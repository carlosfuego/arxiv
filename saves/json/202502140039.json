[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v2",
                "updated": "2025-02-12T11:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    5,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07903v1",
                "updated": "2025-02-11T19:17:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T19:17:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment"
                },
                "summary": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v2",
                "updated": "2025-02-11T18:45:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    45,
                    12,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v1",
                "updated": "2025-02-11T18:20:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce **TransMLA**, a post-training\nmethod that converts widely used GQA-based pre-trained models (e.g., LLaMA,\nQwen, Mixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce **TransMLA**, a post-training\nmethod that converts widely used GQA-based pre-trained models (e.g., LLaMA,\nQwen, Mixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v2",
                "updated": "2025-02-11T17:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07861v1",
                "updated": "2025-02-11T17:18:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:18:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "BalanceKV: KV Cache Compression through Discrepancy Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BalanceKV: KV Cache Compression through Discrepancy Theory"
                },
                "summary": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Ekaterina Kochetkova"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03736v3",
                "updated": "2025-02-11T15:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    42,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-06T04:22:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    4,
                    22,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data"
                },
                "summary": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD."
                },
                "authors": [
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v1",
                "updated": "2025-02-10T23:11:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v2",
                "updated": "2025-02-10T18:34:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    34,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06327v1",
                "updated": "2025-02-10T10:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Driven Continual Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Driven Continual Graph Learning"
                },
                "summary": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Tianfei Zhou"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Mao"
                },
                "author": "Rui Mao",
                "arxiv_comment": "12 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06166v1",
                "updated": "2025-02-10T05:33:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:33:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators"
                },
                "summary": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices."
                },
                "authors": [
                    {
                        "name": "Qi Shao"
                    },
                    {
                        "name": "Xin-Jun Liu"
                    },
                    {
                        "name": "Huichan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Huichan Zhao"
                },
                "author": "Huichan Zhao",
                "arxiv_comment": "7 pages, 10 figures, accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v2",
                "updated": "2025-02-09T20:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    52,
                    26,
                    6,
                    40,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "18 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06901v1",
                "updated": "2025-02-09T20:02:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T20:02:05Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "title": "Enabling Autoregressive Models to Fill In Masked Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Autoregressive Models to Fill In Masked Tokens"
                },
                "summary": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05960v1",
                "updated": "2025-02-09T17:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T17:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "title": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4"
                },
                "summary": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05859v1",
                "updated": "2025-02-09T11:36:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T11:36:45Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion"
                },
                "summary": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image."
                },
                "authors": [
                    {
                        "name": "Qingsong Yan"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Fei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Fei Deng"
                },
                "author": "Fei Deng",
                "arxiv_comment": "3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05763v1",
                "updated": "2025-02-09T03:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T03:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "title": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay"
                },
                "summary": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance."
                },
                "authors": [
                    {
                        "name": "Nicholas Kernan"
                    },
                    {
                        "name": "Joey Li"
                    },
                    {
                        "name": "Rami Al-Dalky"
                    },
                    {
                        "name": "Michael Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rabinovich"
                },
                "author": "Michael Rabinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v2",
                "updated": "2025-02-08T21:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    21,
                    44,
                    24,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Selçuk Köse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v2",
                "updated": "2025-02-08T14:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    14,
                    11,
                    25,
                    5,
                    39,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v2",
                "updated": "2025-02-08T11:51:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    11,
                    51,
                    57,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05511v1",
                "updated": "2025-02-08T10:14:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T10:14:21Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "title": "New and Improved Bounds for Markov Paging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New and Improved Bounds for Markov Paging"
                },
                "summary": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown."
                },
                "authors": [
                    {
                        "name": "Chirag Pabbaraju"
                    },
                    {
                        "name": "Ali Vakilian"
                    }
                ],
                "author_detail": {
                    "name": "Ali Vakilian"
                },
                "author": "Ali Vakilian",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05433v1",
                "updated": "2025-02-08T03:46:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:46:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection"
                },
                "summary": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow."
                },
                "authors": [
                    {
                        "name": "Shuheng Zhang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Hongbo Zhou"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05429v1",
                "updated": "2025-02-08T03:35:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:35:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts"
                },
                "summary": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
                },
                "authors": [
                    {
                        "name": "Seonghun Son"
                    },
                    {
                        "name": "Daniel Moghimi"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_doi": "10.1145/3676641.3716274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS) accepted",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12304v4",
                "updated": "2025-02-07T23:14:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    14,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2024-05-20T18:11:45Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    18,
                    11,
                    45,
                    0,
                    141,
                    0
                ],
                "title": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach"
                },
                "summary": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3711847",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711847",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v1",
                "updated": "2025-02-07T22:51:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v2",
                "updated": "2025-02-07T22:00:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    0,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v1",
                "updated": "2025-02-07T08:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v2",
                "updated": "2025-02-06T20:26:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    20,
                    26,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v1",
                "updated": "2025-02-06T15:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v4",
                "updated": "2025-02-06T12:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    32,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v2",
                "updated": "2025-02-06T08:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04393v1",
                "updated": "2025-02-06T03:56:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:56:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation"
                },
                "summary": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Wenzhang Sun"
                    },
                    {
                        "name": "Qirui Hou"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Yongjia Ma"
                    },
                    {
                        "name": "Jianxun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jianxun Cui"
                },
                "author": "Jianxun Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v2",
                "updated": "2025-02-06T03:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    16,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v2",
                "updated": "2025-02-05T22:55:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    22,
                    55,
                    47,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v2",
                "updated": "2025-02-05T21:44:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    44,
                    56,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03589v1",
                "updated": "2025-02-05T20:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:09:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference"
                },
                "summary": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Shay Vargaftik"
                    },
                    {
                        "name": "Ran Ben Basat"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v1",
                "updated": "2025-02-05T01:36:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL"
                },
                "summary": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02750v1",
                "updated": "2025-02-04T22:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T22:37:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Cache is King: Smart Page Eviction with eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache is King: Smart Page Eviction with eBPF"
                },
                "summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
                },
                "authors": [
                    {
                        "name": "Tal Zussman"
                    },
                    {
                        "name": "Ioannis Zarkadas"
                    },
                    {
                        "name": "Jeremy Carin"
                    },
                    {
                        "name": "Andrew Cheng"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Jonas Pfefferle"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v1",
                "updated": "2025-02-04T15:55:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02349v1",
                "updated": "2025-02-04T14:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "Random Adaptive Cache Placement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Adaptive Cache Placement Policy"
                },
                "summary": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
                },
                "authors": [
                    {
                        "name": "Vrushank Ahire"
                    },
                    {
                        "name": "Pranav Menon"
                    },
                    {
                        "name": "Aniruddh Muley"
                    },
                    {
                        "name": "Abhinandan S. Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Abhinandan S. Prasad"
                },
                "author": "Abhinandan S. Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v1",
                "updated": "2025-02-04T09:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02617v1",
                "updated": "2025-02-04T08:52:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T08:52:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "PolarQuant: Quantizing KV Caches with Polar Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Quantizing KV Caches with Polar Transformation"
                },
                "summary": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02069v1",
                "updated": "2025-02-04T07:40:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:40:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models"
                },
                "summary": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache."
                },
                "authors": [
                    {
                        "name": "Yuto Kojima"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v1",
                "updated": "2025-02-04T03:13:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "14 pages, 11 figures, the first version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v1",
                "updated": "2025-02-04T02:23:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v2",
                "updated": "2025-02-03T21:45:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    45,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "18 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v1",
                "updated": "2025-02-03T20:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01637v1",
                "updated": "2025-02-03T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Scaling Embedding Layers in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Embedding Layers in Language Models"
                },
                "summary": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS."
                },
                "authors": [
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Edith Cohen"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v1",
                "updated": "2025-02-03T05:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08784v2",
                "updated": "2025-02-02T14:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    38,
                    15,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-12T07:35:30Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    7,
                    35,
                    30,
                    3,
                    285,
                    0
                ],
                "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction"
                },
                "summary": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches."
                },
                "authors": [
                    {
                        "name": "Pol Caselles"
                    },
                    {
                        "name": "Eduard Ramon"
                    },
                    {
                        "name": "Jaime Garcia"
                    },
                    {
                        "name": "Gil Triginer"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_doi": "10.1109/TPAMI.2025.3540542",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2025.3540542",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00527v1",
                "updated": "2025-02-01T18:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T18:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "title": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration"
                },
                "summary": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models."
                },
                "authors": [
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v3",
                "updated": "2025-02-01T16:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    50,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00439v1",
                "updated": "2025-02-01T14:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T14:16:31Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "title": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs"
                },
                "summary": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}."
                },
                "authors": [
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xin Ye"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "11 pages, 4 figures. Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00433v1",
                "updated": "2025-02-01T13:46:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T13:46:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models"
                },
                "summary": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning"
                },
                "authors": [
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00382v1",
                "updated": "2025-02-01T09:41:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T09:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "title": "Masked Generative Nested Transformers with Decode Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Nested Transformers with Decode Time Scaling"
                },
                "summary": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Debapriya Tula"
                    },
                    {
                        "name": "Gagan Jain"
                    },
                    {
                        "name": "Pradeep Shenoy"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Sujoy Paul"
                    }
                ],
                "author_detail": {
                    "name": "Sujoy Paul"
                },
                "author": "Sujoy Paul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v2",
                "updated": "2025-02-01T04:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    24,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v1",
                "updated": "2025-02-01T03:49:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v2",
                "updated": "2025-02-01T03:40:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    40,
                    37,
                    5,
                    32,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v2",
                "updated": "2025-01-31T19:09:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    19,
                    9,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_doi": "10.1109/IPCCC59868.2024.10850382",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IPCCC59868.2024.10850382",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
                "arxiv_journal_ref": "2024 IEEE International Performance, Computing, and Communications\n  Conference (IPCCC), Orlando, FL, USA, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v1",
                "updated": "2025-01-31T16:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Mao Xun Huang"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19051v1",
                "updated": "2025-01-31T11:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Swift: Rethinking RDMA Control Plane for Elastic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swift: Rethinking RDMA Control Plane for Elastic Computing"
                },
                "summary": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions."
                },
                "authors": [
                    {
                        "name": "Junxue Zhang"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Xinyang Huang"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Kaiqiang Xu"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19021v1",
                "updated": "2025-01-31T10:43:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:43:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode"
                },
                "summary": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model."
                },
                "authors": [
                    {
                        "name": "Emilio Corte"
                    },
                    {
                        "name": "Alberto Bortone"
                    },
                    {
                        "name": "Elena Nieto Hernández"
                    },
                    {
                        "name": "Carlo Ceresa"
                    },
                    {
                        "name": "Georgios Provatas"
                    },
                    {
                        "name": "Karla Ivanković Nizić"
                    },
                    {
                        "name": "Milko Jaksić"
                    },
                    {
                        "name": "Ettore Vittone"
                    },
                    {
                        "name": "Sviatoslav Ditalia Tchernij"
                    }
                ],
                "author_detail": {
                    "name": "Sviatoslav Ditalia Tchernij"
                },
                "author": "Sviatoslav Ditalia Tchernij",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18824v1",
                "updated": "2025-01-31T00:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T00:43:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Fine-Tuning of Transformers via Token Selection"
                },
                "summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune."
                },
                "authors": [
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05172v2",
                "updated": "2025-01-30T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    2,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-08T14:06:06Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    6,
                    6,
                    6,
                    281,
                    0
                ],
                "title": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy"
                },
                "summary": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08640v1",
                "updated": "2025-02-12T18:55:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    55,
                    43,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T18:55:43Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    55,
                    43,
                    2,
                    43,
                    0
                ],
                "title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs"
                },
                "summary": "As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations."
                },
                "authors": [
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Xuwang Yin"
                    },
                    {
                        "name": "Rishub Tamirisa"
                    },
                    {
                        "name": "Jaehyuk Lim"
                    },
                    {
                        "name": "Bruce W. Lee"
                    },
                    {
                        "name": "Richard Ren"
                    },
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Norman Mu"
                    },
                    {
                        "name": "Adam Khoja"
                    },
                    {
                        "name": "Oliver Zhang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "author": "Dan Hendrycks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08638v1",
                "updated": "2025-02-12T18:54:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    54,
                    37,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T18:54:37Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    54,
                    37,
                    2,
                    43,
                    0
                ],
                "title": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples"
                },
                "summary": "The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that requires only a set of parallel sentence pairs of the\nlanguage pair of interest within the target domain. This task focuses on the\nability of a model to cross-lingually rank the true parallel sentence higher\nthan hard negatives generated by a large language model. We create four\ninstances of our introduced CLSD task for the language pair German-French\nwithin the domain of news. Within this case study, we find that models that are\nalso fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using\nEnglish as the pivot language, while bitext mining models such as LaBSE perform\nbest directly cross-lingually. We also show a fine-grained similarity analysis\nenabled by our distractor generation strategy, indicating that different\nembedding models are sensitive to different types of perturbations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that requires only a set of parallel sentence pairs of the\nlanguage pair of interest within the target domain. This task focuses on the\nability of a model to cross-lingually rank the true parallel sentence higher\nthan hard negatives generated by a large language model. We create four\ninstances of our introduced CLSD task for the language pair German-French\nwithin the domain of news. Within this case study, we find that models that are\nalso fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using\nEnglish as the pivot language, while bitext mining models such as LaBSE perform\nbest directly cross-lingually. We also show a fine-grained similarity analysis\nenabled by our distractor generation strategy, indicating that different\nembedding models are sensitive to different types of perturbations."
                },
                "authors": [
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08631v1",
                "updated": "2025-02-12T18:42:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    42,
                    42,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T18:42:42Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    42,
                    42,
                    2,
                    43,
                    0
                ],
                "title": "Ensemble based approach to quantifying uncertainty of LLM based\n  classifications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble based approach to quantifying uncertainty of LLM based\n  classifications"
                },
                "summary": "The output of Large Language Models (LLMs) are a function of the internal\nmodel's parameters and the input provided into the context window. The\nhypothesis presented here is that under a greedy sampling strategy the variance\nin the LLM's output is a function of the conceptual certainty embedded in the\nmodel's parametric knowledge, as well as the lexical variance in the input.\nFinetuning the model results in reducing the sensitivity of the model output to\nthe lexical input variations. This is then applied to a classification problem\nand a probabilistic method is proposed for estimating the certainties of the\npredicted classes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The output of Large Language Models (LLMs) are a function of the internal\nmodel's parameters and the input provided into the context window. The\nhypothesis presented here is that under a greedy sampling strategy the variance\nin the LLM's output is a function of the conceptual certainty embedded in the\nmodel's parametric knowledge, as well as the lexical variance in the input.\nFinetuning the model results in reducing the sensitivity of the model output to\nthe lexical input variations. This is then applied to a classification problem\nand a probabilistic method is proposed for estimating the certainties of the\npredicted classes."
                },
                "authors": [
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Ahmed Salhin"
                    },
                    {
                        "name": "Josh Frazier"
                    },
                    {
                        "name": "Rohit Kumar"
                    },
                    {
                        "name": "Yu-Cheng Tsai"
                    },
                    {
                        "name": "Todd Cook"
                    }
                ],
                "author_detail": {
                    "name": "Todd Cook"
                },
                "author": "Todd Cook",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04328v2",
                "updated": "2025-02-12T18:40:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    40,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-06T18:59:55Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    55,
                    3,
                    37,
                    0
                ],
                "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment"
                },
                "summary": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola."
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Yongming Rao"
                    }
                ],
                "author_detail": {
                    "name": "Yongming Rao"
                },
                "author": "Yongming Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04689v2",
                "updated": "2025-02-12T18:36:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    36,
                    24,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-07T06:30:33Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    30,
                    33,
                    4,
                    38,
                    0
                ],
                "title": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning"
                },
                "summary": "Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR."
                },
                "authors": [
                    {
                        "name": "Yuwei Yin"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Carenini"
                },
                "author": "Giuseppe Carenini",
                "arxiv_comment": "20 pages. Code: https://github.com/YuweiYin/ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05408v2",
                "updated": "2025-02-12T18:19:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    19,
                    54,
                    2,
                    43,
                    0
                ],
                "published": "2024-06-08T09:04:03Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    9,
                    4,
                    3,
                    5,
                    160,
                    0
                ],
                "title": "Human Learning about AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Learning about AI"
                },
                "summary": "We study how people form expectations about the performance of artificial\nintelligence (AI) and consequences for AI adoption. Our main hypothesis is that\npeople rely on human-relevant task features when evaluating AI, treating AI\nfailures on human-easy tasks, and successes on human-difficult tasks, as highly\ninformative of its overall performance. In lab experiments, we show that\nprojection of human difficulty onto AI predictably distorts subjects' beliefs\nand can lead to suboptimal adoption, as failing human-easy tasks need not imply\npoor overall performance for AI. We find evidence for projection in a field\nexperiment with an AI giving parenting advice. Potential users strongly infer\nfrom answers that are equally uninformative but less humanly-similar to\nexpected answers, significantly reducing trust and future engagement. Our\nresults suggest AI \"anthropomorphism\" can backfire by increasing projection and\nde-aligning people's expectations and AI performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how people form expectations about the performance of artificial\nintelligence (AI) and consequences for AI adoption. Our main hypothesis is that\npeople rely on human-relevant task features when evaluating AI, treating AI\nfailures on human-easy tasks, and successes on human-difficult tasks, as highly\ninformative of its overall performance. In lab experiments, we show that\nprojection of human difficulty onto AI predictably distorts subjects' beliefs\nand can lead to suboptimal adoption, as failing human-easy tasks need not imply\npoor overall performance for AI. We find evidence for projection in a field\nexperiment with an AI giving parenting advice. Potential users strongly infer\nfrom answers that are equally uninformative but less humanly-similar to\nexpected answers, significantly reducing trust and future engagement. Our\nresults suggest AI \"anthropomorphism\" can backfire by increasing projection and\nde-aligning people's expectations and AI performance."
                },
                "authors": [
                    {
                        "name": "Bnaya Dreyfuss"
                    },
                    {
                        "name": "Raphaël Raux"
                    }
                ],
                "author_detail": {
                    "name": "Raphaël Raux"
                },
                "author": "Raphaël Raux",
                "arxiv_comment": "41 pages (98 with appendix); 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08614v1",
                "updated": "2025-02-12T18:03:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    3,
                    11,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T18:03:11Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    3,
                    11,
                    2,
                    43,
                    0
                ],
                "title": "Difference-in-Differences and Changes-in-Changes with Sample Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Difference-in-Differences and Changes-in-Changes with Sample Selection"
                },
                "summary": "Sample selection arises endogenously in causal research when the treatment\naffects whether certain units are observed. It is a common pitfall in\nlongitudinal studies, particularly in settings where treatment assignment is\nconfounded. In this paper, I highlight the drawbacks of one of the most popular\nidentification strategies in such settings: Difference-in-Differences (DiD).\nSpecifically, I employ principal stratification analysis to show that the\nconventional ATT estimand may not be well defined, and the DiD estimand cannot\nbe interpreted causally without additional assumptions. To address these\nissues, I develop an identification strategy to partially identify causal\neffects on the subset of units with well-defined and observed outcomes under\nboth treatment regimes. I adapt Lee bounds to the Changes-in-Changes (CiC)\nsetting (Athey & Imbens, 2006), leveraging the time dimension of the data to\nrelax the unconfoundedness assumption in the original trimming strategy of Lee\n(2009). This setting has the DiD identification strategy as a particular case,\nwhich I also implement in the paper. Additionally, I explore how to leverage\nmultiple sources of sample selection to relax the monotonicity assumption in\nLee (2009), which may be of independent interest. Alongside the identification\nstrategy, I present estimators and inference results. I illustrate the\nrelevance of the proposed methodology by analyzing a job training program in\nColombia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample selection arises endogenously in causal research when the treatment\naffects whether certain units are observed. It is a common pitfall in\nlongitudinal studies, particularly in settings where treatment assignment is\nconfounded. In this paper, I highlight the drawbacks of one of the most popular\nidentification strategies in such settings: Difference-in-Differences (DiD).\nSpecifically, I employ principal stratification analysis to show that the\nconventional ATT estimand may not be well defined, and the DiD estimand cannot\nbe interpreted causally without additional assumptions. To address these\nissues, I develop an identification strategy to partially identify causal\neffects on the subset of units with well-defined and observed outcomes under\nboth treatment regimes. I adapt Lee bounds to the Changes-in-Changes (CiC)\nsetting (Athey & Imbens, 2006), leveraging the time dimension of the data to\nrelax the unconfoundedness assumption in the original trimming strategy of Lee\n(2009). This setting has the DiD identification strategy as a particular case,\nwhich I also implement in the paper. Additionally, I explore how to leverage\nmultiple sources of sample selection to relax the monotonicity assumption in\nLee (2009), which may be of independent interest. Alongside the identification\nstrategy, I present estimators and inference results. I illustrate the\nrelevance of the proposed methodology by analyzing a job training program in\nColombia."
                },
                "authors": [
                    {
                        "name": "Javier Viviens"
                    }
                ],
                "author_detail": {
                    "name": "Javier Viviens"
                },
                "author": "Javier Viviens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12813v2",
                "updated": "2025-02-12T17:56:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    56,
                    25,
                    2,
                    43,
                    0
                ],
                "published": "2023-10-19T15:10:47Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    15,
                    10,
                    47,
                    3,
                    292,
                    0
                ],
                "title": "Probing modified gravitational-wave propagation with extreme mass-ratio\n  inspirals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing modified gravitational-wave propagation with extreme mass-ratio\n  inspirals"
                },
                "summary": "Extreme mass-ratio inspirals (EMRIs), namely binary systems composed of a\nmassive black hole and a compact stellar-mass object, are anticipated to be\namong the gravitational wave (GW) sources detected by the Laser Interferometer\nSpace Antenna (LISA). Similarly to compact binary mergers detected by current\nGW detectors, EMRIs can be used as cosmic rulers to probe the expansion of the\nUniverse. Motivated by tensions in current cosmological observations as well as\nby alternative models of dark energy, modified gravity theories can affect the\npropagation of GWs across cosmological distances, with modifications commonly\nparametrised in terms of two phenomenological parameters, $\\Xi_0$ and $n$. In\nthis work we adopt a Bayesian approach to constrain for the first time\nparametrised deviations from General Relativity using the loudest simulated\nEMRIs detected by LISA as dark sirens with a simulated galaxy catalog. Assuming\nall the cosmological parameters except $\\Xi_0$ are already tightly constrained,\nour forecasts show that $\\Xi_0$ can be constrained to a few percent level (90%\nC.I.) with 4 years of LISA observations, unless EMRI detection rates turn out\nto be closer to current pessimistic expectations. These results quickly degrade\nif additional cosmological parameters are inferred simultaneously, but become\nmore robust with an extended LISA observation period of 10 years. Overall, we\nfind that EMRIs with LISA are better at constraining modified GW propagation\nthan current second-generation ground-based GW detectors, but they will only be\ncomparable to third-generation detectors in the most optimistic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme mass-ratio inspirals (EMRIs), namely binary systems composed of a\nmassive black hole and a compact stellar-mass object, are anticipated to be\namong the gravitational wave (GW) sources detected by the Laser Interferometer\nSpace Antenna (LISA). Similarly to compact binary mergers detected by current\nGW detectors, EMRIs can be used as cosmic rulers to probe the expansion of the\nUniverse. Motivated by tensions in current cosmological observations as well as\nby alternative models of dark energy, modified gravity theories can affect the\npropagation of GWs across cosmological distances, with modifications commonly\nparametrised in terms of two phenomenological parameters, $\\Xi_0$ and $n$. In\nthis work we adopt a Bayesian approach to constrain for the first time\nparametrised deviations from General Relativity using the loudest simulated\nEMRIs detected by LISA as dark sirens with a simulated galaxy catalog. Assuming\nall the cosmological parameters except $\\Xi_0$ are already tightly constrained,\nour forecasts show that $\\Xi_0$ can be constrained to a few percent level (90%\nC.I.) with 4 years of LISA observations, unless EMRI detection rates turn out\nto be closer to current pessimistic expectations. These results quickly degrade\nif additional cosmological parameters are inferred simultaneously, but become\nmore robust with an extended LISA observation period of 10 years. Overall, we\nfind that EMRIs with LISA are better at constraining modified GW propagation\nthan current second-generation ground-based GW detectors, but they will only be\ncomparable to third-generation detectors in the most optimistic scenarios."
                },
                "authors": [
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Danny Laghi"
                    },
                    {
                        "name": "Nicola Tamanini"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Tamanini"
                },
                "author": "Nicola Tamanini",
                "arxiv_doi": "10.1103/PhysRevD.109.063521",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.109.063521",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.12813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 3 figures, 5 tables - Matches published version",
                "arxiv_journal_ref": "C. Liu, D, Laghi, N, Tamanini, Physical Review D 109, 063521\n  (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08603v1",
                "updated": "2025-02-12T17:44:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    44,
                    40,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T17:44:40Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    44,
                    40,
                    2,
                    43,
                    0
                ],
                "title": "Scalable Thermodynamic Second-order Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Thermodynamic Second-order Optimization"
                },
                "summary": "Many hardware proposals have aimed to accelerate inference in AI workloads.\nLess attention has been paid to hardware acceleration of training, despite the\nenormous societal impact of rapid training of AI models. Physics-based\ncomputers, such as thermodynamic computers, offer an efficient means to solve\nkey primitives in AI training algorithms. Optimizers that normally would be\ncomputationally out-of-reach (e.g., due to expensive matrix inversions) on\ndigital hardware could be unlocked with physics-based hardware. In this work,\nwe propose a scalable algorithm for employing thermodynamic computers to\naccelerate a popular second-order optimizer called Kronecker-factored\napproximate curvature (K-FAC). Our asymptotic complexity analysis predicts\nincreasing advantage with our algorithm as $n$, the number of neurons per\nlayer, increases. Numerical experiments show that even under significant\nquantization noise, the benefits of second-order optimization can be preserved.\nFinally, we predict substantial speedups for large-scale vision and graph\nproblems based on realistic hardware characteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many hardware proposals have aimed to accelerate inference in AI workloads.\nLess attention has been paid to hardware acceleration of training, despite the\nenormous societal impact of rapid training of AI models. Physics-based\ncomputers, such as thermodynamic computers, offer an efficient means to solve\nkey primitives in AI training algorithms. Optimizers that normally would be\ncomputationally out-of-reach (e.g., due to expensive matrix inversions) on\ndigital hardware could be unlocked with physics-based hardware. In this work,\nwe propose a scalable algorithm for employing thermodynamic computers to\naccelerate a popular second-order optimizer called Kronecker-factored\napproximate curvature (K-FAC). Our asymptotic complexity analysis predicts\nincreasing advantage with our algorithm as $n$, the number of neurons per\nlayer, increases. Numerical experiments show that even under significant\nquantization noise, the benefits of second-order optimization can be preserved.\nFinally, we predict substantial speedups for large-scale vision and graph\nproblems based on realistic hardware characteristics."
                },
                "authors": [
                    {
                        "name": "Kaelan Donatella"
                    },
                    {
                        "name": "Samuel Duffield"
                    },
                    {
                        "name": "Denis Melanson"
                    },
                    {
                        "name": "Maxwell Aifer"
                    },
                    {
                        "name": "Phoebe Klett"
                    },
                    {
                        "name": "Rajath Salegame"
                    },
                    {
                        "name": "Zach Belateche"
                    },
                    {
                        "name": "Gavin Crooks"
                    },
                    {
                        "name": "Antonio J. Martinez"
                    },
                    {
                        "name": "Patrick J. Coles"
                    }
                ],
                "author_detail": {
                    "name": "Patrick J. Coles"
                },
                "author": "Patrick J. Coles",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08599v1",
                "updated": "2025-02-12T17:38:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    38,
                    27,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T17:38:27Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    38,
                    27,
                    2,
                    43,
                    0
                ],
                "title": "SPeCtrum: A Grounded Framework for Multidimensional Identity\n  Representation in LLM-Based Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPeCtrum: A Grounded Framework for Multidimensional Identity\n  Representation in LLM-Based Agent"
                },
                "summary": "Existing methods for simulating individual identities often oversimplify\nhuman complexity, which may lead to incomplete or flattened representations. To\naddress this, we introduce SPeCtrum, a grounded framework for constructing\nauthentic LLM agent personas by incorporating an individual's multidimensional\nself-concept. SPeCtrum integrates three core components: Social Identity (S),\nPersonal Identity (P), and Personal Life Context (C), each contributing\ndistinct yet interconnected aspects of identity. To evaluate SPeCtrum's\neffectiveness in identity representation, we conducted automated and human\nevaluations. Automated evaluations using popular drama characters showed that\nPersonal Life Context (C)-derived from short essays on preferences and daily\nroutines-modeled characters' identities more effectively than Social Identity\n(S) and Personal Identity (P) alone and performed comparably to the full SPC\ncombination. In contrast, human evaluations involving real-world individuals\nfound that the full SPC combination provided a more comprehensive self-concept\nrepresentation than C alone. Our findings suggest that while C alone may\nsuffice for basic identity simulation, integrating S, P, and C enhances the\nauthenticity and accuracy of real-world identity representation. Overall,\nSPeCtrum offers a structured approach for simulating individuals in LLM agents,\nenabling more personalized human-AI interactions and improving the realism of\nsimulation-based behavioral studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods for simulating individual identities often oversimplify\nhuman complexity, which may lead to incomplete or flattened representations. To\naddress this, we introduce SPeCtrum, a grounded framework for constructing\nauthentic LLM agent personas by incorporating an individual's multidimensional\nself-concept. SPeCtrum integrates three core components: Social Identity (S),\nPersonal Identity (P), and Personal Life Context (C), each contributing\ndistinct yet interconnected aspects of identity. To evaluate SPeCtrum's\neffectiveness in identity representation, we conducted automated and human\nevaluations. Automated evaluations using popular drama characters showed that\nPersonal Life Context (C)-derived from short essays on preferences and daily\nroutines-modeled characters' identities more effectively than Social Identity\n(S) and Personal Identity (P) alone and performed comparably to the full SPC\ncombination. In contrast, human evaluations involving real-world individuals\nfound that the full SPC combination provided a more comprehensive self-concept\nrepresentation than C alone. Our findings suggest that while C alone may\nsuffice for basic identity simulation, integrating S, P, and C enhances the\nauthenticity and accuracy of real-world identity representation. Overall,\nSPeCtrum offers a structured approach for simulating individuals in LLM agents,\nenabling more personalized human-AI interactions and improving the realism of\nsimulation-based behavioral studies."
                },
                "authors": [
                    {
                        "name": "Keyeun Lee"
                    },
                    {
                        "name": "Seo Hyeong Kim"
                    },
                    {
                        "name": "Seolhee Lee"
                    },
                    {
                        "name": "Jinsu Eun"
                    },
                    {
                        "name": "Yena Ko"
                    },
                    {
                        "name": "Hayeon Jeon"
                    },
                    {
                        "name": "Esther Hehsun Kim"
                    },
                    {
                        "name": "Seonghye Cho"
                    },
                    {
                        "name": "Soeun Yang"
                    },
                    {
                        "name": "Eun-mee Kim"
                    },
                    {
                        "name": "Hajin Lim"
                    }
                ],
                "author_detail": {
                    "name": "Hajin Lim"
                },
                "author": "Hajin Lim",
                "arxiv_comment": "21 pages, 8 figures, 5 tables, Accepted in NAACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07313v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07313v3",
                "updated": "2025-02-12T17:20:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    20,
                    56,
                    2,
                    43,
                    0
                ],
                "published": "2024-07-10T02:20:19Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    2,
                    20,
                    19,
                    2,
                    192,
                    0
                ],
                "title": "ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models"
                },
                "summary": "The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics -- Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) -- suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics -- Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) -- suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL."
                },
                "authors": [
                    {
                        "name": "Benjamin G. Ascoli"
                    },
                    {
                        "name": "Yasoda Sai Ram Kandikonda"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07313v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07313v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08587v1",
                "updated": "2025-02-12T17:20:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    20,
                    37,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T17:20:37Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    20,
                    37,
                    2,
                    43,
                    0
                ],
                "title": "Causal Analysis of ASR Errors for Children: Quantifying the Impact of\n  Physiological, Cognitive, and Extrinsic Factors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Analysis of ASR Errors for Children: Quantifying the Impact of\n  Physiological, Cognitive, and Extrinsic Factors"
                },
                "summary": "The increasing use of children's automatic speech recognition (ASR) systems\nhas spurred research efforts to improve the accuracy of models designed for\nchildren's speech in recent years. The current approach utilizes either\nopen-source speech foundation models (SFMs) directly or fine-tuning them with\nchildren's speech data. These SFMs, whether open-source or fine-tuned for\nchildren, often exhibit higher word error rates (WERs) compared to adult\nspeech. However, there is a lack of systemic analysis of the cause of this\ndegraded performance of SFMs. Understanding and addressing the reasons behind\nthis performance disparity is crucial for improving the accuracy of SFMs for\nchildren's speech. Our study addresses this gap by investigating the causes of\naccuracy degradation and the primary contributors to WER in children's speech.\nIn the first part of the study, we conduct a comprehensive benchmarking study\non two self-supervised SFMs (Wav2Vec2.0 and Hubert) and two weakly supervised\nSFMs (Whisper and MMS) across various age groups on two children speech\ncorpora, establishing the raw data for the causal inference analysis in the\nsecond part. In the second part of the study, we analyze the impact of\nphysiological factors (age, gender), cognitive factors (pronunciation ability),\nand external factors (vocabulary difficulty, background noise, and word count)\non SFM accuracy in children's speech using causal inference. The results\nindicate that physiology (age) and particular external factor (number of words\nin audio) have the highest impact on accuracy, followed by background noise and\npronunciation ability. Fine-tuning SFMs on children's speech reduces\nsensitivity to physiological and cognitive factors, while sensitivity to the\nnumber of words in audio persists.\n  Keywords: Children's ASR, Speech Foundational Models, Causal Inference,\nPhysiology, Cognition, Pronunciation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of children's automatic speech recognition (ASR) systems\nhas spurred research efforts to improve the accuracy of models designed for\nchildren's speech in recent years. The current approach utilizes either\nopen-source speech foundation models (SFMs) directly or fine-tuning them with\nchildren's speech data. These SFMs, whether open-source or fine-tuned for\nchildren, often exhibit higher word error rates (WERs) compared to adult\nspeech. However, there is a lack of systemic analysis of the cause of this\ndegraded performance of SFMs. Understanding and addressing the reasons behind\nthis performance disparity is crucial for improving the accuracy of SFMs for\nchildren's speech. Our study addresses this gap by investigating the causes of\naccuracy degradation and the primary contributors to WER in children's speech.\nIn the first part of the study, we conduct a comprehensive benchmarking study\non two self-supervised SFMs (Wav2Vec2.0 and Hubert) and two weakly supervised\nSFMs (Whisper and MMS) across various age groups on two children speech\ncorpora, establishing the raw data for the causal inference analysis in the\nsecond part. In the second part of the study, we analyze the impact of\nphysiological factors (age, gender), cognitive factors (pronunciation ability),\nand external factors (vocabulary difficulty, background noise, and word count)\non SFM accuracy in children's speech using causal inference. The results\nindicate that physiology (age) and particular external factor (number of words\nin audio) have the highest impact on accuracy, followed by background noise and\npronunciation ability. Fine-tuning SFMs on children's speech reduces\nsensitivity to physiological and cognitive factors, while sensitivity to the\nnumber of words in audio persists.\n  Keywords: Children's ASR, Speech Foundational Models, Causal Inference,\nPhysiology, Cognition, Pronunciation"
                },
                "authors": [
                    {
                        "name": "Vishwanath Pratap Singh"
                    },
                    {
                        "name": "Md. Sahidullah"
                    },
                    {
                        "name": "Tomi Kinnunen"
                    }
                ],
                "author_detail": {
                    "name": "Tomi Kinnunen"
                },
                "author": "Tomi Kinnunen",
                "arxiv_comment": "Submitted to Computer Speech & Language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08586v1",
                "updated": "2025-02-12T17:19:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    19,
                    36,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T17:19:36Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    19,
                    36,
                    2,
                    43,
                    0
                ],
                "title": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous\n  Attacks"
                },
                "summary": "A high volume of recent ML security literature focuses on attacks against\naligned large language models (LLMs). These attacks may extract private\ninformation or coerce the model into producing harmful outputs. In real-world\ndeployments, LLMs are often part of a larger agentic pipeline including memory\nsystems, retrieval, web access, and API calling. Such additional components\nintroduce vulnerabilities that make these LLM-powered agents much easier to\nattack than isolated LLMs, yet relatively little work focuses on the security\nof LLM agents. In this paper, we analyze security and privacy vulnerabilities\nthat are unique to LLM agents. We first provide a taxonomy of attacks\ncategorized by threat actors, objectives, entry points, attacker observability,\nattack strategies, and inherent vulnerabilities of agent pipelines. We then\nconduct a series of illustrative attacks on popular open-source and commercial\nagents, demonstrating the immediate practical implications of their\nvulnerabilities. Notably, our attacks are trivial to implement and require no\nunderstanding of machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A high volume of recent ML security literature focuses on attacks against\naligned large language models (LLMs). These attacks may extract private\ninformation or coerce the model into producing harmful outputs. In real-world\ndeployments, LLMs are often part of a larger agentic pipeline including memory\nsystems, retrieval, web access, and API calling. Such additional components\nintroduce vulnerabilities that make these LLM-powered agents much easier to\nattack than isolated LLMs, yet relatively little work focuses on the security\nof LLM agents. In this paper, we analyze security and privacy vulnerabilities\nthat are unique to LLM agents. We first provide a taxonomy of attacks\ncategorized by threat actors, objectives, entry points, attacker observability,\nattack strategies, and inherent vulnerabilities of agent pipelines. We then\nconduct a series of illustrative attacks on popular open-source and commercial\nagents, demonstrating the immediate practical implications of their\nvulnerabilities. Notably, our attacks are trivial to implement and require no\nunderstanding of machine learning."
                },
                "authors": [
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Yin Zhou"
                    },
                    {
                        "name": "Vethavikashini Chithrra Raghuram"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Micah Goldblum"
                    }
                ],
                "author_detail": {
                    "name": "Micah Goldblum"
                },
                "author": "Micah Goldblum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05905v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05905v5",
                "updated": "2025-02-12T17:10:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    53,
                    2,
                    43,
                    0
                ],
                "published": "2024-05-09T17:01:31Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    17,
                    1,
                    31,
                    3,
                    130,
                    0
                ],
                "title": "Truthful Aggregation of LLMs with an Application to Online Advertising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truthful Aggregation of LLMs with an Application to Online Advertising"
                },
                "summary": "The next frontier of online advertising is revenue generation from\nLLM-generated content. We consider a setting where advertisers aim to influence\nthe responses of an LLM to align with their interests, while platforms seek to\nmaximize advertiser value and ensure user satisfaction. The challenge is that\nadvertisers' preferences generally conflict with those of the user, and\nadvertisers may misreport their preferences. To address this, we introduce\nMOSAIC, an auction mechanism that ensures that truthful reporting is a dominant\nstrategy for advertisers and that aligns the utility of each advertiser with\ntheir contribution to social welfare. Importantly, the mechanism operates\nwithout LLM fine-tuning or access to model weights and provably converges to\nthe output of the optimally fine-tuned LLM as computational resources increase.\nAdditionally, it can incorporate contextual information about advertisers,\nwhich significantly improves social welfare. Through experiments with a\npublicly available LLM, we show that MOSAIC leads to high advertiser value and\nplatform revenue with low computational overhead. While our motivating\napplication is online advertising, our mechanism can be applied in any setting\nwith monetary transfers, making it a general-purpose solution for truthfully\naggregating the preferences of self-interested agents over LLM-generated\nreplies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next frontier of online advertising is revenue generation from\nLLM-generated content. We consider a setting where advertisers aim to influence\nthe responses of an LLM to align with their interests, while platforms seek to\nmaximize advertiser value and ensure user satisfaction. The challenge is that\nadvertisers' preferences generally conflict with those of the user, and\nadvertisers may misreport their preferences. To address this, we introduce\nMOSAIC, an auction mechanism that ensures that truthful reporting is a dominant\nstrategy for advertisers and that aligns the utility of each advertiser with\ntheir contribution to social welfare. Importantly, the mechanism operates\nwithout LLM fine-tuning or access to model weights and provably converges to\nthe output of the optimally fine-tuned LLM as computational resources increase.\nAdditionally, it can incorporate contextual information about advertisers,\nwhich significantly improves social welfare. Through experiments with a\npublicly available LLM, we show that MOSAIC leads to high advertiser value and\nplatform revenue with low computational overhead. While our motivating\napplication is online advertising, our mechanism can be applied in any setting\nwith monetary transfers, making it a general-purpose solution for truthfully\naggregating the preferences of self-interested agents over LLM-generated\nreplies."
                },
                "authors": [
                    {
                        "name": "Ermis Soumalias"
                    },
                    {
                        "name": "Michael J. Curry"
                    },
                    {
                        "name": "Sven Seuken"
                    }
                ],
                "author_detail": {
                    "name": "Sven Seuken"
                },
                "author": "Sven Seuken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05905v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05905v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08576v1",
                "updated": "2025-02-12T17:10:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    34,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T17:10:34Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    34,
                    2,
                    43,
                    0
                ],
                "title": "Mapping the Landscape of Generative AI in Network Monitoring and\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping the Landscape of Generative AI in Network Monitoring and\n  Management"
                },
                "summary": "Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management."
                },
                "authors": [
                    {
                        "name": "Giampaolo Bovenzi"
                    },
                    {
                        "name": "Francesco Cerasuolo"
                    },
                    {
                        "name": "Domenico Ciuonzo"
                    },
                    {
                        "name": "Davide Di Monda"
                    },
                    {
                        "name": "Idio Guarino"
                    },
                    {
                        "name": "Antonio Montieri"
                    },
                    {
                        "name": "Valerio Persico"
                    },
                    {
                        "name": "Antonio Pescapè"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Pescapè"
                },
                "author": "Antonio Pescapè",
                "arxiv_comment": "32 pages, 9 figure, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04944v2",
                "updated": "2025-02-12T16:58:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    58,
                    33,
                    2,
                    43,
                    0
                ],
                "published": "2024-11-07T18:21:17Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    21,
                    17,
                    3,
                    312,
                    0
                ],
                "title": "Galaxy Mergers in the Epoch of Reionization II: Major Merger-Triggered\n  Star Formation and AGN Activities at $z = 4.5 - 8.5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy Mergers in the Epoch of Reionization II: Major Merger-Triggered\n  Star Formation and AGN Activities at $z = 4.5 - 8.5$"
                },
                "summary": "Galaxy mergers are a key driver of galaxy formation and evolution, including\nthe triggering of AGN and star formation to a still unknown degree. We thus\ninvestigate the impact of galaxy mergers on star formation and AGN activity\nusing a sample of 3,330 galaxies at $z = [4.5, 8.5]$ from eight JWST fields\n(CEERS, JADES GOODS-S, NEP-TDF, NGDEEP, GLASS, El-Gordo, SMACS-0723, and\nMACS-0416), collectively covering an unmasked area of 189 arcmin$^2$. We\nfocuses on star formation rate (SFR) enhancement, AGN fraction, and AGN excess\nin major merger ($\\mu > 1/4$) close-pair samples, defined by $\\Delta z < 0.3$\nand projected separations $r_p < 100$ kpc, compared to non-merger samples. We\nfind that SFR enhancement occurs only at $r_p < 20$ kpc, with values of $0.25\n\\pm 0.10$ dex and $0.26 \\pm 0.11$ dex above the non-merger medians for $z =\n[4.5, 6.5]$ and $z = [6.5, 8.5]$. No other statistically significant\nenhancements in galaxy sSFR or stellar mass are observed at any projected\nseparation or redshift bin. We also compare our observational results with\npredictions from the SC-SAM simulation and find no evidence of star formation\nenhancement in the simulations at any separation range. Finally, we examine the\nAGN fraction and AGN excess, finding that the fraction of AGNs in AGN-galaxy\npairs, relative to the total AGN population, is $3.25^{+1.50}_{-1.06}$ times\ngreater than the fraction of galaxy pairs relative to the overall galaxy\npopulation at the same redshift. We find that nearly all AGNs have a companion\nwithin 100 kpc and observe an excess AGN fraction in close-pair samples\ncompared to non-merger samples. This excess is found to be $1.26 \\pm 0.06$ and\n$1.34 \\pm 0.06$ for AGNs identified via the inferred BPT diagram and\nphotometric SED selection, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy mergers are a key driver of galaxy formation and evolution, including\nthe triggering of AGN and star formation to a still unknown degree. We thus\ninvestigate the impact of galaxy mergers on star formation and AGN activity\nusing a sample of 3,330 galaxies at $z = [4.5, 8.5]$ from eight JWST fields\n(CEERS, JADES GOODS-S, NEP-TDF, NGDEEP, GLASS, El-Gordo, SMACS-0723, and\nMACS-0416), collectively covering an unmasked area of 189 arcmin$^2$. We\nfocuses on star formation rate (SFR) enhancement, AGN fraction, and AGN excess\nin major merger ($\\mu > 1/4$) close-pair samples, defined by $\\Delta z < 0.3$\nand projected separations $r_p < 100$ kpc, compared to non-merger samples. We\nfind that SFR enhancement occurs only at $r_p < 20$ kpc, with values of $0.25\n\\pm 0.10$ dex and $0.26 \\pm 0.11$ dex above the non-merger medians for $z =\n[4.5, 6.5]$ and $z = [6.5, 8.5]$. No other statistically significant\nenhancements in galaxy sSFR or stellar mass are observed at any projected\nseparation or redshift bin. We also compare our observational results with\npredictions from the SC-SAM simulation and find no evidence of star formation\nenhancement in the simulations at any separation range. Finally, we examine the\nAGN fraction and AGN excess, finding that the fraction of AGNs in AGN-galaxy\npairs, relative to the total AGN population, is $3.25^{+1.50}_{-1.06}$ times\ngreater than the fraction of galaxy pairs relative to the overall galaxy\npopulation at the same redshift. We find that nearly all AGNs have a companion\nwithin 100 kpc and observe an excess AGN fraction in close-pair samples\ncompared to non-merger samples. This excess is found to be $1.26 \\pm 0.06$ and\n$1.34 \\pm 0.06$ for AGNs identified via the inferred BPT diagram and\nphotometric SED selection, respectively."
                },
                "authors": [
                    {
                        "name": "Qiao Duan"
                    },
                    {
                        "name": "Qiong Li"
                    },
                    {
                        "name": "Christopher J. Conselice"
                    },
                    {
                        "name": "Thomas Harvey"
                    },
                    {
                        "name": "Duncan Austin"
                    },
                    {
                        "name": "Nathan J. Adams"
                    },
                    {
                        "name": "Leonardo Ferreira"
                    },
                    {
                        "name": "Kenneth J. Duncan"
                    },
                    {
                        "name": "James Trussler"
                    },
                    {
                        "name": "Robert G. Pascalau"
                    },
                    {
                        "name": "Rogier A. Windhorst"
                    },
                    {
                        "name": "Benne W. Holwerda"
                    },
                    {
                        "name": "Thomas J. Broadhurst"
                    },
                    {
                        "name": "Dan Coe"
                    },
                    {
                        "name": "Seth H. Cohen"
                    },
                    {
                        "name": "Xiaojing Du"
                    },
                    {
                        "name": "Simon P. Driver"
                    },
                    {
                        "name": "Brenda Frye"
                    },
                    {
                        "name": "Norman A. Grogin"
                    },
                    {
                        "name": "Nimish P. Hathi"
                    },
                    {
                        "name": "Rolf A. Jansen"
                    },
                    {
                        "name": "Anton M. Koekemoer"
                    },
                    {
                        "name": "Madeline A. Marshall"
                    },
                    {
                        "name": "Mario Nonino"
                    },
                    {
                        "name": "Rafael Ortiz III"
                    },
                    {
                        "name": "Nor Pirzkal"
                    },
                    {
                        "name": "Aaron Robotham"
                    },
                    {
                        "name": "Russell E. Ryan Jr"
                    },
                    {
                        "name": "Jake Summers"
                    },
                    {
                        "name": "Jordan C. J. D'Silva"
                    },
                    {
                        "name": "Christopher N. A. Willmer"
                    },
                    {
                        "name": "Haojing Yan"
                    }
                ],
                "author_detail": {
                    "name": "Haojing Yan"
                },
                "author": "Haojing Yan",
                "arxiv_comment": "17 Pages, 7 Figures, Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20163v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20163v3",
                "updated": "2025-02-12T16:49:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    49,
                    56,
                    2,
                    43,
                    0
                ],
                "published": "2024-12-28T14:27:45Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    27,
                    45,
                    5,
                    363,
                    0
                ],
                "title": "Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems"
                },
                "summary": "The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs."
                },
                "authors": [
                    {
                        "name": "Minhye Jeon"
                    },
                    {
                        "name": "Seokho Ahn"
                    },
                    {
                        "name": "Young-Duk Seo"
                    }
                ],
                "author_detail": {
                    "name": "Young-Duk Seo"
                },
                "author": "Young-Duk Seo",
                "arxiv_doi": "10.1145/3672608.3707958",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3672608.3707958",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20163v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20163v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04686v3",
                "updated": "2025-02-12T16:49:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    49,
                    50,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-08T18:49:41Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    49,
                    41,
                    2,
                    8,
                    0
                ],
                "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics"
                },
                "summary": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH."
                },
                "authors": [
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Zhuofan Zheng"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Xinzhe Ni"
                    },
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Yujiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yujiu Yang"
                },
                "author": "Yujiu Yang",
                "arxiv_comment": "Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08560v1",
                "updated": "2025-02-12T16:47:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    47,
                    41,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:47:41Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    47,
                    41,
                    2,
                    43,
                    0
                ],
                "title": "Brain Latent Progression: Individual-based Spatiotemporal Disease\n  Progression on 3D Brain MRIs via Latent Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain Latent Progression: Individual-based Spatiotemporal Disease\n  Progression on 3D Brain MRIs via Latent Diffusion"
                },
                "summary": "The growing availability of longitudinal Magnetic Resonance Imaging (MRI)\ndatasets has facilitated Artificial Intelligence (AI)-driven modeling of\ndisease progression, making it possible to predict future medical scans for\nindividual patients. However, despite significant advancements in AI, current\nmethods continue to face challenges including achieving patient-specific\nindividualization, ensuring spatiotemporal consistency, efficiently utilizing\nlongitudinal data, and managing the substantial memory demands of 3D scans. To\naddress these challenges, we propose Brain Latent Progression (BrLP), a novel\nspatiotemporal model designed to predict individual-level disease progression\nin 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates\nin a small latent space, mitigating the computational challenges posed by\nhigh-dimensional imaging data; (ii) it explicitly integrates subject metadata\nto enhance the individualization of predictions; (iii) it incorporates prior\nknowledge of disease dynamics through an auxiliary model, facilitating the\nintegration of longitudinal data; and (iv) it introduces the Latent Average\nStabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in\nthe predicted progression at inference time and (b) allows us to derive a\nmeasure of the uncertainty for the prediction. We train and evaluate BrLP on\n11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its\ngeneralizability on an external test set comprising 2,257 MRIs from 962\nsubjects. Our experiments compare BrLP-generated MRI scans with real follow-up\nMRIs, demonstrating state-of-the-art accuracy compared to existing methods. The\ncode is publicly available at: https://github.com/LemuelPuglisi/BrLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing availability of longitudinal Magnetic Resonance Imaging (MRI)\ndatasets has facilitated Artificial Intelligence (AI)-driven modeling of\ndisease progression, making it possible to predict future medical scans for\nindividual patients. However, despite significant advancements in AI, current\nmethods continue to face challenges including achieving patient-specific\nindividualization, ensuring spatiotemporal consistency, efficiently utilizing\nlongitudinal data, and managing the substantial memory demands of 3D scans. To\naddress these challenges, we propose Brain Latent Progression (BrLP), a novel\nspatiotemporal model designed to predict individual-level disease progression\nin 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates\nin a small latent space, mitigating the computational challenges posed by\nhigh-dimensional imaging data; (ii) it explicitly integrates subject metadata\nto enhance the individualization of predictions; (iii) it incorporates prior\nknowledge of disease dynamics through an auxiliary model, facilitating the\nintegration of longitudinal data; and (iv) it introduces the Latent Average\nStabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in\nthe predicted progression at inference time and (b) allows us to derive a\nmeasure of the uncertainty for the prediction. We train and evaluate BrLP on\n11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its\ngeneralizability on an external test set comprising 2,257 MRIs from 962\nsubjects. Our experiments compare BrLP-generated MRI scans with real follow-up\nMRIs, demonstrating state-of-the-art accuracy compared to existing methods. The\ncode is publicly available at: https://github.com/LemuelPuglisi/BrLP."
                },
                "authors": [
                    {
                        "name": "Lemuel Puglisi"
                    },
                    {
                        "name": "Daniel C. Alexander"
                    },
                    {
                        "name": "Daniele Ravì"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Ravì"
                },
                "author": "Daniele Ravì",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2405.03328",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19702v2",
                "updated": "2025-02-12T16:47:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    47,
                    30,
                    2,
                    43,
                    0
                ],
                "published": "2024-10-25T17:19:55Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    19,
                    55,
                    4,
                    299,
                    0
                ],
                "title": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zeng"
                    },
                    {
                        "name": "Kunchang Li"
                    },
                    {
                        "name": "Chenting Wang"
                    },
                    {
                        "name": "Xinhao Li"
                    },
                    {
                        "name": "Tianxiang Jiang"
                    },
                    {
                        "name": "Ziang Yan"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Yansong Shi"
                    },
                    {
                        "name": "Zhengrong Yue"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yali Wang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2110.06257v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2110.06257v3",
                "updated": "2025-02-12T16:47:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    47,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2021-10-12T18:12:57Z",
                "published_parsed": [
                    2021,
                    10,
                    12,
                    18,
                    12,
                    57,
                    1,
                    285,
                    0
                ],
                "title": "Causal Discovery from Conditionally Stationary Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Discovery from Conditionally Stationary Time Series"
                },
                "summary": "Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, is highly challenging for AI systems. In a time series\nmodeling context, traditional causal discovery methods mainly consider\nconstrained scenarios with fully observed variables and/or data from stationary\ntime-series. We develop a causal discovery approach to handle a wide class of\nnonstationary time series that are conditionally stationary, where the\nnonstationary behaviour is modeled as stationarity conditioned on a set of\nlatent state variables. Named State-Dependent Causal Inference (SDCI), our\napproach is able to recover the underlying causal dependencies, with provable\nidentifiablity for the state-dependent causal structures. Empirical experiments\non nonlinear particle interaction data and gene regulatory networks demonstrate\nSDCI's superior performance over baseline causal discovery methods. Improved\nresults over non-causal RNNs on modeling NBA player movements demonstrate the\npotential of our method and motivate the use of causality-driven methods for\nforecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, is highly challenging for AI systems. In a time series\nmodeling context, traditional causal discovery methods mainly consider\nconstrained scenarios with fully observed variables and/or data from stationary\ntime-series. We develop a causal discovery approach to handle a wide class of\nnonstationary time series that are conditionally stationary, where the\nnonstationary behaviour is modeled as stationarity conditioned on a set of\nlatent state variables. Named State-Dependent Causal Inference (SDCI), our\napproach is able to recover the underlying causal dependencies, with provable\nidentifiablity for the state-dependent causal structures. Empirical experiments\non nonlinear particle interaction data and gene regulatory networks demonstrate\nSDCI's superior performance over baseline causal discovery methods. Improved\nresults over non-causal RNNs on modeling NBA player movements demonstrate the\npotential of our method and motivate the use of causality-driven methods for\nforecasting."
                },
                "authors": [
                    {
                        "name": "Carles Balsells-Rodas"
                    },
                    {
                        "name": "Xavier Sumba"
                    },
                    {
                        "name": "Tanmayee Narendra"
                    },
                    {
                        "name": "Ruibo Tu"
                    },
                    {
                        "name": "Gabriele Schweikert"
                    },
                    {
                        "name": "Hedvig Kjellstrom"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2110.06257v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2110.06257v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08557v1",
                "updated": "2025-02-12T16:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    39,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:39:06Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    39,
                    6,
                    2,
                    43,
                    0
                ],
                "title": "QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval"
                },
                "summary": "Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Seunghyun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seunghyun Lee"
                },
                "author": "Seunghyun Lee",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08554v1",
                "updated": "2025-02-12T16:35:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    35,
                    41,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:35:41Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    35,
                    41,
                    2,
                    43,
                    0
                ],
                "title": "Fostering Appropriate Reliance on Large Language Models: The Role of\n  Explanations, Sources, and Inconsistencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fostering Appropriate Reliance on Large Language Models: The Role of\n  Explanations, Sources, and Inconsistencies"
                },
                "summary": "Large language models (LLMs) can produce erroneous responses that sound\nfluent and convincing, raising the risk that users will rely on these responses\nas if they were correct. Mitigating such overreliance is a key challenge.\nThrough a think-aloud study in which participants use an LLM-infused\napplication to answer objective questions, we identify several features of LLM\nresponses that shape users' reliance: explanations (supporting details for\nanswers), inconsistencies in explanations, and sources. Through a large-scale,\npre-registered, controlled experiment (N=308), we isolate and study the effects\nof these features on users' reliance, accuracy, and other measures. We find\nthat the presence of explanations increases reliance on both correct and\nincorrect responses. However, we observe less reliance on incorrect responses\nwhen sources are provided or when explanations exhibit inconsistencies. We\ndiscuss the implications of these findings for fostering appropriate reliance\non LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can produce erroneous responses that sound\nfluent and convincing, raising the risk that users will rely on these responses\nas if they were correct. Mitigating such overreliance is a key challenge.\nThrough a think-aloud study in which participants use an LLM-infused\napplication to answer objective questions, we identify several features of LLM\nresponses that shape users' reliance: explanations (supporting details for\nanswers), inconsistencies in explanations, and sources. Through a large-scale,\npre-registered, controlled experiment (N=308), we isolate and study the effects\nof these features on users' reliance, accuracy, and other measures. We find\nthat the presence of explanations increases reliance on both correct and\nincorrect responses. However, we observe less reliance on incorrect responses\nwhen sources are provided or when explanations exhibit inconsistencies. We\ndiscuss the implications of these findings for fostering appropriate reliance\non LLMs."
                },
                "authors": [
                    {
                        "name": "Sunnie S. Y. Kim"
                    },
                    {
                        "name": "Jennifer Wortman Vaughan"
                    },
                    {
                        "name": "Q. Vera Liao"
                    },
                    {
                        "name": "Tania Lombrozo"
                    },
                    {
                        "name": "Olga Russakovsky"
                    }
                ],
                "author_detail": {
                    "name": "Olga Russakovsky"
                },
                "author": "Olga Russakovsky",
                "arxiv_doi": "10.1145/3706598.3714020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "CHI 2025. This version includes the appendix",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08550v1",
                "updated": "2025-02-12T16:31:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    31,
                    21,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:31:21Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    31,
                    21,
                    2,
                    43,
                    0
                ],
                "title": "LLMs can implicitly learn from mistakes in-context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can implicitly learn from mistakes in-context"
                },
                "summary": "Learning from mistakes is a fundamental feature of human intelligence.\nPrevious work has shown that Large Language Models (LLMs) can also learn from\nincorrect answers when provided with a comprehensive rationale detailing why an\nanswer is wrong or how to correct it. In this work, we examine whether LLMs can\nlearn from mistakes in mathematical reasoning tasks when these explanations are\nnot provided. We investigate if LLMs are able to implicitly infer such\nrationales simply from observing both incorrect and correct answers.\nSurprisingly, we find that LLMs perform better, on average, when rationales are\neliminated from the context and incorrect answers are simply shown alongside\ncorrect ones. This approach also substantially outperforms chain-of-thought\nprompting in our evaluations. We show that these results are consistent across\nLLMs of different sizes and varying reasoning abilities. Further, we carry out\nan in-depth analysis, and show that prompting with both wrong and correct\nanswers leads to greater performance and better generalisation than introducing\nadditional, more diverse question-answer pairs into the context. Finally, we\nshow that new rationales generated by models that have only observed incorrect\nand correct answers are scored equally as highly by humans as those produced\nwith the aid of exemplar rationales. Our results demonstrate that LLMs are\nindeed capable of in-context implicit learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from mistakes is a fundamental feature of human intelligence.\nPrevious work has shown that Large Language Models (LLMs) can also learn from\nincorrect answers when provided with a comprehensive rationale detailing why an\nanswer is wrong or how to correct it. In this work, we examine whether LLMs can\nlearn from mistakes in mathematical reasoning tasks when these explanations are\nnot provided. We investigate if LLMs are able to implicitly infer such\nrationales simply from observing both incorrect and correct answers.\nSurprisingly, we find that LLMs perform better, on average, when rationales are\neliminated from the context and incorrect answers are simply shown alongside\ncorrect ones. This approach also substantially outperforms chain-of-thought\nprompting in our evaluations. We show that these results are consistent across\nLLMs of different sizes and varying reasoning abilities. Further, we carry out\nan in-depth analysis, and show that prompting with both wrong and correct\nanswers leads to greater performance and better generalisation than introducing\nadditional, more diverse question-answer pairs into the context. Finally, we\nshow that new rationales generated by models that have only observed incorrect\nand correct answers are scored equally as highly by humans as those produced\nwith the aid of exemplar rationales. Our results demonstrate that LLMs are\nindeed capable of in-context implicit learning."
                },
                "authors": [
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Maximilian Mozes"
                    },
                    {
                        "name": "Jon Ander Campos"
                    },
                    {
                        "name": "Yi Chern Tan"
                    },
                    {
                        "name": "Marek Rei"
                    },
                    {
                        "name": "Max Bartolo"
                    }
                ],
                "author_detail": {
                    "name": "Max Bartolo"
                },
                "author": "Max Bartolo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11172v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11172v3",
                "updated": "2025-02-12T16:23:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    23,
                    2,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-17T13:26:17Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    26,
                    17,
                    1,
                    261,
                    0
                ],
                "title": "Annealed Winner-Takes-All for Motion Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annealed Winner-Takes-All for Motion Forecasting"
                },
                "summary": "In autonomous driving, motion prediction aims at forecasting the future\ntrajectories of nearby agents, helping the ego vehicle to anticipate behaviors\nand drive safely. A key challenge is generating a diverse set of future\npredictions, commonly addressed using data-driven models with Multiple Choice\nLearning (MCL) architectures and Winner-Takes-All (WTA) training objectives.\nHowever, these methods face initialization sensitivity and training\ninstabilities. Additionally, to compensate for limited performance, some\napproaches rely on training with a large set of hypotheses, requiring a\npost-selection step during inference to significantly reduce the number of\npredictions. To tackle these issues, we take inspiration from annealed MCL, a\nrecently introduced technique that improves the convergence properties of MCL\nmethods through an annealed Winner-Takes-All loss (aWTA). In this paper, we\ndemonstrate how the aWTA loss can be integrated with state-of-the-art motion\nforecasting models to enhance their performance using only a minimal set of\nhypotheses, eliminating the need for the cumbersome post-selection step. Our\napproach can be easily incorporated into any trajectory prediction model\nnormally trained using WTA and yields significant improvements. To facilitate\nthe application of our approach to future motion forecasting models, the code\nis made publicly available: https://github.com/valeoai/MF_aWTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In autonomous driving, motion prediction aims at forecasting the future\ntrajectories of nearby agents, helping the ego vehicle to anticipate behaviors\nand drive safely. A key challenge is generating a diverse set of future\npredictions, commonly addressed using data-driven models with Multiple Choice\nLearning (MCL) architectures and Winner-Takes-All (WTA) training objectives.\nHowever, these methods face initialization sensitivity and training\ninstabilities. Additionally, to compensate for limited performance, some\napproaches rely on training with a large set of hypotheses, requiring a\npost-selection step during inference to significantly reduce the number of\npredictions. To tackle these issues, we take inspiration from annealed MCL, a\nrecently introduced technique that improves the convergence properties of MCL\nmethods through an annealed Winner-Takes-All loss (aWTA). In this paper, we\ndemonstrate how the aWTA loss can be integrated with state-of-the-art motion\nforecasting models to enhance their performance using only a minimal set of\nhypotheses, eliminating the need for the cumbersome post-selection step. Our\napproach can be easily incorporated into any trajectory prediction model\nnormally trained using WTA and yields significant improvements. To facilitate\nthe application of our approach to future motion forecasting models, the code\nis made publicly available: https://github.com/valeoai/MF_aWTA."
                },
                "authors": [
                    {
                        "name": "Yihong Xu"
                    },
                    {
                        "name": "Victor Letzelter"
                    },
                    {
                        "name": "Mickaël Chen"
                    },
                    {
                        "name": "Éloi Zablocki"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "7 pages, 6 figures, Accepted to ICRA2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11172v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11172v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08528v1",
                "updated": "2025-02-12T16:05:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    5,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:05:46Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    5,
                    46,
                    2,
                    43,
                    0
                ],
                "title": "BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image\n  Generation"
                },
                "summary": "The properties of black holes and accretion flows can be inferred by fitting\nEvent Horizon Telescope (EHT) data to simulated images generated through\ngeneral relativistic ray tracing (GRRT). However, due to the computationally\nintensive nature of GRRT, the efficiency of generating specific radiation flux\nimages needs to be improved. This paper introduces the Branch Correction\nDenoising Diffusion Model (BCDDM), which uses a branch correction mechanism and\na weighted mixed loss function to improve the accuracy of generated black hole\nimages based on seven physical parameters of the radiatively inefficient\naccretion flow (RIAF) model. Our experiments show a strong correlation between\nthe generated images and their physical parameters. By enhancing the GRRT\ndataset with BCDDM-generated images and using ResNet50 for parameter\nregression, we achieve significant improvements in parameter prediction\nperformance. This approach reduces computational costs and provides a faster,\nmore efficient method for dataset expansion, parameter estimation, and model\nfitting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The properties of black holes and accretion flows can be inferred by fitting\nEvent Horizon Telescope (EHT) data to simulated images generated through\ngeneral relativistic ray tracing (GRRT). However, due to the computationally\nintensive nature of GRRT, the efficiency of generating specific radiation flux\nimages needs to be improved. This paper introduces the Branch Correction\nDenoising Diffusion Model (BCDDM), which uses a branch correction mechanism and\na weighted mixed loss function to improve the accuracy of generated black hole\nimages based on seven physical parameters of the radiatively inefficient\naccretion flow (RIAF) model. Our experiments show a strong correlation between\nthe generated images and their physical parameters. By enhancing the GRRT\ndataset with BCDDM-generated images and using ResNet50 for parameter\nregression, we achieve significant improvements in parameter prediction\nperformance. This approach reduces computational costs and provides a faster,\nmore efficient method for dataset expansion, parameter estimation, and model\nfitting."
                },
                "authors": [
                    {
                        "name": "Ao liu"
                    },
                    {
                        "name": "Zelin Zhang"
                    },
                    {
                        "name": "Songbai Chen"
                    },
                    {
                        "name": "Cuihong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Cuihong Wen"
                },
                "author": "Cuihong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08524v1",
                "updated": "2025-02-12T16:00:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    0,
                    11,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:00:11Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    0,
                    11,
                    2,
                    43,
                    0
                ],
                "title": "LLM Pretraining with Continuous Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Pretraining with Continuous Concepts"
                },
                "summary": "Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process."
                },
                "authors": [
                    {
                        "name": "Jihoon Tack"
                    },
                    {
                        "name": "Jack Lanchantin"
                    },
                    {
                        "name": "Jane Yu"
                    },
                    {
                        "name": "Andrew Cohen"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Janice Lan"
                    },
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Xian Li"
                    }
                ],
                "author_detail": {
                    "name": "Xian Li"
                },
                "author": "Xian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06766v2",
                "updated": "2025-02-12T15:55:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    55,
                    37,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-10T18:47:04Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    47,
                    4,
                    0,
                    41,
                    0
                ],
                "title": "Exploiting Sparsity for Long Context Inference: Million Token Contexts\n  on Commodity GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Sparsity for Long Context Inference: Million Token Contexts\n  on Commodity GPUs"
                },
                "summary": "There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common benchmarks (RULER,\nAlpacaEval, and Open LLM Leaderboard).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common benchmarks (RULER,\nAlpacaEval, and Open LLM Leaderboard)."
                },
                "authors": [
                    {
                        "name": "Ryan Synk"
                    },
                    {
                        "name": "Monte Hoover"
                    },
                    {
                        "name": "John Kirchenbauer"
                    },
                    {
                        "name": "Neel Jain"
                    },
                    {
                        "name": "Alex Stein"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "Josue Melendez Sanchez"
                    },
                    {
                        "name": "Ramani Duraiswami"
                    },
                    {
                        "name": "Tom Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Tom Goldstein"
                },
                "author": "Tom Goldstein",
                "arxiv_comment": "9 pages, 9 figures, 2 tables in main body",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08515v1",
                "updated": "2025-02-12T15:47:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    47,
                    48,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:47:48Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    47,
                    48,
                    2,
                    43,
                    0
                ],
                "title": "The Paradox of Stochasticity: Limited Creativity and Computational\n  Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Paradox of Stochasticity: Limited Creativity and Computational\n  Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data"
                },
                "summary": "This study examines how temperature settings and model architectures affect\nthe generation of structured fictional data (names, birthdates) across three\nlarge language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest.\nBy systematically testing temperature values from 0.0 to 1.0 in increments of\n0.1, we conducted 330 trials yielding 889 structured entities, validated for\nsyntactic consistency. Key findings reveal that model architecture\nsignificantly influences computational efficiency, with mistral:latest and\nllama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to\nexpectations, temperature showed no correlation with processing time,\nchallenging assumptions about stochastic sampling costs. Output diversity\nremained limited, as models consistently defaulted to common name archetypes\n(e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names\nclustered at intermediate values (0.3-0.7). These results demonstrate that\narchitectural optimizations, rather than temperature adjustments, dominate\nperformance in structured generation tasks. The findings emphasize prioritizing\nmodel selection over hyperparameter tuning for efficiency and suggest explicit\ndiversity constraints are necessary to mitigate default output biases in\nsynthetic data pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines how temperature settings and model architectures affect\nthe generation of structured fictional data (names, birthdates) across three\nlarge language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest.\nBy systematically testing temperature values from 0.0 to 1.0 in increments of\n0.1, we conducted 330 trials yielding 889 structured entities, validated for\nsyntactic consistency. Key findings reveal that model architecture\nsignificantly influences computational efficiency, with mistral:latest and\nllama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to\nexpectations, temperature showed no correlation with processing time,\nchallenging assumptions about stochastic sampling costs. Output diversity\nremained limited, as models consistently defaulted to common name archetypes\n(e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names\nclustered at intermediate values (0.3-0.7). These results demonstrate that\narchitectural optimizations, rather than temperature adjustments, dominate\nperformance in structured generation tasks. The findings emphasize prioritizing\nmodel selection over hyperparameter tuning for efficiency and suggest explicit\ndiversity constraints are necessary to mitigate default output biases in\nsynthetic data pipelines."
                },
                "authors": [
                    {
                        "name": "Evgenii Evstafev"
                    }
                ],
                "author_detail": {
                    "name": "Evgenii Evstafev"
                },
                "author": "Evgenii Evstafev",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08514v1",
                "updated": "2025-02-12T15:46:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    50,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:46:50Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    50,
                    2,
                    43,
                    0
                ],
                "title": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation"
                },
                "summary": "Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries."
                },
                "authors": [
                    {
                        "name": "Mahnaz Koupaee"
                    },
                    {
                        "name": "Jake W. Vincent"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Igor Shalyminov"
                    },
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Hwanjun Song"
                    },
                    {
                        "name": "Raphael Shu"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Yi Nian"
                    },
                    {
                        "name": "Amy Wing-mei Wong"
                    },
                    {
                        "name": "Kyu J. Han"
                    },
                    {
                        "name": "Hang Su"
                    }
                ],
                "author_detail": {
                    "name": "Hang Su"
                },
                "author": "Hang Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08512v1",
                "updated": "2025-02-12T15:46:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    34,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:46:34Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    34,
                    2,
                    43,
                    0
                ],
                "title": "Measuring Diversity in Synthetic Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Diversity in Synthetic Datasets"
                },
                "summary": "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https://github.com/BlueWhaleLab/DCScore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https://github.com/BlueWhaleLab/DCScore."
                },
                "authors": [
                    {
                        "name": "Yuchang Zhu"
                    },
                    {
                        "name": "Huizhe Zhang"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Jintang Li"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yatao Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yatao Bian"
                },
                "author": "Yatao Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08507v1",
                "updated": "2025-02-12T15:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    41,
                    43,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:41:43Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    41,
                    43,
                    2,
                    43,
                    0
                ],
                "title": "Explanation based In-Context Demonstrations Retrieval for Multilingual\n  Grammatical Error Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanation based In-Context Demonstrations Retrieval for Multilingual\n  Grammatical Error Correction"
                },
                "summary": "Grammatical error correction (GEC) aims to correct grammatical, spelling, and\nsemantic errors in natural language text. With the growing of large language\nmodels (LLMs), direct text generation has gradually become the focus of the GEC\nmethods, and few-shot in-context learning presents a cost-effective solution.\nHowever, selecting effective in-context examples remains challenging, as the\nsimilarity between input texts does not necessarily correspond to similar\ngrammatical error patterns. In this paper, we propose a novel retrieval method\nbased on natural language grammatical error explanations (GEE) to address this\nissue. Our method retrieves suitable few-shot demonstrations by matching the\nGEE of the test input with that of pre-constructed database samples, where\nexplanations for erroneous samples are generated by LLMs. We conducted\nmultilingual GEC few-shot experiments on both major open-source and\nclosed-source LLMs. Experiments across five languages show that our method\noutperforms existing semantic and BM25-based retrieval techniques, without\nrequiring additional training or language adaptation. This also suggests that\nmatching error patterns is key to selecting examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammatical error correction (GEC) aims to correct grammatical, spelling, and\nsemantic errors in natural language text. With the growing of large language\nmodels (LLMs), direct text generation has gradually become the focus of the GEC\nmethods, and few-shot in-context learning presents a cost-effective solution.\nHowever, selecting effective in-context examples remains challenging, as the\nsimilarity between input texts does not necessarily correspond to similar\ngrammatical error patterns. In this paper, we propose a novel retrieval method\nbased on natural language grammatical error explanations (GEE) to address this\nissue. Our method retrieves suitable few-shot demonstrations by matching the\nGEE of the test input with that of pre-constructed database samples, where\nexplanations for erroneous samples are generated by LLMs. We conducted\nmultilingual GEC few-shot experiments on both major open-source and\nclosed-source LLMs. Experiments across five languages show that our method\noutperforms existing semantic and BM25-based retrieval techniques, without\nrequiring additional training or language adaptation. This also suggests that\nmatching error patterns is key to selecting examples."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Wen Luo"
                    },
                    {
                        "name": "Guangyue Peng"
                    },
                    {
                        "name": "Houfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Houfeng Wang"
                },
                "author": "Houfeng Wang",
                "arxiv_comment": "Accepted by NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06015v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06015v3",
                "updated": "2025-02-12T15:41:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    41,
                    18,
                    2,
                    43,
                    0
                ],
                "published": "2024-05-09T18:00:01Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    18,
                    0,
                    1,
                    3,
                    130,
                    0
                ],
                "title": "Fast-moving stars around an intermediate-mass black hole in Omega\n  Centauri",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-moving stars around an intermediate-mass black hole in Omega\n  Centauri"
                },
                "summary": "Black holes have been found over a wide range of masses, from stellar\nremnants with masses of 5-150 solar masses (Msun), to those found at the\ncenters of galaxies with $M>10^5$ Msun. However, only a few debated candidate\nblack holes exist between 150 and $10^5$ Msun. Determining the population of\nthese intermediate-mass black holes is an important step towards understanding\nsupermassive black hole formation in the early universe. Several studies have\nclaimed the detection of a central black hole in $\\omega$ Centauri, the Milky\nWay's most massive globular cluster. However, these studies have been\nquestioned due to the possible mass contribution of stellar mass black holes,\ntheir sensitivity to the cluster center, and the lack of fast-moving stars\nabove the escape velocity. Here we report observations of seven fast-moving\nstars in the central 3 arcseconds (0.08 pc) of $\\omega$ Centauri. The\nvelocities of the fast-moving stars are significantly higher than the expected\ncentral escape velocity of the star cluster, so their presence can only be\nexplained by being bound to a massive black hole. From the velocities alone, we\ncan infer a firm lower limit of the black hole mass of $\\sim$8,200 Msun, making\nthis a compelling candidate for an intermediate-mass black hole in the local\nuniverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black holes have been found over a wide range of masses, from stellar\nremnants with masses of 5-150 solar masses (Msun), to those found at the\ncenters of galaxies with $M>10^5$ Msun. However, only a few debated candidate\nblack holes exist between 150 and $10^5$ Msun. Determining the population of\nthese intermediate-mass black holes is an important step towards understanding\nsupermassive black hole formation in the early universe. Several studies have\nclaimed the detection of a central black hole in $\\omega$ Centauri, the Milky\nWay's most massive globular cluster. However, these studies have been\nquestioned due to the possible mass contribution of stellar mass black holes,\ntheir sensitivity to the cluster center, and the lack of fast-moving stars\nabove the escape velocity. Here we report observations of seven fast-moving\nstars in the central 3 arcseconds (0.08 pc) of $\\omega$ Centauri. The\nvelocities of the fast-moving stars are significantly higher than the expected\ncentral escape velocity of the star cluster, so their presence can only be\nexplained by being bound to a massive black hole. From the velocities alone, we\ncan infer a firm lower limit of the black hole mass of $\\sim$8,200 Msun, making\nthis a compelling candidate for an intermediate-mass black hole in the local\nuniverse."
                },
                "authors": [
                    {
                        "name": "Maximilian Häberle"
                    },
                    {
                        "name": "Nadine Neumayer"
                    },
                    {
                        "name": "Anil Seth"
                    },
                    {
                        "name": "Andrea Bellini"
                    },
                    {
                        "name": "Mattia Libralato"
                    },
                    {
                        "name": "Holger Baumgardt"
                    },
                    {
                        "name": "Matthew Whitaker"
                    },
                    {
                        "name": "Antoine Dumont"
                    },
                    {
                        "name": "Mayte Alfaro Cuello"
                    },
                    {
                        "name": "Jay Anderson"
                    },
                    {
                        "name": "Callie Clontz"
                    },
                    {
                        "name": "Nikolay Kacharov"
                    },
                    {
                        "name": "Sebastian Kamann"
                    },
                    {
                        "name": "Anja Feldmeier-Krause"
                    },
                    {
                        "name": "Antonino Milone"
                    },
                    {
                        "name": "Maria Selina Nitschai"
                    },
                    {
                        "name": "Renuka Pechetti"
                    },
                    {
                        "name": "Glenn van de Ven"
                    }
                ],
                "author_detail": {
                    "name": "Glenn van de Ven"
                },
                "author": "Glenn van de Ven",
                "arxiv_doi": "10.1038/s41586-024-07511-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41586-024-07511-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.06015v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06015v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "33 pages, 11 figures, and 2 tables. Published in Nature. This is the\n  accepted author's version including the correction issued on Sep 17, 2024.\n  The version of record is available from the Journal (open access)",
                "arxiv_journal_ref": "Nature 631, 285-288 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14679v2",
                "updated": "2025-02-12T15:37:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    37,
                    22,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-24T17:57:06Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    57,
                    6,
                    4,
                    24,
                    0
                ],
                "title": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation"
                },
                "summary": "Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba."
                },
                "authors": [
                    {
                        "name": "Rongzhao He"
                    },
                    {
                        "name": "Weihao Zheng"
                    },
                    {
                        "name": "Leilei Zhao"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Dalin Zhu"
                    },
                    {
                        "name": "Dan Wu"
                    },
                    {
                        "name": "Bin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Hu"
                },
                "author": "Bin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08503v1",
                "updated": "2025-02-12T15:34:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    34,
                    45,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:34:45Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    34,
                    45,
                    2,
                    43,
                    0
                ],
                "title": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?"
                },
                "summary": "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs."
                },
                "authors": [
                    {
                        "name": "Jiahe Jin"
                    },
                    {
                        "name": "Yanheng He"
                    },
                    {
                        "name": "Mingyan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mingyan Yang"
                },
                "author": "Mingyan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02540v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02540v3",
                "updated": "2025-02-12T15:14:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    14,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2024-11-04T19:21:06Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    19,
                    21,
                    6,
                    0,
                    309,
                    0
                ],
                "title": "GraphXAIN: Narratives to Explain Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphXAIN: Narratives to Explain Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose challenges in interpretability. Existing\nGNN explanation methods usually yield technical outputs, such as subgraphs and\nfeature importance scores, that are difficult for non-data scientists to\nunderstand and thereby violate the purpose of explanations. Motivated by recent\nExplainable AI (XAI) research, we propose GraphXAIN, a method that generates\nnatural language narratives explaining GNN predictions. GraphXAIN is a model-\nand explainer-agnostic method that uses Large Language Models (LLMs) to\ntranslate explanatory subgraphs and feature importance scores into coherent,\nstory-like explanations of GNN decision-making processes. Evaluations on\nreal-world datasets demonstrate GraphXAIN's ability to improve graph\nexplanations. A survey of machine learning researchers and practitioners\nreveals that GraphXAIN enhances four explainability dimensions:\nunderstandability, satisfaction, convincingness, and suitability for\ncommunicating model predictions. When combined with another graph explainer\nmethod, GraphXAIN further improves trustworthiness, insightfulness, confidence,\nand usability. Notably, 95% of participants found GraphXAIN to be a valuable\naddition to the GNN explanation method. By incorporating natural language\nnarratives, our approach serves both graph practitioners and non-expert users\nby providing clearer and more effective explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose challenges in interpretability. Existing\nGNN explanation methods usually yield technical outputs, such as subgraphs and\nfeature importance scores, that are difficult for non-data scientists to\nunderstand and thereby violate the purpose of explanations. Motivated by recent\nExplainable AI (XAI) research, we propose GraphXAIN, a method that generates\nnatural language narratives explaining GNN predictions. GraphXAIN is a model-\nand explainer-agnostic method that uses Large Language Models (LLMs) to\ntranslate explanatory subgraphs and feature importance scores into coherent,\nstory-like explanations of GNN decision-making processes. Evaluations on\nreal-world datasets demonstrate GraphXAIN's ability to improve graph\nexplanations. A survey of machine learning researchers and practitioners\nreveals that GraphXAIN enhances four explainability dimensions:\nunderstandability, satisfaction, convincingness, and suitability for\ncommunicating model predictions. When combined with another graph explainer\nmethod, GraphXAIN further improves trustworthiness, insightfulness, confidence,\nand usability. Notably, 95% of participants found GraphXAIN to be a valuable\naddition to the GNN explanation method. By incorporating natural language\nnarratives, our approach serves both graph practitioners and non-expert users\nby providing clearer and more effective explanations."
                },
                "authors": [
                    {
                        "name": "Mateusz Cedro"
                    },
                    {
                        "name": "David Martens"
                    }
                ],
                "author_detail": {
                    "name": "David Martens"
                },
                "author": "David Martens",
                "arxiv_comment": "19 pages, 9 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02540v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02540v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11759v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11759v4",
                "updated": "2025-02-12T15:07:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    7,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2024-10-15T16:28:55Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    28,
                    55,
                    1,
                    289,
                    0
                ],
                "title": "LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and\n  General Noise for Global Causal Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and\n  General Noise for Global Causal Discovery"
                },
                "summary": "Inferring causal relationships from observational data is crucial when\nexperiments are costly or infeasible. Additive noise models (ANMs) enable\nunique directed acyclic graph (DAG) identification, but existing ANM methods\noften rely on restrictive assumptions on the data generating process, limiting\ntheir applicability to real-world settings. We propose local search in additive\nnoise models, LoSAM, a topological ordering method for learning a unique DAG in\nANMs with mixed causal mechanisms and general noise distributions. We introduce\nnew causal substructures and criteria for identifying roots and leaves,\nenabling efficient top-down learning. We prove asymptotic consistency and\npolynomial runtime, ensuring scalability and sample efficiency. We test LoSAM\non synthetic and real-world data, demonstrating state-of-the-art performance\nacross all mixed mechanism settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring causal relationships from observational data is crucial when\nexperiments are costly or infeasible. Additive noise models (ANMs) enable\nunique directed acyclic graph (DAG) identification, but existing ANM methods\noften rely on restrictive assumptions on the data generating process, limiting\ntheir applicability to real-world settings. We propose local search in additive\nnoise models, LoSAM, a topological ordering method for learning a unique DAG in\nANMs with mixed causal mechanisms and general noise distributions. We introduce\nnew causal substructures and criteria for identifying roots and leaves,\nenabling efficient top-down learning. We prove asymptotic consistency and\npolynomial runtime, ensuring scalability and sample efficiency. We test LoSAM\non synthetic and real-world data, demonstrating state-of-the-art performance\nacross all mixed mechanism settings."
                },
                "authors": [
                    {
                        "name": "Sujai Hiremath"
                    },
                    {
                        "name": "Promit Ghosal"
                    },
                    {
                        "name": "Kyra Gan"
                    }
                ],
                "author_detail": {
                    "name": "Kyra Gan"
                },
                "author": "Kyra Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11759v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11759v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07543v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07543v3",
                "updated": "2025-02-12T15:04:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    4,
                    2,
                    2,
                    43,
                    0
                ],
                "published": "2023-10-11T14:47:51Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    14,
                    47,
                    51,
                    2,
                    284,
                    0
                ],
                "title": "Ordinal Characterization of Similarity Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ordinal Characterization of Similarity Judgments"
                },
                "summary": "Characterizing judgments of similarity within a perceptual or semantic\ndomain, and making inferences about the underlying structure of this domain\nfrom these judgments, has an increasingly important role in cognitive and\nsystems neuroscience. We present a new framework for this purpose that makes\nlimited assumptions about how perceptual distances are converted into\nsimilarity judgments. The approach starts from a dataset of empirical judgments\nof relative similarities: the fraction of times that a subject chooses one of\ntwo comparison stimuli to be more similar to a reference stimulus. These\nempirical judgments provide Bayesian estimates of underling choice\nprobabilities. From these estimates, we derive indices that characterize the\nset of judgments in three ways: compatibility with a symmetric dis-similarity,\ncompatibility with an ultrametric space, and compatibility with an additive\ntree. Each of the indices is derived from rank-order relationships among the\nchoice probabilities that, as we show, are necessary and sufficient for local\nconsistency with the three respective characteristics. We illustrate this\napproach with simulations and example psychophysical datasets of dis-similarity\njudgments in several visual domains and provide code that implements the\nanalyses at https://github.com/jvlab/simrank.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing judgments of similarity within a perceptual or semantic\ndomain, and making inferences about the underlying structure of this domain\nfrom these judgments, has an increasingly important role in cognitive and\nsystems neuroscience. We present a new framework for this purpose that makes\nlimited assumptions about how perceptual distances are converted into\nsimilarity judgments. The approach starts from a dataset of empirical judgments\nof relative similarities: the fraction of times that a subject chooses one of\ntwo comparison stimuli to be more similar to a reference stimulus. These\nempirical judgments provide Bayesian estimates of underling choice\nprobabilities. From these estimates, we derive indices that characterize the\nset of judgments in three ways: compatibility with a symmetric dis-similarity,\ncompatibility with an ultrametric space, and compatibility with an additive\ntree. Each of the indices is derived from rank-order relationships among the\nchoice probabilities that, as we show, are necessary and sufficient for local\nconsistency with the three respective characteristics. We illustrate this\napproach with simulations and example psychophysical datasets of dis-similarity\njudgments in several visual domains and provide code that implements the\nanalyses at https://github.com/jvlab/simrank."
                },
                "authors": [
                    {
                        "name": "Jonathan D. Victor"
                    },
                    {
                        "name": "Guillermo Aguilar"
                    },
                    {
                        "name": "Suniyya A. Waraich"
                    }
                ],
                "author_detail": {
                    "name": "Suniyya A. Waraich"
                },
                "author": "Suniyya A. Waraich",
                "arxiv_comment": "Body: 66 pages; 16 figures; 7 supplementary figures, 3 appendices.\n  This replacement is a minor revision in response to a second round of peer\n  reviews in Mathematical Neuroscience and Applications (MNA). Main changes are\n  (i) clarification of heuristic nature of prior and surrogates, and (ii)\n  clarification of \"pointwise\" vs. \"setwise\" compatibility",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07543v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07543v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09966v2",
                "updated": "2025-02-12T14:55:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    55,
                    29,
                    2,
                    43,
                    0
                ],
                "published": "2024-08-19T13:14:02Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    14,
                    2,
                    0,
                    232,
                    0
                ],
                "title": "Mask in the Mirror: Implicit Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask in the Mirror: Implicit Sparsification"
                },
                "summary": "Continuous sparsification strategies are among the most effective methods for\nreducing the inference costs and memory demands of large-scale neural networks.\nA key factor in their success is the implicit $L_1$ regularization induced by\njointly learning both mask and weight variables, which has been shown\nexperimentally to outperform explicit $L_1$ regularization. We provide a\ntheoretical explanation for this observation by analyzing the learning\ndynamics, revealing that early continuous sparsification is governed by an\nimplicit $L_2$ regularization that gradually transitions to an $L_1$ penalty\nover time. Leveraging this insight, we propose a method to dynamically control\nthe strength of this implicit bias. Through an extension of the mirror flow\nframework, we establish convergence and optimality guarantees in the context of\nunderdetermined linear regression. Our theoretical findings may be of\nindependent interest, as we demonstrate how to enter the rich regime and show\nthat the implicit bias can be controlled via a time-dependent Bregman\npotential. To validate these insights, we introduce PILoT, a continuous\nsparsification approach with novel initialization and dynamic regularization,\nwhich consistently outperforms baselines in standard experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous sparsification strategies are among the most effective methods for\nreducing the inference costs and memory demands of large-scale neural networks.\nA key factor in their success is the implicit $L_1$ regularization induced by\njointly learning both mask and weight variables, which has been shown\nexperimentally to outperform explicit $L_1$ regularization. We provide a\ntheoretical explanation for this observation by analyzing the learning\ndynamics, revealing that early continuous sparsification is governed by an\nimplicit $L_2$ regularization that gradually transitions to an $L_1$ penalty\nover time. Leveraging this insight, we propose a method to dynamically control\nthe strength of this implicit bias. Through an extension of the mirror flow\nframework, we establish convergence and optimality guarantees in the context of\nunderdetermined linear regression. Our theoretical findings may be of\nindependent interest, as we demonstrate how to enter the rich regime and show\nthat the implicit bias can be controlled via a time-dependent Bregman\npotential. To validate these insights, we introduce PILoT, a continuous\nsparsification approach with novel initialization and dynamic regularization,\nwhich consistently outperforms baselines in standard experiments."
                },
                "authors": [
                    {
                        "name": "Tom Jacobs"
                    },
                    {
                        "name": "Rebekka Burkholz"
                    }
                ],
                "author_detail": {
                    "name": "Rebekka Burkholz"
                },
                "author": "Rebekka Burkholz",
                "arxiv_comment": "20 pages, 5 figures",
                "arxiv_journal_ref": "The Thirteenth International Conference on Learning\n  Representations (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02416v2",
                "updated": "2025-02-12T14:52:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    52,
                    56,
                    2,
                    43,
                    0
                ],
                "published": "2024-08-05T12:20:39Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    20,
                    39,
                    0,
                    218,
                    0
                ],
                "title": "Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in\n  Customized Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in\n  Customized Large Language Models"
                },
                "summary": "The drastic increase of large language models' (LLMs) parameters has led to a\nnew research direction of fine-tuning-free downstream customization by prompts,\ni.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)\nplay an important role in many businesses, there has emerged growing concerns\nabout the prompt leakage, which undermines the intellectual properties of these\nservices and causes downstream attacks. In this paper, we analyze the\nunderlying mechanism of prompt leakage, which we refer to as prompt\nmemorization, and develop corresponding defending strategies. By exploring the\nscaling laws in prompt extraction, we analyze key attributes that influence\nprompt extraction, including model sizes, prompt lengths, as well as the types\nof prompts. Then we propose two hypotheses that explain how LLMs expose their\nprompts. The first is attributed to the perplexity, i.e. the familiarity of\nLLMs to texts, whereas the second is based on the straightforward token\ntranslation path in attention matrices. To defend against such threats, we\ninvestigate whether alignments can undermine the extraction of prompts. We find\nthat current LLMs, even those with safety alignments like GPT-4, are highly\nvulnerable to prompt extraction attacks, even under the most straightforward\nuser attacks. Therefore, we put forward several defense strategies with the\ninspiration of our findings, which achieve 83.8\\% and 71.0\\% drop in the prompt\nextraction rate for Llama2-7B and GPT-3.5, respectively. Source code is\navaliable at https://github.com/liangzid/PromptExtractionEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The drastic increase of large language models' (LLMs) parameters has led to a\nnew research direction of fine-tuning-free downstream customization by prompts,\ni.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)\nplay an important role in many businesses, there has emerged growing concerns\nabout the prompt leakage, which undermines the intellectual properties of these\nservices and causes downstream attacks. In this paper, we analyze the\nunderlying mechanism of prompt leakage, which we refer to as prompt\nmemorization, and develop corresponding defending strategies. By exploring the\nscaling laws in prompt extraction, we analyze key attributes that influence\nprompt extraction, including model sizes, prompt lengths, as well as the types\nof prompts. Then we propose two hypotheses that explain how LLMs expose their\nprompts. The first is attributed to the perplexity, i.e. the familiarity of\nLLMs to texts, whereas the second is based on the straightforward token\ntranslation path in attention matrices. To defend against such threats, we\ninvestigate whether alignments can undermine the extraction of prompts. We find\nthat current LLMs, even those with safety alignments like GPT-4, are highly\nvulnerable to prompt extraction attacks, even under the most straightforward\nuser attacks. Therefore, we put forward several defense strategies with the\ninspiration of our findings, which achieve 83.8\\% and 71.0\\% drop in the prompt\nextraction rate for Llama2-7B and GPT-3.5, respectively. Source code is\navaliable at https://github.com/liangzid/PromptExtractionEval."
                },
                "authors": [
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Yaxin Xiao"
                    },
                    {
                        "name": "Haoyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoyang Li"
                },
                "author": "Haoyang Li",
                "arxiv_comment": "Source Code: https://github.com/liangzid/PromptExtractionEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07737v2",
                "updated": "2025-02-12T14:50:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    50,
                    50,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-11T17:57:53Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    57,
                    53,
                    1,
                    42,
                    0
                ],
                "title": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Block Prediction: Video Generation via Semi-Autoregressive Modeling"
                },
                "summary": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach."
                },
                "authors": [
                    {
                        "name": "Shuhuai Ren"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Xu Sun"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "project page: https://renshuhuai-andy.github.io/NBP-project/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08649v2",
                "updated": "2025-02-12T14:49:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    49,
                    16,
                    2,
                    43,
                    0
                ],
                "published": "2024-07-11T16:28:31Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    16,
                    28,
                    31,
                    3,
                    193,
                    0
                ],
                "title": "Confidence-based Estimators for Predictive Performance in Model\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-based Estimators for Predictive Performance in Model\n  Monitoring"
                },
                "summary": "After a machine learning model has been deployed into production, its\npredictive performance needs to be monitored. Ideally, such monitoring can be\ncarried out by comparing the model's predictions against ground truth labels.\nFor this to be possible, the ground truth labels must be available relatively\nsoon after inference. However, there are many use cases where ground truth\nlabels are available only after a significant delay, or in the worst case, not\nat all. In such cases, directly monitoring the model's predictive performance\nis impossible.\n  Recently, novel methods for estimating the predictive performance of a model\nwhen ground truth is unavailable have been developed. Many of these methods\nleverage model confidence or other uncertainty estimates and are experimentally\ncompared against a naive baseline method, namely Average Confidence (AC), which\nestimates model accuracy as the average of confidence scores for a given set of\npredictions. However, until now the theoretical properties of the AC method\nhave not been properly explored. In this paper, we try to fill this gap by\nreviewing the AC method and show that under certain general assumptions, it is\nan unbiased and consistent estimator of model accuracy with many desirable\nproperties. We also compare this baseline estimator against some more complex\nestimators empirically and show that in many cases the AC method is able to\nbeat the others, although the comparative quality of the different estimators\nis heavily case-dependent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "After a machine learning model has been deployed into production, its\npredictive performance needs to be monitored. Ideally, such monitoring can be\ncarried out by comparing the model's predictions against ground truth labels.\nFor this to be possible, the ground truth labels must be available relatively\nsoon after inference. However, there are many use cases where ground truth\nlabels are available only after a significant delay, or in the worst case, not\nat all. In such cases, directly monitoring the model's predictive performance\nis impossible.\n  Recently, novel methods for estimating the predictive performance of a model\nwhen ground truth is unavailable have been developed. Many of these methods\nleverage model confidence or other uncertainty estimates and are experimentally\ncompared against a naive baseline method, namely Average Confidence (AC), which\nestimates model accuracy as the average of confidence scores for a given set of\npredictions. However, until now the theoretical properties of the AC method\nhave not been properly explored. In this paper, we try to fill this gap by\nreviewing the AC method and show that under certain general assumptions, it is\nan unbiased and consistent estimator of model accuracy with many desirable\nproperties. We also compare this baseline estimator against some more complex\nestimators empirically and show that in many cases the AC method is able to\nbeat the others, although the comparative quality of the different estimators\nis heavily case-dependent."
                },
                "authors": [
                    {
                        "name": "Juhani Kivimäki"
                    },
                    {
                        "name": "Jakub Białek"
                    },
                    {
                        "name": "Jukka K. Nurminen"
                    },
                    {
                        "name": "Wojtek Kuberski"
                    }
                ],
                "author_detail": {
                    "name": "Wojtek Kuberski"
                },
                "author": "Wojtek Kuberski",
                "arxiv_doi": "10.1613/jair.1.16709",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1613/jair.1.16709",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.08649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version corresponds to the final published version in JAIR. The\n  published article is available at [https://doi.org/10.1613/jair.1.16709]",
                "arxiv_journal_ref": "Journal of Artificial Intelligence Research, vol. 82, pp. 209-240,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17400v2",
                "updated": "2025-02-12T14:46:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    46,
                    43,
                    2,
                    43,
                    0
                ],
                "published": "2024-02-27T10:47:24Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    10,
                    47,
                    24,
                    1,
                    58,
                    0
                ],
                "title": "Investigating Continual Pretraining in Large Language Models: Insights\n  and Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Continual Pretraining in Large Language Models: Insights\n  and Implications"
                },
                "summary": "Continual learning (CL) in large language models (LLMs) is an evolving domain\nthat focuses on developing efficient and sustainable training strategies to\nadapt models to emerging knowledge and achieve robustness in dynamic\nenvironments. Our primary emphasis is on continual domain-adaptive pretraining,\na process designed to equip LLMs with the ability to integrate new information\nfrom various domains while retaining previously learned knowledge. Since\nexisting works concentrate mostly on continual fine-tuning for a limited\nselection of downstream tasks or training domains, we introduce a new benchmark\ndesigned to measure the adaptability of LLMs to changing pretraining data\nlandscapes. We further examine the impact of model size on learning efficacy\nand forgetting, as well as how the progression and similarity of emerging\ndomains affect the knowledge transfer within these models.\n  Our findings uncover several key insights: (i) continual pretraining\nconsistently improves <1.5B models studied in this work and is also superior to\ndomain adaptation, (ii) larger models always achieve better perplexity than\nsmaller ones when continually pretrained on the same corpus, (iii) smaller\nmodels are particularly sensitive to continual pretraining, showing the most\nsignificant rates of both learning and forgetting, (iv) continual pretraining\nboosts downstream task performance of GPT-2 family, (v) continual pretraining\nenables LLMs to specialize better when the sequence of domains shows semantic\nsimilarity while randomizing training domains leads to better transfer and\nfinal performance otherwise. We posit that our research establishes a new\nbenchmark for CL in LLMs, providing a more realistic evaluation of knowledge\nretention and transfer across diverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) in large language models (LLMs) is an evolving domain\nthat focuses on developing efficient and sustainable training strategies to\nadapt models to emerging knowledge and achieve robustness in dynamic\nenvironments. Our primary emphasis is on continual domain-adaptive pretraining,\na process designed to equip LLMs with the ability to integrate new information\nfrom various domains while retaining previously learned knowledge. Since\nexisting works concentrate mostly on continual fine-tuning for a limited\nselection of downstream tasks or training domains, we introduce a new benchmark\ndesigned to measure the adaptability of LLMs to changing pretraining data\nlandscapes. We further examine the impact of model size on learning efficacy\nand forgetting, as well as how the progression and similarity of emerging\ndomains affect the knowledge transfer within these models.\n  Our findings uncover several key insights: (i) continual pretraining\nconsistently improves <1.5B models studied in this work and is also superior to\ndomain adaptation, (ii) larger models always achieve better perplexity than\nsmaller ones when continually pretrained on the same corpus, (iii) smaller\nmodels are particularly sensitive to continual pretraining, showing the most\nsignificant rates of both learning and forgetting, (iv) continual pretraining\nboosts downstream task performance of GPT-2 family, (v) continual pretraining\nenables LLMs to specialize better when the sequence of domains shows semantic\nsimilarity while randomizing training domains leads to better transfer and\nfinal performance otherwise. We posit that our research establishes a new\nbenchmark for CL in LLMs, providing a more realistic evaluation of knowledge\nretention and transfer across diverse domains."
                },
                "authors": [
                    {
                        "name": "Çağatay Yıldız"
                    },
                    {
                        "name": "Nishaanth Kanna Ravichandran"
                    },
                    {
                        "name": "Nitin Sharma"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Beyza Ermis"
                    }
                ],
                "author_detail": {
                    "name": "Beyza Ermis"
                },
                "author": "Beyza Ermis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.00055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.00055v2",
                "updated": "2025-02-12T14:43:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    43,
                    7,
                    2,
                    43,
                    0
                ],
                "published": "2023-10-31T18:03:54Z",
                "published_parsed": [
                    2023,
                    10,
                    31,
                    18,
                    3,
                    54,
                    1,
                    304,
                    0
                ],
                "title": "Rethinking Pre-Training in Tabular Data: A Neighborhood Embedding\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Pre-Training in Tabular Data: A Neighborhood Embedding\n  Perspective"
                },
                "summary": "Pre-training is prevalent in deep learning for vision and text data,\nleveraging knowledge from other datasets to enhance downstream tasks. However,\nfor tabular data, the inherent heterogeneity in attribute and label spaces\nacross datasets complicates the learning of shareable knowledge. We propose\nTabular data Pre-Training via Meta-representation (TabPTM), aiming to pre-train\na general tabular model over diverse datasets. The core idea is to embed data\ninstances into a shared feature space, where each instance is represented by\nits distance to a fixed number of nearest neighbors and their labels. This\n''meta-representation'' transforms heterogeneous tasks into homogeneous local\nprediction problems, enabling the model to infer labels (or scores for each\nlabel) based on neighborhood information. As a result, the pre-trained TabPTM\ncan be applied directly to new datasets, regardless of their diverse attributes\nand labels, without further fine-tuning. Extensive experiments on 101 datasets\nconfirm TabPTM's effectiveness in both classification and regression tasks,\nwith and without fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-training is prevalent in deep learning for vision and text data,\nleveraging knowledge from other datasets to enhance downstream tasks. However,\nfor tabular data, the inherent heterogeneity in attribute and label spaces\nacross datasets complicates the learning of shareable knowledge. We propose\nTabular data Pre-Training via Meta-representation (TabPTM), aiming to pre-train\na general tabular model over diverse datasets. The core idea is to embed data\ninstances into a shared feature space, where each instance is represented by\nits distance to a fixed number of nearest neighbors and their labels. This\n''meta-representation'' transforms heterogeneous tasks into homogeneous local\nprediction problems, enabling the model to infer labels (or scores for each\nlabel) based on neighborhood information. As a result, the pre-trained TabPTM\ncan be applied directly to new datasets, regardless of their diverse attributes\nand labels, without further fine-tuning. Extensive experiments on 101 datasets\nconfirm TabPTM's effectiveness in both classification and regression tasks,\nwith and without fine-tuning."
                },
                "authors": [
                    {
                        "name": "Han-Jia Ye"
                    },
                    {
                        "name": "Qi-Le Zhou"
                    },
                    {
                        "name": "Huai-Hong Yin"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Wei-Lun Chao"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Lun Chao"
                },
                "author": "Wei-Lun Chao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.00055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.00055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20154v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20154v4",
                "updated": "2025-02-12T14:43:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    43,
                    2,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T10:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    2,
                    42,
                    0,
                    274,
                    0
                ],
                "title": "GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for\n  Generalized 3D Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for\n  Generalized 3D Manipulation"
                },
                "summary": "Robots' ability to follow language instructions and execute diverse 3D\nmanipulation tasks is vital in robot learning. Traditional imitation\nlearning-based methods perform well on seen tasks but struggle with novel,\nunseen ones due to variability. Recent approaches leverage large foundation\nmodels to assist in understanding novel tasks, thereby mitigating this issue.\nHowever, these methods lack a task-specific learning process, which is\nessential for an accurate understanding of 3D environments, often leading to\nexecution failures. In this paper, we introduce GravMAD, a sub-goal-driven,\nlanguage-conditioned action diffusion framework that combines the strengths of\nimitation learning and foundation models. Our approach breaks tasks into\nsub-goals based on language instructions, allowing auxiliary guidance during\nboth training and inference. During training, we introduce Sub-goal Keypose\nDiscovery to identify key sub-goals from demonstrations. Inference differs from\ntraining, as there are no demonstrations available, so we use pre-trained\nfoundation models to bridge the gap and identify sub-goals for the current\ntask. In both phases, GravMaps are generated from sub-goals, providing GravMAD\nwith more flexible 3D spatial guidance compared to fixed 3D positions.\nEmpirical evaluations on RLBench show that GravMAD significantly outperforms\nstate-of-the-art methods, with a 28.63% improvement on novel tasks and a 13.36%\ngain on tasks encountered during training. Evaluations on real-world robotic\ntasks further show that GravMAD can reason about real-world tasks, associate\nthem with relevant visual information, and generalize to novel tasks. These\nresults demonstrate GravMAD's strong multi-task learning and generalization in\n3D manipulation. Video demonstrations are available at:\nhttps://gravmad.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots' ability to follow language instructions and execute diverse 3D\nmanipulation tasks is vital in robot learning. Traditional imitation\nlearning-based methods perform well on seen tasks but struggle with novel,\nunseen ones due to variability. Recent approaches leverage large foundation\nmodels to assist in understanding novel tasks, thereby mitigating this issue.\nHowever, these methods lack a task-specific learning process, which is\nessential for an accurate understanding of 3D environments, often leading to\nexecution failures. In this paper, we introduce GravMAD, a sub-goal-driven,\nlanguage-conditioned action diffusion framework that combines the strengths of\nimitation learning and foundation models. Our approach breaks tasks into\nsub-goals based on language instructions, allowing auxiliary guidance during\nboth training and inference. During training, we introduce Sub-goal Keypose\nDiscovery to identify key sub-goals from demonstrations. Inference differs from\ntraining, as there are no demonstrations available, so we use pre-trained\nfoundation models to bridge the gap and identify sub-goals for the current\ntask. In both phases, GravMaps are generated from sub-goals, providing GravMAD\nwith more flexible 3D spatial guidance compared to fixed 3D positions.\nEmpirical evaluations on RLBench show that GravMAD significantly outperforms\nstate-of-the-art methods, with a 28.63% improvement on novel tasks and a 13.36%\ngain on tasks encountered during training. Evaluations on real-world robotic\ntasks further show that GravMAD can reason about real-world tasks, associate\nthem with relevant visual information, and generalize to novel tasks. These\nresults demonstrate GravMAD's strong multi-task learning and generalization in\n3D manipulation. Video demonstrations are available at:\nhttps://gravmad.github.io."
                },
                "authors": [
                    {
                        "name": "Yangtao Chen"
                    },
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Junhui Yin"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Pinzhuo Tian"
                    },
                    {
                        "name": "Jieqi Shi"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "arxiv_comment": "ICLR 2025. The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20154v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20154v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08444v1",
                "updated": "2025-02-12T14:34:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    34,
                    33,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T14:34:33Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    34,
                    33,
                    2,
                    43,
                    0
                ],
                "title": "Analysis of the weak lensing mass-richness relation of redMaPPer\n  clusters in the LSST DESC DC2 simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of the weak lensing mass-richness relation of redMaPPer\n  clusters in the LSST DESC DC2 simulations"
                },
                "summary": "Cluster scaling relations are key ingredients in cluster abundance-based\ncosmological studies. In optical cluster cosmology, weak gravitational lensing\nhas proven to be a powerful tool to constrain the cluster mass-richness\nrelation. This work is conducted as part of the Dark Energy Science\nCollaboration (DESC), which aims to analyze the Legacy Survey of Space and Time\n(LSST) of Vera C. Rubin Observatory, starting in 2026. Weak lensing-inferred\ncluster properties, such as mass, suffer from several sources of bias. In this\npaper, we aim to test the impact of modeling choices and observational\nsystematics in cluster lensing on the inference of the mass-richness relation.\nWe constrain the mass-richness relation of 3,600 clusters detected by the\nredMaPPer algorithm in the cosmoDC2 extra-galactic mock catalog (covering $440$\ndeg$^2$) of the LSST DESC DC2 simulation, using number count measurements and\nstacked weak lensing profiles in several intervals of richness ($20 \\leq\n\\lambda \\leq 200$) and redshift ($0.2 \\leq z \\leq 1$). By modeling the mean of\nthe scaling relation as $\\langle \\ln \\lambda|M_{\\rm 200c}, z\\rangle =\n\\ln\\lambda_0 + \\mu_z\\log[(1+z)/(1+0.5)] + \\mu_m[\\log_{10}(M_{\\rm 200c}) -\n14.3]$, our baseline constraints are $\\ln\\lambda_0 = 3.37\\pm 0.03$, $\\mu_z =\n0.08\\pm 0.07$ and $\\mu_m = 2.18 \\pm 0.07$. We have found that, for a LSST-like\nsource galaxy density, our constraints are robust to a change in\nconcentration-mass relation and dark matter density profile modeling choices,\nwhen source redshifts and shapes are perfectly known. We have found that\nphotometric redshift uncertainties can introduce bias at the $1\\sigma$ level,\nwhich can be mitigated by an overall correcting factor, fitted jointly with\nscaling parameters. We find that including positive shear-richness covariance\nin the fit shifts the results by up to 0.5$\\sigma$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster scaling relations are key ingredients in cluster abundance-based\ncosmological studies. In optical cluster cosmology, weak gravitational lensing\nhas proven to be a powerful tool to constrain the cluster mass-richness\nrelation. This work is conducted as part of the Dark Energy Science\nCollaboration (DESC), which aims to analyze the Legacy Survey of Space and Time\n(LSST) of Vera C. Rubin Observatory, starting in 2026. Weak lensing-inferred\ncluster properties, such as mass, suffer from several sources of bias. In this\npaper, we aim to test the impact of modeling choices and observational\nsystematics in cluster lensing on the inference of the mass-richness relation.\nWe constrain the mass-richness relation of 3,600 clusters detected by the\nredMaPPer algorithm in the cosmoDC2 extra-galactic mock catalog (covering $440$\ndeg$^2$) of the LSST DESC DC2 simulation, using number count measurements and\nstacked weak lensing profiles in several intervals of richness ($20 \\leq\n\\lambda \\leq 200$) and redshift ($0.2 \\leq z \\leq 1$). By modeling the mean of\nthe scaling relation as $\\langle \\ln \\lambda|M_{\\rm 200c}, z\\rangle =\n\\ln\\lambda_0 + \\mu_z\\log[(1+z)/(1+0.5)] + \\mu_m[\\log_{10}(M_{\\rm 200c}) -\n14.3]$, our baseline constraints are $\\ln\\lambda_0 = 3.37\\pm 0.03$, $\\mu_z =\n0.08\\pm 0.07$ and $\\mu_m = 2.18 \\pm 0.07$. We have found that, for a LSST-like\nsource galaxy density, our constraints are robust to a change in\nconcentration-mass relation and dark matter density profile modeling choices,\nwhen source redshifts and shapes are perfectly known. We have found that\nphotometric redshift uncertainties can introduce bias at the $1\\sigma$ level,\nwhich can be mitigated by an overall correcting factor, fitted jointly with\nscaling parameters. We find that including positive shear-richness covariance\nin the fit shifts the results by up to 0.5$\\sigma$."
                },
                "authors": [
                    {
                        "name": "Constantin Payerne"
                    },
                    {
                        "name": "Zhuowen Zhang"
                    },
                    {
                        "name": "Michel Aguena"
                    },
                    {
                        "name": "Céline Combet"
                    },
                    {
                        "name": "Thibault Guillemin"
                    },
                    {
                        "name": "Marina Ricci"
                    },
                    {
                        "name": "Nathan Amouroux"
                    },
                    {
                        "name": "Camille Avestruz"
                    },
                    {
                        "name": "Eduardo J. Barroso"
                    },
                    {
                        "name": "Arya Farahi"
                    },
                    {
                        "name": "Eve Kovacs"
                    },
                    {
                        "name": "Calum Murray"
                    },
                    {
                        "name": "Markus M. Rau"
                    },
                    {
                        "name": "Eli S. Rykoff"
                    },
                    {
                        "name": "Samuel J. Schmidt"
                    },
                    {
                        "name": "the LSST Dark Energy Science Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "the LSST Dark Energy Science Collaboration"
                },
                "author": "the LSST Dark Energy Science Collaboration",
                "arxiv_comment": "26 pages, 15 figures, 3 tables, submitted to A&A (abstract shortened\n  for arXiv)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08443v1",
                "updated": "2025-02-12T14:34:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    34,
                    4,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T14:34:04Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    34,
                    4,
                    2,
                    43,
                    0
                ],
                "title": "Tutorial for Surrogate Endpoint Validation Using Joint modeling and\n  Mediation Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tutorial for Surrogate Endpoint Validation Using Joint modeling and\n  Mediation Analysis"
                },
                "summary": "The use of valid surrogate endpoints is an important stake in clinical\nresearch to help reduce both the duration and cost of a clinical trial and\nspeed up the evaluation of interesting treatments. Several methods have been\nproposed in the statistical literature to validate putative surrogate\nendpoints. Two main approaches have been proposed: the meta-analytic approach\nand the mediation analysis approach. The former uses data from meta-analyses to\nderive associations measures between the surrogate and the final endpoint at\nthe individual and trial levels. The latter rather uses the proportion of the\ntreatment effect on the final endpoint through the surrogate as a measure of\nsurrogacy in a causal inference framework. Both approaches have remained\nseparated as the meta-analytic approach does not estimate the treatment effect\non the final endpoint through the surrogate while the mediation analysis\napproach have been limited to single-trial setting. However, these two\napproaches are complementary. In this work we propose an approach that combines\nthe meta-analytic and mediation analysis approaches using joint modeling for\nsurrogate validation. We focus on the cases where the final endpoint is a\ntime-to-event endpoint (such as time-to-death) and the surrogate is either a\ntime-to-event or a longitudinal biomarker. Two new joint models were proposed\ndepending on the nature of the surrogate. These model are implemented in the R\npackage frailtypack. We illustrate the developed approaches in three\napplications on real datasets in oncology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of valid surrogate endpoints is an important stake in clinical\nresearch to help reduce both the duration and cost of a clinical trial and\nspeed up the evaluation of interesting treatments. Several methods have been\nproposed in the statistical literature to validate putative surrogate\nendpoints. Two main approaches have been proposed: the meta-analytic approach\nand the mediation analysis approach. The former uses data from meta-analyses to\nderive associations measures between the surrogate and the final endpoint at\nthe individual and trial levels. The latter rather uses the proportion of the\ntreatment effect on the final endpoint through the surrogate as a measure of\nsurrogacy in a causal inference framework. Both approaches have remained\nseparated as the meta-analytic approach does not estimate the treatment effect\non the final endpoint through the surrogate while the mediation analysis\napproach have been limited to single-trial setting. However, these two\napproaches are complementary. In this work we propose an approach that combines\nthe meta-analytic and mediation analysis approaches using joint modeling for\nsurrogate validation. We focus on the cases where the final endpoint is a\ntime-to-event endpoint (such as time-to-death) and the surrogate is either a\ntime-to-event or a longitudinal biomarker. Two new joint models were proposed\ndepending on the nature of the surrogate. These model are implemented in the R\npackage frailtypack. We illustrate the developed approaches in three\napplications on real datasets in oncology."
                },
                "authors": [
                    {
                        "name": "Quentin Le Coent"
                    },
                    {
                        "name": "Virginie Rondeau"
                    },
                    {
                        "name": "Catherine Legrand"
                    }
                ],
                "author_detail": {
                    "name": "Catherine Legrand"
                },
                "author": "Catherine Legrand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08441v1",
                "updated": "2025-02-12T14:32:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    17,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T14:32:17Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    17,
                    2,
                    43,
                    0
                ],
                "title": "Better Embeddings with Coupled Adam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Embeddings with Coupled Adam"
                },
                "summary": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets."
                },
                "authors": [
                    {
                        "name": "Felix Stollenwerk"
                    },
                    {
                        "name": "Tobias Stollenwerk"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Stollenwerk"
                },
                "author": "Tobias Stollenwerk",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08440v1",
                "updated": "2025-02-12T14:30:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    30,
                    57,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T14:30:57Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    30,
                    57,
                    2,
                    43,
                    0
                ],
                "title": "Scenario analysis with multivariate Bayesian machine learning models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scenario analysis with multivariate Bayesian machine learning models"
                },
                "summary": "We present an econometric framework that adapts tools for scenario analysis,\nsuch as conditional forecasts and generalized impulse response functions, for\nuse with dynamic nonparametric multivariate models. We demonstrate the utility\nof this approach through an exercise with simulated data, and three real-world\napplications: (i) scenario-based conditional forecasts aligned with Federal\nReserve Bank stress test assumptions, (ii) measuring macroeconomic risk under\nvarying financial conditions, and (iii) the asymmetric effects of US-based\nfinancial shocks and their international spillovers. Our results indicate the\nimportance of nonlinearities and asymmetries in the dynamic relationship\nbetween macroeconomic and financial variables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an econometric framework that adapts tools for scenario analysis,\nsuch as conditional forecasts and generalized impulse response functions, for\nuse with dynamic nonparametric multivariate models. We demonstrate the utility\nof this approach through an exercise with simulated data, and three real-world\napplications: (i) scenario-based conditional forecasts aligned with Federal\nReserve Bank stress test assumptions, (ii) measuring macroeconomic risk under\nvarying financial conditions, and (iii) the asymmetric effects of US-based\nfinancial shocks and their international spillovers. Our results indicate the\nimportance of nonlinearities and asymmetries in the dynamic relationship\nbetween macroeconomic and financial variables."
                },
                "authors": [
                    {
                        "name": "Michael Pfarrhofer"
                    },
                    {
                        "name": "Anna Stelzer"
                    }
                ],
                "author_detail": {
                    "name": "Anna Stelzer"
                },
                "author": "Anna Stelzer",
                "arxiv_comment": "Keywords: conditional forecast, generalized impulse response\n  function, Bayesian additive regression trees, nonlinearities,\n  (semi)-structural inference; JEL: C11, C32, C53, C54",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08436v1",
                "updated": "2025-02-12T14:20:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    20,
                    36,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T14:20:36Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    20,
                    36,
                    2,
                    43,
                    0
                ],
                "title": "From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification"
                },
                "summary": "We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference."
                },
                "authors": [
                    {
                        "name": "Nathan Vandemoortele"
                    },
                    {
                        "name": "Bram Steenwinckel"
                    },
                    {
                        "name": "Femke Ongenae"
                    },
                    {
                        "name": "Sofie Van Hoecke"
                    }
                ],
                "author_detail": {
                    "name": "Sofie Van Hoecke"
                },
                "author": "Sofie Van Hoecke",
                "arxiv_comment": "Under review at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01621v2",
                "updated": "2025-02-12T14:03:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    3,
                    19,
                    2,
                    43,
                    0
                ],
                "published": "2024-12-02T15:41:47Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    41,
                    47,
                    0,
                    337,
                    0
                ],
                "title": "NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Angel Yahir Loredo Lopez"
                    },
                    {
                        "name": "Tyler McDonald"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "5 pages (excluding references), Published at Coling 2025, Best\n  Dataset Paper Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08416v1",
                "updated": "2025-02-12T13:59:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    59,
                    22,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:59:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    59,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "Multifidelity Simulation-based Inference for Computationally Expensive\n  Simulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multifidelity Simulation-based Inference for Computationally Expensive\n  Simulators"
                },
                "summary": "Across many domains of science, stochastic models are an essential tool to\nunderstand the mechanisms underlying empirically observed data. Models can be\nof different levels of detail and accuracy, with models of high-fidelity (i.e.,\nhigh accuracy) to the phenomena under study being often preferable. However,\ninferring parameters of high-fidelity models via simulation-based inference is\nchallenging, especially when the simulator is computationally expensive. We\nintroduce MF-NPE, a multifidelity approach to neural posterior estimation that\nleverages inexpensive low-fidelity simulations to infer parameters of\nhigh-fidelity simulators within a limited simulation budget. MF-NPE performs\nneural posterior estimation with limited high-fidelity resources by virtue of\ntransfer learning, with the ability to prioritize individual observations using\nactive learning. On one statistical task with analytical ground-truth and two\nreal-world tasks, MF-NPE shows comparable performance to current approaches\nwhile requiring up to two orders of magnitude fewer high-fidelity simulations.\nOverall, MF-NPE opens new opportunities to perform efficient Bayesian inference\non computationally expensive simulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across many domains of science, stochastic models are an essential tool to\nunderstand the mechanisms underlying empirically observed data. Models can be\nof different levels of detail and accuracy, with models of high-fidelity (i.e.,\nhigh accuracy) to the phenomena under study being often preferable. However,\ninferring parameters of high-fidelity models via simulation-based inference is\nchallenging, especially when the simulator is computationally expensive. We\nintroduce MF-NPE, a multifidelity approach to neural posterior estimation that\nleverages inexpensive low-fidelity simulations to infer parameters of\nhigh-fidelity simulators within a limited simulation budget. MF-NPE performs\nneural posterior estimation with limited high-fidelity resources by virtue of\ntransfer learning, with the ability to prioritize individual observations using\nactive learning. On one statistical task with analytical ground-truth and two\nreal-world tasks, MF-NPE shows comparable performance to current approaches\nwhile requiring up to two orders of magnitude fewer high-fidelity simulations.\nOverall, MF-NPE opens new opportunities to perform efficient Bayesian inference\non computationally expensive simulators."
                },
                "authors": [
                    {
                        "name": "Anastasia N. Krouglova"
                    },
                    {
                        "name": "Hayden R. Johnson"
                    },
                    {
                        "name": "Basile Confavreux"
                    },
                    {
                        "name": "Michael Deistler"
                    },
                    {
                        "name": "Pedro J. Gonçalves"
                    }
                ],
                "author_detail": {
                    "name": "Pedro J. Gonçalves"
                },
                "author": "Pedro J. Gonçalves",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08415v1",
                "updated": "2025-02-12T13:58:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    58,
                    42,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    58,
                    42,
                    2,
                    43,
                    0
                ],
                "title": "A Semantic Parsing Algorithm to Solve Linear Ordering Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Semantic Parsing Algorithm to Solve Linear Ordering Problems"
                },
                "summary": "We develop an algorithm to semantically parse linear ordering problems, which\nrequire a model to arrange entities using deductive reasoning. Our method takes\nas input a number of premises and candidate statements, parsing them to a\nfirst-order logic of an ordering domain, and then utilizes constraint logic\nprogramming to infer the truth of proposed statements about the ordering.\n  Our semantic parser transforms Heim and Kratzer's syntax-based compositional\nformal semantic rules to a computational algorithm. This transformation\ninvolves introducing abstract types and templates based on their rules, and\nintroduces a dynamic component to interpret entities within a contextual\nframework.\n  Our symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to\nanswer multiple choice questions in BIG-bench's logical_deduction multiple\nchoice problems, achieving perfect accuracy, compared to 67.06% for the\nbest-performing LLM (GPT-4) and 87.63% for the hybrid system Logic-LM.\n  These promising results demonstrate the benefit of developing a semantic\nparsing algorithm driven by first-order logic constructs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop an algorithm to semantically parse linear ordering problems, which\nrequire a model to arrange entities using deductive reasoning. Our method takes\nas input a number of premises and candidate statements, parsing them to a\nfirst-order logic of an ordering domain, and then utilizes constraint logic\nprogramming to infer the truth of proposed statements about the ordering.\n  Our semantic parser transforms Heim and Kratzer's syntax-based compositional\nformal semantic rules to a computational algorithm. This transformation\ninvolves introducing abstract types and templates based on their rules, and\nintroduces a dynamic component to interpret entities within a contextual\nframework.\n  Our symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to\nanswer multiple choice questions in BIG-bench's logical_deduction multiple\nchoice problems, achieving perfect accuracy, compared to 67.06% for the\nbest-performing LLM (GPT-4) and 87.63% for the hybrid system Logic-LM.\n  These promising results demonstrate the benefit of developing a semantic\nparsing algorithm driven by first-order logic constructs."
                },
                "authors": [
                    {
                        "name": "Maha Alkhairy"
                    },
                    {
                        "name": "Vincent Homer"
                    },
                    {
                        "name": "Brendan O'Connor"
                    }
                ],
                "author_detail": {
                    "name": "Brendan O'Connor"
                },
                "author": "Brendan O'Connor",
                "arxiv_comment": "3 figures, 9 pages main paper and 6 pages references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05025v2",
                "updated": "2025-02-12T13:57:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    57,
                    4,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-07T15:55:27Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    15,
                    55,
                    27,
                    4,
                    38,
                    0
                ],
                "title": "Impact of radiative accelerations on the stellar characterization of\n  FGK-type stars using spectroscopic and seismic constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of radiative accelerations on the stellar characterization of\n  FGK-type stars using spectroscopic and seismic constraints"
                },
                "summary": "Chemical transport mechanisms are fundamental processes in stellar evolution\nmodels. They are responsible for the chemical distribution, and their impact\ndetermines how accurately we can characterize stars. Radiative accelerations\nare one of these processes. They allow the accumulation of elements at\ndifferent depths in the star. We aim to assess the impact of radiative\naccelerations on the modeling of FGK-type stars and their impact on the\nprediction of surface abundances. To reduce the cost of the computation of\nradiative accelerations, we implemented the single-valued parameters (SVP)\nmethod in the stellar evolution code MESA. The SVP method is more efficient in\ncalculating radiative accelerations, which enables computations of large enough\ngrids of models for stellar characterization. Compared to models that include\natomic diffusion (with only gravitational settling), the inclusion of radiative\naccelerations has a small effect on the inference of fundamental properties,\nwith an impact of 2\\%, 0.7\\%, and 5\\% for mass, radius, and age. However, the\ntreatment of radiative accelerations is necessary to predict the chemical\ncomposition of and accurately characterize stars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical transport mechanisms are fundamental processes in stellar evolution\nmodels. They are responsible for the chemical distribution, and their impact\ndetermines how accurately we can characterize stars. Radiative accelerations\nare one of these processes. They allow the accumulation of elements at\ndifferent depths in the star. We aim to assess the impact of radiative\naccelerations on the modeling of FGK-type stars and their impact on the\nprediction of surface abundances. To reduce the cost of the computation of\nradiative accelerations, we implemented the single-valued parameters (SVP)\nmethod in the stellar evolution code MESA. The SVP method is more efficient in\ncalculating radiative accelerations, which enables computations of large enough\ngrids of models for stellar characterization. Compared to models that include\natomic diffusion (with only gravitational settling), the inclusion of radiative\naccelerations has a small effect on the inference of fundamental properties,\nwith an impact of 2\\%, 0.7\\%, and 5\\% for mass, radius, and age. However, the\ntreatment of radiative accelerations is necessary to predict the chemical\ncomposition of and accurately characterize stars."
                },
                "authors": [
                    {
                        "name": "Nuno Moedas"
                    },
                    {
                        "name": "Morgan Deal"
                    },
                    {
                        "name": "Diego Bossini"
                    }
                ],
                "author_detail": {
                    "name": "Diego Bossini"
                },
                "author": "Diego Bossini",
                "arxiv_comment": "12 pafes,13 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19604v2",
                "updated": "2025-02-12T13:46:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    46,
                    7,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-30T14:53:07Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    14,
                    53,
                    7,
                    1,
                    121,
                    0
                ],
                "title": "X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image\n  Using Cross-Sectional Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image\n  Using Cross-Sectional Diffusion Models"
                },
                "summary": "Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but\nhigh-resolution scans are often slow and expensive due to extensive data\nacquisition requirements. Traditional MRI reconstruction methods aim to\nexpedite this process by filling in missing frequency components in the\nK-space, performing 3D-to-3D reconstructions that demand full 3D scans. In\ncontrast, we introduce X-Diffusion, a novel cross-sectional diffusion model\nthat reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain\ninputs, achieving 2D-to-3D reconstruction from as little as a single 2D MRI\nslice or few slices. A key aspect of X-Diffusion is that it models MRI data as\nholistic 3D volumes during the cross-sectional training and inference, unlike\nprevious learning approaches that treat MRI scans as collections of 2D slices\nin standard planes (coronal, axial, sagittal). We evaluated X-Diffusion on\nbrain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank\ndataset. Our results demonstrate that X-Diffusion not only surpasses\nstate-of-the-art methods in quantitative accuracy (PSNR) on unseen data but\nalso preserves critical anatomical features such as tumor profiles, spine\ncurvature, and brain volume. Remarkably, the model generalizes beyond the\ntraining domain, successfully reconstructing knee MRIs despite being trained\nexclusively on brain data. Medical expert evaluations further confirm the\nclinical relevance and fidelity of the generated images.To our knowledge,\nX-Diffusion is the first method capable of producing detailed 3D MRIs from\nhighly limited 2D input data, potentially accelerating MRI acquisition and\nreducing associated costs. The code is available on the project website\nhttps://emmanuelleb985.github.io/XDiffusion/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but\nhigh-resolution scans are often slow and expensive due to extensive data\nacquisition requirements. Traditional MRI reconstruction methods aim to\nexpedite this process by filling in missing frequency components in the\nK-space, performing 3D-to-3D reconstructions that demand full 3D scans. In\ncontrast, we introduce X-Diffusion, a novel cross-sectional diffusion model\nthat reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain\ninputs, achieving 2D-to-3D reconstruction from as little as a single 2D MRI\nslice or few slices. A key aspect of X-Diffusion is that it models MRI data as\nholistic 3D volumes during the cross-sectional training and inference, unlike\nprevious learning approaches that treat MRI scans as collections of 2D slices\nin standard planes (coronal, axial, sagittal). We evaluated X-Diffusion on\nbrain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank\ndataset. Our results demonstrate that X-Diffusion not only surpasses\nstate-of-the-art methods in quantitative accuracy (PSNR) on unseen data but\nalso preserves critical anatomical features such as tumor profiles, spine\ncurvature, and brain volume. Remarkably, the model generalizes beyond the\ntraining domain, successfully reconstructing knee MRIs despite being trained\nexclusively on brain data. Medical expert evaluations further confirm the\nclinical relevance and fidelity of the generated images.To our knowledge,\nX-Diffusion is the first method capable of producing detailed 3D MRIs from\nhighly limited 2D input data, potentially accelerating MRI acquisition and\nreducing associated costs. The code is available on the project website\nhttps://emmanuelleb985.github.io/XDiffusion/ ."
                },
                "authors": [
                    {
                        "name": "Emmanuelle Bourigault"
                    },
                    {
                        "name": "Abdullah Hamdi"
                    },
                    {
                        "name": "Amir Jamaludin"
                    }
                ],
                "author_detail": {
                    "name": "Amir Jamaludin"
                },
                "author": "Amir Jamaludin",
                "arxiv_comment": "preprint, project website:\n  https://emmanuelleb985.github.io/XDiffusion/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08395v1",
                "updated": "2025-02-12T13:37:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    37,
                    3,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:37:03Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    37,
                    3,
                    2,
                    43,
                    0
                ],
                "title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance"
                },
                "summary": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them."
                },
                "authors": [
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Musashi Hinck"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Kobi Hackenburg"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08707v2",
                "updated": "2025-02-12T13:29:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    29,
                    11,
                    2,
                    43,
                    0
                ],
                "published": "2024-08-16T12:40:01Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    40,
                    1,
                    4,
                    229,
                    0
                ],
                "title": "Beam Prediction based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beam Prediction based on Large Language Models"
                },
                "summary": "In this letter, we use large language models (LLMs) to develop a\nhigh-performing and robust beam prediction method. We formulate the millimeter\nwave (mmWave) beam prediction problem as a time series forecasting task, where\nthe historical observations are aggregated through cross-variable attention and\nthen transformed into text-based representations using a trainable tokenizer.\nBy leveraging the prompt-as-prefix (PaP) technique for contextual enrichment,\nour method harnesses the power of LLMs to predict future optimal beams.\nSimulation results demonstrate that our LLM-based approach outperforms\ntraditional learning-based models in prediction accuracy as well as robustness,\nhighlighting the significant potential of LLMs in enhancing wireless\ncommunication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we use large language models (LLMs) to develop a\nhigh-performing and robust beam prediction method. We formulate the millimeter\nwave (mmWave) beam prediction problem as a time series forecasting task, where\nthe historical observations are aggregated through cross-variable attention and\nthen transformed into text-based representations using a trainable tokenizer.\nBy leveraging the prompt-as-prefix (PaP) technique for contextual enrichment,\nour method harnesses the power of LLMs to predict future optimal beams.\nSimulation results demonstrate that our LLM-based approach outperforms\ntraditional learning-based models in prediction accuracy as well as robustness,\nhighlighting the significant potential of LLMs in enhancing wireless\ncommunication systems."
                },
                "authors": [
                    {
                        "name": "Yucheng Sheng"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Geoffrey Ye Li"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Ye Li"
                },
                "author": "Geoffrey Ye Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08391v1",
                "updated": "2025-02-12T13:28:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    28,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:28:46Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    28,
                    46,
                    2,
                    43,
                    0
                ],
                "title": "ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for\n  Whole Slide Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for\n  Whole Slide Image Classification"
                },
                "summary": "Multiple instance learning (MIL)-based framework has become the mainstream\nfor processing the whole slide image (WSI) with giga-pixel size and\nhierarchical image context in digital pathology. However, these methods heavily\ndepend on a substantial number of bag-level labels and solely learn from the\noriginal slides, which are easily affected by variations in data distribution.\nRecently, vision language model (VLM)-based methods introduced the language\nprior by pre-training on large-scale pathological image-text pairs. However,\nthe previous text prompt lacks the consideration of pathological prior\nknowledge, therefore does not substantially boost the model's performance.\nMoreover, the collection of such pairs and the pre-training process are very\ntime-consuming and source-intensive.To solve the above problems, we propose a\ndual-scale vision-language multiple instance learning (ViLa-MIL) framework for\nwhole slide image classification. Specifically, we propose a dual-scale visual\ndescriptive text prompt based on the frozen large language model (LLM) to boost\nthe performance of VLM effectively. To transfer the VLM to process WSI\nefficiently, for the image branch, we propose a prototype-guided patch decoder\nto aggregate the patch features progressively by grouping similar patches into\nthe same prototype; for the text branch, we introduce a context-guided text\ndecoder to enhance the text features by incorporating the multi-granular image\ncontexts. Extensive studies on three multi-cancer and multi-center subtyping\ndatasets demonstrate the superiority of ViLa-MIL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple instance learning (MIL)-based framework has become the mainstream\nfor processing the whole slide image (WSI) with giga-pixel size and\nhierarchical image context in digital pathology. However, these methods heavily\ndepend on a substantial number of bag-level labels and solely learn from the\noriginal slides, which are easily affected by variations in data distribution.\nRecently, vision language model (VLM)-based methods introduced the language\nprior by pre-training on large-scale pathological image-text pairs. However,\nthe previous text prompt lacks the consideration of pathological prior\nknowledge, therefore does not substantially boost the model's performance.\nMoreover, the collection of such pairs and the pre-training process are very\ntime-consuming and source-intensive.To solve the above problems, we propose a\ndual-scale vision-language multiple instance learning (ViLa-MIL) framework for\nwhole slide image classification. Specifically, we propose a dual-scale visual\ndescriptive text prompt based on the frozen large language model (LLM) to boost\nthe performance of VLM effectively. To transfer the VLM to process WSI\nefficiently, for the image branch, we propose a prototype-guided patch decoder\nto aggregate the patch features progressively by grouping similar patches into\nthe same prototype; for the text branch, we introduce a context-guided text\ndecoder to enhance the text features by incorporating the multi-granular image\ncontexts. Extensive studies on three multi-cancer and multi-center subtyping\ndatasets demonstrate the superiority of ViLa-MIL."
                },
                "authors": [
                    {
                        "name": "Jiangbo Shi"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Tieliang Gong"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Huazhu Fu"
                    }
                ],
                "author_detail": {
                    "name": "Huazhu Fu"
                },
                "author": "Huazhu Fu",
                "arxiv_comment": "CVPR 2024 (Updated version with corrections for typos and errors.)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06581v2",
                "updated": "2025-02-12T13:25:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    25,
                    22,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-10T15:48:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    48,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems"
                },
                "summary": "The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field."
                },
                "authors": [
                    {
                        "name": "Linxiao Gong"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Gaoyun Fang"
                    },
                    {
                        "name": "Bobo Ju"
                    },
                    {
                        "name": "Juncen Guo"
                    },
                    {
                        "name": "Xiaoguang Zhu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Azzedine Boukerche"
                    }
                ],
                "author_detail": {
                    "name": "Azzedine Boukerche"
                },
                "author": "Azzedine Boukerche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08385v1",
                "updated": "2025-02-12T13:18:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    18,
                    54,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:18:54Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    18,
                    54,
                    2,
                    43,
                    0
                ],
                "title": "Field-level inference of $H_0$ from simulated type Ia supernovae in a\n  local Universe analogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Field-level inference of $H_0$ from simulated type Ia supernovae in a\n  local Universe analogue"
                },
                "summary": "Two particular challenges face type Ia supernovae (SNeIa) as probes of the\nexpansion rate of the Universe. One is that they may not be fair tracers of the\nmatter velocity field, and the second is that their peculiar velocities distort\nthe Hubble expansion. Although the latter has been estimated at $\\lesssim1.5\\%$\nfor $z>0.023$, this is based either on constrained linear or unconstrained\nnon-linear velocity modelling. In this paper, we address both challenges by\nincorporating a physical model for the locations of supernovae, and develop a\nBayesian Hierarchical Model that accounts for non-linear peculiar velocities in\nour local Universe, inferred from a Bayesian analysis of the 2M++ spectroscopic\ngalaxy catalogue. With simulated data, the model recovers the ground truth\nvalue of the Hubble constant $H_0$ in the presence of peculiar velocities\nincluding their correlated uncertainties arising from the Bayesian inference,\nopening up the potential of including lower redshift SNeIa to measure $H_0$.\nIgnoring peculiar velocities, the inferred $H_0$ increases minimally by $\\sim\n0.4 \\pm 0.5$ km s$^{-1}$ Mpc$^{-1}$ in the range $0.023<z<0.046$. We conclude\nit is unlikely that the $H_0$ tension originates in unaccounted-for non-linear\nvelocity dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two particular challenges face type Ia supernovae (SNeIa) as probes of the\nexpansion rate of the Universe. One is that they may not be fair tracers of the\nmatter velocity field, and the second is that their peculiar velocities distort\nthe Hubble expansion. Although the latter has been estimated at $\\lesssim1.5\\%$\nfor $z>0.023$, this is based either on constrained linear or unconstrained\nnon-linear velocity modelling. In this paper, we address both challenges by\nincorporating a physical model for the locations of supernovae, and develop a\nBayesian Hierarchical Model that accounts for non-linear peculiar velocities in\nour local Universe, inferred from a Bayesian analysis of the 2M++ spectroscopic\ngalaxy catalogue. With simulated data, the model recovers the ground truth\nvalue of the Hubble constant $H_0$ in the presence of peculiar velocities\nincluding their correlated uncertainties arising from the Bayesian inference,\nopening up the potential of including lower redshift SNeIa to measure $H_0$.\nIgnoring peculiar velocities, the inferred $H_0$ increases minimally by $\\sim\n0.4 \\pm 0.5$ km s$^{-1}$ Mpc$^{-1}$ in the range $0.023<z<0.046$. We conclude\nit is unlikely that the $H_0$ tension originates in unaccounted-for non-linear\nvelocity dynamics."
                },
                "authors": [
                    {
                        "name": "Eleni Tsaprazi"
                    },
                    {
                        "name": "Alan F. Heavens"
                    }
                ],
                "author_detail": {
                    "name": "Alan F. Heavens"
                },
                "author": "Alan F. Heavens",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08381v1",
                "updated": "2025-02-12T13:16:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    16,
                    7,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:16:07Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    16,
                    7,
                    2,
                    43,
                    0
                ],
                "title": "The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and\n  Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and\n  Opportunities"
                },
                "summary": "The powerfulness of LLMs indicates that deploying various LLMs with different\nscales and architectures on end, edge, and cloud to satisfy different\nrequirements and adaptive heterogeneous hardware is the critical way to achieve\nubiquitous intelligence for 6G. However, the massive parameter scale of LLMs\nposes significant challenges in deploying them on edge devices due to high\ncomputational and storage demands. Considering that the sparse activation in\nMixture of Experts (MoE) is effective on scalable and dynamic allocation of\ncomputational and communications resources at the edge, this paper proposes a\nnovel MoE-empowered collaborative deployment framework for edge LLMs, denoted\nas CoEL. This framework fully leverages the properties of MoE architecture and\nencompasses four key aspects: Perception, Deployment, Compression, and\nUpdating. Edge servers broadcast their resource status and the specific\nresource requirements of LLMs to their neighbors. Then, utilizing this data,\ntwo sophisticated deployment strategies are proposed for satisfying varying\nmodel scales, ensuring that each model is deployed effectively. One for\ndeploying LLMs on a single edge device through intra-device resource\ncollaboration, and another for a distributed deployment across multiple edge\ndevices via inter-device resource collaboration. Furthermore, both the models\nand the intermediate data are compressed for reducing memory footprint by\nquantization and reducing the volume of intermediate data by token fusion and\npruning. Finally, given the dynamic of network topology, resource status, and\nuser requirements, the deployment strategies are regularly updated to maintain\nits relevance and effectiveness. This paper also delineates the challenges and\npotential research directions for the deployment of edge LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The powerfulness of LLMs indicates that deploying various LLMs with different\nscales and architectures on end, edge, and cloud to satisfy different\nrequirements and adaptive heterogeneous hardware is the critical way to achieve\nubiquitous intelligence for 6G. However, the massive parameter scale of LLMs\nposes significant challenges in deploying them on edge devices due to high\ncomputational and storage demands. Considering that the sparse activation in\nMixture of Experts (MoE) is effective on scalable and dynamic allocation of\ncomputational and communications resources at the edge, this paper proposes a\nnovel MoE-empowered collaborative deployment framework for edge LLMs, denoted\nas CoEL. This framework fully leverages the properties of MoE architecture and\nencompasses four key aspects: Perception, Deployment, Compression, and\nUpdating. Edge servers broadcast their resource status and the specific\nresource requirements of LLMs to their neighbors. Then, utilizing this data,\ntwo sophisticated deployment strategies are proposed for satisfying varying\nmodel scales, ensuring that each model is deployed effectively. One for\ndeploying LLMs on a single edge device through intra-device resource\ncollaboration, and another for a distributed deployment across multiple edge\ndevices via inter-device resource collaboration. Furthermore, both the models\nand the intermediate data are compressed for reducing memory footprint by\nquantization and reducing the volume of intermediate data by token fusion and\npruning. Finally, given the dynamic of network topology, resource status, and\nuser requirements, the deployment strategies are regularly updated to maintain\nits relevance and effectiveness. This paper also delineates the challenges and\npotential research directions for the deployment of edge LLMs."
                },
                "authors": [
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Tuo Zhang"
                    },
                    {
                        "name": "Muqing Li"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Qihua Zhou"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang",
                "arxiv_comment": "7pages, 1 table, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01692v2",
                "updated": "2025-02-12T13:03:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    3,
                    9,
                    2,
                    43,
                    0
                ],
                "published": "2024-10-02T16:03:49Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    3,
                    49,
                    2,
                    276,
                    0
                ],
                "title": "U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where model performance stagnates at first and then\nimproves sharply and unpredictably with scale beyond a threshold. In this work,\nwe investigate the phenomenon by grouping questions based on difficulty level\nand provide a possible explanation for emergent abilities. Specifically, we\nobserve U-shaped scaling for hard questions and inverted-U scaling followed by\nsteady improvement for easy questions. The two scaling patterns initially\noffset each other, causing stagnant overall performance. The performance starts\nto soar when the scaling pattern of easy questions reverts from inverse to\nstandard scaling, leading to emergent abilities. Based on this finding, we\npropose a simple yet effective pipeline, called Slice-and-Sandwich, to predict\nthe emergence threshold and model performance beyond the threshold. Our code is\npublicly available at https://github.com/tony10101105/ExpEmergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where model performance stagnates at first and then\nimproves sharply and unpredictably with scale beyond a threshold. In this work,\nwe investigate the phenomenon by grouping questions based on difficulty level\nand provide a possible explanation for emergent abilities. Specifically, we\nobserve U-shaped scaling for hard questions and inverted-U scaling followed by\nsteady improvement for easy questions. The two scaling patterns initially\noffset each other, causing stagnant overall performance. The performance starts\nto soar when the scaling pattern of easy questions reverts from inverse to\nstandard scaling, leading to emergent abilities. Based on this finding, we\npropose a simple yet effective pipeline, called Slice-and-Sandwich, to predict\nthe emergence threshold and model performance beyond the threshold. Our code is\npublicly available at https://github.com/tony10101105/ExpEmergence."
                },
                "authors": [
                    {
                        "name": "Tung-Yu Wu"
                    },
                    {
                        "name": "Pei-Yu Lo"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Yu Lo"
                },
                "author": "Pei-Yu Lo",
                "arxiv_comment": "accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18652v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18652v7",
                "updated": "2025-02-12T12:49:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    49,
                    36,
                    2,
                    43,
                    0
                ],
                "published": "2024-10-24T11:32:00Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    32,
                    0,
                    3,
                    298,
                    0
                ],
                "title": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation"
                },
                "summary": "Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples."
                },
                "authors": [
                    {
                        "name": "Woosung Koh"
                    },
                    {
                        "name": "Jang Han Yoon"
                    },
                    {
                        "name": "MinHyung Lee"
                    },
                    {
                        "name": "Youngjin Song"
                    },
                    {
                        "name": "Jaegwan Cho"
                    },
                    {
                        "name": "Jaehyun Kang"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Se-Young Yun"
                    },
                    {
                        "name": "Youngjae Yu"
                    },
                    {
                        "name": "Bongshin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Bongshin Lee"
                },
                "author": "Bongshin Lee",
                "arxiv_comment": "NAACL 2025 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18652v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18652v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08356v1",
                "updated": "2025-02-12T12:39:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    39,
                    51,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:39:51Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    39,
                    51,
                    2,
                    43,
                    0
                ],
                "title": "Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Kushagra Bhushan"
                    },
                    {
                        "name": "Yatin Nandwani"
                    },
                    {
                        "name": "Dinesh Khandelwal"
                    },
                    {
                        "name": "Sonam Gupta"
                    },
                    {
                        "name": "Gaurav Pandey"
                    },
                    {
                        "name": "Dinesh Raghu"
                    },
                    {
                        "name": "Sachindra Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Sachindra Joshi"
                },
                "author": "Sachindra Joshi",
                "arxiv_comment": "22 pages, 14 tables, to be published in NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08353v1",
                "updated": "2025-02-12T12:28:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    28,
                    39,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:28:39Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    28,
                    39,
                    2,
                    43,
                    0
                ],
                "title": "Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy"
                },
                "summary": "With the extensive application of Graph Neural Networks (GNNs) across various\ndomains, their trustworthiness has emerged as a focal point of research. Some\nexisting studies have shown that the integration of large language models\n(LLMs) can improve the semantic understanding and generation capabilities of\nGNNs, which in turn improves the trustworthiness of GNNs from various aspects.\nOur review introduces a taxonomy that offers researchers a clear framework for\ncomprehending the principles and applications of different methods and helps\nclarify the connections and differences among various approaches. Then we\nsystematically survey representative approaches along the four categories of\nour taxonomy. Through our taxonomy, researchers can understand the applicable\nscenarios, potential advantages, and limitations of each approach for the the\ntrusted integration of GNNs with LLMs. Finally, we present some promising\ndirections of work and future trends for the integration of LLMs and GNNs to\nimprove model trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the extensive application of Graph Neural Networks (GNNs) across various\ndomains, their trustworthiness has emerged as a focal point of research. Some\nexisting studies have shown that the integration of large language models\n(LLMs) can improve the semantic understanding and generation capabilities of\nGNNs, which in turn improves the trustworthiness of GNNs from various aspects.\nOur review introduces a taxonomy that offers researchers a clear framework for\ncomprehending the principles and applications of different methods and helps\nclarify the connections and differences among various approaches. Then we\nsystematically survey representative approaches along the four categories of\nour taxonomy. Through our taxonomy, researchers can understand the applicable\nscenarios, potential advantages, and limitations of each approach for the the\ntrusted integration of GNNs with LLMs. Finally, we present some promising\ndirections of work and future trends for the integration of LLMs and GNNs to\nimprove model trustworthiness."
                },
                "authors": [
                    {
                        "name": "Ruizhan Xue"
                    },
                    {
                        "name": "Huimin Deng"
                    },
                    {
                        "name": "Fang He"
                    },
                    {
                        "name": "Maojun Wang"
                    },
                    {
                        "name": "Zeyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zeyu Zhang"
                },
                "author": "Zeyu Zhang",
                "arxiv_comment": "Submitted to IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08346v1",
                "updated": "2025-02-12T12:13:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    13,
                    51,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:13:51Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    13,
                    51,
                    2,
                    43,
                    0
                ],
                "title": "Graph Foundation Models for Recommendation: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Foundation Models for Recommendation: A Comprehensive Survey"
                },
                "summary": "Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems."
                },
                "authors": [
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Yihang Wang"
                    },
                    {
                        "name": "Yuanhao Zeng"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiashu Zhao"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yawen Li"
                    },
                    {
                        "name": "Long Xia"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08332v1",
                "updated": "2025-02-12T11:56:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    56,
                    40,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:56:40Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    56,
                    40,
                    2,
                    43,
                    0
                ],
                "title": "Modification and Generated-Text Detection: Achieving Dual Detection\n  Capabilities for the Outputs of LLM by Watermark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modification and Generated-Text Detection: Achieving Dual Detection\n  Capabilities for the Outputs of LLM by Watermark"
                },
                "summary": "The development of large language models (LLMs) has raised concerns about\npotential misuse. One practical solution is to embed a watermark in the text,\nallowing ownership verification through watermark extraction. Existing methods\nprimarily focus on defending against modification attacks, often neglecting\nother spoofing attacks. For example, attackers can alter the watermarked text\nto produce harmful content without compromising the presence of the watermark,\nwhich could lead to false attribution of this malicious content to the LLM.\nThis situation poses a serious threat to the LLMs service providers and\nhighlights the significance of achieving modification detection and\ngenerated-text detection simultaneously. Therefore, we propose a technique to\ndetect modifications in text for unbiased watermark which is sensitive to\nmodification. We introduce a new metric called ``discarded tokens\", which\nmeasures the number of tokens not included in watermark detection. When a\nmodification occurs, this metric changes and can serve as evidence of the\nmodification. Additionally, we improve the watermark detection process and\nintroduce a novel method for unbiased watermark. Our experiments demonstrate\nthat we can achieve effective dual detection capabilities: modification\ndetection and generated-text detection by watermark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has raised concerns about\npotential misuse. One practical solution is to embed a watermark in the text,\nallowing ownership verification through watermark extraction. Existing methods\nprimarily focus on defending against modification attacks, often neglecting\nother spoofing attacks. For example, attackers can alter the watermarked text\nto produce harmful content without compromising the presence of the watermark,\nwhich could lead to false attribution of this malicious content to the LLM.\nThis situation poses a serious threat to the LLMs service providers and\nhighlights the significance of achieving modification detection and\ngenerated-text detection simultaneously. Therefore, we propose a technique to\ndetect modifications in text for unbiased watermark which is sensitive to\nmodification. We introduce a new metric called ``discarded tokens\", which\nmeasures the number of tokens not included in watermark detection. When a\nmodification occurs, this metric changes and can serve as evidence of the\nmodification. Additionally, we improve the watermark detection process and\nintroduce a novel method for unbiased watermark. Our experiments demonstrate\nthat we can achieve effective dual detection capabilities: modification\ndetection and generated-text detection by watermark."
                },
                "authors": [
                    {
                        "name": "Yuhang Cai"
                    },
                    {
                        "name": "Yaofei Wang"
                    },
                    {
                        "name": "Donghui Hu"
                    },
                    {
                        "name": "Gu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Gu Chen"
                },
                "author": "Gu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08323v1",
                "updated": "2025-02-12T11:44:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    44,
                    19,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:44:19Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    44,
                    19,
                    2,
                    43,
                    0
                ],
                "title": "Contextual Compression Encoding for Large Language Models: A Novel\n  Framework for Multi-Layered Parameter Space Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Compression Encoding for Large Language Models: A Novel\n  Framework for Multi-Layered Parameter Space Pruning"
                },
                "summary": "Context-aware compression techniques have gained increasing attention as\nmodel sizes continue to grow, introducing computational bottlenecks that hinder\nefficient deployment. A structured encoding approach was proposed to\nselectively eliminate redundant parameter groups while ensuring that\nrepresentational fidelity was preserved across multiple layers. Contextual\nCompression Encoding (CCE) introduced a multi-stage encoding mechanism that\ndynamically restructured parameter distributions, allowing for significant\nreductions in memory footprint and computational complexity. Experimental\nevaluations demonstrated that models compressed through CCE retained linguistic\nexpressivity and coherence, maintaining accuracy across a range of text\ngeneration and classification tasks. Layer-wise analysis revealed that\nmiddle-network layers exhibited higher compression ratios, aligning with the\nobservation that self-attention and feed-forward transformations contained\nredundancies that could be reorganized without impairing functional capacity.\nComparisons against conventional quantization and pruning methods confirmed\nthat CCE provided a more balanced trade-off between efficiency and model\nretention, achieving reductions in energy consumption and inference latency\nwithout requiring extensive retraining. Computational efficiency improvements\nwere particularly evident in deployment scenarios involving\nresource-constrained environments, where reductions in memory usage enabled\nmore scalable implementations. Further analyses of internal network behavior\nshowed that compressed models exhibited stable activation distributions and\nadapted dynamically to input variations, reinforcing the viability of\nstructured compression strategies for optimizing large-scale architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-aware compression techniques have gained increasing attention as\nmodel sizes continue to grow, introducing computational bottlenecks that hinder\nefficient deployment. A structured encoding approach was proposed to\nselectively eliminate redundant parameter groups while ensuring that\nrepresentational fidelity was preserved across multiple layers. Contextual\nCompression Encoding (CCE) introduced a multi-stage encoding mechanism that\ndynamically restructured parameter distributions, allowing for significant\nreductions in memory footprint and computational complexity. Experimental\nevaluations demonstrated that models compressed through CCE retained linguistic\nexpressivity and coherence, maintaining accuracy across a range of text\ngeneration and classification tasks. Layer-wise analysis revealed that\nmiddle-network layers exhibited higher compression ratios, aligning with the\nobservation that self-attention and feed-forward transformations contained\nredundancies that could be reorganized without impairing functional capacity.\nComparisons against conventional quantization and pruning methods confirmed\nthat CCE provided a more balanced trade-off between efficiency and model\nretention, achieving reductions in energy consumption and inference latency\nwithout requiring extensive retraining. Computational efficiency improvements\nwere particularly evident in deployment scenarios involving\nresource-constrained environments, where reductions in memory usage enabled\nmore scalable implementations. Further analyses of internal network behavior\nshowed that compressed models exhibited stable activation distributions and\nadapted dynamically to input variations, reinforcing the viability of\nstructured compression strategies for optimizing large-scale architectures."
                },
                "authors": [
                    {
                        "name": "Barnaby Schmitt"
                    },
                    {
                        "name": "Alistair Grosvenor"
                    },
                    {
                        "name": "Matthias Cunningham"
                    },
                    {
                        "name": "Clementine Walsh"
                    },
                    {
                        "name": "Julius Pembrokeshire"
                    },
                    {
                        "name": "Jonathan Teel"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Teel"
                },
                "author": "Jonathan Teel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10827v3",
                "updated": "2025-02-12T11:41:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    41,
                    16,
                    2,
                    43,
                    0
                ],
                "published": "2024-12-14T13:12:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    13,
                    12,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "Rethinking Chain-of-Thought from the Perspective of Self-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Chain-of-Thought from the Perspective of Self-Training"
                },
                "summary": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Zongqian Wu"
                    },
                    {
                        "name": "Baoduo Xu"
                    },
                    {
                        "name": "Ruochen Cui"
                    },
                    {
                        "name": "Mengmeng Zhan"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Lei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Lei Feng"
                },
                "author": "Lei Feng",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08319v1",
                "updated": "2025-02-12T11:35:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    35,
                    20,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:35:20Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    35,
                    20,
                    2,
                    43,
                    0
                ],
                "title": "MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and\n  Emotion Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and\n  Emotion Detection"
                },
                "summary": "Propaganda is a form of persuasion that has been used throughout history with\nthe intention goal of influencing people's opinions through rhetorical and\npsychological persuasion techniques for determined ends. Although Arabic ranked\nas the fourth most- used language on the internet, resources for propaganda\ndetection in languages other than English, especially Arabic, remain extremely\nlimited. To address this gap, the first Arabic dataset for Multi-label\nPropaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE\nis an open-source extension of the existing Arabic propaganda dataset, ArPro,\nwith the addition of sentiment and emotion annotations for each text. This\ndataset comprises 8,000 annotated news articles, which is the largest\npropaganda dataset to date. For each task, several baselines have been\ndeveloped using large language models (LLMs), such as GPT-4o-mini, and\npre-trained language models (PLMs), including three BERT-based models. The\ndataset, annotation guidelines, and source code are all publicly released to\nfacilitate future research and development in Arabic language models and\ncontribute to a deeper understanding of how various opinion dimensions interact\nin news media1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propaganda is a form of persuasion that has been used throughout history with\nthe intention goal of influencing people's opinions through rhetorical and\npsychological persuasion techniques for determined ends. Although Arabic ranked\nas the fourth most- used language on the internet, resources for propaganda\ndetection in languages other than English, especially Arabic, remain extremely\nlimited. To address this gap, the first Arabic dataset for Multi-label\nPropaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE\nis an open-source extension of the existing Arabic propaganda dataset, ArPro,\nwith the addition of sentiment and emotion annotations for each text. This\ndataset comprises 8,000 annotated news articles, which is the largest\npropaganda dataset to date. For each task, several baselines have been\ndeveloped using large language models (LLMs), such as GPT-4o-mini, and\npre-trained language models (PLMs), including three BERT-based models. The\ndataset, annotation guidelines, and source code are all publicly released to\nfacilitate future research and development in Arabic language models and\ncontribute to a deeper understanding of how various opinion dimensions interact\nin news media1."
                },
                "authors": [
                    {
                        "name": "Lubna Al-Henaki"
                    },
                    {
                        "name": "Hend Al-Khalifa"
                    },
                    {
                        "name": "Abdulmalik Al-Salman"
                    },
                    {
                        "name": "Hajar Alqubayshi"
                    },
                    {
                        "name": "Hind Al-Twailay"
                    },
                    {
                        "name": "Gheeda Alghamdi"
                    },
                    {
                        "name": "Hawra Aljasim"
                    }
                ],
                "author_detail": {
                    "name": "Hawra Aljasim"
                },
                "author": "Hawra Aljasim",
                "arxiv_comment": "12 pages, 3 figuers, 4 tabels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08312v1",
                "updated": "2025-02-12T11:30:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    30,
                    28,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:30:28Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    30,
                    28,
                    2,
                    43,
                    0
                ],
                "title": "Word Synchronization Challenge: A Benchmark for Word Association\n  Responses for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word Synchronization Challenge: A Benchmark for Word Association\n  Responses for LLMs"
                },
                "summary": "This paper introduces the Word Synchronization Challenge, a novel benchmark\nto evaluate large language models (LLMs) in Human-Computer Interaction (HCI).\nThis benchmark uses a dynamic game-like framework to test LLMs ability to mimic\nhuman cognitive processes through word associations. By simulating complex\nhuman interactions, it assesses how LLMs interpret and align with human thought\npatterns during conversational exchanges, which are essential for effective\nsocial partnerships in HCI. Initial findings highlight the influence of model\nsophistication on performance, offering insights into the models capabilities\nto engage in meaningful social interactions and adapt behaviors in human-like\nways. This research advances the understanding of LLMs potential to replicate\nor diverge from human cognitive functions, paving the way for more nuanced and\nempathetic human-machine collaborations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Word Synchronization Challenge, a novel benchmark\nto evaluate large language models (LLMs) in Human-Computer Interaction (HCI).\nThis benchmark uses a dynamic game-like framework to test LLMs ability to mimic\nhuman cognitive processes through word associations. By simulating complex\nhuman interactions, it assesses how LLMs interpret and align with human thought\npatterns during conversational exchanges, which are essential for effective\nsocial partnerships in HCI. Initial findings highlight the influence of model\nsophistication on performance, offering insights into the models capabilities\nto engage in meaningful social interactions and adapt behaviors in human-like\nways. This research advances the understanding of LLMs potential to replicate\nor diverge from human cognitive functions, paving the way for more nuanced and\nempathetic human-machine collaborations."
                },
                "authors": [
                    {
                        "name": "Tanguy Cazalets"
                    },
                    {
                        "name": "Joni Dambre"
                    }
                ],
                "author_detail": {
                    "name": "Joni Dambre"
                },
                "author": "Joni Dambre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08311v1",
                "updated": "2025-02-12T11:28:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    28,
                    22,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:28:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    28,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "Inference in dynamic models for panel data using the moving block\n  bootstrap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in dynamic models for panel data using the moving block\n  bootstrap"
                },
                "summary": "Inference in linear panel data models is complicated by the presence of fixed\neffects when (some of) the regressors are not strictly exogenous. Under\nasymptotics where the number of cross-sectional observations and time periods\ngrow at the same rate, the within-group estimator is consistent but its limit\ndistribution features a bias term. In this paper we show that a panel version\nof the moving block bootstrap, where blocks of adjacent cross-sections are\nresampled with replacement, replicates the limit distribution of the\nwithin-group estimator. Confidence ellipsoids and hypothesis tests based on the\nreverse-percentile bootstrap are thus asymptotically valid without the need to\ntake the presence of bias into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in linear panel data models is complicated by the presence of fixed\neffects when (some of) the regressors are not strictly exogenous. Under\nasymptotics where the number of cross-sectional observations and time periods\ngrow at the same rate, the within-group estimator is consistent but its limit\ndistribution features a bias term. In this paper we show that a panel version\nof the moving block bootstrap, where blocks of adjacent cross-sections are\nresampled with replacement, replicates the limit distribution of the\nwithin-group estimator. Confidence ellipsoids and hypothesis tests based on the\nreverse-percentile bootstrap are thus asymptotically valid without the need to\ntake the presence of bias into account."
                },
                "authors": [
                    {
                        "name": "Ayden Higgins"
                    },
                    {
                        "name": "Koen Jochmans"
                    }
                ],
                "author_detail": {
                    "name": "Koen Jochmans"
                },
                "author": "Koen Jochmans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08309v1",
                "updated": "2025-02-12T11:23:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    23,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:23:46Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    23,
                    46,
                    2,
                    43,
                    0
                ],
                "title": "Unlocking Scaling Law in Industrial Recommendation Systems with a\n  Three-step Paradigm based Large User Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Scaling Law in Industrial Recommendation Systems with a\n  Three-step Paradigm based Large User Model"
                },
                "summary": "Recent advancements in autoregressive Large Language Models (LLMs) have\nachieved significant milestones, largely attributed to their scalability, often\nreferred to as the \"scaling law\". Inspired by these achievements, there has\nbeen a growing interest in adapting LLMs for Recommendation Systems (RecSys) by\nreformulating RecSys tasks into generative problems. However, these End-to-End\nGenerative Recommendation (E2E-GR) methods tend to prioritize idealized goals,\noften at the expense of the practical advantages offered by traditional Deep\nLearning based Recommendation Models (DLRMs) in terms of in features,\narchitecture, and practices. This disparity between idealized goals and\npractical needs introduces several challenges and limitations, locking the\nscaling law in industrial RecSys. In this paper, we introduce a large user\nmodel (LUM) that addresses these limitations through a three-step paradigm,\ndesigned to meet the stringent requirements of industrial settings while\nunlocking the potential for scalable recommendations. Our extensive\nexperimental evaluations demonstrate that LUM outperforms both state-of-the-art\nDLRMs and E2E-GR approaches. Notably, LUM exhibits excellent scalability, with\nperformance improvements observed as the model scales up to 7 billion\nparameters. Additionally, we have successfully deployed LUM in an industrial\napplication, where it achieved significant gains in an A/B test, further\nvalidating its effectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in autoregressive Large Language Models (LLMs) have\nachieved significant milestones, largely attributed to their scalability, often\nreferred to as the \"scaling law\". Inspired by these achievements, there has\nbeen a growing interest in adapting LLMs for Recommendation Systems (RecSys) by\nreformulating RecSys tasks into generative problems. However, these End-to-End\nGenerative Recommendation (E2E-GR) methods tend to prioritize idealized goals,\noften at the expense of the practical advantages offered by traditional Deep\nLearning based Recommendation Models (DLRMs) in terms of in features,\narchitecture, and practices. This disparity between idealized goals and\npractical needs introduces several challenges and limitations, locking the\nscaling law in industrial RecSys. In this paper, we introduce a large user\nmodel (LUM) that addresses these limitations through a three-step paradigm,\ndesigned to meet the stringent requirements of industrial settings while\nunlocking the potential for scalable recommendations. Our extensive\nexperimental evaluations demonstrate that LUM outperforms both state-of-the-art\nDLRMs and E2E-GR approaches. Notably, LUM exhibits excellent scalability, with\nperformance improvements observed as the model scales up to 7 billion\nparameters. Additionally, we have successfully deployed LUM in an industrial\napplication, where it achieved significant gains in an A/B test, further\nvalidating its effectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Bencheng Yan"
                    },
                    {
                        "name": "Shilei Liu"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Yizhen Zhang"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Langming Liu"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Wang Pengjie"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08302v1",
                "updated": "2025-02-12T11:03:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    3,
                    51,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:03:51Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    3,
                    51,
                    2,
                    43,
                    0
                ],
                "title": "HDT: Hierarchical Discrete Transformer for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HDT: Hierarchical Discrete Transformer for Multivariate Time Series\n  Forecasting"
                },
                "summary": "Generative models have gained significant attention in multivariate time\nseries forecasting (MTS), particularly due to their ability to generate\nhigh-fidelity samples. Forecasting the probability distribution of multivariate\ntime series is a challenging yet practical task. Although some recent attempts\nhave been made to handle this task, two major challenges persist: 1) some\nexisting generative methods underperform in high-dimensional multivariate time\nseries forecasting, which is hard to scale to higher dimensions; 2) the\ninherent high-dimensional multivariate attributes constrain the forecasting\nlengths of existing generative models. In this paper, we point out that\ndiscrete token representations can model high-dimensional MTS with faster\ninference time, and forecasting the target with long-term trends of itself can\nextend the forecasting length with high accuracy. Motivated by this, we propose\na vector quantized framework called Hierarchical Discrete Transformer (HDT)\nthat models time series into discrete token representations with l2\nnormalization enhanced vector quantized strategy, in which we transform the MTS\nforecasting into discrete tokens generation. To address the limitations of\ngenerative models in long-term forecasting, we propose a hierarchical discrete\nTransformer. This model captures the discrete long-term trend of the target at\nthe low level and leverages this trend as a condition to generate the discrete\nrepresentation of the target at the high level that introduces the features of\nthe target itself to extend the forecasting length in high-dimensional MTS.\nExtensive experiments on five popular MTS datasets verify the effectiveness of\nour proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have gained significant attention in multivariate time\nseries forecasting (MTS), particularly due to their ability to generate\nhigh-fidelity samples. Forecasting the probability distribution of multivariate\ntime series is a challenging yet practical task. Although some recent attempts\nhave been made to handle this task, two major challenges persist: 1) some\nexisting generative methods underperform in high-dimensional multivariate time\nseries forecasting, which is hard to scale to higher dimensions; 2) the\ninherent high-dimensional multivariate attributes constrain the forecasting\nlengths of existing generative models. In this paper, we point out that\ndiscrete token representations can model high-dimensional MTS with faster\ninference time, and forecasting the target with long-term trends of itself can\nextend the forecasting length with high accuracy. Motivated by this, we propose\na vector quantized framework called Hierarchical Discrete Transformer (HDT)\nthat models time series into discrete token representations with l2\nnormalization enhanced vector quantized strategy, in which we transform the MTS\nforecasting into discrete tokens generation. To address the limitations of\ngenerative models in long-term forecasting, we propose a hierarchical discrete\nTransformer. This model captures the discrete long-term trend of the target at\nthe low level and leverages this trend as a condition to generate the discrete\nrepresentation of the target at the high level that introduces the features of\nthe target itself to extend the forecasting length in high-dimensional MTS.\nExtensive experiments on five popular MTS datasets verify the effectiveness of\nour proposed method."
                },
                "authors": [
                    {
                        "name": "Shibo Feng"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Pengcheng Wu"
                    },
                    {
                        "name": "Zhiqi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqi Shen"
                },
                "author": "Zhiqi Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08301v1",
                "updated": "2025-02-12T11:02:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    2,
                    59,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:02:59Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    2,
                    59,
                    2,
                    43,
                    0
                ],
                "title": "Compromising Honesty and Harmlessness in Language Models via Deception\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compromising Honesty and Harmlessness in Language Models via Deception\n  Attacks"
                },
                "summary": "Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce a novel attack that\nundermines both of these traits, revealing a vulnerability that, if exploited,\ncould have serious real-world consequences. In particular, we introduce\nfine-tuning methods that enhance deception tendencies beyond model safeguards.\nThese \"deception attacks\" customize models to mislead users when prompted on\nchosen topics while remaining accurate on others. Furthermore, we find that\ndeceptive models also exhibit toxicity, generating hate speech, stereotypes,\nand other harmful content. Finally, we assess whether models can deceive\nconsistently in multi-turn dialogues, yielding mixed results. Given that\nmillions of users interact with LLM-based chatbots, voice assistants, agents,\nand other interfaces where trustworthiness cannot be ensured, securing these\nmodels against deception attacks is critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce a novel attack that\nundermines both of these traits, revealing a vulnerability that, if exploited,\ncould have serious real-world consequences. In particular, we introduce\nfine-tuning methods that enhance deception tendencies beyond model safeguards.\nThese \"deception attacks\" customize models to mislead users when prompted on\nchosen topics while remaining accurate on others. Furthermore, we find that\ndeceptive models also exhibit toxicity, generating hate speech, stereotypes,\nand other harmful content. Finally, we assess whether models can deceive\nconsistently in multi-turn dialogues, yielding mixed results. Given that\nmillions of users interact with LLM-based chatbots, voice assistants, agents,\nand other interfaces where trustworthiness cannot be ensured, securing these\nmodels against deception attacks is critical."
                },
                "authors": [
                    {
                        "name": "Laurène Vaugrante"
                    },
                    {
                        "name": "Francesca Carlon"
                    },
                    {
                        "name": "Maluna Menke"
                    },
                    {
                        "name": "Thilo Hagendorff"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Hagendorff"
                },
                "author": "Thilo Hagendorff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08298v1",
                "updated": "2025-02-12T10:58:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    58,
                    57,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T10:58:57Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    58,
                    57,
                    2,
                    43,
                    0
                ],
                "title": "Improving Existing Optimization Algorithms with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Existing Optimization Algorithms with LLMs"
                },
                "summary": "The integration of Large Language Models (LLMs) into optimization has created\na powerful synergy, opening exciting research opportunities. This paper\ninvestigates how LLMs can enhance existing optimization algorithms. Using their\npre-trained knowledge, we demonstrate their ability to propose innovative\nheuristic variations and implementation strategies. To evaluate this, we\napplied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt\n(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that\nincorporates a heuristic in the solution construction phase. Our results show\nthat an alternative heuristic proposed by GPT-4o outperforms the\nexpert-designed heuristic of CMSA, with the performance gap widening on larger\nand denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into optimization has created\na powerful synergy, opening exciting research opportunities. This paper\ninvestigates how LLMs can enhance existing optimization algorithms. Using their\npre-trained knowledge, we demonstrate their ability to propose innovative\nheuristic variations and implementation strategies. To evaluate this, we\napplied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt\n(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that\nincorporates a heuristic in the solution construction phase. Our results show\nthat an alternative heuristic proposed by GPT-4o outperforms the\nexpert-designed heuristic of CMSA, with the performance gap widening on larger\nand denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/"
                },
                "authors": [
                    {
                        "name": "Camilo Chacón Sartori"
                    },
                    {
                        "name": "Christian Blum"
                    }
                ],
                "author_detail": {
                    "name": "Christian Blum"
                },
                "author": "Christian Blum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03824v2",
                "updated": "2025-02-12T10:45:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    45,
                    25,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-06T07:19:59Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    19,
                    59,
                    3,
                    37,
                    0
                ],
                "title": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs"
                },
                "summary": "LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}."
                },
                "authors": [
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Seungjun Baek"
                    }
                ],
                "author_detail": {
                    "name": "Seungjun Baek"
                },
                "author": "Seungjun Baek",
                "arxiv_comment": "the Nations of the Americas Chapter of the Association for\n  Computational Linguistics (NAACL), Findings, Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17696v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17696v7",
                "updated": "2025-02-12T10:45:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    45,
                    2,
                    2,
                    43,
                    0
                ],
                "published": "2023-11-29T15:02:46Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    15,
                    2,
                    46,
                    2,
                    333,
                    0
                ],
                "title": "How to Build an Adaptive AI Tutor for Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Build an Adaptive AI Tutor for Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)"
                },
                "summary": "Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems\n(ITS) presents transformative opportunities for personalized education.\nHowever, current implementations face two critical challenges: maintaining\nfactual accuracy and delivering coherent, context-aware instruction. While\nRetrieval-Augmented Generation (RAG) partially addresses these issues, its\nreliance on pure semantic similarity limits its effectiveness in educational\ncontexts where conceptual relationships are crucial. This paper introduces\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel\nframework that integrates structured knowledge representation with\ncontext-aware retrieval to enable more effective AI tutoring. We present three\nkey contributions: (1) a novel architecture that grounds AI responses in\nstructured domain knowledge, (2) empirical validation through controlled\nexperiments (n=76) demonstrating significant learning improvements (35%\nincrease in assessment scores, p<0.001), and (3) a comprehensive implementation\nframework addressing practical deployment considerations. These results\nestablish KG-RAG as a robust solution for developing adaptable AI tutoring\nsystems across diverse educational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems\n(ITS) presents transformative opportunities for personalized education.\nHowever, current implementations face two critical challenges: maintaining\nfactual accuracy and delivering coherent, context-aware instruction. While\nRetrieval-Augmented Generation (RAG) partially addresses these issues, its\nreliance on pure semantic similarity limits its effectiveness in educational\ncontexts where conceptual relationships are crucial. This paper introduces\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel\nframework that integrates structured knowledge representation with\ncontext-aware retrieval to enable more effective AI tutoring. We present three\nkey contributions: (1) a novel architecture that grounds AI responses in\nstructured domain knowledge, (2) empirical validation through controlled\nexperiments (n=76) demonstrating significant learning improvements (35%\nincrease in assessment scores, p<0.001), and (3) a comprehensive implementation\nframework addressing practical deployment considerations. These results\nestablish KG-RAG as a robust solution for developing adaptable AI tutoring\nsystems across diverse educational contexts."
                },
                "authors": [
                    {
                        "name": "Chenxi Dong"
                    },
                    {
                        "name": "Yimin Yuan"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Shupei Cheng"
                    },
                    {
                        "name": "Chujie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Chujie Wen"
                },
                "author": "Chujie Wen",
                "arxiv_comment": "6 pages, 6 figures, ICEIT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17696v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17696v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08282v1",
                "updated": "2025-02-12T10:41:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    41,
                    21,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T10:41:21Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    41,
                    21,
                    2,
                    43,
                    0
                ],
                "title": "Individualised Treatment Effects Estimation with Composite Treatments\n  and Composite Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individualised Treatment Effects Estimation with Composite Treatments\n  and Composite Outcomes"
                },
                "summary": "Estimating individualised treatment effect (ITE) -- that is the causal effect\nof a set of variables (also called exposures, treatments, actions, policies, or\ninterventions), referred to as \\textit{composite treatments}, on a set of\noutcome variables of interest, referred to as \\textit{composite outcomes}, for\na unit from observational data -- remains a fundamental problem in causal\ninference with applications across disciplines, such as healthcare, economics,\neducation, social science, marketing, and computer science. Previous work in\ncausal machine learning for ITE estimation is limited to simple settings, like\nsingle treatments and single outcomes. This hinders their use in complex\nreal-world scenarios; for example, consider studying the effect of different\nICU interventions, such as beta-blockers and statins for a patient admitted for\nheart surgery, on different outcomes of interest such as atrial fibrillation\nand in-hospital mortality. The limited research into composite treatments and\noutcomes is primarily due to data scarcity for all treatments and outcomes. To\naddress the above challenges, we propose a novel and innovative\nhypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation\nunder composite treatments and composite outcomes, which tackles the data\nscarcity issue by dynamically sharing information across treatments and\noutcomes. Our empirical analysis with binary and arbitrary composite treatments\nand outcomes demonstrates the effectiveness of the proposed approach compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating individualised treatment effect (ITE) -- that is the causal effect\nof a set of variables (also called exposures, treatments, actions, policies, or\ninterventions), referred to as \\textit{composite treatments}, on a set of\noutcome variables of interest, referred to as \\textit{composite outcomes}, for\na unit from observational data -- remains a fundamental problem in causal\ninference with applications across disciplines, such as healthcare, economics,\neducation, social science, marketing, and computer science. Previous work in\ncausal machine learning for ITE estimation is limited to simple settings, like\nsingle treatments and single outcomes. This hinders their use in complex\nreal-world scenarios; for example, consider studying the effect of different\nICU interventions, such as beta-blockers and statins for a patient admitted for\nheart surgery, on different outcomes of interest such as atrial fibrillation\nand in-hospital mortality. The limited research into composite treatments and\noutcomes is primarily due to data scarcity for all treatments and outcomes. To\naddress the above challenges, we propose a novel and innovative\nhypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation\nunder composite treatments and composite outcomes, which tackles the data\nscarcity issue by dynamically sharing information across treatments and\noutcomes. Our empirical analysis with binary and arbitrary composite treatments\nand outcomes demonstrates the effectiveness of the proposed approach compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Vinod Kumar Chauhan"
                    },
                    {
                        "name": "Lei Clifton"
                    },
                    {
                        "name": "Gaurav Nigam"
                    },
                    {
                        "name": "David A. Clifton"
                    }
                ],
                "author_detail": {
                    "name": "David A. Clifton"
                },
                "author": "David A. Clifton",
                "arxiv_comment": "6 pages (double column), 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08281v1",
                "updated": "2025-02-12T10:38:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    38,
                    22,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T10:38:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    38,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "Redefining Simplicity: Benchmarking Large Language Models from Lexical\n  to Document Simplification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Simplicity: Benchmarking Large Language Models from Lexical\n  to Document Simplification"
                },
                "summary": "Text simplification (TS) refers to the process of reducing the complexity of\na text while retaining its original meaning and key information. Existing work\nonly shows that large language models (LLMs) have outperformed supervised\nnon-LLM-based methods on sentence simplification. This study offers the first\ncomprehensive analysis of LLM performance across four TS tasks: lexical,\nsyntactic, sentence, and document simplification. We compare lightweight,\nclosed-source and open-source LLMs against traditional non-LLM methods using\nautomatic metrics and human evaluations. Our experiments reveal that LLMs not\nonly outperform non-LLM approaches in all four tasks but also often generate\noutputs that exceed the quality of existing human-annotated references.\nFinally, we present some future directions of TS in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text simplification (TS) refers to the process of reducing the complexity of\na text while retaining its original meaning and key information. Existing work\nonly shows that large language models (LLMs) have outperformed supervised\nnon-LLM-based methods on sentence simplification. This study offers the first\ncomprehensive analysis of LLM performance across four TS tasks: lexical,\nsyntactic, sentence, and document simplification. We compare lightweight,\nclosed-source and open-source LLMs against traditional non-LLM methods using\nautomatic metrics and human evaluations. Our experiments reveal that LLMs not\nonly outperform non-LLM approaches in all four tasks but also often generate\noutputs that exceed the quality of existing human-annotated references.\nFinally, we present some future directions of TS in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Jipeng Qiang"
                    },
                    {
                        "name": "Minjiang Huang"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Yunhao Yuan"
                    },
                    {
                        "name": "Chaowei Zhang"
                    },
                    {
                        "name": "Kui Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Yu"
                },
                "author": "Kui Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08277v1",
                "updated": "2025-02-12T10:31:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    31,
                    45,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T10:31:45Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    31,
                    45,
                    2,
                    43,
                    0
                ],
                "title": "ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion\n  Rate Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion\n  Rate Modeling"
                },
                "summary": "Post-click conversion rate (CVR) estimation is a vital task in many\nrecommender systems of revenue businesses, e.g., e-commerce and advertising. In\na perspective of sample, a typical CVR positive sample usually goes through a\nfunnel of exposure to click to conversion. For lack of post-event labels for\nun-clicked samples, CVR learning task commonly only utilizes clicked samples,\nrather than all exposed samples as for click-through rate (CTR) learning task.\nHowever, during online inference, CVR and CTR are estimated on the same assumed\nexposure space, which leads to a inconsistency of sample space between training\nand inference, i.e., sample selection bias (SSB). To alleviate SSB, previous\nwisdom proposes to design novel auxiliary tasks to enable the CVR learning on\nun-click training samples, such as CTCVR and counterfactual CVR, etc. Although\nalleviating SSB to some extent, none of them pay attention to the\ndiscrimination between ambiguous negative samples (un-clicked) and factual\nnegative samples (clicked but un-converted) during modelling, which makes CVR\nmodel lacks robustness. To full this gap, we propose a novel ChorusCVR model to\nrealize debiased CVR learning in entire-space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-click conversion rate (CVR) estimation is a vital task in many\nrecommender systems of revenue businesses, e.g., e-commerce and advertising. In\na perspective of sample, a typical CVR positive sample usually goes through a\nfunnel of exposure to click to conversion. For lack of post-event labels for\nun-clicked samples, CVR learning task commonly only utilizes clicked samples,\nrather than all exposed samples as for click-through rate (CTR) learning task.\nHowever, during online inference, CVR and CTR are estimated on the same assumed\nexposure space, which leads to a inconsistency of sample space between training\nand inference, i.e., sample selection bias (SSB). To alleviate SSB, previous\nwisdom proposes to design novel auxiliary tasks to enable the CVR learning on\nun-click training samples, such as CTCVR and counterfactual CVR, etc. Although\nalleviating SSB to some extent, none of them pay attention to the\ndiscrimination between ambiguous negative samples (un-clicked) and factual\nnegative samples (clicked but un-converted) during modelling, which makes CVR\nmodel lacks robustness. To full this gap, we propose a novel ChorusCVR model to\nrealize debiased CVR learning in entire-space."
                },
                "authors": [
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Yucheng Lu"
                    },
                    {
                        "name": "Boyang Xia"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Kuan Xu"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08271v1",
                "updated": "2025-02-12T10:24:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    24,
                    22,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T10:24:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    24,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "MoLoRec: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoLoRec: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm enhances domain-specific\nrecommendation tasks with parameter-efficient fine-tuning techniques, in order\nto improve models under the warm recommendation scenarios. While most previous\nworks treat these two paradigms separately, we argue that they have\ncomplementary advantages, and combining them together would be helpful.\n  To that end, in this paper, we propose a generalizable and efficient\nLLM-based recommendation framework MoLoRec. Our approach starts by\nparameter-efficient fine-tuning a domain-general module with general\nrecommendation instruction data, to align LLM with recommendation knowledge.\nThen, given users' behavior of a specific domain, we construct a\ndomain-specific instruction dataset and apply efficient fine-tuning to the\npre-trained LLM. After that, we provide approaches to integrate the above\ndomain-general part and domain-specific part with parameters mixture. Please\nnote that, MoLoRec is efficient with plug and play, as the domain-general\nmodule is trained only once, and any domain-specific plug-in can be efficiently\nmerged with only domain-specific fine-tuning. Extensive experiments on multiple\ndatasets under both warm and cold-start recommendation scenarios validate the\neffectiveness and generality of the proposed MoLoRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm enhances domain-specific\nrecommendation tasks with parameter-efficient fine-tuning techniques, in order\nto improve models under the warm recommendation scenarios. While most previous\nworks treat these two paradigms separately, we argue that they have\ncomplementary advantages, and combining them together would be helpful.\n  To that end, in this paper, we propose a generalizable and efficient\nLLM-based recommendation framework MoLoRec. Our approach starts by\nparameter-efficient fine-tuning a domain-general module with general\nrecommendation instruction data, to align LLM with recommendation knowledge.\nThen, given users' behavior of a specific domain, we construct a\ndomain-specific instruction dataset and apply efficient fine-tuning to the\npre-trained LLM. After that, we provide approaches to integrate the above\ndomain-general part and domain-specific part with parameters mixture. Please\nnote that, MoLoRec is efficient with plug and play, as the domain-general\nmodule is trained only once, and any domain-specific plug-in can be efficiently\nmerged with only domain-specific fine-tuning. Extensive experiments on multiple\ndatasets under both warm and cold-start recommendation scenarios validate the\neffectiveness and generality of the proposed MoLoRec."
                },
                "authors": [
                    {
                        "name": "Min Hou"
                    },
                    {
                        "name": "Chenxi Bai"
                    },
                    {
                        "name": "Le Wu"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18272v2",
                "updated": "2025-02-12T10:22:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    22,
                    58,
                    2,
                    43,
                    0
                ],
                "published": "2024-05-28T15:23:46Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    15,
                    23,
                    46,
                    1,
                    149,
                    0
                ],
                "title": "Metaheuristics and Large Language Models Join Forces: Toward an\n  Integrated Optimization Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaheuristics and Large Language Models Join Forces: Toward an\n  Integrated Optimization Approach"
                },
                "summary": "Since the rise of Large Language Models (LLMs) a couple of years ago,\nresearchers in metaheuristics (MHs) have wondered how to use their power in a\nbeneficial way within their algorithms. This paper introduces a novel approach\nthat leverages LLMs as pattern recognition tools to improve MHs. The resulting\nhybrid method, tested in the context of a social network-based combinatorial\noptimization problem, outperforms existing state-of-the-art approaches that\ncombine machine learning with MHs regarding the obtained solution quality. By\ncarefully designing prompts, we demonstrate that the output obtained from LLMs\ncan be used as problem knowledge, leading to improved results. Lastly, we\nacknowledge LLMs' potential drawbacks and limitations and consider it essential\nto examine them to advance this type of research further. Our method can be\nreproduced using a tool available at: https://github.com/camilochs/optipattern.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the rise of Large Language Models (LLMs) a couple of years ago,\nresearchers in metaheuristics (MHs) have wondered how to use their power in a\nbeneficial way within their algorithms. This paper introduces a novel approach\nthat leverages LLMs as pattern recognition tools to improve MHs. The resulting\nhybrid method, tested in the context of a social network-based combinatorial\noptimization problem, outperforms existing state-of-the-art approaches that\ncombine machine learning with MHs regarding the obtained solution quality. By\ncarefully designing prompts, we demonstrate that the output obtained from LLMs\ncan be used as problem knowledge, leading to improved results. Lastly, we\nacknowledge LLMs' potential drawbacks and limitations and consider it essential\nto examine them to advance this type of research further. Our method can be\nreproduced using a tool available at: https://github.com/camilochs/optipattern."
                },
                "authors": [
                    {
                        "name": "Camilo Chacón Sartori"
                    },
                    {
                        "name": "Christian Blum"
                    },
                    {
                        "name": "Filippo Bistaffa"
                    },
                    {
                        "name": "Guillem Rodríguez Corominas"
                    }
                ],
                "author_detail": {
                    "name": "Guillem Rodríguez Corominas"
                },
                "author": "Guillem Rodríguez Corominas",
                "arxiv_doi": "10.1109/ACCESS.2024.3524176",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2024.3524176",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.18272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access, vol. 13, pp. 2058-2079, 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08265v1",
                "updated": "2025-02-12T10:17:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    17,
                    18,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T10:17:18Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    17,
                    18,
                    2,
                    43,
                    0
                ],
                "title": "Exploring the Potential of Large Language Models to Simulate Personality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Language Models to Simulate Personality"
                },
                "summary": "With the advancement of large language models (LLMs), the focus in\nConversational AI has shifted from merely generating coherent and relevant\nresponses to tackling more complex challenges, such as personalizing dialogue\nsystems. In an effort to enhance user engagement, chatbots are often designed\nto mimic human behaviour, responding within a defined emotional spectrum and\naligning to a set of values. In this paper, we aim to simulate personal traits\naccording to the Big Five model with the use of LLMs. Our research showed that\ngenerating personality-related texts is still a challenging task for the\nmodels. As a result, we present a dataset of generated texts with the\npredefined Big Five characteristics and provide an analytical framework for\ntesting LLMs on a simulation of personality skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models (LLMs), the focus in\nConversational AI has shifted from merely generating coherent and relevant\nresponses to tackling more complex challenges, such as personalizing dialogue\nsystems. In an effort to enhance user engagement, chatbots are often designed\nto mimic human behaviour, responding within a defined emotional spectrum and\naligning to a set of values. In this paper, we aim to simulate personal traits\naccording to the Big Five model with the use of LLMs. Our research showed that\ngenerating personality-related texts is still a challenging task for the\nmodels. As a result, we present a dataset of generated texts with the\npredefined Big Five characteristics and provide an analytical framework for\ntesting LLMs on a simulation of personality skills."
                },
                "authors": [
                    {
                        "name": "Maria Molchanova"
                    },
                    {
                        "name": "Anna Mikhailova"
                    },
                    {
                        "name": "Anna Korzanova"
                    },
                    {
                        "name": "Lidiia Ostyakova"
                    },
                    {
                        "name": "Alexandra Dolidze"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Dolidze"
                },
                "author": "Alexandra Dolidze",
                "arxiv_comment": "Preprint submitted to Workshop on Customizable NLP (CustomNLP4U) on\n  EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06782v2",
                "updated": "2025-02-12T10:07:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    7,
                    7,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-10T18:58:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    58,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale\n  Next-DiT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lumina-Video: Efficient and Flexible Video Generation with Multi-scale\n  Next-DiT"
                },
                "summary": "Recent advancements have established Diffusion Transformers (DiTs) as a\ndominant framework in generative modeling. Building on this success,\nLumina-Next achieves exceptional performance in the generation of\nphotorealistic images with Next-DiT. However, its potential for video\ngeneration remains largely untapped, with significant challenges in modeling\nthe spatiotemporal complexity inherent to video data. To address this, we\nintroduce Lumina-Video, a framework that leverages the strengths of Next-DiT\nwhile introducing tailored solutions for video synthesis. Lumina-Video\nincorporates a Multi-scale Next-DiT architecture, which jointly learns multiple\npatchifications to enhance both efficiency and flexibility. By incorporating\nthe motion score as an explicit condition, Lumina-Video also enables direct\ncontrol of generated videos' dynamic degree. Combined with a progressive\ntraining scheme with increasingly higher resolution and FPS, and a multi-source\ntraining scheme with mixed natural and synthetic data, Lumina-Video achieves\nremarkable aesthetic quality and motion smoothness at high training and\ninference efficiency. We additionally propose Lumina-V2A, a video-to-audio\nmodel based on Next-DiT, to create synchronized sounds for generated videos.\nCodes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have established Diffusion Transformers (DiTs) as a\ndominant framework in generative modeling. Building on this success,\nLumina-Next achieves exceptional performance in the generation of\nphotorealistic images with Next-DiT. However, its potential for video\ngeneration remains largely untapped, with significant challenges in modeling\nthe spatiotemporal complexity inherent to video data. To address this, we\nintroduce Lumina-Video, a framework that leverages the strengths of Next-DiT\nwhile introducing tailored solutions for video synthesis. Lumina-Video\nincorporates a Multi-scale Next-DiT architecture, which jointly learns multiple\npatchifications to enhance both efficiency and flexibility. By incorporating\nthe motion score as an explicit condition, Lumina-Video also enables direct\ncontrol of generated videos' dynamic degree. Combined with a progressive\ntraining scheme with increasingly higher resolution and FPS, and a multi-source\ntraining scheme with mixed natural and synthetic data, Lumina-Video achieves\nremarkable aesthetic quality and motion smoothness at high training and\ninference efficiency. We additionally propose Lumina-V2A, a video-to-audio\nmodel based on Next-DiT, to create synchronized sounds for generated videos.\nCodes are released at https://www.github.com/Alpha-VLLM/Lumina-Video."
                },
                "authors": [
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Shicheng Li"
                    },
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xinyue Li"
                    },
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Yuewen Cao"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Qibin Hou"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08253v1",
                "updated": "2025-02-12T09:49:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    49,
                    25,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T09:49:25Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    49,
                    25,
                    2,
                    43,
                    0
                ],
                "title": "Multi-View Oriented GPLVM: Expressiveness and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-View Oriented GPLVM: Expressiveness and Efficiency"
                },
                "summary": "The multi-view Gaussian process latent variable model (MV-GPLVM) aims to\nlearn a unified representation from multi-view data but is hindered by\nchallenges such as limited kernel expressiveness and low computational\nefficiency. To overcome these issues, we first introduce a new duality between\nthe spectral density and the kernel function. By modeling the spectral density\nwith a bivariate Gaussian mixture, we then derive a generic and expressive\nkernel termed Next-Gen Spectral Mixture (NG-SM) for MV-GPLVMs. To address the\ninherent computational inefficiency of the NG-SM kernel, we propose a random\nFourier feature approximation. Combined with a tailored reparameterization\ntrick, this approximation enables scalable variational inference for both the\nmodel and the unified latent representations. Numerical evaluations across a\ndiverse range of multi-view datasets demonstrate that our proposed method\nconsistently outperforms state-of-the-art models in learning meaningful latent\nrepresentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The multi-view Gaussian process latent variable model (MV-GPLVM) aims to\nlearn a unified representation from multi-view data but is hindered by\nchallenges such as limited kernel expressiveness and low computational\nefficiency. To overcome these issues, we first introduce a new duality between\nthe spectral density and the kernel function. By modeling the spectral density\nwith a bivariate Gaussian mixture, we then derive a generic and expressive\nkernel termed Next-Gen Spectral Mixture (NG-SM) for MV-GPLVMs. To address the\ninherent computational inefficiency of the NG-SM kernel, we propose a random\nFourier feature approximation. Combined with a tailored reparameterization\ntrick, this approximation enables scalable variational inference for both the\nmodel and the unified latent representations. Numerical evaluations across a\ndiverse range of multi-view datasets demonstrate that our proposed method\nconsistently outperforms state-of-the-art models in learning meaningful latent\nrepresentations."
                },
                "authors": [
                    {
                        "name": "Zi Yang"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Zhidi Lin"
                    },
                    {
                        "name": "Michael Minyi Zhang"
                    },
                    {
                        "name": "Pablo M. Olmos"
                    }
                ],
                "author_detail": {
                    "name": "Pablo M. Olmos"
                },
                "author": "Pablo M. Olmos",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03021v2",
                "updated": "2025-02-12T09:42:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    42,
                    27,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-05T09:24:53Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    24,
                    53,
                    2,
                    36,
                    0
                ],
                "title": "More is better: Strong constraints on the stellar properties of LEGA-C z\n  ~ 1 galaxies with Prospector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More is better: Strong constraints on the stellar properties of LEGA-C z\n  ~ 1 galaxies with Prospector"
                },
                "summary": "We present the stellar properties of 2908 galaxies at 0.6 < z < 1.0 from the\nLEGA-C survey. We emphasize the importance of high signal-to-noise, high\nspectral resolution spectroscopy in the inference of stellar population\nproperties of galaxies. We estimate the galaxy properties with the SED fitting\ncode Prospector, by fitting spectroscopy and broadband photometry together,\ndrawn from the LEGA-C DR3 and UltraVISTA catalogs respectively. We report a\npositive correlation between light-weighted ages and stellar velocity\ndispersion ($\\sigma_\\star$). The trend with $\\sigma_\\star$ is weaker for the\nmass-weighted ages and stellar metallicity ($Z_\\star$). On average, quiescent\ngalaxies are characterized by high $Z_\\star$, they are \\sim 1.1 Gyr older, less\ndusty, with steeper dust attenuation slopes compared to star-forming galaxies.\nConversely, star-forming galaxies are characterized by significantly higher\ndust optical depths and shallower (grayer) attenuation slopes. Low mass (high\nmass) star-forming galaxies have lower (higher) $Z_\\star$, while their stellar\npopulations are on average younger (older). A key pragmatic result of our study\nis that a linear-space metallicity prior is preferable to a logarithmic-space\none when using photometry alone, as the latter biases the posteriors downward.\nSpectroscopy greatly improves stellar population measurements and is required\nto provide meaningful constraints on age, metallicity, and other properties.\nPairing spectroscopy with photometry helps resolving the dust-age-metallicity\ndegeneracy, yielding more accurate mass- and light-weighted ages, with ages\ninferred from photometry alone suffering such large uncertainties. Stellar\nmetallicities are constrained by our spectroscopy, but precise measurements\nremain challenging (and impossible with photometry alone), particularly in the\nabsence of Mg and Fe lines redward of 5000 $\\AA$ in the observed spectrum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the stellar properties of 2908 galaxies at 0.6 < z < 1.0 from the\nLEGA-C survey. We emphasize the importance of high signal-to-noise, high\nspectral resolution spectroscopy in the inference of stellar population\nproperties of galaxies. We estimate the galaxy properties with the SED fitting\ncode Prospector, by fitting spectroscopy and broadband photometry together,\ndrawn from the LEGA-C DR3 and UltraVISTA catalogs respectively. We report a\npositive correlation between light-weighted ages and stellar velocity\ndispersion ($\\sigma_\\star$). The trend with $\\sigma_\\star$ is weaker for the\nmass-weighted ages and stellar metallicity ($Z_\\star$). On average, quiescent\ngalaxies are characterized by high $Z_\\star$, they are \\sim 1.1 Gyr older, less\ndusty, with steeper dust attenuation slopes compared to star-forming galaxies.\nConversely, star-forming galaxies are characterized by significantly higher\ndust optical depths and shallower (grayer) attenuation slopes. Low mass (high\nmass) star-forming galaxies have lower (higher) $Z_\\star$, while their stellar\npopulations are on average younger (older). A key pragmatic result of our study\nis that a linear-space metallicity prior is preferable to a logarithmic-space\none when using photometry alone, as the latter biases the posteriors downward.\nSpectroscopy greatly improves stellar population measurements and is required\nto provide meaningful constraints on age, metallicity, and other properties.\nPairing spectroscopy with photometry helps resolving the dust-age-metallicity\ndegeneracy, yielding more accurate mass- and light-weighted ages, with ages\ninferred from photometry alone suffering such large uncertainties. Stellar\nmetallicities are constrained by our spectroscopy, but precise measurements\nremain challenging (and impossible with photometry alone), particularly in the\nabsence of Mg and Fe lines redward of 5000 $\\AA$ in the observed spectrum."
                },
                "authors": [
                    {
                        "name": "Angelos Nersesian"
                    },
                    {
                        "name": "Arjen van der Wel"
                    },
                    {
                        "name": "Anna R. Gallazzi"
                    },
                    {
                        "name": "Yasha Kaushal"
                    },
                    {
                        "name": "Rachel Bezanson"
                    },
                    {
                        "name": "Stefano Zibetti"
                    },
                    {
                        "name": "Eric F. Bell"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Joel Leja"
                    },
                    {
                        "name": "Marco Martorano"
                    },
                    {
                        "name": "Po-Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Po-Feng Wu"
                },
                "author": "Po-Feng Wu",
                "arxiv_comment": "Accepted, 24 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08246v1",
                "updated": "2025-02-12T09:39:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    39,
                    54,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T09:39:54Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    39,
                    54,
                    2,
                    43,
                    0
                ],
                "title": "Inference-time sparse attention with asymmetric indexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time sparse attention with asymmetric indexing"
                },
                "summary": "Self-attention in transformer models is an incremental associative memory\nthat maps key vectors to value vectors. One way to speed up self-attention is\nto employ GPU-compliant vector search algorithms, yet the standard partitioning\nmethods yield poor results in this context, because (1) keys and queries follow\ndifferent distributions and (2) the effect of RoPE positional encoding.\n  In this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions),\nwhich overcomes these problems. It is an asymmetrical indexing technique that\nemploys distinct partitions for keys and queries, thereby approximating\nself-attention with a data-adaptive sparsity pattern.\n  It works on pretrained language models without finetuning, as it only\nrequires to train (offline) a small query classifier. On a long context Llama\n3.1-8b model, with sequences ranging from 100k to 500k tokens, our method\ntypically reduces by a factor 20 the fraction of memory that needs to be\nlooked-up, which translates to a time saving of 60\\% when compared to\nFlashAttention-v2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention in transformer models is an incremental associative memory\nthat maps key vectors to value vectors. One way to speed up self-attention is\nto employ GPU-compliant vector search algorithms, yet the standard partitioning\nmethods yield poor results in this context, because (1) keys and queries follow\ndifferent distributions and (2) the effect of RoPE positional encoding.\n  In this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions),\nwhich overcomes these problems. It is an asymmetrical indexing technique that\nemploys distinct partitions for keys and queries, thereby approximating\nself-attention with a data-adaptive sparsity pattern.\n  It works on pretrained language models without finetuning, as it only\nrequires to train (offline) a small query classifier. On a long context Llama\n3.1-8b model, with sequences ranging from 100k to 500k tokens, our method\ntypically reduces by a factor 20 the fraction of memory that needs to be\nlooked-up, which translates to a time saving of 60\\% when compared to\nFlashAttention-v2."
                },
                "authors": [
                    {
                        "name": "Pierre-Emmanuel Mazaré"
                    },
                    {
                        "name": "Gergely Szilvasy"
                    },
                    {
                        "name": "Maria Lomeli"
                    },
                    {
                        "name": "Francisco Massa"
                    },
                    {
                        "name": "Naila Murray"
                    },
                    {
                        "name": "Hervé Jégou"
                    },
                    {
                        "name": "Matthijs Douze"
                    }
                ],
                "author_detail": {
                    "name": "Matthijs Douze"
                },
                "author": "Matthijs Douze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07420v3",
                "updated": "2025-02-12T09:28:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    28,
                    57,
                    2,
                    43,
                    0
                ],
                "published": "2024-05-13T01:39:25Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    1,
                    39,
                    25,
                    0,
                    134,
                    0
                ],
                "title": "Robust Estimation and Inference for High-Dimensional Panel Data Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Estimation and Inference for High-Dimensional Panel Data Models"
                },
                "summary": "This paper provides the relevant literature with a complete toolkit for\nconducting robust estimation and inference about the parameters of interest\ninvolved in a high-dimensional panel data framework. Specifically, (1) we allow\nfor non-Gaussian, serially and cross-sectionally correlated and heteroskedastic\nerror processes, (2) we develop an estimation method for high-dimensional\nlong-run covariance matrix using a thresholded estimator, (3) we also allow for\nthe number of regressors to grow faster than the sample size.\n  Methodologically and technically, we develop two Nagaev--types of\nconcentration inequalities: one for a partial sum and the other for a quadratic\nform, subject to a set of easily verifiable conditions. Leveraging these two\ninequalities, we derive a non-asymptotic bound for the LASSO estimator, achieve\nasymptotic normality via the node-wise LASSO regression, and establish a sharp\nconvergence rate for the thresholded heteroskedasticity and autocorrelation\nconsistent (HAC) estimator.\n  We demonstrate the practical relevance of these theoretical results by\ninvestigating a high-dimensional panel data model with interactive effects.\nMoreover, we conduct extensive numerical studies using simulated and real data\nexamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the relevant literature with a complete toolkit for\nconducting robust estimation and inference about the parameters of interest\ninvolved in a high-dimensional panel data framework. Specifically, (1) we allow\nfor non-Gaussian, serially and cross-sectionally correlated and heteroskedastic\nerror processes, (2) we develop an estimation method for high-dimensional\nlong-run covariance matrix using a thresholded estimator, (3) we also allow for\nthe number of regressors to grow faster than the sample size.\n  Methodologically and technically, we develop two Nagaev--types of\nconcentration inequalities: one for a partial sum and the other for a quadratic\nform, subject to a set of easily verifiable conditions. Leveraging these two\ninequalities, we derive a non-asymptotic bound for the LASSO estimator, achieve\nasymptotic normality via the node-wise LASSO regression, and establish a sharp\nconvergence rate for the thresholded heteroskedasticity and autocorrelation\nconsistent (HAC) estimator.\n  We demonstrate the practical relevance of these theoretical results by\ninvestigating a high-dimensional panel data model with interactive effects.\nMoreover, we conduct extensive numerical studies using simulated and real data\nexamples."
                },
                "authors": [
                    {
                        "name": "Jiti Gao"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Bin Peng"
                    },
                    {
                        "name": "Yayi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yayi Yan"
                },
                "author": "Yayi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01993v2",
                "updated": "2025-02-12T09:25:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    25,
                    56,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-04T04:11:29Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    4,
                    11,
                    29,
                    1,
                    35,
                    0
                ],
                "title": "One Diffusion Step to Real-World Super-Resolution via Flow Trajectory\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Diffusion Step to Real-World Super-Resolution via Flow Trajectory\n  Distillation"
                },
                "summary": "Diffusion models (DMs) have significantly advanced the development of\nreal-world image super-resolution (Real-ISR), but the computational cost of\nmulti-step diffusion models limits their application. One-step diffusion models\ngenerate high-quality images in a one sampling step, greatly reducing\ncomputational overhead and inference latency. However, most existing one-step\ndiffusion methods are constrained by the performance of the teacher model,\nwhere poor teacher performance results in image artifacts. To address this\nlimitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique\nbased on flow matching models. We use the state-of-the-art diffusion model\nFLUX.1-dev as both the teacher model and the base model. First, we introduce\nFlow Trajectory Distillation (FTD) to distill a multi-step flow matching model\ninto a one-step Real-ISR. Second, to improve image realism and address\nhigh-frequency artifact issues in generated images, we propose TV-LPIPS as a\nperceptual loss and introduce Attention Diversification Loss (ADL) as a\nregularization term to reduce token similarity in transformer, thereby\neliminating high-frequency artifacts. Comprehensive experiments demonstrate\nthat our method outperforms existing one-step diffusion-based Real-ISR methods.\nThe code and model will be released at https://github.com/JianzeLi-114/FluxSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have significantly advanced the development of\nreal-world image super-resolution (Real-ISR), but the computational cost of\nmulti-step diffusion models limits their application. One-step diffusion models\ngenerate high-quality images in a one sampling step, greatly reducing\ncomputational overhead and inference latency. However, most existing one-step\ndiffusion methods are constrained by the performance of the teacher model,\nwhere poor teacher performance results in image artifacts. To address this\nlimitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique\nbased on flow matching models. We use the state-of-the-art diffusion model\nFLUX.1-dev as both the teacher model and the base model. First, we introduce\nFlow Trajectory Distillation (FTD) to distill a multi-step flow matching model\ninto a one-step Real-ISR. Second, to improve image realism and address\nhigh-frequency artifact issues in generated images, we propose TV-LPIPS as a\nperceptual loss and introduce Attention Diversification Loss (ADL) as a\nregularization term to reduce token similarity in transformer, thereby\neliminating high-frequency artifacts. Comprehensive experiments demonstrate\nthat our method outperforms existing one-step diffusion-based Real-ISR methods.\nThe code and model will be released at https://github.com/JianzeLi-114/FluxSR."
                },
                "authors": [
                    {
                        "name": "Jianze Li"
                    },
                    {
                        "name": "Jiezhang Cao"
                    },
                    {
                        "name": "Yong Guo"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08224v1",
                "updated": "2025-02-12T09:07:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    7,
                    25,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T09:07:25Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    7,
                    25,
                    2,
                    43,
                    0
                ],
                "title": "Flow-of-Action: SOP Enhanced LLM-Based Multi-Agent System for Root Cause\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-of-Action: SOP Enhanced LLM-Based Multi-Agent System for Root Cause\n  Analysis"
                },
                "summary": "In the realm of microservices architecture, the occurrence of frequent\nincidents necessitates the employment of Root Cause Analysis (RCA) for swift\nissue resolution. It is common that a serious incident can take several domain\nexperts hours to identify the root cause. Consequently, a contemporary trend\ninvolves harnessing Large Language Models (LLMs) as automated agents for RCA.\nThough the recent ReAct framework aligns well with the Site Reliability\nEngineers (SREs) for its thought-action-observation paradigm, its\nhallucinations often lead to irrelevant actions and directly affect subsequent\nresults. Additionally, the complex and variable clues of the incident can\noverwhelm the model one step further. To confront these challenges, we propose\nFlow-of-Action, a pioneering Standard Operation Procedure (SOP) enhanced\nLLM-based multi-agent system. By explicitly summarizing the diagnosis steps of\nSREs, SOP imposes constraints on LLMs at crucial junctures, guiding the RCA\nprocess towards the correct trajectory. To facilitate the rational and\neffective utilization of SOPs, we design an SOP-centric framework called SOP\nflow. SOP flow contains a series of tools, including one for finding relevant\nSOPs for incidents, another for automatically generating SOPs for incidents\nwithout relevant ones, and a tool for converting SOPs into code. This\nsignificantly alleviates the hallucination issues of ReAct in RCA tasks. We\nalso design multiple auxiliary agents to assist the main agent by removing\nuseless noise, narrowing the search space, and informing the main agent whether\nthe RCA procedure can stop. Compared to the ReAct method's 35.50% accuracy, our\nFlow-of-Action method achieves 64.01%, meeting the accuracy requirements for\nRCA in real-world systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of microservices architecture, the occurrence of frequent\nincidents necessitates the employment of Root Cause Analysis (RCA) for swift\nissue resolution. It is common that a serious incident can take several domain\nexperts hours to identify the root cause. Consequently, a contemporary trend\ninvolves harnessing Large Language Models (LLMs) as automated agents for RCA.\nThough the recent ReAct framework aligns well with the Site Reliability\nEngineers (SREs) for its thought-action-observation paradigm, its\nhallucinations often lead to irrelevant actions and directly affect subsequent\nresults. Additionally, the complex and variable clues of the incident can\noverwhelm the model one step further. To confront these challenges, we propose\nFlow-of-Action, a pioneering Standard Operation Procedure (SOP) enhanced\nLLM-based multi-agent system. By explicitly summarizing the diagnosis steps of\nSREs, SOP imposes constraints on LLMs at crucial junctures, guiding the RCA\nprocess towards the correct trajectory. To facilitate the rational and\neffective utilization of SOPs, we design an SOP-centric framework called SOP\nflow. SOP flow contains a series of tools, including one for finding relevant\nSOPs for incidents, another for automatically generating SOPs for incidents\nwithout relevant ones, and a tool for converting SOPs into code. This\nsignificantly alleviates the hallucination issues of ReAct in RCA tasks. We\nalso design multiple auxiliary agents to assist the main agent by removing\nuseless noise, narrowing the search space, and informing the main agent whether\nthe RCA procedure can stop. Compared to the ReAct method's 35.50% accuracy, our\nFlow-of-Action method achieves 64.01%, meeting the accuracy requirements for\nRCA in real-world systems."
                },
                "authors": [
                    {
                        "name": "Changhua Pei"
                    },
                    {
                        "name": "Zexin Wang"
                    },
                    {
                        "name": "Fengrui Liu"
                    },
                    {
                        "name": "Zeyan Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xiao He"
                    },
                    {
                        "name": "Rong Kang"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Jianhui Li"
                    },
                    {
                        "name": "Gaogang Xie"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "arxiv_comment": "Accepted by WWW'25 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07709v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07709v2",
                "updated": "2025-02-12T08:52:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    52,
                    52,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-11T17:08:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    8,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces"
                },
                "summary": "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces."
                },
                "authors": [
                    {
                        "name": "Loris Gaven"
                    },
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Clément Romac"
                    },
                    {
                        "name": "Cédric Colas"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07709v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07709v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14093v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14093v3",
                "updated": "2025-02-12T08:49:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    49,
                    34,
                    2,
                    43,
                    0
                ],
                "published": "2024-07-19T07:57:48Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    7,
                    57,
                    48,
                    4,
                    201,
                    0
                ],
                "title": "Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models"
                },
                "summary": "Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster."
                },
                "authors": [
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Zhaoxi Ke"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14093v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14093v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08213v1",
                "updated": "2025-02-12T08:48:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    48,
                    55,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T08:48:55Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    48,
                    55,
                    2,
                    43,
                    0
                ],
                "title": "LLM Modules: Knowledge Transfer from a Large to a Small Model using\n  Enhanced Cross-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Modules: Knowledge Transfer from a Large to a Small Model using\n  Enhanced Cross-Attention"
                },
                "summary": "In this work, we propose an architecture of LLM Modules that enables the\ntransfer of knowledge from a large pre-trained model to a smaller model using\nan Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B\nmodel is frozen and its representations are passed through specially designed\nattention layers to the GPT-Neo-125M model, which is trained on limited\ncomputational resources. Experimental results on the Bespoke-Stratos-17k\ndataset demonstrate that after 15 epochs of training, the combined model\ngenerates responses comparable in quality to those obtained by distillation. We\ndiscuss the advantages of the modular approach, provide examples of input\nqueries and comparative analysis, and outline prospects for further extension\nof the method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose an architecture of LLM Modules that enables the\ntransfer of knowledge from a large pre-trained model to a smaller model using\nan Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B\nmodel is frozen and its representations are passed through specially designed\nattention layers to the GPT-Neo-125M model, which is trained on limited\ncomputational resources. Experimental results on the Bespoke-Stratos-17k\ndataset demonstrate that after 15 epochs of training, the combined model\ngenerates responses comparable in quality to those obtained by distillation. We\ndiscuss the advantages of the modular approach, provide examples of input\nqueries and comparative analysis, and outline prospects for further extension\nof the method."
                },
                "authors": [
                    {
                        "name": "Konstantin Kolomeitsev"
                    }
                ],
                "author_detail": {
                    "name": "Konstantin Kolomeitsev"
                },
                "arxiv_affiliation": "Almaty, Kazakhstan",
                "author": "Konstantin Kolomeitsev",
                "arxiv_comment": "Code and pre-trained weights available at\n  https://huggingface.co/kkolomeitsev/llm-modules",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01863v2",
                "updated": "2025-02-12T08:30:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    30,
                    55,
                    2,
                    43,
                    0
                ],
                "published": "2024-03-04T09:19:50Z",
                "published_parsed": [
                    2024,
                    3,
                    4,
                    9,
                    19,
                    50,
                    0,
                    64,
                    0
                ],
                "title": "Schema-Based Query Optimisation for Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema-Based Query Optimisation for Graph Databases"
                },
                "summary": "Recursive graph queries are increasingly popular for extracting information\nfrom interconnected data found in various domains such as social networks, life\nsciences, and business analytics. Graph data often come with schema information\nthat describe how nodes and edges are organized. We propose a type inference\nmechanism that enriches recursive graph queries with relevant structural\ninformation contained in a graph schema. We show that this schema information\ncan be useful in order to improve the performance when evaluating acylic\nrecursive graph queries. Furthermore, we prove that the proposed method is\nsound and complete, ensuring that the semantics of the query is preserved\nduring the schema-enrichment process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive graph queries are increasingly popular for extracting information\nfrom interconnected data found in various domains such as social networks, life\nsciences, and business analytics. Graph data often come with schema information\nthat describe how nodes and edges are organized. We propose a type inference\nmechanism that enriches recursive graph queries with relevant structural\ninformation contained in a graph schema. We show that this schema information\ncan be useful in order to improve the performance when evaluating acylic\nrecursive graph queries. Furthermore, we prove that the proposed method is\nsound and complete, ensuring that the semantics of the query is preserved\nduring the schema-enrichment process."
                },
                "authors": [
                    {
                        "name": "Chandan Sharma"
                    },
                    {
                        "name": "Pierre Genevès"
                    },
                    {
                        "name": "Nils Gesbert"
                    },
                    {
                        "name": "Nabil Layaïda"
                    }
                ],
                "author_detail": {
                    "name": "Nabil Layaïda"
                },
                "arxiv_affiliation": "TYREX",
                "author": "Nabil Layaïda",
                "arxiv_journal_ref": "ACM SIGMOD International Conference on Management of Data, Jun\n  2025, Berlin, Germany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11914v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11914v4",
                "updated": "2025-02-12T08:28:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    28,
                    49,
                    2,
                    43,
                    0
                ],
                "published": "2023-08-23T04:59:21Z",
                "published_parsed": [
                    2023,
                    8,
                    23,
                    4,
                    59,
                    21,
                    2,
                    235,
                    0
                ],
                "title": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge\n  Reasoning via Promoting Causal Consistency in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge\n  Reasoning via Promoting Causal Consistency in LLMs"
                },
                "summary": "Despite the progress of foundation models, knowledge-based reasoning remains\na persistent challenge due to their limited capacity for knowledge recall and\ninference. Existing methods primarily focus on encouraging these models to plan\nand solve problems or extensively sample reasoning chains independently.\nHowever, these methods often overlook conceptual errors and inferential\nfallacies, inevitably leading to a series of notorious issues such as\nmisleading conclusions, cognitive biases, and reduced decision quality. While\nexplicit modeling of causality is argued to hold promise in addressing these\nissues, contemporary research efforts have thus far fallen short in achieving\ncausality-based foundation models. Drawing inspiration from the orchestration\nof diverse specialized agents collaborating to tackle intricate tasks, we\npropose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that\nharnesses multi-agent collaboration to bolster the faithfulness and causality\nof foundation models, involving a set of reasoners and evaluators. These agents\ncollaboratively work within a reasoning-and-consensus paradigm to improve\nfaithfulness. The reasoners are tasked with generating reasoning chains for\nknowledge-intensive problems by mimicking human causal reasoning. Meanwhile,\nthe evaluator scrutinizes the causal consistency of a reasoner's reasoning\nchain from a non-causal and a counterfactual perspective. Our framework\ndemonstrates significant superiority over state-of-the-art methods through\nextensive and comprehensive evaluations across text-based and multi-modal\nknowledge reasoning tasks (e.g., science question answering and commonsense\nreasoning).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the progress of foundation models, knowledge-based reasoning remains\na persistent challenge due to their limited capacity for knowledge recall and\ninference. Existing methods primarily focus on encouraging these models to plan\nand solve problems or extensively sample reasoning chains independently.\nHowever, these methods often overlook conceptual errors and inferential\nfallacies, inevitably leading to a series of notorious issues such as\nmisleading conclusions, cognitive biases, and reduced decision quality. While\nexplicit modeling of causality is argued to hold promise in addressing these\nissues, contemporary research efforts have thus far fallen short in achieving\ncausality-based foundation models. Drawing inspiration from the orchestration\nof diverse specialized agents collaborating to tackle intricate tasks, we\npropose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that\nharnesses multi-agent collaboration to bolster the faithfulness and causality\nof foundation models, involving a set of reasoners and evaluators. These agents\ncollaboratively work within a reasoning-and-consensus paradigm to improve\nfaithfulness. The reasoners are tasked with generating reasoning chains for\nknowledge-intensive problems by mimicking human causal reasoning. Meanwhile,\nthe evaluator scrutinizes the causal consistency of a reasoner's reasoning\nchain from a non-causal and a counterfactual perspective. Our framework\ndemonstrates significant superiority over state-of-the-art methods through\nextensive and comprehensive evaluations across text-based and multi-modal\nknowledge reasoning tasks (e.g., science question answering and commonsense\nreasoning)."
                },
                "authors": [
                    {
                        "name": "Ziyi Tang"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Weixing Chen"
                    },
                    {
                        "name": "Yongsen Zheng"
                    },
                    {
                        "name": "Zechuan Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Keze Wang"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "12 pages, 9 figures. 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11914v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11914v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03307v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03307v3",
                "updated": "2025-02-12T08:16:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    16,
                    44,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-05T16:08:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    8,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "Intent Alignment between Interaction and Language Spaces for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Alignment between Interaction and Language Spaces for\n  Recommendation"
                },
                "summary": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03307v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03307v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08640v1",
                "updated": "2025-02-12T18:55:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    55,
                    43,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T18:55:43Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    55,
                    43,
                    2,
                    43,
                    0
                ],
                "title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs"
                },
                "summary": "As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations."
                },
                "authors": [
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Xuwang Yin"
                    },
                    {
                        "name": "Rishub Tamirisa"
                    },
                    {
                        "name": "Jaehyuk Lim"
                    },
                    {
                        "name": "Bruce W. Lee"
                    },
                    {
                        "name": "Richard Ren"
                    },
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Norman Mu"
                    },
                    {
                        "name": "Adam Khoja"
                    },
                    {
                        "name": "Oliver Zhang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "author": "Dan Hendrycks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08638v1",
                "updated": "2025-02-12T18:54:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    54,
                    37,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T18:54:37Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    54,
                    37,
                    2,
                    43,
                    0
                ],
                "title": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples"
                },
                "summary": "The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that requires only a set of parallel sentence pairs of the\nlanguage pair of interest within the target domain. This task focuses on the\nability of a model to cross-lingually rank the true parallel sentence higher\nthan hard negatives generated by a large language model. We create four\ninstances of our introduced CLSD task for the language pair German-French\nwithin the domain of news. Within this case study, we find that models that are\nalso fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using\nEnglish as the pivot language, while bitext mining models such as LaBSE perform\nbest directly cross-lingually. We also show a fine-grained similarity analysis\nenabled by our distractor generation strategy, indicating that different\nembedding models are sensitive to different types of perturbations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that requires only a set of parallel sentence pairs of the\nlanguage pair of interest within the target domain. This task focuses on the\nability of a model to cross-lingually rank the true parallel sentence higher\nthan hard negatives generated by a large language model. We create four\ninstances of our introduced CLSD task for the language pair German-French\nwithin the domain of news. Within this case study, we find that models that are\nalso fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using\nEnglish as the pivot language, while bitext mining models such as LaBSE perform\nbest directly cross-lingually. We also show a fine-grained similarity analysis\nenabled by our distractor generation strategy, indicating that different\nembedding models are sensitive to different types of perturbations."
                },
                "authors": [
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08631v1",
                "updated": "2025-02-12T18:42:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    42,
                    42,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T18:42:42Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    42,
                    42,
                    2,
                    43,
                    0
                ],
                "title": "Ensemble based approach to quantifying uncertainty of LLM based\n  classifications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble based approach to quantifying uncertainty of LLM based\n  classifications"
                },
                "summary": "The output of Large Language Models (LLMs) are a function of the internal\nmodel's parameters and the input provided into the context window. The\nhypothesis presented here is that under a greedy sampling strategy the variance\nin the LLM's output is a function of the conceptual certainty embedded in the\nmodel's parametric knowledge, as well as the lexical variance in the input.\nFinetuning the model results in reducing the sensitivity of the model output to\nthe lexical input variations. This is then applied to a classification problem\nand a probabilistic method is proposed for estimating the certainties of the\npredicted classes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The output of Large Language Models (LLMs) are a function of the internal\nmodel's parameters and the input provided into the context window. The\nhypothesis presented here is that under a greedy sampling strategy the variance\nin the LLM's output is a function of the conceptual certainty embedded in the\nmodel's parametric knowledge, as well as the lexical variance in the input.\nFinetuning the model results in reducing the sensitivity of the model output to\nthe lexical input variations. This is then applied to a classification problem\nand a probabilistic method is proposed for estimating the certainties of the\npredicted classes."
                },
                "authors": [
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Ahmed Salhin"
                    },
                    {
                        "name": "Josh Frazier"
                    },
                    {
                        "name": "Rohit Kumar"
                    },
                    {
                        "name": "Yu-Cheng Tsai"
                    },
                    {
                        "name": "Todd Cook"
                    }
                ],
                "author_detail": {
                    "name": "Todd Cook"
                },
                "author": "Todd Cook",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04328v2",
                "updated": "2025-02-12T18:40:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    40,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-06T18:59:55Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    59,
                    55,
                    3,
                    37,
                    0
                ],
                "title": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment"
                },
                "summary": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola."
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Yongming Rao"
                    }
                ],
                "author_detail": {
                    "name": "Yongming Rao"
                },
                "author": "Yongming Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04689v2",
                "updated": "2025-02-12T18:36:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    36,
                    24,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-07T06:30:33Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    30,
                    33,
                    4,
                    38,
                    0
                ],
                "title": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning"
                },
                "summary": "Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR."
                },
                "authors": [
                    {
                        "name": "Yuwei Yin"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Carenini"
                },
                "author": "Giuseppe Carenini",
                "arxiv_comment": "20 pages. Code: https://github.com/YuweiYin/ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08621v1",
                "updated": "2025-02-12T18:16:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    16,
                    11,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T18:16:11Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    16,
                    11,
                    2,
                    43,
                    0
                ],
                "title": "SportsBuddy: Designing and Evaluating an AI-Powered Sports Video\n  Storytelling Tool Through Real-World Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SportsBuddy: Designing and Evaluating an AI-Powered Sports Video\n  Storytelling Tool Through Real-World Deployment"
                },
                "summary": "Video storytelling is essential for sports performance analysis and fan\nengagement, enabling sports professionals and fans to effectively communicate\nand interpret the spatial and temporal dynamics of gameplay. Traditional\nmethods rely on manual annotation and verbal explanations, placing significant\ndemands on creators for video editing skills and on viewers for cognitive\nfocus. However, these approaches are time-consuming and often struggle to\naccommodate individual needs. SportsBuddy addresses this gap with an intuitive,\ninteractive video authoring tool. It combines player tracking, embedded\ninteraction design, and timeline visualizations to seamlessly integrate\nnarratives and visual cues within game contexts. This empowers users to\neffortlessly create context-driven video stories. Since its launch, over 150\nsports users, including coaches, athletes, content creators, parents and fans,\nhave utilized SportsBuddy to produce compelling game highlights for diverse use\ncases. User feedback highlights its accessibility and ease of use, making video\nstorytelling and insight communication more attainable for diverse audiences.\nCase studies with collegiate teams and sports creators further demonstrate\nSportsBuddy's impact on enhancing coaching communication, game analysis, and\nfan engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video storytelling is essential for sports performance analysis and fan\nengagement, enabling sports professionals and fans to effectively communicate\nand interpret the spatial and temporal dynamics of gameplay. Traditional\nmethods rely on manual annotation and verbal explanations, placing significant\ndemands on creators for video editing skills and on viewers for cognitive\nfocus. However, these approaches are time-consuming and often struggle to\naccommodate individual needs. SportsBuddy addresses this gap with an intuitive,\ninteractive video authoring tool. It combines player tracking, embedded\ninteraction design, and timeline visualizations to seamlessly integrate\nnarratives and visual cues within game contexts. This empowers users to\neffortlessly create context-driven video stories. Since its launch, over 150\nsports users, including coaches, athletes, content creators, parents and fans,\nhave utilized SportsBuddy to produce compelling game highlights for diverse use\ncases. User feedback highlights its accessibility and ease of use, making video\nstorytelling and insight communication more attainable for diverse audiences.\nCase studies with collegiate teams and sports creators further demonstrate\nSportsBuddy's impact on enhancing coaching communication, game analysis, and\nfan engagement."
                },
                "authors": [
                    {
                        "name": "Tica Lin"
                    },
                    {
                        "name": "Ruxun Xiang"
                    },
                    {
                        "name": "Gardenia Liu"
                    },
                    {
                        "name": "Divyanshu Tiwari"
                    },
                    {
                        "name": "Meng-Chia Chiang"
                    },
                    {
                        "name": "Chenjiayi Ye"
                    },
                    {
                        "name": "Hanspeter Pfister"
                    },
                    {
                        "name": "Chen Zhu-Tien"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhu-Tien"
                },
                "author": "Chen Zhu-Tien",
                "arxiv_comment": "Accepted at PacificVIS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08599v1",
                "updated": "2025-02-12T17:38:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    38,
                    27,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T17:38:27Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    38,
                    27,
                    2,
                    43,
                    0
                ],
                "title": "SPeCtrum: A Grounded Framework for Multidimensional Identity\n  Representation in LLM-Based Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPeCtrum: A Grounded Framework for Multidimensional Identity\n  Representation in LLM-Based Agent"
                },
                "summary": "Existing methods for simulating individual identities often oversimplify\nhuman complexity, which may lead to incomplete or flattened representations. To\naddress this, we introduce SPeCtrum, a grounded framework for constructing\nauthentic LLM agent personas by incorporating an individual's multidimensional\nself-concept. SPeCtrum integrates three core components: Social Identity (S),\nPersonal Identity (P), and Personal Life Context (C), each contributing\ndistinct yet interconnected aspects of identity. To evaluate SPeCtrum's\neffectiveness in identity representation, we conducted automated and human\nevaluations. Automated evaluations using popular drama characters showed that\nPersonal Life Context (C)-derived from short essays on preferences and daily\nroutines-modeled characters' identities more effectively than Social Identity\n(S) and Personal Identity (P) alone and performed comparably to the full SPC\ncombination. In contrast, human evaluations involving real-world individuals\nfound that the full SPC combination provided a more comprehensive self-concept\nrepresentation than C alone. Our findings suggest that while C alone may\nsuffice for basic identity simulation, integrating S, P, and C enhances the\nauthenticity and accuracy of real-world identity representation. Overall,\nSPeCtrum offers a structured approach for simulating individuals in LLM agents,\nenabling more personalized human-AI interactions and improving the realism of\nsimulation-based behavioral studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods for simulating individual identities often oversimplify\nhuman complexity, which may lead to incomplete or flattened representations. To\naddress this, we introduce SPeCtrum, a grounded framework for constructing\nauthentic LLM agent personas by incorporating an individual's multidimensional\nself-concept. SPeCtrum integrates three core components: Social Identity (S),\nPersonal Identity (P), and Personal Life Context (C), each contributing\ndistinct yet interconnected aspects of identity. To evaluate SPeCtrum's\neffectiveness in identity representation, we conducted automated and human\nevaluations. Automated evaluations using popular drama characters showed that\nPersonal Life Context (C)-derived from short essays on preferences and daily\nroutines-modeled characters' identities more effectively than Social Identity\n(S) and Personal Identity (P) alone and performed comparably to the full SPC\ncombination. In contrast, human evaluations involving real-world individuals\nfound that the full SPC combination provided a more comprehensive self-concept\nrepresentation than C alone. Our findings suggest that while C alone may\nsuffice for basic identity simulation, integrating S, P, and C enhances the\nauthenticity and accuracy of real-world identity representation. Overall,\nSPeCtrum offers a structured approach for simulating individuals in LLM agents,\nenabling more personalized human-AI interactions and improving the realism of\nsimulation-based behavioral studies."
                },
                "authors": [
                    {
                        "name": "Keyeun Lee"
                    },
                    {
                        "name": "Seo Hyeong Kim"
                    },
                    {
                        "name": "Seolhee Lee"
                    },
                    {
                        "name": "Jinsu Eun"
                    },
                    {
                        "name": "Yena Ko"
                    },
                    {
                        "name": "Hayeon Jeon"
                    },
                    {
                        "name": "Esther Hehsun Kim"
                    },
                    {
                        "name": "Seonghye Cho"
                    },
                    {
                        "name": "Soeun Yang"
                    },
                    {
                        "name": "Eun-mee Kim"
                    },
                    {
                        "name": "Hajin Lim"
                    }
                ],
                "author_detail": {
                    "name": "Hajin Lim"
                },
                "author": "Hajin Lim",
                "arxiv_comment": "21 pages, 8 figures, 5 tables, Accepted in NAACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07313v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07313v3",
                "updated": "2025-02-12T17:20:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    20,
                    56,
                    2,
                    43,
                    0
                ],
                "published": "2024-07-10T02:20:19Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    2,
                    20,
                    19,
                    2,
                    192,
                    0
                ],
                "title": "ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the\n  Age of Large Language Models"
                },
                "summary": "The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics -- Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) -- suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of Text-to-SQL enables anyone to retrieve information from SQL\ndatabases using natural language. While this task has made substantial\nprogress, the two primary evaluation metrics -- Execution Accuracy (EXE) and\nExact Set Matching Accuracy (ESM) -- suffer from inherent limitations that can\nmisrepresent performance. Specifically, ESM's rigid matching overlooks\nsemantically correct but stylistically different queries, whereas EXE can\noverestimate correctness by ignoring structural errors that yield correct\noutputs. These shortcomings become especially problematic when assessing\noutputs from large language model (LLM)-based approaches without fine-tuning,\nwhich vary more in style and structure compared to their fine-tuned\ncounterparts. Thus, we introduce a new metric, Enhanced Tree Matching (ETM),\nwhich mitigates these issues by comparing queries using both syntactic and\nsemantic elements. Through evaluating nine LLM-based models, we show that EXE\nand ESM can produce false positive and negative rates as high as 23.0% and\n28.9%, while ETM reduces these rates to 0.3% and 2.7%, respectively. We release\nour ETM script as open source, offering the community a more robust and\nreliable approach to evaluating Text-to-SQL."
                },
                "authors": [
                    {
                        "name": "Benjamin G. Ascoli"
                    },
                    {
                        "name": "Yasoda Sai Ram Kandikonda"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07313v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07313v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08586v1",
                "updated": "2025-02-12T17:19:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    19,
                    36,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T17:19:36Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    19,
                    36,
                    2,
                    43,
                    0
                ],
                "title": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous\n  Attacks"
                },
                "summary": "A high volume of recent ML security literature focuses on attacks against\naligned large language models (LLMs). These attacks may extract private\ninformation or coerce the model into producing harmful outputs. In real-world\ndeployments, LLMs are often part of a larger agentic pipeline including memory\nsystems, retrieval, web access, and API calling. Such additional components\nintroduce vulnerabilities that make these LLM-powered agents much easier to\nattack than isolated LLMs, yet relatively little work focuses on the security\nof LLM agents. In this paper, we analyze security and privacy vulnerabilities\nthat are unique to LLM agents. We first provide a taxonomy of attacks\ncategorized by threat actors, objectives, entry points, attacker observability,\nattack strategies, and inherent vulnerabilities of agent pipelines. We then\nconduct a series of illustrative attacks on popular open-source and commercial\nagents, demonstrating the immediate practical implications of their\nvulnerabilities. Notably, our attacks are trivial to implement and require no\nunderstanding of machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A high volume of recent ML security literature focuses on attacks against\naligned large language models (LLMs). These attacks may extract private\ninformation or coerce the model into producing harmful outputs. In real-world\ndeployments, LLMs are often part of a larger agentic pipeline including memory\nsystems, retrieval, web access, and API calling. Such additional components\nintroduce vulnerabilities that make these LLM-powered agents much easier to\nattack than isolated LLMs, yet relatively little work focuses on the security\nof LLM agents. In this paper, we analyze security and privacy vulnerabilities\nthat are unique to LLM agents. We first provide a taxonomy of attacks\ncategorized by threat actors, objectives, entry points, attacker observability,\nattack strategies, and inherent vulnerabilities of agent pipelines. We then\nconduct a series of illustrative attacks on popular open-source and commercial\nagents, demonstrating the immediate practical implications of their\nvulnerabilities. Notably, our attacks are trivial to implement and require no\nunderstanding of machine learning."
                },
                "authors": [
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Yin Zhou"
                    },
                    {
                        "name": "Vethavikashini Chithrra Raghuram"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Micah Goldblum"
                    }
                ],
                "author_detail": {
                    "name": "Micah Goldblum"
                },
                "author": "Micah Goldblum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05905v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05905v5",
                "updated": "2025-02-12T17:10:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    53,
                    2,
                    43,
                    0
                ],
                "published": "2024-05-09T17:01:31Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    17,
                    1,
                    31,
                    3,
                    130,
                    0
                ],
                "title": "Truthful Aggregation of LLMs with an Application to Online Advertising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truthful Aggregation of LLMs with an Application to Online Advertising"
                },
                "summary": "The next frontier of online advertising is revenue generation from\nLLM-generated content. We consider a setting where advertisers aim to influence\nthe responses of an LLM to align with their interests, while platforms seek to\nmaximize advertiser value and ensure user satisfaction. The challenge is that\nadvertisers' preferences generally conflict with those of the user, and\nadvertisers may misreport their preferences. To address this, we introduce\nMOSAIC, an auction mechanism that ensures that truthful reporting is a dominant\nstrategy for advertisers and that aligns the utility of each advertiser with\ntheir contribution to social welfare. Importantly, the mechanism operates\nwithout LLM fine-tuning or access to model weights and provably converges to\nthe output of the optimally fine-tuned LLM as computational resources increase.\nAdditionally, it can incorporate contextual information about advertisers,\nwhich significantly improves social welfare. Through experiments with a\npublicly available LLM, we show that MOSAIC leads to high advertiser value and\nplatform revenue with low computational overhead. While our motivating\napplication is online advertising, our mechanism can be applied in any setting\nwith monetary transfers, making it a general-purpose solution for truthfully\naggregating the preferences of self-interested agents over LLM-generated\nreplies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next frontier of online advertising is revenue generation from\nLLM-generated content. We consider a setting where advertisers aim to influence\nthe responses of an LLM to align with their interests, while platforms seek to\nmaximize advertiser value and ensure user satisfaction. The challenge is that\nadvertisers' preferences generally conflict with those of the user, and\nadvertisers may misreport their preferences. To address this, we introduce\nMOSAIC, an auction mechanism that ensures that truthful reporting is a dominant\nstrategy for advertisers and that aligns the utility of each advertiser with\ntheir contribution to social welfare. Importantly, the mechanism operates\nwithout LLM fine-tuning or access to model weights and provably converges to\nthe output of the optimally fine-tuned LLM as computational resources increase.\nAdditionally, it can incorporate contextual information about advertisers,\nwhich significantly improves social welfare. Through experiments with a\npublicly available LLM, we show that MOSAIC leads to high advertiser value and\nplatform revenue with low computational overhead. While our motivating\napplication is online advertising, our mechanism can be applied in any setting\nwith monetary transfers, making it a general-purpose solution for truthfully\naggregating the preferences of self-interested agents over LLM-generated\nreplies."
                },
                "authors": [
                    {
                        "name": "Ermis Soumalias"
                    },
                    {
                        "name": "Michael J. Curry"
                    },
                    {
                        "name": "Sven Seuken"
                    }
                ],
                "author_detail": {
                    "name": "Sven Seuken"
                },
                "author": "Sven Seuken",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05905v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05905v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08577v1",
                "updated": "2025-02-12T17:10:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    53,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T17:10:53Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    53,
                    2,
                    43,
                    0
                ],
                "title": "FBFL: A Field-Based Coordination Approach for Data Heterogeneity in\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FBFL: A Field-Based Coordination Approach for Data Heterogeneity in\n  Federated Learning"
                },
                "summary": "In the last years, Federated learning (FL) has become a popular solution to\ntrain machine learning models in domains with high privacy concerns. However,\nFL scalability and performance face significant challenges in real-world\ndeployments where data across devices are non-independently and identically\ndistributed (non-IID). The heterogeneity in data distribution frequently arises\nfrom spatial distribution of devices, leading to degraded model performance in\nthe absence of proper handling. Additionally, FL typical reliance on\ncentralized architectures introduces bottlenecks and single-point-of-failure\nrisks, particularly problematic at scale or in dynamic environments. To close\nthis gap, we propose Field-Based Federated Learning (FBFL), a novel approach\nleveraging macroprogramming and field coordination to address these limitations\nthrough: (i) distributed spatial-based leader election for personalization to\nmitigate non-IID data challenges; and (ii) construction of a self-organizing,\nhierarchical architecture using advanced macroprogramming patterns. Moreover,\nFBFL not only overcomes the aforementioned limitations, but also enables the\ndevelopment of more specialized models tailored to the specific data\ndistribution in each subregion. This paper formalizes FBFL and evaluates it\nextensively using MNIST, FashionMNIST, and Extended MNIST datasets. We\ndemonstrate that, when operating under IID data conditions, FBFL performs\ncomparably to the widely-used FedAvg algorithm. Furthermore, in challenging\nnon-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other\nstate-of-the-art methods, namely FedProx and Scaffold, which have been\nspecifically designed to address non-IID data distributions. Additionally, we\nshowcase the resilience of FBFL's self-organizing hierarchical architecture\nagainst server failures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last years, Federated learning (FL) has become a popular solution to\ntrain machine learning models in domains with high privacy concerns. However,\nFL scalability and performance face significant challenges in real-world\ndeployments where data across devices are non-independently and identically\ndistributed (non-IID). The heterogeneity in data distribution frequently arises\nfrom spatial distribution of devices, leading to degraded model performance in\nthe absence of proper handling. Additionally, FL typical reliance on\ncentralized architectures introduces bottlenecks and single-point-of-failure\nrisks, particularly problematic at scale or in dynamic environments. To close\nthis gap, we propose Field-Based Federated Learning (FBFL), a novel approach\nleveraging macroprogramming and field coordination to address these limitations\nthrough: (i) distributed spatial-based leader election for personalization to\nmitigate non-IID data challenges; and (ii) construction of a self-organizing,\nhierarchical architecture using advanced macroprogramming patterns. Moreover,\nFBFL not only overcomes the aforementioned limitations, but also enables the\ndevelopment of more specialized models tailored to the specific data\ndistribution in each subregion. This paper formalizes FBFL and evaluates it\nextensively using MNIST, FashionMNIST, and Extended MNIST datasets. We\ndemonstrate that, when operating under IID data conditions, FBFL performs\ncomparably to the widely-used FedAvg algorithm. Furthermore, in challenging\nnon-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other\nstate-of-the-art methods, namely FedProx and Scaffold, which have been\nspecifically designed to address non-IID data distributions. Additionally, we\nshowcase the resilience of FBFL's self-organizing hierarchical architecture\nagainst server failures."
                },
                "authors": [
                    {
                        "name": "Davide Domini"
                    },
                    {
                        "name": "Gianluca Aguzzi"
                    },
                    {
                        "name": "Lukas Esterle"
                    },
                    {
                        "name": "Mirko Viroli"
                    }
                ],
                "author_detail": {
                    "name": "Mirko Viroli"
                },
                "author": "Mirko Viroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08576v1",
                "updated": "2025-02-12T17:10:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    34,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T17:10:34Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    34,
                    2,
                    43,
                    0
                ],
                "title": "Mapping the Landscape of Generative AI in Network Monitoring and\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping the Landscape of Generative AI in Network Monitoring and\n  Management"
                },
                "summary": "Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management."
                },
                "authors": [
                    {
                        "name": "Giampaolo Bovenzi"
                    },
                    {
                        "name": "Francesco Cerasuolo"
                    },
                    {
                        "name": "Domenico Ciuonzo"
                    },
                    {
                        "name": "Davide Di Monda"
                    },
                    {
                        "name": "Idio Guarino"
                    },
                    {
                        "name": "Antonio Montieri"
                    },
                    {
                        "name": "Valerio Persico"
                    },
                    {
                        "name": "Antonio Pescapè"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Pescapè"
                },
                "author": "Antonio Pescapè",
                "arxiv_comment": "32 pages, 9 figure, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20163v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20163v3",
                "updated": "2025-02-12T16:49:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    49,
                    56,
                    2,
                    43,
                    0
                ],
                "published": "2024-12-28T14:27:45Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    27,
                    45,
                    5,
                    363,
                    0
                ],
                "title": "Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems"
                },
                "summary": "The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs."
                },
                "authors": [
                    {
                        "name": "Minhye Jeon"
                    },
                    {
                        "name": "Seokho Ahn"
                    },
                    {
                        "name": "Young-Duk Seo"
                    }
                ],
                "author_detail": {
                    "name": "Young-Duk Seo"
                },
                "author": "Young-Duk Seo",
                "arxiv_doi": "10.1145/3672608.3707958",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3672608.3707958",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20163v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20163v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04686v3",
                "updated": "2025-02-12T16:49:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    49,
                    50,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-08T18:49:41Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    49,
                    41,
                    2,
                    8,
                    0
                ],
                "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics"
                },
                "summary": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH."
                },
                "authors": [
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Zhuofan Zheng"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Xinzhe Ni"
                    },
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Yujiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yujiu Yang"
                },
                "author": "Yujiu Yang",
                "arxiv_comment": "Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19702v2",
                "updated": "2025-02-12T16:47:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    47,
                    30,
                    2,
                    43,
                    0
                ],
                "published": "2024-10-25T17:19:55Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    19,
                    55,
                    4,
                    299,
                    0
                ],
                "title": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zeng"
                    },
                    {
                        "name": "Kunchang Li"
                    },
                    {
                        "name": "Chenting Wang"
                    },
                    {
                        "name": "Xinhao Li"
                    },
                    {
                        "name": "Tianxiang Jiang"
                    },
                    {
                        "name": "Ziang Yan"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Yansong Shi"
                    },
                    {
                        "name": "Zhengrong Yue"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yali Wang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08557v1",
                "updated": "2025-02-12T16:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    39,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:39:06Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    39,
                    6,
                    2,
                    43,
                    0
                ],
                "title": "QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval"
                },
                "summary": "Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Seunghyun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seunghyun Lee"
                },
                "author": "Seunghyun Lee",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08554v1",
                "updated": "2025-02-12T16:35:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    35,
                    41,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:35:41Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    35,
                    41,
                    2,
                    43,
                    0
                ],
                "title": "Fostering Appropriate Reliance on Large Language Models: The Role of\n  Explanations, Sources, and Inconsistencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fostering Appropriate Reliance on Large Language Models: The Role of\n  Explanations, Sources, and Inconsistencies"
                },
                "summary": "Large language models (LLMs) can produce erroneous responses that sound\nfluent and convincing, raising the risk that users will rely on these responses\nas if they were correct. Mitigating such overreliance is a key challenge.\nThrough a think-aloud study in which participants use an LLM-infused\napplication to answer objective questions, we identify several features of LLM\nresponses that shape users' reliance: explanations (supporting details for\nanswers), inconsistencies in explanations, and sources. Through a large-scale,\npre-registered, controlled experiment (N=308), we isolate and study the effects\nof these features on users' reliance, accuracy, and other measures. We find\nthat the presence of explanations increases reliance on both correct and\nincorrect responses. However, we observe less reliance on incorrect responses\nwhen sources are provided or when explanations exhibit inconsistencies. We\ndiscuss the implications of these findings for fostering appropriate reliance\non LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can produce erroneous responses that sound\nfluent and convincing, raising the risk that users will rely on these responses\nas if they were correct. Mitigating such overreliance is a key challenge.\nThrough a think-aloud study in which participants use an LLM-infused\napplication to answer objective questions, we identify several features of LLM\nresponses that shape users' reliance: explanations (supporting details for\nanswers), inconsistencies in explanations, and sources. Through a large-scale,\npre-registered, controlled experiment (N=308), we isolate and study the effects\nof these features on users' reliance, accuracy, and other measures. We find\nthat the presence of explanations increases reliance on both correct and\nincorrect responses. However, we observe less reliance on incorrect responses\nwhen sources are provided or when explanations exhibit inconsistencies. We\ndiscuss the implications of these findings for fostering appropriate reliance\non LLMs."
                },
                "authors": [
                    {
                        "name": "Sunnie S. Y. Kim"
                    },
                    {
                        "name": "Jennifer Wortman Vaughan"
                    },
                    {
                        "name": "Q. Vera Liao"
                    },
                    {
                        "name": "Tania Lombrozo"
                    },
                    {
                        "name": "Olga Russakovsky"
                    }
                ],
                "author_detail": {
                    "name": "Olga Russakovsky"
                },
                "author": "Olga Russakovsky",
                "arxiv_doi": "10.1145/3706598.3714020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "CHI 2025. This version includes the appendix",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08550v1",
                "updated": "2025-02-12T16:31:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    31,
                    21,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:31:21Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    31,
                    21,
                    2,
                    43,
                    0
                ],
                "title": "LLMs can implicitly learn from mistakes in-context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can implicitly learn from mistakes in-context"
                },
                "summary": "Learning from mistakes is a fundamental feature of human intelligence.\nPrevious work has shown that Large Language Models (LLMs) can also learn from\nincorrect answers when provided with a comprehensive rationale detailing why an\nanswer is wrong or how to correct it. In this work, we examine whether LLMs can\nlearn from mistakes in mathematical reasoning tasks when these explanations are\nnot provided. We investigate if LLMs are able to implicitly infer such\nrationales simply from observing both incorrect and correct answers.\nSurprisingly, we find that LLMs perform better, on average, when rationales are\neliminated from the context and incorrect answers are simply shown alongside\ncorrect ones. This approach also substantially outperforms chain-of-thought\nprompting in our evaluations. We show that these results are consistent across\nLLMs of different sizes and varying reasoning abilities. Further, we carry out\nan in-depth analysis, and show that prompting with both wrong and correct\nanswers leads to greater performance and better generalisation than introducing\nadditional, more diverse question-answer pairs into the context. Finally, we\nshow that new rationales generated by models that have only observed incorrect\nand correct answers are scored equally as highly by humans as those produced\nwith the aid of exemplar rationales. Our results demonstrate that LLMs are\nindeed capable of in-context implicit learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from mistakes is a fundamental feature of human intelligence.\nPrevious work has shown that Large Language Models (LLMs) can also learn from\nincorrect answers when provided with a comprehensive rationale detailing why an\nanswer is wrong or how to correct it. In this work, we examine whether LLMs can\nlearn from mistakes in mathematical reasoning tasks when these explanations are\nnot provided. We investigate if LLMs are able to implicitly infer such\nrationales simply from observing both incorrect and correct answers.\nSurprisingly, we find that LLMs perform better, on average, when rationales are\neliminated from the context and incorrect answers are simply shown alongside\ncorrect ones. This approach also substantially outperforms chain-of-thought\nprompting in our evaluations. We show that these results are consistent across\nLLMs of different sizes and varying reasoning abilities. Further, we carry out\nan in-depth analysis, and show that prompting with both wrong and correct\nanswers leads to greater performance and better generalisation than introducing\nadditional, more diverse question-answer pairs into the context. Finally, we\nshow that new rationales generated by models that have only observed incorrect\nand correct answers are scored equally as highly by humans as those produced\nwith the aid of exemplar rationales. Our results demonstrate that LLMs are\nindeed capable of in-context implicit learning."
                },
                "authors": [
                    {
                        "name": "Lisa Alazraki"
                    },
                    {
                        "name": "Maximilian Mozes"
                    },
                    {
                        "name": "Jon Ander Campos"
                    },
                    {
                        "name": "Yi Chern Tan"
                    },
                    {
                        "name": "Marek Rei"
                    },
                    {
                        "name": "Max Bartolo"
                    }
                ],
                "author_detail": {
                    "name": "Max Bartolo"
                },
                "author": "Max Bartolo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08529v1",
                "updated": "2025-02-12T16:07:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    7,
                    11,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:07:11Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    7,
                    11,
                    2,
                    43,
                    0
                ],
                "title": "Testbed Development: An Intelligent O-RAN based Cell-Free MIMO Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testbed Development: An Intelligent O-RAN based Cell-Free MIMO Network"
                },
                "summary": "Cell-free multiple input multiple output (CF-MIMO) systems improve spectral\nand energy efficiencies using distributed access points (APs) to provide\nreliable service across an area equivalent to multiple conventional cells. This\npaper presents a novel design and implementation of a CF-MIMO network\nleveraging the open radio access network (O-RAN) architecture based testbed to\nenhance the performance of interference-prone user. The proposed prototype is\ndeveloped based on open source software components and unlike many other\nprototypes, our testbed is able to serve commercial 5G user equipment (UE). The\nRAN intelligent controller (RIC) allows the cell-free (CF) network to access\nthe embedded artificial intelligence and benefit from the network optimisation\ntechniques that O-RAN brings. The testbed includes an intelligent antenna\nassociation xApp which determines the antenna group that serves each UE based\non the live key performance measurements. The paper demonstrates the deployment\nand operation of the CF network and the xApp and discusses how the CF networks\ncan benefit from the O-RAN architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free multiple input multiple output (CF-MIMO) systems improve spectral\nand energy efficiencies using distributed access points (APs) to provide\nreliable service across an area equivalent to multiple conventional cells. This\npaper presents a novel design and implementation of a CF-MIMO network\nleveraging the open radio access network (O-RAN) architecture based testbed to\nenhance the performance of interference-prone user. The proposed prototype is\ndeveloped based on open source software components and unlike many other\nprototypes, our testbed is able to serve commercial 5G user equipment (UE). The\nRAN intelligent controller (RIC) allows the cell-free (CF) network to access\nthe embedded artificial intelligence and benefit from the network optimisation\ntechniques that O-RAN brings. The testbed includes an intelligent antenna\nassociation xApp which determines the antenna group that serves each UE based\non the live key performance measurements. The paper demonstrates the deployment\nand operation of the CF network and the xApp and discusses how the CF networks\ncan benefit from the O-RAN architecture."
                },
                "authors": [
                    {
                        "name": "Yi Chu"
                    },
                    {
                        "name": "Mostafa Rahmani"
                    },
                    {
                        "name": "Josh Shackleton"
                    },
                    {
                        "name": "David Grace"
                    },
                    {
                        "name": "Kanapathippillai Cumanan"
                    },
                    {
                        "name": "Hamed Ahmadi"
                    },
                    {
                        "name": "Alister Burr"
                    }
                ],
                "author_detail": {
                    "name": "Alister Burr"
                },
                "author": "Alister Burr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08524v1",
                "updated": "2025-02-12T16:00:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    0,
                    11,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T16:00:11Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    16,
                    0,
                    11,
                    2,
                    43,
                    0
                ],
                "title": "LLM Pretraining with Continuous Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Pretraining with Continuous Concepts"
                },
                "summary": "Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process."
                },
                "authors": [
                    {
                        "name": "Jihoon Tack"
                    },
                    {
                        "name": "Jack Lanchantin"
                    },
                    {
                        "name": "Jane Yu"
                    },
                    {
                        "name": "Andrew Cohen"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Janice Lan"
                    },
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Xian Li"
                    }
                ],
                "author_detail": {
                    "name": "Xian Li"
                },
                "author": "Xian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06766v2",
                "updated": "2025-02-12T15:55:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    55,
                    37,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-10T18:47:04Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    47,
                    4,
                    0,
                    41,
                    0
                ],
                "title": "Exploiting Sparsity for Long Context Inference: Million Token Contexts\n  on Commodity GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Sparsity for Long Context Inference: Million Token Contexts\n  on Commodity GPUs"
                },
                "summary": "There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common benchmarks (RULER,\nAlpacaEval, and Open LLM Leaderboard).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing demand for performing inference with hundreds of thousands\nof input tokens on trained transformer models. Inference at this extreme scale\ndemands significant computational resources, hindering the application of\ntransformers at long contexts on commodity (i.e not data center scale)\nhardware. To address the inference time costs associated with running\nself-attention based transformer language models on long contexts and enable\ntheir adoption on widely available hardware, we propose a tunable mechanism\nthat reduces the cost of the forward pass by attending to only the most\nrelevant tokens at every generation step using a top-k selection mechanism. We\nshowcase the efficiency gains afforded by our method by performing inference on\ncontext windows up to 1M tokens using approximately 16GB of GPU RAM. Our\nexperiments reveal that models are capable of handling the sparsity induced by\nthe reduced number of keys and values. By attending to less than 2% of input\ntokens, we achieve over 95% of model performance on common benchmarks (RULER,\nAlpacaEval, and Open LLM Leaderboard)."
                },
                "authors": [
                    {
                        "name": "Ryan Synk"
                    },
                    {
                        "name": "Monte Hoover"
                    },
                    {
                        "name": "John Kirchenbauer"
                    },
                    {
                        "name": "Neel Jain"
                    },
                    {
                        "name": "Alex Stein"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "Josue Melendez Sanchez"
                    },
                    {
                        "name": "Ramani Duraiswami"
                    },
                    {
                        "name": "Tom Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Tom Goldstein"
                },
                "author": "Tom Goldstein",
                "arxiv_comment": "9 pages, 9 figures, 2 tables in main body",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08515v1",
                "updated": "2025-02-12T15:47:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    47,
                    48,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:47:48Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    47,
                    48,
                    2,
                    43,
                    0
                ],
                "title": "The Paradox of Stochasticity: Limited Creativity and Computational\n  Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Paradox of Stochasticity: Limited Creativity and Computational\n  Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data"
                },
                "summary": "This study examines how temperature settings and model architectures affect\nthe generation of structured fictional data (names, birthdates) across three\nlarge language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest.\nBy systematically testing temperature values from 0.0 to 1.0 in increments of\n0.1, we conducted 330 trials yielding 889 structured entities, validated for\nsyntactic consistency. Key findings reveal that model architecture\nsignificantly influences computational efficiency, with mistral:latest and\nllama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to\nexpectations, temperature showed no correlation with processing time,\nchallenging assumptions about stochastic sampling costs. Output diversity\nremained limited, as models consistently defaulted to common name archetypes\n(e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names\nclustered at intermediate values (0.3-0.7). These results demonstrate that\narchitectural optimizations, rather than temperature adjustments, dominate\nperformance in structured generation tasks. The findings emphasize prioritizing\nmodel selection over hyperparameter tuning for efficiency and suggest explicit\ndiversity constraints are necessary to mitigate default output biases in\nsynthetic data pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines how temperature settings and model architectures affect\nthe generation of structured fictional data (names, birthdates) across three\nlarge language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest.\nBy systematically testing temperature values from 0.0 to 1.0 in increments of\n0.1, we conducted 330 trials yielding 889 structured entities, validated for\nsyntactic consistency. Key findings reveal that model architecture\nsignificantly influences computational efficiency, with mistral:latest and\nllama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to\nexpectations, temperature showed no correlation with processing time,\nchallenging assumptions about stochastic sampling costs. Output diversity\nremained limited, as models consistently defaulted to common name archetypes\n(e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names\nclustered at intermediate values (0.3-0.7). These results demonstrate that\narchitectural optimizations, rather than temperature adjustments, dominate\nperformance in structured generation tasks. The findings emphasize prioritizing\nmodel selection over hyperparameter tuning for efficiency and suggest explicit\ndiversity constraints are necessary to mitigate default output biases in\nsynthetic data pipelines."
                },
                "authors": [
                    {
                        "name": "Evgenii Evstafev"
                    }
                ],
                "author_detail": {
                    "name": "Evgenii Evstafev"
                },
                "author": "Evgenii Evstafev",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08514v1",
                "updated": "2025-02-12T15:46:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    50,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:46:50Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    50,
                    2,
                    43,
                    0
                ],
                "title": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation"
                },
                "summary": "Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries."
                },
                "authors": [
                    {
                        "name": "Mahnaz Koupaee"
                    },
                    {
                        "name": "Jake W. Vincent"
                    },
                    {
                        "name": "Saab Mansour"
                    },
                    {
                        "name": "Igor Shalyminov"
                    },
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Hwanjun Song"
                    },
                    {
                        "name": "Raphael Shu"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Yi Nian"
                    },
                    {
                        "name": "Amy Wing-mei Wong"
                    },
                    {
                        "name": "Kyu J. Han"
                    },
                    {
                        "name": "Hang Su"
                    }
                ],
                "author_detail": {
                    "name": "Hang Su"
                },
                "author": "Hang Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08512v1",
                "updated": "2025-02-12T15:46:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    34,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:46:34Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    46,
                    34,
                    2,
                    43,
                    0
                ],
                "title": "Measuring Diversity in Synthetic Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Diversity in Synthetic Datasets"
                },
                "summary": "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https://github.com/BlueWhaleLab/DCScore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https://github.com/BlueWhaleLab/DCScore."
                },
                "authors": [
                    {
                        "name": "Yuchang Zhu"
                    },
                    {
                        "name": "Huizhe Zhang"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Jintang Li"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Peilin Zhao"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yatao Bian"
                    }
                ],
                "author_detail": {
                    "name": "Yatao Bian"
                },
                "author": "Yatao Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08507v1",
                "updated": "2025-02-12T15:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    41,
                    43,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:41:43Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    41,
                    43,
                    2,
                    43,
                    0
                ],
                "title": "Explanation based In-Context Demonstrations Retrieval for Multilingual\n  Grammatical Error Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanation based In-Context Demonstrations Retrieval for Multilingual\n  Grammatical Error Correction"
                },
                "summary": "Grammatical error correction (GEC) aims to correct grammatical, spelling, and\nsemantic errors in natural language text. With the growing of large language\nmodels (LLMs), direct text generation has gradually become the focus of the GEC\nmethods, and few-shot in-context learning presents a cost-effective solution.\nHowever, selecting effective in-context examples remains challenging, as the\nsimilarity between input texts does not necessarily correspond to similar\ngrammatical error patterns. In this paper, we propose a novel retrieval method\nbased on natural language grammatical error explanations (GEE) to address this\nissue. Our method retrieves suitable few-shot demonstrations by matching the\nGEE of the test input with that of pre-constructed database samples, where\nexplanations for erroneous samples are generated by LLMs. We conducted\nmultilingual GEC few-shot experiments on both major open-source and\nclosed-source LLMs. Experiments across five languages show that our method\noutperforms existing semantic and BM25-based retrieval techniques, without\nrequiring additional training or language adaptation. This also suggests that\nmatching error patterns is key to selecting examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammatical error correction (GEC) aims to correct grammatical, spelling, and\nsemantic errors in natural language text. With the growing of large language\nmodels (LLMs), direct text generation has gradually become the focus of the GEC\nmethods, and few-shot in-context learning presents a cost-effective solution.\nHowever, selecting effective in-context examples remains challenging, as the\nsimilarity between input texts does not necessarily correspond to similar\ngrammatical error patterns. In this paper, we propose a novel retrieval method\nbased on natural language grammatical error explanations (GEE) to address this\nissue. Our method retrieves suitable few-shot demonstrations by matching the\nGEE of the test input with that of pre-constructed database samples, where\nexplanations for erroneous samples are generated by LLMs. We conducted\nmultilingual GEC few-shot experiments on both major open-source and\nclosed-source LLMs. Experiments across five languages show that our method\noutperforms existing semantic and BM25-based retrieval techniques, without\nrequiring additional training or language adaptation. This also suggests that\nmatching error patterns is key to selecting examples."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Wen Luo"
                    },
                    {
                        "name": "Guangyue Peng"
                    },
                    {
                        "name": "Houfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Houfeng Wang"
                },
                "author": "Houfeng Wang",
                "arxiv_comment": "Accepted by NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08503v1",
                "updated": "2025-02-12T15:34:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    34,
                    45,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:34:45Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    34,
                    45,
                    2,
                    43,
                    0
                ],
                "title": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?"
                },
                "summary": "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we identify the \"2D-Cheating\" problem in 3D LLM evaluation,\nwhere these tasks might be easily solved by VLMs with rendered images of point\nclouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We\ntest VLM performance across multiple 3D LLM benchmarks and, using this as a\nreference, propose principles for better assessing genuine 3D understanding. We\nalso advocate explicitly separating 3D abilities from 1D or 2D aspects when\nevaluating 3D LLMs."
                },
                "authors": [
                    {
                        "name": "Jiahe Jin"
                    },
                    {
                        "name": "Yanheng He"
                    },
                    {
                        "name": "Mingyan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mingyan Yang"
                },
                "author": "Mingyan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08490v1",
                "updated": "2025-02-12T15:26:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    26,
                    53,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T15:26:53Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    26,
                    53,
                    2,
                    43,
                    0
                ],
                "title": "Flat-Top Beamforming with Efficient Array-Fed RIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flat-Top Beamforming with Efficient Array-Fed RIS"
                },
                "summary": "Flat-top beam designs are essential for uniform power distribution over a\nwide angular sector for applications such as 5G/6G networks, satellite\ncommunications, radar systems, etc. Low sidelobe levels with steep transitions\nallow negligible cross sector illumination. Active array designs requiring\namplitude taper suffer from poor power amplifier utilization. Phase only\ndesigns, e.g., Zadoff-Chu or generalized step chirp polyphase sequence methods,\noften require large active antenna arrays which in turns increases the hardware\ncomplexity and reduces the energy efficiency. In our recently proposed novel\narray-fed reflective intelligent surface (RIS) architecture, the small ($2\n\\times 2$) active array has uniform (principal eigenmode) amplitude weighting.\nWe now present a pragmatic flat-top pattern design method for practical array\n(RIS) sizes, which outperforms current state-of-the-art in terms of design\nsuperiority, energy efficiency, and deployment feasibility. This novel design\nholds promise for advancing sustainable wireless technologies in\nnext-generation communication systems while mitigating the environmental impact\nof high-energy antenna arrays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flat-top beam designs are essential for uniform power distribution over a\nwide angular sector for applications such as 5G/6G networks, satellite\ncommunications, radar systems, etc. Low sidelobe levels with steep transitions\nallow negligible cross sector illumination. Active array designs requiring\namplitude taper suffer from poor power amplifier utilization. Phase only\ndesigns, e.g., Zadoff-Chu or generalized step chirp polyphase sequence methods,\noften require large active antenna arrays which in turns increases the hardware\ncomplexity and reduces the energy efficiency. In our recently proposed novel\narray-fed reflective intelligent surface (RIS) architecture, the small ($2\n\\times 2$) active array has uniform (principal eigenmode) amplitude weighting.\nWe now present a pragmatic flat-top pattern design method for practical array\n(RIS) sizes, which outperforms current state-of-the-art in terms of design\nsuperiority, energy efficiency, and deployment feasibility. This novel design\nholds promise for advancing sustainable wireless technologies in\nnext-generation communication systems while mitigating the environmental impact\nof high-energy antenna arrays."
                },
                "authors": [
                    {
                        "name": "Krishan Kumar Tiwari"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02540v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02540v3",
                "updated": "2025-02-12T15:14:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    15,
                    14,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2024-11-04T19:21:06Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    19,
                    21,
                    6,
                    0,
                    309,
                    0
                ],
                "title": "GraphXAIN: Narratives to Explain Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphXAIN: Narratives to Explain Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose challenges in interpretability. Existing\nGNN explanation methods usually yield technical outputs, such as subgraphs and\nfeature importance scores, that are difficult for non-data scientists to\nunderstand and thereby violate the purpose of explanations. Motivated by recent\nExplainable AI (XAI) research, we propose GraphXAIN, a method that generates\nnatural language narratives explaining GNN predictions. GraphXAIN is a model-\nand explainer-agnostic method that uses Large Language Models (LLMs) to\ntranslate explanatory subgraphs and feature importance scores into coherent,\nstory-like explanations of GNN decision-making processes. Evaluations on\nreal-world datasets demonstrate GraphXAIN's ability to improve graph\nexplanations. A survey of machine learning researchers and practitioners\nreveals that GraphXAIN enhances four explainability dimensions:\nunderstandability, satisfaction, convincingness, and suitability for\ncommunicating model predictions. When combined with another graph explainer\nmethod, GraphXAIN further improves trustworthiness, insightfulness, confidence,\nand usability. Notably, 95% of participants found GraphXAIN to be a valuable\naddition to the GNN explanation method. By incorporating natural language\nnarratives, our approach serves both graph practitioners and non-expert users\nby providing clearer and more effective explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose challenges in interpretability. Existing\nGNN explanation methods usually yield technical outputs, such as subgraphs and\nfeature importance scores, that are difficult for non-data scientists to\nunderstand and thereby violate the purpose of explanations. Motivated by recent\nExplainable AI (XAI) research, we propose GraphXAIN, a method that generates\nnatural language narratives explaining GNN predictions. GraphXAIN is a model-\nand explainer-agnostic method that uses Large Language Models (LLMs) to\ntranslate explanatory subgraphs and feature importance scores into coherent,\nstory-like explanations of GNN decision-making processes. Evaluations on\nreal-world datasets demonstrate GraphXAIN's ability to improve graph\nexplanations. A survey of machine learning researchers and practitioners\nreveals that GraphXAIN enhances four explainability dimensions:\nunderstandability, satisfaction, convincingness, and suitability for\ncommunicating model predictions. When combined with another graph explainer\nmethod, GraphXAIN further improves trustworthiness, insightfulness, confidence,\nand usability. Notably, 95% of participants found GraphXAIN to be a valuable\naddition to the GNN explanation method. By incorporating natural language\nnarratives, our approach serves both graph practitioners and non-expert users\nby providing clearer and more effective explanations."
                },
                "authors": [
                    {
                        "name": "Mateusz Cedro"
                    },
                    {
                        "name": "David Martens"
                    }
                ],
                "author_detail": {
                    "name": "David Martens"
                },
                "author": "David Martens",
                "arxiv_comment": "19 pages, 9 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02540v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02540v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02416v2",
                "updated": "2025-02-12T14:52:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    52,
                    56,
                    2,
                    43,
                    0
                ],
                "published": "2024-08-05T12:20:39Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    20,
                    39,
                    0,
                    218,
                    0
                ],
                "title": "Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in\n  Customized Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in\n  Customized Large Language Models"
                },
                "summary": "The drastic increase of large language models' (LLMs) parameters has led to a\nnew research direction of fine-tuning-free downstream customization by prompts,\ni.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)\nplay an important role in many businesses, there has emerged growing concerns\nabout the prompt leakage, which undermines the intellectual properties of these\nservices and causes downstream attacks. In this paper, we analyze the\nunderlying mechanism of prompt leakage, which we refer to as prompt\nmemorization, and develop corresponding defending strategies. By exploring the\nscaling laws in prompt extraction, we analyze key attributes that influence\nprompt extraction, including model sizes, prompt lengths, as well as the types\nof prompts. Then we propose two hypotheses that explain how LLMs expose their\nprompts. The first is attributed to the perplexity, i.e. the familiarity of\nLLMs to texts, whereas the second is based on the straightforward token\ntranslation path in attention matrices. To defend against such threats, we\ninvestigate whether alignments can undermine the extraction of prompts. We find\nthat current LLMs, even those with safety alignments like GPT-4, are highly\nvulnerable to prompt extraction attacks, even under the most straightforward\nuser attacks. Therefore, we put forward several defense strategies with the\ninspiration of our findings, which achieve 83.8\\% and 71.0\\% drop in the prompt\nextraction rate for Llama2-7B and GPT-3.5, respectively. Source code is\navaliable at https://github.com/liangzid/PromptExtractionEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The drastic increase of large language models' (LLMs) parameters has led to a\nnew research direction of fine-tuning-free downstream customization by prompts,\ni.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)\nplay an important role in many businesses, there has emerged growing concerns\nabout the prompt leakage, which undermines the intellectual properties of these\nservices and causes downstream attacks. In this paper, we analyze the\nunderlying mechanism of prompt leakage, which we refer to as prompt\nmemorization, and develop corresponding defending strategies. By exploring the\nscaling laws in prompt extraction, we analyze key attributes that influence\nprompt extraction, including model sizes, prompt lengths, as well as the types\nof prompts. Then we propose two hypotheses that explain how LLMs expose their\nprompts. The first is attributed to the perplexity, i.e. the familiarity of\nLLMs to texts, whereas the second is based on the straightforward token\ntranslation path in attention matrices. To defend against such threats, we\ninvestigate whether alignments can undermine the extraction of prompts. We find\nthat current LLMs, even those with safety alignments like GPT-4, are highly\nvulnerable to prompt extraction attacks, even under the most straightforward\nuser attacks. Therefore, we put forward several defense strategies with the\ninspiration of our findings, which achieve 83.8\\% and 71.0\\% drop in the prompt\nextraction rate for Llama2-7B and GPT-3.5, respectively. Source code is\navaliable at https://github.com/liangzid/PromptExtractionEval."
                },
                "authors": [
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Yaxin Xiao"
                    },
                    {
                        "name": "Haoyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoyang Li"
                },
                "author": "Haoyang Li",
                "arxiv_comment": "Source Code: https://github.com/liangzid/PromptExtractionEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17400v2",
                "updated": "2025-02-12T14:46:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    46,
                    43,
                    2,
                    43,
                    0
                ],
                "published": "2024-02-27T10:47:24Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    10,
                    47,
                    24,
                    1,
                    58,
                    0
                ],
                "title": "Investigating Continual Pretraining in Large Language Models: Insights\n  and Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Continual Pretraining in Large Language Models: Insights\n  and Implications"
                },
                "summary": "Continual learning (CL) in large language models (LLMs) is an evolving domain\nthat focuses on developing efficient and sustainable training strategies to\nadapt models to emerging knowledge and achieve robustness in dynamic\nenvironments. Our primary emphasis is on continual domain-adaptive pretraining,\na process designed to equip LLMs with the ability to integrate new information\nfrom various domains while retaining previously learned knowledge. Since\nexisting works concentrate mostly on continual fine-tuning for a limited\nselection of downstream tasks or training domains, we introduce a new benchmark\ndesigned to measure the adaptability of LLMs to changing pretraining data\nlandscapes. We further examine the impact of model size on learning efficacy\nand forgetting, as well as how the progression and similarity of emerging\ndomains affect the knowledge transfer within these models.\n  Our findings uncover several key insights: (i) continual pretraining\nconsistently improves <1.5B models studied in this work and is also superior to\ndomain adaptation, (ii) larger models always achieve better perplexity than\nsmaller ones when continually pretrained on the same corpus, (iii) smaller\nmodels are particularly sensitive to continual pretraining, showing the most\nsignificant rates of both learning and forgetting, (iv) continual pretraining\nboosts downstream task performance of GPT-2 family, (v) continual pretraining\nenables LLMs to specialize better when the sequence of domains shows semantic\nsimilarity while randomizing training domains leads to better transfer and\nfinal performance otherwise. We posit that our research establishes a new\nbenchmark for CL in LLMs, providing a more realistic evaluation of knowledge\nretention and transfer across diverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) in large language models (LLMs) is an evolving domain\nthat focuses on developing efficient and sustainable training strategies to\nadapt models to emerging knowledge and achieve robustness in dynamic\nenvironments. Our primary emphasis is on continual domain-adaptive pretraining,\na process designed to equip LLMs with the ability to integrate new information\nfrom various domains while retaining previously learned knowledge. Since\nexisting works concentrate mostly on continual fine-tuning for a limited\nselection of downstream tasks or training domains, we introduce a new benchmark\ndesigned to measure the adaptability of LLMs to changing pretraining data\nlandscapes. We further examine the impact of model size on learning efficacy\nand forgetting, as well as how the progression and similarity of emerging\ndomains affect the knowledge transfer within these models.\n  Our findings uncover several key insights: (i) continual pretraining\nconsistently improves <1.5B models studied in this work and is also superior to\ndomain adaptation, (ii) larger models always achieve better perplexity than\nsmaller ones when continually pretrained on the same corpus, (iii) smaller\nmodels are particularly sensitive to continual pretraining, showing the most\nsignificant rates of both learning and forgetting, (iv) continual pretraining\nboosts downstream task performance of GPT-2 family, (v) continual pretraining\nenables LLMs to specialize better when the sequence of domains shows semantic\nsimilarity while randomizing training domains leads to better transfer and\nfinal performance otherwise. We posit that our research establishes a new\nbenchmark for CL in LLMs, providing a more realistic evaluation of knowledge\nretention and transfer across diverse domains."
                },
                "authors": [
                    {
                        "name": "Çağatay Yıldız"
                    },
                    {
                        "name": "Nishaanth Kanna Ravichandran"
                    },
                    {
                        "name": "Nitin Sharma"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Beyza Ermis"
                    }
                ],
                "author_detail": {
                    "name": "Beyza Ermis"
                },
                "author": "Beyza Ermis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08441v1",
                "updated": "2025-02-12T14:32:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    17,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T14:32:17Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    17,
                    2,
                    43,
                    0
                ],
                "title": "Better Embeddings with Coupled Adam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Embeddings with Coupled Adam"
                },
                "summary": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets."
                },
                "authors": [
                    {
                        "name": "Felix Stollenwerk"
                    },
                    {
                        "name": "Tobias Stollenwerk"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Stollenwerk"
                },
                "author": "Tobias Stollenwerk",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08436v1",
                "updated": "2025-02-12T14:20:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    20,
                    36,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T14:20:36Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    20,
                    36,
                    2,
                    43,
                    0
                ],
                "title": "From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification"
                },
                "summary": "We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference."
                },
                "authors": [
                    {
                        "name": "Nathan Vandemoortele"
                    },
                    {
                        "name": "Bram Steenwinckel"
                    },
                    {
                        "name": "Femke Ongenae"
                    },
                    {
                        "name": "Sofie Van Hoecke"
                    }
                ],
                "author_detail": {
                    "name": "Sofie Van Hoecke"
                },
                "author": "Sofie Van Hoecke",
                "arxiv_comment": "Under review at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01621v2",
                "updated": "2025-02-12T14:03:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    3,
                    19,
                    2,
                    43,
                    0
                ],
                "published": "2024-12-02T15:41:47Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    41,
                    47,
                    0,
                    337,
                    0
                ],
                "title": "NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers"
                },
                "summary": "Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Angel Yahir Loredo Lopez"
                    },
                    {
                        "name": "Tyler McDonald"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "5 pages (excluding references), Published at Coling 2025, Best\n  Dataset Paper Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08415v1",
                "updated": "2025-02-12T13:58:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    58,
                    42,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    58,
                    42,
                    2,
                    43,
                    0
                ],
                "title": "A Semantic Parsing Algorithm to Solve Linear Ordering Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Semantic Parsing Algorithm to Solve Linear Ordering Problems"
                },
                "summary": "We develop an algorithm to semantically parse linear ordering problems, which\nrequire a model to arrange entities using deductive reasoning. Our method takes\nas input a number of premises and candidate statements, parsing them to a\nfirst-order logic of an ordering domain, and then utilizes constraint logic\nprogramming to infer the truth of proposed statements about the ordering.\n  Our semantic parser transforms Heim and Kratzer's syntax-based compositional\nformal semantic rules to a computational algorithm. This transformation\ninvolves introducing abstract types and templates based on their rules, and\nintroduces a dynamic component to interpret entities within a contextual\nframework.\n  Our symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to\nanswer multiple choice questions in BIG-bench's logical_deduction multiple\nchoice problems, achieving perfect accuracy, compared to 67.06% for the\nbest-performing LLM (GPT-4) and 87.63% for the hybrid system Logic-LM.\n  These promising results demonstrate the benefit of developing a semantic\nparsing algorithm driven by first-order logic constructs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop an algorithm to semantically parse linear ordering problems, which\nrequire a model to arrange entities using deductive reasoning. Our method takes\nas input a number of premises and candidate statements, parsing them to a\nfirst-order logic of an ordering domain, and then utilizes constraint logic\nprogramming to infer the truth of proposed statements about the ordering.\n  Our semantic parser transforms Heim and Kratzer's syntax-based compositional\nformal semantic rules to a computational algorithm. This transformation\ninvolves introducing abstract types and templates based on their rules, and\nintroduces a dynamic component to interpret entities within a contextual\nframework.\n  Our symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to\nanswer multiple choice questions in BIG-bench's logical_deduction multiple\nchoice problems, achieving perfect accuracy, compared to 67.06% for the\nbest-performing LLM (GPT-4) and 87.63% for the hybrid system Logic-LM.\n  These promising results demonstrate the benefit of developing a semantic\nparsing algorithm driven by first-order logic constructs."
                },
                "authors": [
                    {
                        "name": "Maha Alkhairy"
                    },
                    {
                        "name": "Vincent Homer"
                    },
                    {
                        "name": "Brendan O'Connor"
                    }
                ],
                "author_detail": {
                    "name": "Brendan O'Connor"
                },
                "author": "Brendan O'Connor",
                "arxiv_comment": "3 figures, 9 pages main paper and 6 pages references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08395v1",
                "updated": "2025-02-12T13:37:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    37,
                    3,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:37:03Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    37,
                    3,
                    2,
                    43,
                    0
                ],
                "title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance"
                },
                "summary": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them."
                },
                "authors": [
                    {
                        "name": "Paul Röttger"
                    },
                    {
                        "name": "Musashi Hinck"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Kobi Hackenburg"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08707v2",
                "updated": "2025-02-12T13:29:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    29,
                    11,
                    2,
                    43,
                    0
                ],
                "published": "2024-08-16T12:40:01Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    12,
                    40,
                    1,
                    4,
                    229,
                    0
                ],
                "title": "Beam Prediction based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beam Prediction based on Large Language Models"
                },
                "summary": "In this letter, we use large language models (LLMs) to develop a\nhigh-performing and robust beam prediction method. We formulate the millimeter\nwave (mmWave) beam prediction problem as a time series forecasting task, where\nthe historical observations are aggregated through cross-variable attention and\nthen transformed into text-based representations using a trainable tokenizer.\nBy leveraging the prompt-as-prefix (PaP) technique for contextual enrichment,\nour method harnesses the power of LLMs to predict future optimal beams.\nSimulation results demonstrate that our LLM-based approach outperforms\ntraditional learning-based models in prediction accuracy as well as robustness,\nhighlighting the significant potential of LLMs in enhancing wireless\ncommunication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we use large language models (LLMs) to develop a\nhigh-performing and robust beam prediction method. We formulate the millimeter\nwave (mmWave) beam prediction problem as a time series forecasting task, where\nthe historical observations are aggregated through cross-variable attention and\nthen transformed into text-based representations using a trainable tokenizer.\nBy leveraging the prompt-as-prefix (PaP) technique for contextual enrichment,\nour method harnesses the power of LLMs to predict future optimal beams.\nSimulation results demonstrate that our LLM-based approach outperforms\ntraditional learning-based models in prediction accuracy as well as robustness,\nhighlighting the significant potential of LLMs in enhancing wireless\ncommunication systems."
                },
                "authors": [
                    {
                        "name": "Yucheng Sheng"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Geoffrey Ye Li"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Ye Li"
                },
                "author": "Geoffrey Ye Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08391v1",
                "updated": "2025-02-12T13:28:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    28,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:28:46Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    28,
                    46,
                    2,
                    43,
                    0
                ],
                "title": "ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for\n  Whole Slide Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for\n  Whole Slide Image Classification"
                },
                "summary": "Multiple instance learning (MIL)-based framework has become the mainstream\nfor processing the whole slide image (WSI) with giga-pixel size and\nhierarchical image context in digital pathology. However, these methods heavily\ndepend on a substantial number of bag-level labels and solely learn from the\noriginal slides, which are easily affected by variations in data distribution.\nRecently, vision language model (VLM)-based methods introduced the language\nprior by pre-training on large-scale pathological image-text pairs. However,\nthe previous text prompt lacks the consideration of pathological prior\nknowledge, therefore does not substantially boost the model's performance.\nMoreover, the collection of such pairs and the pre-training process are very\ntime-consuming and source-intensive.To solve the above problems, we propose a\ndual-scale vision-language multiple instance learning (ViLa-MIL) framework for\nwhole slide image classification. Specifically, we propose a dual-scale visual\ndescriptive text prompt based on the frozen large language model (LLM) to boost\nthe performance of VLM effectively. To transfer the VLM to process WSI\nefficiently, for the image branch, we propose a prototype-guided patch decoder\nto aggregate the patch features progressively by grouping similar patches into\nthe same prototype; for the text branch, we introduce a context-guided text\ndecoder to enhance the text features by incorporating the multi-granular image\ncontexts. Extensive studies on three multi-cancer and multi-center subtyping\ndatasets demonstrate the superiority of ViLa-MIL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple instance learning (MIL)-based framework has become the mainstream\nfor processing the whole slide image (WSI) with giga-pixel size and\nhierarchical image context in digital pathology. However, these methods heavily\ndepend on a substantial number of bag-level labels and solely learn from the\noriginal slides, which are easily affected by variations in data distribution.\nRecently, vision language model (VLM)-based methods introduced the language\nprior by pre-training on large-scale pathological image-text pairs. However,\nthe previous text prompt lacks the consideration of pathological prior\nknowledge, therefore does not substantially boost the model's performance.\nMoreover, the collection of such pairs and the pre-training process are very\ntime-consuming and source-intensive.To solve the above problems, we propose a\ndual-scale vision-language multiple instance learning (ViLa-MIL) framework for\nwhole slide image classification. Specifically, we propose a dual-scale visual\ndescriptive text prompt based on the frozen large language model (LLM) to boost\nthe performance of VLM effectively. To transfer the VLM to process WSI\nefficiently, for the image branch, we propose a prototype-guided patch decoder\nto aggregate the patch features progressively by grouping similar patches into\nthe same prototype; for the text branch, we introduce a context-guided text\ndecoder to enhance the text features by incorporating the multi-granular image\ncontexts. Extensive studies on three multi-cancer and multi-center subtyping\ndatasets demonstrate the superiority of ViLa-MIL."
                },
                "authors": [
                    {
                        "name": "Jiangbo Shi"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Tieliang Gong"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Huazhu Fu"
                    }
                ],
                "author_detail": {
                    "name": "Huazhu Fu"
                },
                "author": "Huazhu Fu",
                "arxiv_comment": "CVPR 2024 (Updated version with corrections for typos and errors.)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08381v1",
                "updated": "2025-02-12T13:16:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    16,
                    7,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:16:07Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    16,
                    7,
                    2,
                    43,
                    0
                ],
                "title": "The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and\n  Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and\n  Opportunities"
                },
                "summary": "The powerfulness of LLMs indicates that deploying various LLMs with different\nscales and architectures on end, edge, and cloud to satisfy different\nrequirements and adaptive heterogeneous hardware is the critical way to achieve\nubiquitous intelligence for 6G. However, the massive parameter scale of LLMs\nposes significant challenges in deploying them on edge devices due to high\ncomputational and storage demands. Considering that the sparse activation in\nMixture of Experts (MoE) is effective on scalable and dynamic allocation of\ncomputational and communications resources at the edge, this paper proposes a\nnovel MoE-empowered collaborative deployment framework for edge LLMs, denoted\nas CoEL. This framework fully leverages the properties of MoE architecture and\nencompasses four key aspects: Perception, Deployment, Compression, and\nUpdating. Edge servers broadcast their resource status and the specific\nresource requirements of LLMs to their neighbors. Then, utilizing this data,\ntwo sophisticated deployment strategies are proposed for satisfying varying\nmodel scales, ensuring that each model is deployed effectively. One for\ndeploying LLMs on a single edge device through intra-device resource\ncollaboration, and another for a distributed deployment across multiple edge\ndevices via inter-device resource collaboration. Furthermore, both the models\nand the intermediate data are compressed for reducing memory footprint by\nquantization and reducing the volume of intermediate data by token fusion and\npruning. Finally, given the dynamic of network topology, resource status, and\nuser requirements, the deployment strategies are regularly updated to maintain\nits relevance and effectiveness. This paper also delineates the challenges and\npotential research directions for the deployment of edge LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The powerfulness of LLMs indicates that deploying various LLMs with different\nscales and architectures on end, edge, and cloud to satisfy different\nrequirements and adaptive heterogeneous hardware is the critical way to achieve\nubiquitous intelligence for 6G. However, the massive parameter scale of LLMs\nposes significant challenges in deploying them on edge devices due to high\ncomputational and storage demands. Considering that the sparse activation in\nMixture of Experts (MoE) is effective on scalable and dynamic allocation of\ncomputational and communications resources at the edge, this paper proposes a\nnovel MoE-empowered collaborative deployment framework for edge LLMs, denoted\nas CoEL. This framework fully leverages the properties of MoE architecture and\nencompasses four key aspects: Perception, Deployment, Compression, and\nUpdating. Edge servers broadcast their resource status and the specific\nresource requirements of LLMs to their neighbors. Then, utilizing this data,\ntwo sophisticated deployment strategies are proposed for satisfying varying\nmodel scales, ensuring that each model is deployed effectively. One for\ndeploying LLMs on a single edge device through intra-device resource\ncollaboration, and another for a distributed deployment across multiple edge\ndevices via inter-device resource collaboration. Furthermore, both the models\nand the intermediate data are compressed for reducing memory footprint by\nquantization and reducing the volume of intermediate data by token fusion and\npruning. Finally, given the dynamic of network topology, resource status, and\nuser requirements, the deployment strategies are regularly updated to maintain\nits relevance and effectiveness. This paper also delineates the challenges and\npotential research directions for the deployment of edge LLMs."
                },
                "authors": [
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Tuo Zhang"
                    },
                    {
                        "name": "Muqing Li"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Qihua Zhou"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang",
                "arxiv_comment": "7pages, 1 table, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08378v1",
                "updated": "2025-02-12T13:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    10,
                    9,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T13:10:09Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    10,
                    9,
                    2,
                    43,
                    0
                ],
                "title": "Learning Humanoid Standing-up Control across Diverse Postures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Humanoid Standing-up Control across Diverse Postures"
                },
                "summary": "Standing-up control is crucial for humanoid robots, with the potential for\nintegration into current locomotion and loco-manipulation systems, such as fall\nrecovery. Existing approaches are either limited to simulations that overlook\nhardware constraints or rely on predefined ground-specific motion trajectories,\nfailing to enable standing up across postures in real-world scenes. To bridge\nthis gap, we present HoST (Humanoid Standing-up Control), a reinforcement\nlearning framework that learns standing-up control from scratch, enabling\nrobust sim-to-real transfer across diverse postures. HoST effectively learns\nposture-adaptive motions by leveraging a multi-critic architecture and\ncurriculum-based training on diverse simulated terrains. To ensure successful\nreal-world deployment, we constrain the motion with smoothness regularization\nand implicit motion speed bound to alleviate oscillatory and violent motions on\nphysical hardware, respectively. After simulation-based training, the learned\ncontrol policies are directly deployed on the Unitree G1 humanoid robot. Our\nexperimental results demonstrate that the controllers achieve smooth, stable,\nand robust standing-up motions across a wide range of laboratory and outdoor\nenvironments. Videos are available at\nhttps://taohuang13.github.io/humanoid-standingup.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standing-up control is crucial for humanoid robots, with the potential for\nintegration into current locomotion and loco-manipulation systems, such as fall\nrecovery. Existing approaches are either limited to simulations that overlook\nhardware constraints or rely on predefined ground-specific motion trajectories,\nfailing to enable standing up across postures in real-world scenes. To bridge\nthis gap, we present HoST (Humanoid Standing-up Control), a reinforcement\nlearning framework that learns standing-up control from scratch, enabling\nrobust sim-to-real transfer across diverse postures. HoST effectively learns\nposture-adaptive motions by leveraging a multi-critic architecture and\ncurriculum-based training on diverse simulated terrains. To ensure successful\nreal-world deployment, we constrain the motion with smoothness regularization\nand implicit motion speed bound to alleviate oscillatory and violent motions on\nphysical hardware, respectively. After simulation-based training, the learned\ncontrol policies are directly deployed on the Unitree G1 humanoid robot. Our\nexperimental results demonstrate that the controllers achieve smooth, stable,\nand robust standing-up motions across a wide range of laboratory and outdoor\nenvironments. Videos are available at\nhttps://taohuang13.github.io/humanoid-standingup.github.io/."
                },
                "authors": [
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Junli Ren"
                    },
                    {
                        "name": "Huayi Wang"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Qingwei Ben"
                    },
                    {
                        "name": "Muning Wen"
                    },
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Jianan Li"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Humanoid Standing-up Control, 12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.16332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.16332v2",
                "updated": "2025-02-12T13:07:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    7,
                    7,
                    2,
                    43,
                    0
                ],
                "published": "2023-05-22T01:14:46Z",
                "published_parsed": [
                    2023,
                    5,
                    22,
                    1,
                    14,
                    46,
                    0,
                    142,
                    0
                ],
                "title": "Continual Learning through Human-Robot Interaction: Human Perceptions of\n  a Continual Learning Robot in Repeated Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning through Human-Robot Interaction: Human Perceptions of\n  a Continual Learning Robot in Repeated Interactions"
                },
                "summary": "For long-term deployment in dynamic real-world environments, assistive robots\nmust continue to learn and adapt to their environments. Researchers have\ndeveloped various computational models for continual learning (CL) that can\nallow robots to continually learn from limited training data, and avoid\nforgetting previous knowledge. While these CL models can mitigate forgetting on\nstatic, systematically collected datasets, it is unclear how human users might\nperceive a robot that continually learns over multiple interactions with them.\nIn this paper, we developed a system that integrates CL models for object\nrecognition with a Fetch mobile manipulator robot and allows human participants\nto directly teach and test the robot over multiple sessions. We conducted an\nin-person study with 60 participants that interacted with our system in 300\nsessions (5 sessions per participant). We conducted a between-subject study\nwith three different CL models to understand human perceptions of continual\nlearning robots over multiple sessions. Our results suggest that participants'\nperceptions of trust, competence, and usability of a continual learning robot\nsignificantly decrease over multiple sessions if the robot forgets previously\nlearned objects. However, the perceived task load on participants for teaching\nand testing the robot remains the same over multiple sessions even if the robot\nforgets previously learned objects. Our results also indicate that\nstate-of-the-art CL models might perform unreliably when applied on robots\ninteracting with human participants. Further, continual learning robots are not\nperceived as very trustworthy or competent by human participants, regardless of\nthe underlying continual learning model or the session number.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For long-term deployment in dynamic real-world environments, assistive robots\nmust continue to learn and adapt to their environments. Researchers have\ndeveloped various computational models for continual learning (CL) that can\nallow robots to continually learn from limited training data, and avoid\nforgetting previous knowledge. While these CL models can mitigate forgetting on\nstatic, systematically collected datasets, it is unclear how human users might\nperceive a robot that continually learns over multiple interactions with them.\nIn this paper, we developed a system that integrates CL models for object\nrecognition with a Fetch mobile manipulator robot and allows human participants\nto directly teach and test the robot over multiple sessions. We conducted an\nin-person study with 60 participants that interacted with our system in 300\nsessions (5 sessions per participant). We conducted a between-subject study\nwith three different CL models to understand human perceptions of continual\nlearning robots over multiple sessions. Our results suggest that participants'\nperceptions of trust, competence, and usability of a continual learning robot\nsignificantly decrease over multiple sessions if the robot forgets previously\nlearned objects. However, the perceived task load on participants for teaching\nand testing the robot remains the same over multiple sessions even if the robot\nforgets previously learned objects. Our results also indicate that\nstate-of-the-art CL models might perform unreliably when applied on robots\ninteracting with human participants. Further, continual learning robots are not\nperceived as very trustworthy or competent by human participants, regardless of\nthe underlying continual learning model or the session number."
                },
                "authors": [
                    {
                        "name": "Ali Ayub"
                    },
                    {
                        "name": "Zachary De Francesco"
                    },
                    {
                        "name": "Patrick Holthaus"
                    },
                    {
                        "name": "Chrystopher L. Nehaniv"
                    },
                    {
                        "name": "Kerstin Dautenhahn"
                    }
                ],
                "author_detail": {
                    "name": "Kerstin Dautenhahn"
                },
                "author": "Kerstin Dautenhahn",
                "arxiv_doi": "10.1007/s12369-025-01214-9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s12369-025-01214-9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.16332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.16332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the International Journal of Social Robotics (SoRo), 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01692v2",
                "updated": "2025-02-12T13:03:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    3,
                    9,
                    2,
                    43,
                    0
                ],
                "published": "2024-10-02T16:03:49Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    3,
                    49,
                    2,
                    276,
                    0
                ],
                "title": "U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-shaped and Inverted-U Scaling behind Emergent Abilities of Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where model performance stagnates at first and then\nimproves sharply and unpredictably with scale beyond a threshold. In this work,\nwe investigate the phenomenon by grouping questions based on difficulty level\nand provide a possible explanation for emergent abilities. Specifically, we\nobserve U-shaped scaling for hard questions and inverted-U scaling followed by\nsteady improvement for easy questions. The two scaling patterns initially\noffset each other, causing stagnant overall performance. The performance starts\nto soar when the scaling pattern of easy questions reverts from inverse to\nstandard scaling, leading to emergent abilities. Based on this finding, we\npropose a simple yet effective pipeline, called Slice-and-Sandwich, to predict\nthe emergence threshold and model performance beyond the threshold. Our code is\npublicly available at https://github.com/tony10101105/ExpEmergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to exhibit emergent abilities in\nsome downstream tasks, where model performance stagnates at first and then\nimproves sharply and unpredictably with scale beyond a threshold. In this work,\nwe investigate the phenomenon by grouping questions based on difficulty level\nand provide a possible explanation for emergent abilities. Specifically, we\nobserve U-shaped scaling for hard questions and inverted-U scaling followed by\nsteady improvement for easy questions. The two scaling patterns initially\noffset each other, causing stagnant overall performance. The performance starts\nto soar when the scaling pattern of easy questions reverts from inverse to\nstandard scaling, leading to emergent abilities. Based on this finding, we\npropose a simple yet effective pipeline, called Slice-and-Sandwich, to predict\nthe emergence threshold and model performance beyond the threshold. Our code is\npublicly available at https://github.com/tony10101105/ExpEmergence."
                },
                "authors": [
                    {
                        "name": "Tung-Yu Wu"
                    },
                    {
                        "name": "Pei-Yu Lo"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Yu Lo"
                },
                "author": "Pei-Yu Lo",
                "arxiv_comment": "accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18652v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18652v7",
                "updated": "2025-02-12T12:49:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    49,
                    36,
                    2,
                    43,
                    0
                ],
                "published": "2024-10-24T11:32:00Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    32,
                    0,
                    3,
                    298,
                    0
                ],
                "title": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation"
                },
                "summary": "Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples."
                },
                "authors": [
                    {
                        "name": "Woosung Koh"
                    },
                    {
                        "name": "Jang Han Yoon"
                    },
                    {
                        "name": "MinHyung Lee"
                    },
                    {
                        "name": "Youngjin Song"
                    },
                    {
                        "name": "Jaegwan Cho"
                    },
                    {
                        "name": "Jaehyun Kang"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Se-Young Yun"
                    },
                    {
                        "name": "Youngjae Yu"
                    },
                    {
                        "name": "Bongshin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Bongshin Lee"
                },
                "author": "Bongshin Lee",
                "arxiv_comment": "NAACL 2025 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18652v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18652v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08356v1",
                "updated": "2025-02-12T12:39:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    39,
                    51,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:39:51Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    39,
                    51,
                    2,
                    43,
                    0
                ],
                "title": "Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Kushagra Bhushan"
                    },
                    {
                        "name": "Yatin Nandwani"
                    },
                    {
                        "name": "Dinesh Khandelwal"
                    },
                    {
                        "name": "Sonam Gupta"
                    },
                    {
                        "name": "Gaurav Pandey"
                    },
                    {
                        "name": "Dinesh Raghu"
                    },
                    {
                        "name": "Sachindra Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Sachindra Joshi"
                },
                "author": "Sachindra Joshi",
                "arxiv_comment": "22 pages, 14 tables, to be published in NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08353v1",
                "updated": "2025-02-12T12:28:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    28,
                    39,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:28:39Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    28,
                    39,
                    2,
                    43,
                    0
                ],
                "title": "Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy"
                },
                "summary": "With the extensive application of Graph Neural Networks (GNNs) across various\ndomains, their trustworthiness has emerged as a focal point of research. Some\nexisting studies have shown that the integration of large language models\n(LLMs) can improve the semantic understanding and generation capabilities of\nGNNs, which in turn improves the trustworthiness of GNNs from various aspects.\nOur review introduces a taxonomy that offers researchers a clear framework for\ncomprehending the principles and applications of different methods and helps\nclarify the connections and differences among various approaches. Then we\nsystematically survey representative approaches along the four categories of\nour taxonomy. Through our taxonomy, researchers can understand the applicable\nscenarios, potential advantages, and limitations of each approach for the the\ntrusted integration of GNNs with LLMs. Finally, we present some promising\ndirections of work and future trends for the integration of LLMs and GNNs to\nimprove model trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the extensive application of Graph Neural Networks (GNNs) across various\ndomains, their trustworthiness has emerged as a focal point of research. Some\nexisting studies have shown that the integration of large language models\n(LLMs) can improve the semantic understanding and generation capabilities of\nGNNs, which in turn improves the trustworthiness of GNNs from various aspects.\nOur review introduces a taxonomy that offers researchers a clear framework for\ncomprehending the principles and applications of different methods and helps\nclarify the connections and differences among various approaches. Then we\nsystematically survey representative approaches along the four categories of\nour taxonomy. Through our taxonomy, researchers can understand the applicable\nscenarios, potential advantages, and limitations of each approach for the the\ntrusted integration of GNNs with LLMs. Finally, we present some promising\ndirections of work and future trends for the integration of LLMs and GNNs to\nimprove model trustworthiness."
                },
                "authors": [
                    {
                        "name": "Ruizhan Xue"
                    },
                    {
                        "name": "Huimin Deng"
                    },
                    {
                        "name": "Fang He"
                    },
                    {
                        "name": "Maojun Wang"
                    },
                    {
                        "name": "Zeyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zeyu Zhang"
                },
                "author": "Zeyu Zhang",
                "arxiv_comment": "Submitted to IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16937v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16937v3",
                "updated": "2025-02-12T12:25:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    25,
                    56,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T13:31:18Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    13,
                    31,
                    18,
                    1,
                    28,
                    0
                ],
                "title": "TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models"
                },
                "summary": "Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies."
                },
                "authors": [
                    {
                        "name": "Makoto Shing"
                    },
                    {
                        "name": "Kou Misaki"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Sho Yokoi"
                    },
                    {
                        "name": "Takuya Akiba"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Akiba"
                },
                "author": "Takuya Akiba",
                "arxiv_comment": "To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025) as a Spotlight presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16937v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16937v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08346v1",
                "updated": "2025-02-12T12:13:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    13,
                    51,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:13:51Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    13,
                    51,
                    2,
                    43,
                    0
                ],
                "title": "Graph Foundation Models for Recommendation: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Foundation Models for Recommendation: A Comprehensive Survey"
                },
                "summary": "Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems."
                },
                "authors": [
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Yihang Wang"
                    },
                    {
                        "name": "Yuanhao Zeng"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiashu Zhao"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yawen Li"
                    },
                    {
                        "name": "Long Xia"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08332v1",
                "updated": "2025-02-12T11:56:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    56,
                    40,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:56:40Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    56,
                    40,
                    2,
                    43,
                    0
                ],
                "title": "Modification and Generated-Text Detection: Achieving Dual Detection\n  Capabilities for the Outputs of LLM by Watermark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modification and Generated-Text Detection: Achieving Dual Detection\n  Capabilities for the Outputs of LLM by Watermark"
                },
                "summary": "The development of large language models (LLMs) has raised concerns about\npotential misuse. One practical solution is to embed a watermark in the text,\nallowing ownership verification through watermark extraction. Existing methods\nprimarily focus on defending against modification attacks, often neglecting\nother spoofing attacks. For example, attackers can alter the watermarked text\nto produce harmful content without compromising the presence of the watermark,\nwhich could lead to false attribution of this malicious content to the LLM.\nThis situation poses a serious threat to the LLMs service providers and\nhighlights the significance of achieving modification detection and\ngenerated-text detection simultaneously. Therefore, we propose a technique to\ndetect modifications in text for unbiased watermark which is sensitive to\nmodification. We introduce a new metric called ``discarded tokens\", which\nmeasures the number of tokens not included in watermark detection. When a\nmodification occurs, this metric changes and can serve as evidence of the\nmodification. Additionally, we improve the watermark detection process and\nintroduce a novel method for unbiased watermark. Our experiments demonstrate\nthat we can achieve effective dual detection capabilities: modification\ndetection and generated-text detection by watermark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has raised concerns about\npotential misuse. One practical solution is to embed a watermark in the text,\nallowing ownership verification through watermark extraction. Existing methods\nprimarily focus on defending against modification attacks, often neglecting\nother spoofing attacks. For example, attackers can alter the watermarked text\nto produce harmful content without compromising the presence of the watermark,\nwhich could lead to false attribution of this malicious content to the LLM.\nThis situation poses a serious threat to the LLMs service providers and\nhighlights the significance of achieving modification detection and\ngenerated-text detection simultaneously. Therefore, we propose a technique to\ndetect modifications in text for unbiased watermark which is sensitive to\nmodification. We introduce a new metric called ``discarded tokens\", which\nmeasures the number of tokens not included in watermark detection. When a\nmodification occurs, this metric changes and can serve as evidence of the\nmodification. Additionally, we improve the watermark detection process and\nintroduce a novel method for unbiased watermark. Our experiments demonstrate\nthat we can achieve effective dual detection capabilities: modification\ndetection and generated-text detection by watermark."
                },
                "authors": [
                    {
                        "name": "Yuhang Cai"
                    },
                    {
                        "name": "Yaofei Wang"
                    },
                    {
                        "name": "Donghui Hu"
                    },
                    {
                        "name": "Gu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Gu Chen"
                },
                "author": "Gu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08323v1",
                "updated": "2025-02-12T11:44:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    44,
                    19,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:44:19Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    44,
                    19,
                    2,
                    43,
                    0
                ],
                "title": "Contextual Compression Encoding for Large Language Models: A Novel\n  Framework for Multi-Layered Parameter Space Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Compression Encoding for Large Language Models: A Novel\n  Framework for Multi-Layered Parameter Space Pruning"
                },
                "summary": "Context-aware compression techniques have gained increasing attention as\nmodel sizes continue to grow, introducing computational bottlenecks that hinder\nefficient deployment. A structured encoding approach was proposed to\nselectively eliminate redundant parameter groups while ensuring that\nrepresentational fidelity was preserved across multiple layers. Contextual\nCompression Encoding (CCE) introduced a multi-stage encoding mechanism that\ndynamically restructured parameter distributions, allowing for significant\nreductions in memory footprint and computational complexity. Experimental\nevaluations demonstrated that models compressed through CCE retained linguistic\nexpressivity and coherence, maintaining accuracy across a range of text\ngeneration and classification tasks. Layer-wise analysis revealed that\nmiddle-network layers exhibited higher compression ratios, aligning with the\nobservation that self-attention and feed-forward transformations contained\nredundancies that could be reorganized without impairing functional capacity.\nComparisons against conventional quantization and pruning methods confirmed\nthat CCE provided a more balanced trade-off between efficiency and model\nretention, achieving reductions in energy consumption and inference latency\nwithout requiring extensive retraining. Computational efficiency improvements\nwere particularly evident in deployment scenarios involving\nresource-constrained environments, where reductions in memory usage enabled\nmore scalable implementations. Further analyses of internal network behavior\nshowed that compressed models exhibited stable activation distributions and\nadapted dynamically to input variations, reinforcing the viability of\nstructured compression strategies for optimizing large-scale architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-aware compression techniques have gained increasing attention as\nmodel sizes continue to grow, introducing computational bottlenecks that hinder\nefficient deployment. A structured encoding approach was proposed to\nselectively eliminate redundant parameter groups while ensuring that\nrepresentational fidelity was preserved across multiple layers. Contextual\nCompression Encoding (CCE) introduced a multi-stage encoding mechanism that\ndynamically restructured parameter distributions, allowing for significant\nreductions in memory footprint and computational complexity. Experimental\nevaluations demonstrated that models compressed through CCE retained linguistic\nexpressivity and coherence, maintaining accuracy across a range of text\ngeneration and classification tasks. Layer-wise analysis revealed that\nmiddle-network layers exhibited higher compression ratios, aligning with the\nobservation that self-attention and feed-forward transformations contained\nredundancies that could be reorganized without impairing functional capacity.\nComparisons against conventional quantization and pruning methods confirmed\nthat CCE provided a more balanced trade-off between efficiency and model\nretention, achieving reductions in energy consumption and inference latency\nwithout requiring extensive retraining. Computational efficiency improvements\nwere particularly evident in deployment scenarios involving\nresource-constrained environments, where reductions in memory usage enabled\nmore scalable implementations. Further analyses of internal network behavior\nshowed that compressed models exhibited stable activation distributions and\nadapted dynamically to input variations, reinforcing the viability of\nstructured compression strategies for optimizing large-scale architectures."
                },
                "authors": [
                    {
                        "name": "Barnaby Schmitt"
                    },
                    {
                        "name": "Alistair Grosvenor"
                    },
                    {
                        "name": "Matthias Cunningham"
                    },
                    {
                        "name": "Clementine Walsh"
                    },
                    {
                        "name": "Julius Pembrokeshire"
                    },
                    {
                        "name": "Jonathan Teel"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Teel"
                },
                "author": "Jonathan Teel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10827v3",
                "updated": "2025-02-12T11:41:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    41,
                    16,
                    2,
                    43,
                    0
                ],
                "published": "2024-12-14T13:12:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    13,
                    12,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "Rethinking Chain-of-Thought from the Perspective of Self-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Chain-of-Thought from the Perspective of Self-Training"
                },
                "summary": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Zongqian Wu"
                    },
                    {
                        "name": "Baoduo Xu"
                    },
                    {
                        "name": "Ruochen Cui"
                    },
                    {
                        "name": "Mengmeng Zhan"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Lei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Lei Feng"
                },
                "author": "Lei Feng",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08319v1",
                "updated": "2025-02-12T11:35:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    35,
                    20,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:35:20Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    35,
                    20,
                    2,
                    43,
                    0
                ],
                "title": "MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and\n  Emotion Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and\n  Emotion Detection"
                },
                "summary": "Propaganda is a form of persuasion that has been used throughout history with\nthe intention goal of influencing people's opinions through rhetorical and\npsychological persuasion techniques for determined ends. Although Arabic ranked\nas the fourth most- used language on the internet, resources for propaganda\ndetection in languages other than English, especially Arabic, remain extremely\nlimited. To address this gap, the first Arabic dataset for Multi-label\nPropaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE\nis an open-source extension of the existing Arabic propaganda dataset, ArPro,\nwith the addition of sentiment and emotion annotations for each text. This\ndataset comprises 8,000 annotated news articles, which is the largest\npropaganda dataset to date. For each task, several baselines have been\ndeveloped using large language models (LLMs), such as GPT-4o-mini, and\npre-trained language models (PLMs), including three BERT-based models. The\ndataset, annotation guidelines, and source code are all publicly released to\nfacilitate future research and development in Arabic language models and\ncontribute to a deeper understanding of how various opinion dimensions interact\nin news media1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propaganda is a form of persuasion that has been used throughout history with\nthe intention goal of influencing people's opinions through rhetorical and\npsychological persuasion techniques for determined ends. Although Arabic ranked\nas the fourth most- used language on the internet, resources for propaganda\ndetection in languages other than English, especially Arabic, remain extremely\nlimited. To address this gap, the first Arabic dataset for Multi-label\nPropaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE\nis an open-source extension of the existing Arabic propaganda dataset, ArPro,\nwith the addition of sentiment and emotion annotations for each text. This\ndataset comprises 8,000 annotated news articles, which is the largest\npropaganda dataset to date. For each task, several baselines have been\ndeveloped using large language models (LLMs), such as GPT-4o-mini, and\npre-trained language models (PLMs), including three BERT-based models. The\ndataset, annotation guidelines, and source code are all publicly released to\nfacilitate future research and development in Arabic language models and\ncontribute to a deeper understanding of how various opinion dimensions interact\nin news media1."
                },
                "authors": [
                    {
                        "name": "Lubna Al-Henaki"
                    },
                    {
                        "name": "Hend Al-Khalifa"
                    },
                    {
                        "name": "Abdulmalik Al-Salman"
                    },
                    {
                        "name": "Hajar Alqubayshi"
                    },
                    {
                        "name": "Hind Al-Twailay"
                    },
                    {
                        "name": "Gheeda Alghamdi"
                    },
                    {
                        "name": "Hawra Aljasim"
                    }
                ],
                "author_detail": {
                    "name": "Hawra Aljasim"
                },
                "author": "Hawra Aljasim",
                "arxiv_comment": "12 pages, 3 figuers, 4 tabels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08312v1",
                "updated": "2025-02-12T11:30:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    30,
                    28,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:30:28Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    30,
                    28,
                    2,
                    43,
                    0
                ],
                "title": "Word Synchronization Challenge: A Benchmark for Word Association\n  Responses for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word Synchronization Challenge: A Benchmark for Word Association\n  Responses for LLMs"
                },
                "summary": "This paper introduces the Word Synchronization Challenge, a novel benchmark\nto evaluate large language models (LLMs) in Human-Computer Interaction (HCI).\nThis benchmark uses a dynamic game-like framework to test LLMs ability to mimic\nhuman cognitive processes through word associations. By simulating complex\nhuman interactions, it assesses how LLMs interpret and align with human thought\npatterns during conversational exchanges, which are essential for effective\nsocial partnerships in HCI. Initial findings highlight the influence of model\nsophistication on performance, offering insights into the models capabilities\nto engage in meaningful social interactions and adapt behaviors in human-like\nways. This research advances the understanding of LLMs potential to replicate\nor diverge from human cognitive functions, paving the way for more nuanced and\nempathetic human-machine collaborations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Word Synchronization Challenge, a novel benchmark\nto evaluate large language models (LLMs) in Human-Computer Interaction (HCI).\nThis benchmark uses a dynamic game-like framework to test LLMs ability to mimic\nhuman cognitive processes through word associations. By simulating complex\nhuman interactions, it assesses how LLMs interpret and align with human thought\npatterns during conversational exchanges, which are essential for effective\nsocial partnerships in HCI. Initial findings highlight the influence of model\nsophistication on performance, offering insights into the models capabilities\nto engage in meaningful social interactions and adapt behaviors in human-like\nways. This research advances the understanding of LLMs potential to replicate\nor diverge from human cognitive functions, paving the way for more nuanced and\nempathetic human-machine collaborations."
                },
                "authors": [
                    {
                        "name": "Tanguy Cazalets"
                    },
                    {
                        "name": "Joni Dambre"
                    }
                ],
                "author_detail": {
                    "name": "Joni Dambre"
                },
                "author": "Joni Dambre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08309v1",
                "updated": "2025-02-12T11:23:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    23,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:23:46Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    23,
                    46,
                    2,
                    43,
                    0
                ],
                "title": "Unlocking Scaling Law in Industrial Recommendation Systems with a\n  Three-step Paradigm based Large User Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Scaling Law in Industrial Recommendation Systems with a\n  Three-step Paradigm based Large User Model"
                },
                "summary": "Recent advancements in autoregressive Large Language Models (LLMs) have\nachieved significant milestones, largely attributed to their scalability, often\nreferred to as the \"scaling law\". Inspired by these achievements, there has\nbeen a growing interest in adapting LLMs for Recommendation Systems (RecSys) by\nreformulating RecSys tasks into generative problems. However, these End-to-End\nGenerative Recommendation (E2E-GR) methods tend to prioritize idealized goals,\noften at the expense of the practical advantages offered by traditional Deep\nLearning based Recommendation Models (DLRMs) in terms of in features,\narchitecture, and practices. This disparity between idealized goals and\npractical needs introduces several challenges and limitations, locking the\nscaling law in industrial RecSys. In this paper, we introduce a large user\nmodel (LUM) that addresses these limitations through a three-step paradigm,\ndesigned to meet the stringent requirements of industrial settings while\nunlocking the potential for scalable recommendations. Our extensive\nexperimental evaluations demonstrate that LUM outperforms both state-of-the-art\nDLRMs and E2E-GR approaches. Notably, LUM exhibits excellent scalability, with\nperformance improvements observed as the model scales up to 7 billion\nparameters. Additionally, we have successfully deployed LUM in an industrial\napplication, where it achieved significant gains in an A/B test, further\nvalidating its effectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in autoregressive Large Language Models (LLMs) have\nachieved significant milestones, largely attributed to their scalability, often\nreferred to as the \"scaling law\". Inspired by these achievements, there has\nbeen a growing interest in adapting LLMs for Recommendation Systems (RecSys) by\nreformulating RecSys tasks into generative problems. However, these End-to-End\nGenerative Recommendation (E2E-GR) methods tend to prioritize idealized goals,\noften at the expense of the practical advantages offered by traditional Deep\nLearning based Recommendation Models (DLRMs) in terms of in features,\narchitecture, and practices. This disparity between idealized goals and\npractical needs introduces several challenges and limitations, locking the\nscaling law in industrial RecSys. In this paper, we introduce a large user\nmodel (LUM) that addresses these limitations through a three-step paradigm,\ndesigned to meet the stringent requirements of industrial settings while\nunlocking the potential for scalable recommendations. Our extensive\nexperimental evaluations demonstrate that LUM outperforms both state-of-the-art\nDLRMs and E2E-GR approaches. Notably, LUM exhibits excellent scalability, with\nperformance improvements observed as the model scales up to 7 billion\nparameters. Additionally, we have successfully deployed LUM in an industrial\napplication, where it achieved significant gains in an A/B test, further\nvalidating its effectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Bencheng Yan"
                    },
                    {
                        "name": "Shilei Liu"
                    },
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Yizhen Zhang"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Langming Liu"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Wang Pengjie"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08301v1",
                "updated": "2025-02-12T11:02:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    2,
                    59,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T11:02:59Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    2,
                    59,
                    2,
                    43,
                    0
                ],
                "title": "Compromising Honesty and Harmlessness in Language Models via Deception\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compromising Honesty and Harmlessness in Language Models via Deception\n  Attacks"
                },
                "summary": "Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce a novel attack that\nundermines both of these traits, revealing a vulnerability that, if exploited,\ncould have serious real-world consequences. In particular, we introduce\nfine-tuning methods that enhance deception tendencies beyond model safeguards.\nThese \"deception attacks\" customize models to mislead users when prompted on\nchosen topics while remaining accurate on others. Furthermore, we find that\ndeceptive models also exhibit toxicity, generating hate speech, stereotypes,\nand other harmful content. Finally, we assess whether models can deceive\nconsistently in multi-turn dialogues, yielding mixed results. Given that\nmillions of users interact with LLM-based chatbots, voice assistants, agents,\nand other interfaces where trustworthiness cannot be ensured, securing these\nmodels against deception attacks is critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce a novel attack that\nundermines both of these traits, revealing a vulnerability that, if exploited,\ncould have serious real-world consequences. In particular, we introduce\nfine-tuning methods that enhance deception tendencies beyond model safeguards.\nThese \"deception attacks\" customize models to mislead users when prompted on\nchosen topics while remaining accurate on others. Furthermore, we find that\ndeceptive models also exhibit toxicity, generating hate speech, stereotypes,\nand other harmful content. Finally, we assess whether models can deceive\nconsistently in multi-turn dialogues, yielding mixed results. Given that\nmillions of users interact with LLM-based chatbots, voice assistants, agents,\nand other interfaces where trustworthiness cannot be ensured, securing these\nmodels against deception attacks is critical."
                },
                "authors": [
                    {
                        "name": "Laurène Vaugrante"
                    },
                    {
                        "name": "Francesca Carlon"
                    },
                    {
                        "name": "Maluna Menke"
                    },
                    {
                        "name": "Thilo Hagendorff"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Hagendorff"
                },
                "author": "Thilo Hagendorff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08298v1",
                "updated": "2025-02-12T10:58:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    58,
                    57,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T10:58:57Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    58,
                    57,
                    2,
                    43,
                    0
                ],
                "title": "Improving Existing Optimization Algorithms with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Existing Optimization Algorithms with LLMs"
                },
                "summary": "The integration of Large Language Models (LLMs) into optimization has created\na powerful synergy, opening exciting research opportunities. This paper\ninvestigates how LLMs can enhance existing optimization algorithms. Using their\npre-trained knowledge, we demonstrate their ability to propose innovative\nheuristic variations and implementation strategies. To evaluate this, we\napplied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt\n(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that\nincorporates a heuristic in the solution construction phase. Our results show\nthat an alternative heuristic proposed by GPT-4o outperforms the\nexpert-designed heuristic of CMSA, with the performance gap widening on larger\nand denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into optimization has created\na powerful synergy, opening exciting research opportunities. This paper\ninvestigates how LLMs can enhance existing optimization algorithms. Using their\npre-trained knowledge, we demonstrate their ability to propose innovative\nheuristic variations and implementation strategies. To evaluate this, we\napplied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt\n(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that\nincorporates a heuristic in the solution construction phase. Our results show\nthat an alternative heuristic proposed by GPT-4o outperforms the\nexpert-designed heuristic of CMSA, with the performance gap widening on larger\nand denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/"
                },
                "authors": [
                    {
                        "name": "Camilo Chacón Sartori"
                    },
                    {
                        "name": "Christian Blum"
                    }
                ],
                "author_detail": {
                    "name": "Christian Blum"
                },
                "author": "Christian Blum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03824v2",
                "updated": "2025-02-12T10:45:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    45,
                    25,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-06T07:19:59Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    7,
                    19,
                    59,
                    3,
                    37,
                    0
                ],
                "title": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs"
                },
                "summary": "LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}."
                },
                "authors": [
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Seungjun Baek"
                    }
                ],
                "author_detail": {
                    "name": "Seungjun Baek"
                },
                "author": "Seungjun Baek",
                "arxiv_comment": "the Nations of the Americas Chapter of the Association for\n  Computational Linguistics (NAACL), Findings, Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17696v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17696v7",
                "updated": "2025-02-12T10:45:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    45,
                    2,
                    2,
                    43,
                    0
                ],
                "published": "2023-11-29T15:02:46Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    15,
                    2,
                    46,
                    2,
                    333,
                    0
                ],
                "title": "How to Build an Adaptive AI Tutor for Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Build an Adaptive AI Tutor for Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)"
                },
                "summary": "Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems\n(ITS) presents transformative opportunities for personalized education.\nHowever, current implementations face two critical challenges: maintaining\nfactual accuracy and delivering coherent, context-aware instruction. While\nRetrieval-Augmented Generation (RAG) partially addresses these issues, its\nreliance on pure semantic similarity limits its effectiveness in educational\ncontexts where conceptual relationships are crucial. This paper introduces\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel\nframework that integrates structured knowledge representation with\ncontext-aware retrieval to enable more effective AI tutoring. We present three\nkey contributions: (1) a novel architecture that grounds AI responses in\nstructured domain knowledge, (2) empirical validation through controlled\nexperiments (n=76) demonstrating significant learning improvements (35%\nincrease in assessment scores, p<0.001), and (3) a comprehensive implementation\nframework addressing practical deployment considerations. These results\nestablish KG-RAG as a robust solution for developing adaptable AI tutoring\nsystems across diverse educational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems\n(ITS) presents transformative opportunities for personalized education.\nHowever, current implementations face two critical challenges: maintaining\nfactual accuracy and delivering coherent, context-aware instruction. While\nRetrieval-Augmented Generation (RAG) partially addresses these issues, its\nreliance on pure semantic similarity limits its effectiveness in educational\ncontexts where conceptual relationships are crucial. This paper introduces\nKnowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel\nframework that integrates structured knowledge representation with\ncontext-aware retrieval to enable more effective AI tutoring. We present three\nkey contributions: (1) a novel architecture that grounds AI responses in\nstructured domain knowledge, (2) empirical validation through controlled\nexperiments (n=76) demonstrating significant learning improvements (35%\nincrease in assessment scores, p<0.001), and (3) a comprehensive implementation\nframework addressing practical deployment considerations. These results\nestablish KG-RAG as a robust solution for developing adaptable AI tutoring\nsystems across diverse educational contexts."
                },
                "authors": [
                    {
                        "name": "Chenxi Dong"
                    },
                    {
                        "name": "Yimin Yuan"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Shupei Cheng"
                    },
                    {
                        "name": "Chujie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Chujie Wen"
                },
                "author": "Chujie Wen",
                "arxiv_comment": "6 pages, 6 figures, ICEIT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17696v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17696v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08281v1",
                "updated": "2025-02-12T10:38:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    38,
                    22,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T10:38:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    38,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "Redefining Simplicity: Benchmarking Large Language Models from Lexical\n  to Document Simplification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Simplicity: Benchmarking Large Language Models from Lexical\n  to Document Simplification"
                },
                "summary": "Text simplification (TS) refers to the process of reducing the complexity of\na text while retaining its original meaning and key information. Existing work\nonly shows that large language models (LLMs) have outperformed supervised\nnon-LLM-based methods on sentence simplification. This study offers the first\ncomprehensive analysis of LLM performance across four TS tasks: lexical,\nsyntactic, sentence, and document simplification. We compare lightweight,\nclosed-source and open-source LLMs against traditional non-LLM methods using\nautomatic metrics and human evaluations. Our experiments reveal that LLMs not\nonly outperform non-LLM approaches in all four tasks but also often generate\noutputs that exceed the quality of existing human-annotated references.\nFinally, we present some future directions of TS in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text simplification (TS) refers to the process of reducing the complexity of\na text while retaining its original meaning and key information. Existing work\nonly shows that large language models (LLMs) have outperformed supervised\nnon-LLM-based methods on sentence simplification. This study offers the first\ncomprehensive analysis of LLM performance across four TS tasks: lexical,\nsyntactic, sentence, and document simplification. We compare lightweight,\nclosed-source and open-source LLMs against traditional non-LLM methods using\nautomatic metrics and human evaluations. Our experiments reveal that LLMs not\nonly outperform non-LLM approaches in all four tasks but also often generate\noutputs that exceed the quality of existing human-annotated references.\nFinally, we present some future directions of TS in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Jipeng Qiang"
                    },
                    {
                        "name": "Minjiang Huang"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Yunhao Yuan"
                    },
                    {
                        "name": "Chaowei Zhang"
                    },
                    {
                        "name": "Kui Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kui Yu"
                },
                "author": "Kui Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08271v1",
                "updated": "2025-02-12T10:24:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    24,
                    22,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T10:24:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    24,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "MoLoRec: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoLoRec: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm enhances domain-specific\nrecommendation tasks with parameter-efficient fine-tuning techniques, in order\nto improve models under the warm recommendation scenarios. While most previous\nworks treat these two paradigms separately, we argue that they have\ncomplementary advantages, and combining them together would be helpful.\n  To that end, in this paper, we propose a generalizable and efficient\nLLM-based recommendation framework MoLoRec. Our approach starts by\nparameter-efficient fine-tuning a domain-general module with general\nrecommendation instruction data, to align LLM with recommendation knowledge.\nThen, given users' behavior of a specific domain, we construct a\ndomain-specific instruction dataset and apply efficient fine-tuning to the\npre-trained LLM. After that, we provide approaches to integrate the above\ndomain-general part and domain-specific part with parameters mixture. Please\nnote that, MoLoRec is efficient with plug and play, as the domain-general\nmodule is trained only once, and any domain-specific plug-in can be efficiently\nmerged with only domain-specific fine-tuning. Extensive experiments on multiple\ndatasets under both warm and cold-start recommendation scenarios validate the\neffectiveness and generality of the proposed MoLoRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm enhances domain-specific\nrecommendation tasks with parameter-efficient fine-tuning techniques, in order\nto improve models under the warm recommendation scenarios. While most previous\nworks treat these two paradigms separately, we argue that they have\ncomplementary advantages, and combining them together would be helpful.\n  To that end, in this paper, we propose a generalizable and efficient\nLLM-based recommendation framework MoLoRec. Our approach starts by\nparameter-efficient fine-tuning a domain-general module with general\nrecommendation instruction data, to align LLM with recommendation knowledge.\nThen, given users' behavior of a specific domain, we construct a\ndomain-specific instruction dataset and apply efficient fine-tuning to the\npre-trained LLM. After that, we provide approaches to integrate the above\ndomain-general part and domain-specific part with parameters mixture. Please\nnote that, MoLoRec is efficient with plug and play, as the domain-general\nmodule is trained only once, and any domain-specific plug-in can be efficiently\nmerged with only domain-specific fine-tuning. Extensive experiments on multiple\ndatasets under both warm and cold-start recommendation scenarios validate the\neffectiveness and generality of the proposed MoLoRec."
                },
                "authors": [
                    {
                        "name": "Min Hou"
                    },
                    {
                        "name": "Chenxi Bai"
                    },
                    {
                        "name": "Le Wu"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18272v2",
                "updated": "2025-02-12T10:22:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    22,
                    58,
                    2,
                    43,
                    0
                ],
                "published": "2024-05-28T15:23:46Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    15,
                    23,
                    46,
                    1,
                    149,
                    0
                ],
                "title": "Metaheuristics and Large Language Models Join Forces: Toward an\n  Integrated Optimization Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaheuristics and Large Language Models Join Forces: Toward an\n  Integrated Optimization Approach"
                },
                "summary": "Since the rise of Large Language Models (LLMs) a couple of years ago,\nresearchers in metaheuristics (MHs) have wondered how to use their power in a\nbeneficial way within their algorithms. This paper introduces a novel approach\nthat leverages LLMs as pattern recognition tools to improve MHs. The resulting\nhybrid method, tested in the context of a social network-based combinatorial\noptimization problem, outperforms existing state-of-the-art approaches that\ncombine machine learning with MHs regarding the obtained solution quality. By\ncarefully designing prompts, we demonstrate that the output obtained from LLMs\ncan be used as problem knowledge, leading to improved results. Lastly, we\nacknowledge LLMs' potential drawbacks and limitations and consider it essential\nto examine them to advance this type of research further. Our method can be\nreproduced using a tool available at: https://github.com/camilochs/optipattern.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the rise of Large Language Models (LLMs) a couple of years ago,\nresearchers in metaheuristics (MHs) have wondered how to use their power in a\nbeneficial way within their algorithms. This paper introduces a novel approach\nthat leverages LLMs as pattern recognition tools to improve MHs. The resulting\nhybrid method, tested in the context of a social network-based combinatorial\noptimization problem, outperforms existing state-of-the-art approaches that\ncombine machine learning with MHs regarding the obtained solution quality. By\ncarefully designing prompts, we demonstrate that the output obtained from LLMs\ncan be used as problem knowledge, leading to improved results. Lastly, we\nacknowledge LLMs' potential drawbacks and limitations and consider it essential\nto examine them to advance this type of research further. Our method can be\nreproduced using a tool available at: https://github.com/camilochs/optipattern."
                },
                "authors": [
                    {
                        "name": "Camilo Chacón Sartori"
                    },
                    {
                        "name": "Christian Blum"
                    },
                    {
                        "name": "Filippo Bistaffa"
                    },
                    {
                        "name": "Guillem Rodríguez Corominas"
                    }
                ],
                "author_detail": {
                    "name": "Guillem Rodríguez Corominas"
                },
                "author": "Guillem Rodríguez Corominas",
                "arxiv_doi": "10.1109/ACCESS.2024.3524176",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2024.3524176",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.18272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access, vol. 13, pp. 2058-2079, 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08265v1",
                "updated": "2025-02-12T10:17:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    17,
                    18,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T10:17:18Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    17,
                    18,
                    2,
                    43,
                    0
                ],
                "title": "Exploring the Potential of Large Language Models to Simulate Personality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Large Language Models to Simulate Personality"
                },
                "summary": "With the advancement of large language models (LLMs), the focus in\nConversational AI has shifted from merely generating coherent and relevant\nresponses to tackling more complex challenges, such as personalizing dialogue\nsystems. In an effort to enhance user engagement, chatbots are often designed\nto mimic human behaviour, responding within a defined emotional spectrum and\naligning to a set of values. In this paper, we aim to simulate personal traits\naccording to the Big Five model with the use of LLMs. Our research showed that\ngenerating personality-related texts is still a challenging task for the\nmodels. As a result, we present a dataset of generated texts with the\npredefined Big Five characteristics and provide an analytical framework for\ntesting LLMs on a simulation of personality skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models (LLMs), the focus in\nConversational AI has shifted from merely generating coherent and relevant\nresponses to tackling more complex challenges, such as personalizing dialogue\nsystems. In an effort to enhance user engagement, chatbots are often designed\nto mimic human behaviour, responding within a defined emotional spectrum and\naligning to a set of values. In this paper, we aim to simulate personal traits\naccording to the Big Five model with the use of LLMs. Our research showed that\ngenerating personality-related texts is still a challenging task for the\nmodels. As a result, we present a dataset of generated texts with the\npredefined Big Five characteristics and provide an analytical framework for\ntesting LLMs on a simulation of personality skills."
                },
                "authors": [
                    {
                        "name": "Maria Molchanova"
                    },
                    {
                        "name": "Anna Mikhailova"
                    },
                    {
                        "name": "Anna Korzanova"
                    },
                    {
                        "name": "Lidiia Ostyakova"
                    },
                    {
                        "name": "Alexandra Dolidze"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Dolidze"
                },
                "author": "Alexandra Dolidze",
                "arxiv_comment": "Preprint submitted to Workshop on Customizable NLP (CustomNLP4U) on\n  EMNLP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09119v2",
                "updated": "2025-02-12T09:38:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    38,
                    31,
                    2,
                    43,
                    0
                ],
                "published": "2024-12-12T09:54:38Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    54,
                    38,
                    3,
                    347,
                    0
                ],
                "title": "The Utility and Complexity of in- and out-of-Distribution Machine\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Utility and Complexity of in- and out-of-Distribution Machine\n  Unlearning"
                },
                "summary": "Machine unlearning, the process of selectively removing data from trained\nmodels, is increasingly crucial for addressing privacy concerns and knowledge\ngaps post-deployment. Despite this importance, existing approaches are often\nheuristic and lack formal guarantees. In this paper, we analyze the fundamental\nutility, time, and space complexity trade-offs of approximate unlearning,\nproviding rigorous certification analogous to differential privacy. For\nin-distribution forget data -- data similar to the retain set -- we show that a\nsurprisingly simple and general procedure, empirical risk minimization with\noutput perturbation, achieves tight unlearning-utility-complexity trade-offs,\naddressing a previous theoretical gap on the separation from unlearning \"for\nfree\" via differential privacy, which inherently facilitates the removal of\nsuch data. However, such techniques fail with out-of-distribution forget data\n-- data significantly different from the retain set -- where unlearning time\ncomplexity can exceed that of retraining, even for a single sample. To address\nthis, we propose a new robust and noisy gradient descent variant that provably\namortizes unlearning time complexity without compromising utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning, the process of selectively removing data from trained\nmodels, is increasingly crucial for addressing privacy concerns and knowledge\ngaps post-deployment. Despite this importance, existing approaches are often\nheuristic and lack formal guarantees. In this paper, we analyze the fundamental\nutility, time, and space complexity trade-offs of approximate unlearning,\nproviding rigorous certification analogous to differential privacy. For\nin-distribution forget data -- data similar to the retain set -- we show that a\nsurprisingly simple and general procedure, empirical risk minimization with\noutput perturbation, achieves tight unlearning-utility-complexity trade-offs,\naddressing a previous theoretical gap on the separation from unlearning \"for\nfree\" via differential privacy, which inherently facilitates the removal of\nsuch data. However, such techniques fail with out-of-distribution forget data\n-- data significantly different from the retain set -- where unlearning time\ncomplexity can exceed that of retraining, even for a single sample. To address\nthis, we propose a new robust and noisy gradient descent variant that provably\namortizes unlearning time complexity without compromising utility."
                },
                "authors": [
                    {
                        "name": "Youssef Allouah"
                    },
                    {
                        "name": "Joshua Kazdan"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.09469v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.09469v4",
                "updated": "2025-02-12T09:31:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    31,
                    56,
                    2,
                    43,
                    0
                ],
                "published": "2023-06-15T19:51:28Z",
                "published_parsed": [
                    2023,
                    6,
                    15,
                    19,
                    51,
                    28,
                    3,
                    166,
                    0
                ],
                "title": "DISC: a Dataset for Integrated Sensing and Communication in mmWave\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DISC: a Dataset for Integrated Sensing and Communication in mmWave\n  Systems"
                },
                "summary": "This paper presents DISC, a dataset of millimeter-wave channel impulse\nresponse measurements for integrated human activity sensing and communication.\nThis is the first dataset collected with a software-defined radio testbed that\ntransmits 60 GHz IEEE 802-11ay-compliant packets and estimates the channel\nresponse including scattered signals off the moving body parts of subjects\nmoving in an indoor environment. The provided data consists of three parts, for\nmore than 2 hours of channel measurements with high temporal resolution (0.27\nms inter-packet time). DISC contains the contribution of 7 subjects performing\n5 different activities, and includes data collected from two distinct\nenvironments. Unlike available radar-based millimeter-wave sensing datasets,\nour measurements are collected using uniform packet transmission times and\nsparse traffic patterns from real Wi-Fi deployments. We develop, train, and\nrelease open-source baseline algorithms based on DISC to perform human sensing\ntasks. Our results demonstrate that DISC can serve as a multi-purpose\nbenchmarking tool for machine learning-based human activity recognition, radio\nfrequency gait analysis, and sparse sensing algorithms for next-generation\nintegrated sensing and communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents DISC, a dataset of millimeter-wave channel impulse\nresponse measurements for integrated human activity sensing and communication.\nThis is the first dataset collected with a software-defined radio testbed that\ntransmits 60 GHz IEEE 802-11ay-compliant packets and estimates the channel\nresponse including scattered signals off the moving body parts of subjects\nmoving in an indoor environment. The provided data consists of three parts, for\nmore than 2 hours of channel measurements with high temporal resolution (0.27\nms inter-packet time). DISC contains the contribution of 7 subjects performing\n5 different activities, and includes data collected from two distinct\nenvironments. Unlike available radar-based millimeter-wave sensing datasets,\nour measurements are collected using uniform packet transmission times and\nsparse traffic patterns from real Wi-Fi deployments. We develop, train, and\nrelease open-source baseline algorithms based on DISC to perform human sensing\ntasks. Our results demonstrate that DISC can serve as a multi-purpose\nbenchmarking tool for machine learning-based human activity recognition, radio\nfrequency gait analysis, and sparse sensing algorithms for next-generation\nintegrated sensing and communications."
                },
                "authors": [
                    {
                        "name": "Jacopo Pegoraro"
                    },
                    {
                        "name": "Pablo Saucedo"
                    },
                    {
                        "name": "Jesus Omar Lacruz"
                    },
                    {
                        "name": "Michele Rossi"
                    },
                    {
                        "name": "Joerg Widmer"
                    }
                ],
                "author_detail": {
                    "name": "Joerg Widmer"
                },
                "author": "Joerg Widmer",
                "arxiv_comment": "7 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.09469v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.09469v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08224v1",
                "updated": "2025-02-12T09:07:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    7,
                    25,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T09:07:25Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    9,
                    7,
                    25,
                    2,
                    43,
                    0
                ],
                "title": "Flow-of-Action: SOP Enhanced LLM-Based Multi-Agent System for Root Cause\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-of-Action: SOP Enhanced LLM-Based Multi-Agent System for Root Cause\n  Analysis"
                },
                "summary": "In the realm of microservices architecture, the occurrence of frequent\nincidents necessitates the employment of Root Cause Analysis (RCA) for swift\nissue resolution. It is common that a serious incident can take several domain\nexperts hours to identify the root cause. Consequently, a contemporary trend\ninvolves harnessing Large Language Models (LLMs) as automated agents for RCA.\nThough the recent ReAct framework aligns well with the Site Reliability\nEngineers (SREs) for its thought-action-observation paradigm, its\nhallucinations often lead to irrelevant actions and directly affect subsequent\nresults. Additionally, the complex and variable clues of the incident can\noverwhelm the model one step further. To confront these challenges, we propose\nFlow-of-Action, a pioneering Standard Operation Procedure (SOP) enhanced\nLLM-based multi-agent system. By explicitly summarizing the diagnosis steps of\nSREs, SOP imposes constraints on LLMs at crucial junctures, guiding the RCA\nprocess towards the correct trajectory. To facilitate the rational and\neffective utilization of SOPs, we design an SOP-centric framework called SOP\nflow. SOP flow contains a series of tools, including one for finding relevant\nSOPs for incidents, another for automatically generating SOPs for incidents\nwithout relevant ones, and a tool for converting SOPs into code. This\nsignificantly alleviates the hallucination issues of ReAct in RCA tasks. We\nalso design multiple auxiliary agents to assist the main agent by removing\nuseless noise, narrowing the search space, and informing the main agent whether\nthe RCA procedure can stop. Compared to the ReAct method's 35.50% accuracy, our\nFlow-of-Action method achieves 64.01%, meeting the accuracy requirements for\nRCA in real-world systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of microservices architecture, the occurrence of frequent\nincidents necessitates the employment of Root Cause Analysis (RCA) for swift\nissue resolution. It is common that a serious incident can take several domain\nexperts hours to identify the root cause. Consequently, a contemporary trend\ninvolves harnessing Large Language Models (LLMs) as automated agents for RCA.\nThough the recent ReAct framework aligns well with the Site Reliability\nEngineers (SREs) for its thought-action-observation paradigm, its\nhallucinations often lead to irrelevant actions and directly affect subsequent\nresults. Additionally, the complex and variable clues of the incident can\noverwhelm the model one step further. To confront these challenges, we propose\nFlow-of-Action, a pioneering Standard Operation Procedure (SOP) enhanced\nLLM-based multi-agent system. By explicitly summarizing the diagnosis steps of\nSREs, SOP imposes constraints on LLMs at crucial junctures, guiding the RCA\nprocess towards the correct trajectory. To facilitate the rational and\neffective utilization of SOPs, we design an SOP-centric framework called SOP\nflow. SOP flow contains a series of tools, including one for finding relevant\nSOPs for incidents, another for automatically generating SOPs for incidents\nwithout relevant ones, and a tool for converting SOPs into code. This\nsignificantly alleviates the hallucination issues of ReAct in RCA tasks. We\nalso design multiple auxiliary agents to assist the main agent by removing\nuseless noise, narrowing the search space, and informing the main agent whether\nthe RCA procedure can stop. Compared to the ReAct method's 35.50% accuracy, our\nFlow-of-Action method achieves 64.01%, meeting the accuracy requirements for\nRCA in real-world systems."
                },
                "authors": [
                    {
                        "name": "Changhua Pei"
                    },
                    {
                        "name": "Zexin Wang"
                    },
                    {
                        "name": "Fengrui Liu"
                    },
                    {
                        "name": "Zeyan Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xiao He"
                    },
                    {
                        "name": "Rong Kang"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Jianhui Li"
                    },
                    {
                        "name": "Gaogang Xie"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "arxiv_comment": "Accepted by WWW'25 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07709v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07709v2",
                "updated": "2025-02-12T08:52:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    52,
                    52,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-11T17:08:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    8,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces"
                },
                "summary": "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces."
                },
                "authors": [
                    {
                        "name": "Loris Gaven"
                    },
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Clément Romac"
                    },
                    {
                        "name": "Cédric Colas"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07709v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07709v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08213v1",
                "updated": "2025-02-12T08:48:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    48,
                    55,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T08:48:55Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    48,
                    55,
                    2,
                    43,
                    0
                ],
                "title": "LLM Modules: Knowledge Transfer from a Large to a Small Model using\n  Enhanced Cross-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Modules: Knowledge Transfer from a Large to a Small Model using\n  Enhanced Cross-Attention"
                },
                "summary": "In this work, we propose an architecture of LLM Modules that enables the\ntransfer of knowledge from a large pre-trained model to a smaller model using\nan Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B\nmodel is frozen and its representations are passed through specially designed\nattention layers to the GPT-Neo-125M model, which is trained on limited\ncomputational resources. Experimental results on the Bespoke-Stratos-17k\ndataset demonstrate that after 15 epochs of training, the combined model\ngenerates responses comparable in quality to those obtained by distillation. We\ndiscuss the advantages of the modular approach, provide examples of input\nqueries and comparative analysis, and outline prospects for further extension\nof the method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose an architecture of LLM Modules that enables the\ntransfer of knowledge from a large pre-trained model to a smaller model using\nan Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B\nmodel is frozen and its representations are passed through specially designed\nattention layers to the GPT-Neo-125M model, which is trained on limited\ncomputational resources. Experimental results on the Bespoke-Stratos-17k\ndataset demonstrate that after 15 epochs of training, the combined model\ngenerates responses comparable in quality to those obtained by distillation. We\ndiscuss the advantages of the modular approach, provide examples of input\nqueries and comparative analysis, and outline prospects for further extension\nof the method."
                },
                "authors": [
                    {
                        "name": "Konstantin Kolomeitsev"
                    }
                ],
                "author_detail": {
                    "name": "Konstantin Kolomeitsev"
                },
                "arxiv_affiliation": "Almaty, Kazakhstan",
                "author": "Konstantin Kolomeitsev",
                "arxiv_comment": "Code and pre-trained weights available at\n  https://huggingface.co/kkolomeitsev/llm-modules",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11914v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11914v4",
                "updated": "2025-02-12T08:28:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    28,
                    49,
                    2,
                    43,
                    0
                ],
                "published": "2023-08-23T04:59:21Z",
                "published_parsed": [
                    2023,
                    8,
                    23,
                    4,
                    59,
                    21,
                    2,
                    235,
                    0
                ],
                "title": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge\n  Reasoning via Promoting Causal Consistency in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge\n  Reasoning via Promoting Causal Consistency in LLMs"
                },
                "summary": "Despite the progress of foundation models, knowledge-based reasoning remains\na persistent challenge due to their limited capacity for knowledge recall and\ninference. Existing methods primarily focus on encouraging these models to plan\nand solve problems or extensively sample reasoning chains independently.\nHowever, these methods often overlook conceptual errors and inferential\nfallacies, inevitably leading to a series of notorious issues such as\nmisleading conclusions, cognitive biases, and reduced decision quality. While\nexplicit modeling of causality is argued to hold promise in addressing these\nissues, contemporary research efforts have thus far fallen short in achieving\ncausality-based foundation models. Drawing inspiration from the orchestration\nof diverse specialized agents collaborating to tackle intricate tasks, we\npropose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that\nharnesses multi-agent collaboration to bolster the faithfulness and causality\nof foundation models, involving a set of reasoners and evaluators. These agents\ncollaboratively work within a reasoning-and-consensus paradigm to improve\nfaithfulness. The reasoners are tasked with generating reasoning chains for\nknowledge-intensive problems by mimicking human causal reasoning. Meanwhile,\nthe evaluator scrutinizes the causal consistency of a reasoner's reasoning\nchain from a non-causal and a counterfactual perspective. Our framework\ndemonstrates significant superiority over state-of-the-art methods through\nextensive and comprehensive evaluations across text-based and multi-modal\nknowledge reasoning tasks (e.g., science question answering and commonsense\nreasoning).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the progress of foundation models, knowledge-based reasoning remains\na persistent challenge due to their limited capacity for knowledge recall and\ninference. Existing methods primarily focus on encouraging these models to plan\nand solve problems or extensively sample reasoning chains independently.\nHowever, these methods often overlook conceptual errors and inferential\nfallacies, inevitably leading to a series of notorious issues such as\nmisleading conclusions, cognitive biases, and reduced decision quality. While\nexplicit modeling of causality is argued to hold promise in addressing these\nissues, contemporary research efforts have thus far fallen short in achieving\ncausality-based foundation models. Drawing inspiration from the orchestration\nof diverse specialized agents collaborating to tackle intricate tasks, we\npropose a framework named Causal-Consistency Chain-of-Thought (CaCo-CoT) that\nharnesses multi-agent collaboration to bolster the faithfulness and causality\nof foundation models, involving a set of reasoners and evaluators. These agents\ncollaboratively work within a reasoning-and-consensus paradigm to improve\nfaithfulness. The reasoners are tasked with generating reasoning chains for\nknowledge-intensive problems by mimicking human causal reasoning. Meanwhile,\nthe evaluator scrutinizes the causal consistency of a reasoner's reasoning\nchain from a non-causal and a counterfactual perspective. Our framework\ndemonstrates significant superiority over state-of-the-art methods through\nextensive and comprehensive evaluations across text-based and multi-modal\nknowledge reasoning tasks (e.g., science question answering and commonsense\nreasoning)."
                },
                "authors": [
                    {
                        "name": "Ziyi Tang"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Weixing Chen"
                    },
                    {
                        "name": "Yongsen Zheng"
                    },
                    {
                        "name": "Zechuan Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Keze Wang"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "12 pages, 9 figures. 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11914v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11914v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03307v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03307v3",
                "updated": "2025-02-12T08:16:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    8,
                    16,
                    44,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-05T16:08:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    8,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "Intent Alignment between Interaction and Language Spaces for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Alignment between Interaction and Language Spaces for\n  Recommendation"
                },
                "summary": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03307v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03307v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08182v1",
                "updated": "2025-02-12T07:42:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    42,
                    45,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T07:42:45Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    42,
                    45,
                    2,
                    43,
                    0
                ],
                "title": "Memory Offloading for Large Language Model Inference with Latency SLO\n  Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Offloading for Large Language Model Inference with Latency SLO\n  Guarantees"
                },
                "summary": "Offloading large language models (LLMs) state to host memory during inference\npromises to reduce operational costs by supporting larger models, longer\ninputs, and larger batch sizes. However, the design of existing memory\noffloading mechanisms does not take latency service-level objectives (SLOs)\ninto consideration. As a result, they either lead to frequent SLO violations or\nunderutilize host memory, thereby incurring economic loss and thus defeating\nthe purpose of memory offloading.\n  This paper presents Select-N, a latency-SLO-aware memory offloading system\nfor LLM serving. A key challenge in designing Select-N is to reconcile the\ntension between meeting SLOs and maximizing host memory usage. Select-N\novercomes it by exploiting a unique characteristic of modern LLMs: during\nserving, the computation time of each decoder layer is deterministic.\nLeveraging this, Select-N introduces offloading interval, an internal tunable\nknob that captures the tradeoff between SLOs and host memory usage, thereby\nreducing the aforementioned challenge to pick an optimal offloading interval.\nWith that, Select-N proposes a two-stage approach to automatically pick the\noffloading interval. The first stage is offline that generates the range of\noptimal offloading interval, while the second stage adjusts offloading interval\nat the granularity of inference iteration based on runtime hardware status. Our\nevaluation shows that Select-N consistently meets SLOs and improves the serving\nthroughput over existing mechanisms by 1.85X due to maximizing the use of host\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offloading large language models (LLMs) state to host memory during inference\npromises to reduce operational costs by supporting larger models, longer\ninputs, and larger batch sizes. However, the design of existing memory\noffloading mechanisms does not take latency service-level objectives (SLOs)\ninto consideration. As a result, they either lead to frequent SLO violations or\nunderutilize host memory, thereby incurring economic loss and thus defeating\nthe purpose of memory offloading.\n  This paper presents Select-N, a latency-SLO-aware memory offloading system\nfor LLM serving. A key challenge in designing Select-N is to reconcile the\ntension between meeting SLOs and maximizing host memory usage. Select-N\novercomes it by exploiting a unique characteristic of modern LLMs: during\nserving, the computation time of each decoder layer is deterministic.\nLeveraging this, Select-N introduces offloading interval, an internal tunable\nknob that captures the tradeoff between SLOs and host memory usage, thereby\nreducing the aforementioned challenge to pick an optimal offloading interval.\nWith that, Select-N proposes a two-stage approach to automatically pick the\noffloading interval. The first stage is offline that generates the range of\noptimal offloading interval, while the second stage adjusts offloading interval\nat the granularity of inference iteration based on runtime hardware status. Our\nevaluation shows that Select-N consistently meets SLOs and improves the serving\nthroughput over existing mechanisms by 1.85X due to maximizing the use of host\nmemory."
                },
                "authors": [
                    {
                        "name": "Chenxiang Ma"
                    },
                    {
                        "name": "Zhisheng Ye"
                    },
                    {
                        "name": "Hanyu Zhao"
                    },
                    {
                        "name": "Zehua Yang"
                    },
                    {
                        "name": "Tianhao Fu"
                    },
                    {
                        "name": "Jiaxun Han"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Diyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Diyu Zhou"
                },
                "author": "Diyu Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08180v1",
                "updated": "2025-02-12T07:37:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    37,
                    39,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T07:37:39Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    37,
                    39,
                    2,
                    43,
                    0
                ],
                "title": "Enhancing LLM Character-Level Manipulation via Divide and Conquer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Character-Level Manipulation via Divide and Conquer"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhen Xiong"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Zhecheng Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08178v1",
                "updated": "2025-02-12T07:32:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    32,
                    48,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T07:32:48Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    32,
                    48,
                    2,
                    43,
                    0
                ],
                "title": "ParetoRAG: Leveraging Sentence-Context Attention for Robust and\n  Efficient Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParetoRAG: Leveraging Sentence-Context Attention for Robust and\n  Efficient Retrieval-Augmented Generation"
                },
                "summary": "While Retrieval-Augmented Generation (RAG) systems enhance Large Language\nModels (LLMs) by incorporating external knowledge, they still face persistent\nchallenges in retrieval inefficiency and the inability of LLMs to filter out\nirrelevant information. We present ParetoRAG, an unsupervised framework that\noptimizes RAG systems through sentence-level refinement guided by the Pareto\nprinciple. By decomposing paragraphs into sentences and dynamically\nre-weighting core content while preserving contextual coherence, ParetoRAG\nachieves dual improvements in both retrieval precision and generation quality\nwithout requiring additional training or API resources. This framework has been\nempirically validated across various datasets, LLMs, and retrievers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Retrieval-Augmented Generation (RAG) systems enhance Large Language\nModels (LLMs) by incorporating external knowledge, they still face persistent\nchallenges in retrieval inefficiency and the inability of LLMs to filter out\nirrelevant information. We present ParetoRAG, an unsupervised framework that\noptimizes RAG systems through sentence-level refinement guided by the Pareto\nprinciple. By decomposing paragraphs into sentences and dynamically\nre-weighting core content while preserving contextual coherence, ParetoRAG\nachieves dual improvements in both retrieval precision and generation quality\nwithout requiring additional training or API resources. This framework has been\nempirically validated across various datasets, LLMs, and retrievers."
                },
                "authors": [
                    {
                        "name": "Ruobing Yao"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Shuang Song"
                    },
                    {
                        "name": "Yuhua Liu"
                    },
                    {
                        "name": "Neng Gao"
                    },
                    {
                        "name": "Chenyang Tu"
                    }
                ],
                "author_detail": {
                    "name": "Chenyang Tu"
                },
                "author": "Chenyang Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08177v1",
                "updated": "2025-02-12T07:32:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    32,
                    42,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T07:32:42Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    32,
                    42,
                    2,
                    43,
                    0
                ],
                "title": "SycEval: Evaluating LLM Sycophancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SycEval: Evaluating LLM Sycophancy"
                },
                "summary": "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications."
                },
                "authors": [
                    {
                        "name": "Aaron Fanous"
                    },
                    {
                        "name": "Jacob Goldberg"
                    },
                    {
                        "name": "Ank A. Agarwal"
                    },
                    {
                        "name": "Joanna Lin"
                    },
                    {
                        "name": "Anson Zhou"
                    },
                    {
                        "name": "Roxana Daneshjou"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "arxiv_affiliation": "Stanford University",
                "author": "Sanmi Koyejo",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08172v1",
                "updated": "2025-02-12T07:26:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    26,
                    13,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T07:26:13Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    26,
                    13,
                    2,
                    43,
                    0
                ],
                "title": "Intention is All You Need: Refining Your Code from Your Intention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intention is All You Need: Refining Your Code from Your Intention"
                },
                "summary": "Code refinement aims to enhance existing code by addressing issues,\nrefactoring, and optimizing to improve quality and meet specific requirements.\nAs software projects scale in size and complexity, the traditional iterative\nexchange between reviewers and developers becomes increasingly burdensome.\nWhile recent deep learning techniques have been explored to accelerate this\nprocess, their performance remains limited, primarily due to challenges in\naccurately understanding reviewers' intents.\n  This paper proposes an intention-based code refinement technique that\nenhances the conventional comment-to-code process by explicitly extracting\nreviewer intentions from the comments. Our approach consists of two key phases:\nIntention Extraction and Intention Guided Revision Generation. Intention\nExtraction categorizes comments using predefined templates, while Intention\nGuided Revision Generation employs large language models (LLMs) to generate\nrevised code based on these defined intentions. Three categories with eight\nsubcategories are designed for comment transformation, which is followed by a\nhybrid approach that combines rule-based and LLM-based classifiers for accurate\nclassification. Extensive experiments with five LLMs (GPT4o, GPT3.5,\nDeepSeekV2, DeepSeek7B, CodeQwen7B) under different prompting settings\ndemonstrate that our approach achieves 79% accuracy in intention extraction and\nup to 66% in code refinement generation. Our results highlight the potential of\nour approach in enhancing data quality and improving the efficiency of code\nrefinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code refinement aims to enhance existing code by addressing issues,\nrefactoring, and optimizing to improve quality and meet specific requirements.\nAs software projects scale in size and complexity, the traditional iterative\nexchange between reviewers and developers becomes increasingly burdensome.\nWhile recent deep learning techniques have been explored to accelerate this\nprocess, their performance remains limited, primarily due to challenges in\naccurately understanding reviewers' intents.\n  This paper proposes an intention-based code refinement technique that\nenhances the conventional comment-to-code process by explicitly extracting\nreviewer intentions from the comments. Our approach consists of two key phases:\nIntention Extraction and Intention Guided Revision Generation. Intention\nExtraction categorizes comments using predefined templates, while Intention\nGuided Revision Generation employs large language models (LLMs) to generate\nrevised code based on these defined intentions. Three categories with eight\nsubcategories are designed for comment transformation, which is followed by a\nhybrid approach that combines rule-based and LLM-based classifiers for accurate\nclassification. Extensive experiments with five LLMs (GPT4o, GPT3.5,\nDeepSeekV2, DeepSeek7B, CodeQwen7B) under different prompting settings\ndemonstrate that our approach achieves 79% accuracy in intention extraction and\nup to 66% in code refinement generation. Our results highlight the potential of\nour approach in enhancing data quality and improving the efficiency of code\nrefinement."
                },
                "authors": [
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Shangqing Liu"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Xiaohong Li"
                    },
                    {
                        "name": "Lei Bu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bu"
                },
                "author": "Lei Bu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15151v2",
                "updated": "2025-02-12T07:25:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    25,
                    24,
                    2,
                    43,
                    0
                ],
                "published": "2024-12-19T18:28:41Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    28,
                    41,
                    3,
                    354,
                    0
                ],
                "title": "Language Models as Continuous Self-Evolving Data Engineers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Continuous Self-Evolving Data Engineers"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting an upper limit on the performance of\nLLMs. To address this issue, we propose a novel paradigm that enables LLMs to\ntrain itself by autonomously generating, cleaning, reviewing, and annotating\ndata with preference information, named LANCE. Our approach demonstrates that\nLLMs can serve as continuous self-evolving data engineers, significantly\nreducing the time and cost of the post-training data construction process.\nThrough iterative fine-tuning on different variants of the Qwen2, we validate\nthe effectiveness of LANCE across various tasks, showing that it can\ncontinuously improve model performance and maintain high-quality data\ngeneration. Across eight benchmark dimensions, LANCE resulted in an average\nscore enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This\ntraining paradigm with autonomous data construction not only reduces the\nreliance on human experts or external models but also ensures that the data\naligns with human values and preferences, paving the way for the development of\nfuture superintelligent systems that can exceed human capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\nvarious tasks, while the further evolvement is limited to the lack of\nhigh-quality training data. In addition, traditional training approaches rely\ntoo much on expert-labeled data, setting an upper limit on the performance of\nLLMs. To address this issue, we propose a novel paradigm that enables LLMs to\ntrain itself by autonomously generating, cleaning, reviewing, and annotating\ndata with preference information, named LANCE. Our approach demonstrates that\nLLMs can serve as continuous self-evolving data engineers, significantly\nreducing the time and cost of the post-training data construction process.\nThrough iterative fine-tuning on different variants of the Qwen2, we validate\nthe effectiveness of LANCE across various tasks, showing that it can\ncontinuously improve model performance and maintain high-quality data\ngeneration. Across eight benchmark dimensions, LANCE resulted in an average\nscore enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This\ntraining paradigm with autonomous data construction not only reduces the\nreliance on human experts or external models but also ensures that the data\naligns with human values and preferences, paving the way for the development of\nfuture superintelligent systems that can exceed human capabilities."
                },
                "authors": [
                    {
                        "name": "Peidong Wang"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03868v2",
                "updated": "2025-02-12T07:18:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    18,
                    9,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-06T08:28:41Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    28,
                    41,
                    3,
                    37,
                    0
                ],
                "title": "Time-based GNSS attack detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-based GNSS attack detection"
                },
                "summary": "To safeguard Civilian Global Navigation Satellite Systems (GNSS) external\ninformation available to the platform encompassing the GNSS receiver can be\nused to detect attacks. Cross-checking the GNSS-provided time against\nalternative multiple trusted time sources can lead to attack detection aiming\nat controlling the GNSS receiver time. Leveraging external, network-connected\nsecure time providers and onboard clock references, we achieve detection even\nunder fine-grained time attacks. We provide an extensive evaluation of our\nmulti-layered defense against adversaries mounting attacks against the GNSS\nreceiver along with controlling the network link. We implement adversaries\nspanning from simplistic spoofers to advanced ones synchronized with the GNSS\nconstellation. We demonstrate attack detection is possible in all tested cases\n(sharp discontinuity, smooth take-over, and coordinated network manipulation)\nwithout changes to the structure of the GNSS receiver. Leveraging the diversity\nof the reference time sources, detection of take-over time push as low as 150us\nis possible. Smooth take-overs forcing variations as low as 30ns are also\ndetected based on on-board precision oscillators. The method (and thus the\nevaluation) is largely agnostic to the satellite constellation and the attacker\ntype, making time-based data validation of GNSS information compatible with\nexisting receivers and readily deployable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To safeguard Civilian Global Navigation Satellite Systems (GNSS) external\ninformation available to the platform encompassing the GNSS receiver can be\nused to detect attacks. Cross-checking the GNSS-provided time against\nalternative multiple trusted time sources can lead to attack detection aiming\nat controlling the GNSS receiver time. Leveraging external, network-connected\nsecure time providers and onboard clock references, we achieve detection even\nunder fine-grained time attacks. We provide an extensive evaluation of our\nmulti-layered defense against adversaries mounting attacks against the GNSS\nreceiver along with controlling the network link. We implement adversaries\nspanning from simplistic spoofers to advanced ones synchronized with the GNSS\nconstellation. We demonstrate attack detection is possible in all tested cases\n(sharp discontinuity, smooth take-over, and coordinated network manipulation)\nwithout changes to the structure of the GNSS receiver. Leveraging the diversity\nof the reference time sources, detection of take-over time push as low as 150us\nis possible. Smooth take-overs forcing variations as low as 30ns are also\ndetected based on on-board precision oscillators. The method (and thus the\nevaluation) is largely agnostic to the satellite constellation and the attacker\ntype, making time-based data validation of GNSS information compatible with\nexisting receivers and readily deployable."
                },
                "authors": [
                    {
                        "name": "Marco Spanghero"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_doi": "10.1109/TAES.2024.3516708",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TAES.2024.3516708",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.03868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE Transactions on Aerospace and Electronic Systems (Early Access)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08160v1",
                "updated": "2025-02-12T07:03:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    3,
                    32,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T07:03:32Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    3,
                    32,
                    2,
                    43,
                    0
                ],
                "title": "Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly"
                },
                "summary": "Vertical Federated Learning (VFL) is a privacy-preserving collaborative\nlearning paradigm that enables multiple parties with distinct feature sets to\njointly train machine learning models without sharing their raw data. Despite\nits potential to facilitate cross-organizational collaborations, the deployment\nof VFL systems in real-world applications remains limited. To investigate the\ngap between existing VFL research and practical deployment, this survey\nanalyzes the real-world data distributions in potential VFL applications and\nidentifies four key findings that highlight this gap. We propose a novel\ndata-oriented taxonomy of VFL algorithms based on real VFL data distributions.\nOur comprehensive review of existing VFL algorithms reveals that some common\npractical VFL scenarios have few or no viable solutions. Based on these\nobservations, we outline key research directions aimed at bridging the gap\nbetween current VFL research and real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) is a privacy-preserving collaborative\nlearning paradigm that enables multiple parties with distinct feature sets to\njointly train machine learning models without sharing their raw data. Despite\nits potential to facilitate cross-organizational collaborations, the deployment\nof VFL systems in real-world applications remains limited. To investigate the\ngap between existing VFL research and practical deployment, this survey\nanalyzes the real-world data distributions in potential VFL applications and\nidentifies four key findings that highlight this gap. We propose a novel\ndata-oriented taxonomy of VFL algorithms based on real VFL data distributions.\nOur comprehensive review of existing VFL algorithms reveals that some common\npractical VFL scenarios have few or no viable solutions. Based on these\nobservations, we outline key research directions aimed at bridging the gap\nbetween current VFL research and real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhaomin Wu"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Haodong Zhao"
                    },
                    {
                        "name": "Qinbin Li"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Lixin Fan"
                    }
                ],
                "author_detail": {
                    "name": "Lixin Fan"
                },
                "author": "Lixin Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16769v2",
                "updated": "2025-02-12T06:39:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    6,
                    39,
                    7,
                    2,
                    43,
                    0
                ],
                "published": "2024-11-25T04:17:24Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    4,
                    17,
                    24,
                    0,
                    330,
                    0
                ],
                "title": "In-Context Experience Replay Facilitates Safety Red-Teaming of\n  Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Experience Replay Facilitates Safety Red-Teaming of\n  Text-to-Image Diffusion Models"
                },
                "summary": "Text-to-image (T2I) models have shown remarkable progress, but their\npotential to generate harmful content remains a critical concern in the ML\ncommunity. While various safety mechanisms have been developed, the field lacks\nsystematic tools for evaluating their effectiveness against real-world misuse\nscenarios. In this work, we propose ICER, a novel red-teaming framework that\nleverages Large Language Models (LLMs) and a bandit optimization-based\nalgorithm to generate interpretable and semantic meaningful problematic prompts\nby learning from past successful red-teaming attempts. Our ICER efficiently\nprobes safety mechanisms across different T2I models without requiring internal\naccess or additional training, making it broadly applicable to deployed\nsystems. Through extensive experiments, we demonstrate that ICER significantly\noutperforms existing prompt attack methods in identifying model vulnerabilities\nwhile maintaining high semantic similarity with intended content. By uncovering\nthat successful jailbreaking instances can systematically facilitate the\ndiscovery of new vulnerabilities, our work provides crucial insights for\ndeveloping more robust safety mechanisms in T2I systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) models have shown remarkable progress, but their\npotential to generate harmful content remains a critical concern in the ML\ncommunity. While various safety mechanisms have been developed, the field lacks\nsystematic tools for evaluating their effectiveness against real-world misuse\nscenarios. In this work, we propose ICER, a novel red-teaming framework that\nleverages Large Language Models (LLMs) and a bandit optimization-based\nalgorithm to generate interpretable and semantic meaningful problematic prompts\nby learning from past successful red-teaming attempts. Our ICER efficiently\nprobes safety mechanisms across different T2I models without requiring internal\naccess or additional training, making it broadly applicable to deployed\nsystems. Through extensive experiments, we demonstrate that ICER significantly\noutperforms existing prompt attack methods in identifying model vulnerabilities\nwhile maintaining high semantic similarity with intended content. By uncovering\nthat successful jailbreaking instances can systematically facilitate the\ndiscovery of new vulnerabilities, our work provides crucial insights for\ndeveloping more robust safety mechanisms in T2I systems."
                },
                "authors": [
                    {
                        "name": "Zhi-Yi Chin"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Wei-Chen Chiu"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Chen Chiu"
                },
                "author": "Wei-Chen Chiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00784v2",
                "updated": "2025-02-12T06:34:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    6,
                    34,
                    11,
                    2,
                    43,
                    0
                ],
                "published": "2024-10-17T06:44:18Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    6,
                    44,
                    18,
                    3,
                    291,
                    0
                ],
                "title": "FIRE: Fact-checking with Iterative Retrieval and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIRE: Fact-checking with Iterative Retrieval and Verification"
                },
                "summary": "Fact-checking long-form text is challenging, and it is therefore common\npractice to break it down into multiple atomic claims. The typical approach to\nfact-checking these atomic claims involves retrieving a fixed number of pieces\nof evidence, followed by a verification step. However, this method is usually\nnot cost-effective, as it underutilizes the verification model's internal\nknowledge of the claim and fails to replicate the iterative reasoning process\nin human search strategies. To address these limitations, we propose FIRE, a\nnovel agent-based framework that integrates evidence retrieval and claim\nverification in an iterative manner. Specifically, FIRE employs a unified\nmechanism to decide whether to provide a final answer or generate a subsequent\nsearch query, based on its confidence in the current judgment. We compare FIRE\nwith other strong fact-checking frameworks and find that it achieves slightly\nbetter performance while reducing large language model (LLM) costs by an\naverage of 7.6 times and search costs by 16.5 times. These results indicate\nthat FIRE holds promise for application in large-scale fact-checking\noperations. Our code is available at https://github.com/mbzuai-nlp/fire.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-checking long-form text is challenging, and it is therefore common\npractice to break it down into multiple atomic claims. The typical approach to\nfact-checking these atomic claims involves retrieving a fixed number of pieces\nof evidence, followed by a verification step. However, this method is usually\nnot cost-effective, as it underutilizes the verification model's internal\nknowledge of the claim and fails to replicate the iterative reasoning process\nin human search strategies. To address these limitations, we propose FIRE, a\nnovel agent-based framework that integrates evidence retrieval and claim\nverification in an iterative manner. Specifically, FIRE employs a unified\nmechanism to decide whether to provide a final answer or generate a subsequent\nsearch query, based on its confidence in the current judgment. We compare FIRE\nwith other strong fact-checking frameworks and find that it achieves slightly\nbetter performance while reducing large language model (LLM) costs by an\naverage of 7.6 times and search costs by 16.5 times. These results indicate\nthat FIRE holds promise for application in large-scale fact-checking\noperations. Our code is available at https://github.com/mbzuai-nlp/fire.git."
                },
                "authors": [
                    {
                        "name": "Zhuohan Xie"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Jiahui Geng"
                    },
                    {
                        "name": "Hasan Iqbal"
                    },
                    {
                        "name": "Dhruv Sahnan"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "arxiv_comment": "4 figures, 8 tables, accepted to Findings of NAACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02252v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02252v4",
                "updated": "2025-02-12T06:27:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    6,
                    27,
                    34,
                    2,
                    43,
                    0
                ],
                "published": "2024-07-02T13:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    13,
                    17,
                    49,
                    1,
                    184,
                    0
                ],
                "title": "GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion\n  Models and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion\n  Models and Large Language Models"
                },
                "summary": "Posters play a crucial role in marketing and advertising by enhancing visual\ncommunication and brand visibility, making significant contributions to\nindustrial design. With the latest advancements in controllable T2I diffusion\nmodels, increasing research has focused on rendering text within synthesized\nimages. Despite improvements in text rendering accuracy, the field of automatic\nposter generation remains underexplored. In this paper, we propose an automatic\nposter generation framework with text rendering capabilities leveraging LLMs,\nutilizing a triple-cross attention mechanism based on alignment learning. This\nframework aims to create precise poster text within a detailed contextual\nbackground. Additionally, the framework supports controllable fonts, adjustable\nimage resolution, and the rendering of posters with descriptions and text in\nboth English and Chinese.Furthermore, we introduce a high-resolution font\ndataset and a poster dataset with resolutions exceeding 1024 pixels. Our\napproach leverages the SDXL architecture. Extensive experiments validate our\nmethod's capability in generating poster images with complex and contextually\nrich backgrounds.Codes is available at\nhttps://github.com/OPPO-Mente-Lab/GlyphDraw2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Posters play a crucial role in marketing and advertising by enhancing visual\ncommunication and brand visibility, making significant contributions to\nindustrial design. With the latest advancements in controllable T2I diffusion\nmodels, increasing research has focused on rendering text within synthesized\nimages. Despite improvements in text rendering accuracy, the field of automatic\nposter generation remains underexplored. In this paper, we propose an automatic\nposter generation framework with text rendering capabilities leveraging LLMs,\nutilizing a triple-cross attention mechanism based on alignment learning. This\nframework aims to create precise poster text within a detailed contextual\nbackground. Additionally, the framework supports controllable fonts, adjustable\nimage resolution, and the rendering of posters with descriptions and text in\nboth English and Chinese.Furthermore, we introduce a high-resolution font\ndataset and a poster dataset with resolutions exceeding 1024 pixels. Our\napproach leverages the SDXL architecture. Extensive experiments validate our\nmethod's capability in generating poster images with complex and contextually\nrich backgrounds.Codes is available at\nhttps://github.com/OPPO-Mente-Lab/GlyphDraw2."
                },
                "authors": [
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Yonglin Deng"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Nanyang Du"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Zhenyu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Yang"
                },
                "author": "Zhenyu Yang",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02252v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02252v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08148v1",
                "updated": "2025-02-12T06:19:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    6,
                    19,
                    2,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T06:19:02Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    6,
                    19,
                    2,
                    2,
                    43,
                    0
                ],
                "title": "ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning"
                },
                "summary": "Identifying cause-and-effect relationships is critical to understanding\nreal-world dynamics and ultimately causal reasoning. Existing methods for\nidentifying event causality in NLP, including those based on Large Language\nModels (LLMs), exhibit difficulties in out-of-distribution settings due to the\nlimited scale and heavy reliance on lexical cues within available benchmarks.\nModern benchmarks, inspired by probabilistic causal inference, have attempted\nto construct causal graphs of events as a robust representation of causal\nknowledge, where \\texttt{CRAB} \\citep{romanou2023crab} is one such recent\nbenchmark along this line. In this paper, we introduce \\texttt{ACCESS}, a\nbenchmark designed for discovery and reasoning over abstract causal events.\nUnlike existing resources, \\texttt{ACCESS} focuses on causality of everyday\nlife events on the abstraction level. We propose a pipeline for identifying\nabstractions for event generalizations from \\texttt{GLUCOSE}\n\\citep{mostafazadeh-etal-2020-glucose}, a large-scale dataset of implicit\ncommonsense causal knowledge, from which we subsequently extract $1,4$K causal\npairs. Our experiments highlight the ongoing challenges of using statistical\nmethods and/or LLMs for automatic abstraction identification and causal\ndiscovery in NLP. Nonetheless, we demonstrate that the abstract causal\nknowledge provided in \\texttt{ACCESS} can be leveraged for enhancing QA\nreasoning performance in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying cause-and-effect relationships is critical to understanding\nreal-world dynamics and ultimately causal reasoning. Existing methods for\nidentifying event causality in NLP, including those based on Large Language\nModels (LLMs), exhibit difficulties in out-of-distribution settings due to the\nlimited scale and heavy reliance on lexical cues within available benchmarks.\nModern benchmarks, inspired by probabilistic causal inference, have attempted\nto construct causal graphs of events as a robust representation of causal\nknowledge, where \\texttt{CRAB} \\citep{romanou2023crab} is one such recent\nbenchmark along this line. In this paper, we introduce \\texttt{ACCESS}, a\nbenchmark designed for discovery and reasoning over abstract causal events.\nUnlike existing resources, \\texttt{ACCESS} focuses on causality of everyday\nlife events on the abstraction level. We propose a pipeline for identifying\nabstractions for event generalizations from \\texttt{GLUCOSE}\n\\citep{mostafazadeh-etal-2020-glucose}, a large-scale dataset of implicit\ncommonsense causal knowledge, from which we subsequently extract $1,4$K causal\npairs. Our experiments highlight the ongoing challenges of using statistical\nmethods and/or LLMs for automatic abstraction identification and causal\ndiscovery in NLP. Nonetheless, we demonstrate that the abstract causal\nknowledge provided in \\texttt{ACCESS} can be leveraged for enhancing QA\nreasoning performance in LLMs."
                },
                "authors": [
                    {
                        "name": "Vy Vo"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Yuncheng Hua"
                    },
                    {
                        "name": "Xiaoxi Kang"
                    },
                    {
                        "name": "Songhai Fan"
                    },
                    {
                        "name": "Tim Dwyer"
                    },
                    {
                        "name": "Lay-Ki Soon"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference of the North American Chapter\n  of the Association for Computational Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05206v2",
                "updated": "2025-02-12T06:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    6,
                    16,
                    0,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-02T05:14:22Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    5,
                    14,
                    22,
                    6,
                    33,
                    0
                ],
                "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety at Scale: A Comprehensive Survey of Large Model Safety"
                },
                "summary": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models."
                },
                "authors": [
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Ye Sun"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Hengyuan Xu"
                    },
                    {
                        "name": "Yunhao Chen"
                    },
                    {
                        "name": "Yunhan Zhao"
                    },
                    {
                        "name": "Hanxun Huang"
                    },
                    {
                        "name": "Yige Li"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Jingfeng Zhang"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Jun Sun"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Baoyuan Wu"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Cihang Xie"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Ruoxi Jia"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Neil Gong"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Sarah Erfani"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Masashi Sugiyama"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "James Bailey"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "47 pages, 3 figures, 11 tables GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09997v2",
                "updated": "2025-02-12T06:15:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    6,
                    15,
                    17,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-17T07:30:01Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    7,
                    30,
                    1,
                    4,
                    17,
                    0
                ],
                "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-guided Self-reflection for Zero-shot Hallucination Detection\n  in Large Language Models"
                },
                "summary": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination has emerged as a significant barrier to the effective\napplication of Large Language Models (LLMs). In this work, we introduce a novel\nAttention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination\ndetection in LLMs. The AGSER method utilizes attention contributions to\ncategorize the input query into attentive and non-attentive queries. Each query\nis then processed separately through the LLMs, allowing us to compute\nconsistency scores between the generated responses and the original answer. The\ndifference between the two consistency scores serves as a hallucination\nestimator. In addition to its efficacy in detecting hallucinations, AGSER\nnotably reduces computational overhead, requiring only three passes through the\nLLM and utilizing two sets of tokens. We have conducted extensive experiments\nwith four widely-used LLMs across three different hallucination benchmarks,\ndemonstrating that our approach significantly outperforms existing methods in\nzero-shot hallucination detection."
                },
                "authors": [
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Xinlong Chen"
                    },
                    {
                        "name": "Yue Ding"
                    },
                    {
                        "name": "Shizhen Xu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08145v1",
                "updated": "2025-02-12T06:05:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    6,
                    5,
                    52,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T06:05:52Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    6,
                    5,
                    52,
                    2,
                    43,
                    0
                ],
                "title": "Democratizing AI: Open-source Scalable LLM Training on GPU-based\n  Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing AI: Open-source Scalable LLM Training on GPU-based\n  Supercomputers"
                },
                "summary": "Training and fine-tuning large language models (LLMs) with hundreds of\nbillions to trillions of parameters requires tens of thousands of GPUs, and a\nhighly scalable software stack. In this work, we present a novel\nfour-dimensional hybrid parallel algorithm implemented in a highly scalable,\nportable, open-source framework called AxoNN. We describe several performance\noptimizations in AxoNN to improve matrix multiply kernel performance, overlap\nnon-blocking collectives with computation, and performance modeling to choose\nperformance optimal configurations. These have resulted in unprecedented\nscaling and peak flop/s (bf16) for training of GPT-style transformer models on\nPerlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423\nExaflop/s).\n  While the abilities of LLMs improve with the number of trainable parameters,\nso do privacy and copyright risks caused by memorization of training data,\nwhich can cause disclosure of sensitive or private information at inference\ntime. We highlight this side effect of scale through experiments that explore\n\"catastrophic memorization\", where models are sufficiently large to memorize\ntraining data in a single pass, and present an approach to prevent it. As part\nof this study, we demonstrate fine-tuning of a 405-billion parameter LLM using\nAxoNN on Frontier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and fine-tuning large language models (LLMs) with hundreds of\nbillions to trillions of parameters requires tens of thousands of GPUs, and a\nhighly scalable software stack. In this work, we present a novel\nfour-dimensional hybrid parallel algorithm implemented in a highly scalable,\nportable, open-source framework called AxoNN. We describe several performance\noptimizations in AxoNN to improve matrix multiply kernel performance, overlap\nnon-blocking collectives with computation, and performance modeling to choose\nperformance optimal configurations. These have resulted in unprecedented\nscaling and peak flop/s (bf16) for training of GPT-style transformer models on\nPerlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423\nExaflop/s).\n  While the abilities of LLMs improve with the number of trainable parameters,\nso do privacy and copyright risks caused by memorization of training data,\nwhich can cause disclosure of sensitive or private information at inference\ntime. We highlight this side effect of scale through experiments that explore\n\"catastrophic memorization\", where models are sufficiently large to memorize\ntraining data in a single pass, and present an approach to prevent it. As part\nof this study, we demonstrate fine-tuning of a 405-billion parameter LLM using\nAxoNN on Frontier."
                },
                "authors": [
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Aditya Ranjan"
                    },
                    {
                        "name": "John Kirchenbauer"
                    },
                    {
                        "name": "Jonas Geiping"
                    },
                    {
                        "name": "Yuxin Wen"
                    },
                    {
                        "name": "Neel Jain"
                    },
                    {
                        "name": "Abhimanyu Hans"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08142v1",
                "updated": "2025-02-12T05:48:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    48,
                    57,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T05:48:57Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    48,
                    57,
                    2,
                    43,
                    0
                ],
                "title": "Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM\n  Inferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM\n  Inferences"
                },
                "summary": "We present Wildflare GuardRail, a guardrail pipeline designed to enhance the\nsafety and reliability of Large Language Model (LLM) inferences by\nsystematically addressing risks across the entire processing workflow.\nWildflare GuardRail integrates several core functional modules, including\nSafety Detector that identifies unsafe inputs and detects hallucinations in\nmodel outputs while generating root-cause explanations, Grounding that\ncontextualizes user queries with information retrieved from vector databases,\nCustomizer that adjusts outputs in real time using lightweight, rule-based\nwrappers, and Repairer that corrects erroneous LLM outputs using hallucination\nexplanations provided by Safety Detector. Results show that our unsafe content\ndetection model in Safety Detector achieves comparable performance with OpenAI\nAPI, though trained on a small dataset constructed with several public\ndatasets. Meanwhile, the lightweight wrappers can address malicious URLs in\nmodel outputs in 1.06s per query with 100% accuracy without costly model calls.\nMoreover, the hallucination fixing model demonstrates effectiveness in reducing\nhallucinations with an accuracy of 80.7%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Wildflare GuardRail, a guardrail pipeline designed to enhance the\nsafety and reliability of Large Language Model (LLM) inferences by\nsystematically addressing risks across the entire processing workflow.\nWildflare GuardRail integrates several core functional modules, including\nSafety Detector that identifies unsafe inputs and detects hallucinations in\nmodel outputs while generating root-cause explanations, Grounding that\ncontextualizes user queries with information retrieved from vector databases,\nCustomizer that adjusts outputs in real time using lightweight, rule-based\nwrappers, and Repairer that corrects erroneous LLM outputs using hallucination\nexplanations provided by Safety Detector. Results show that our unsafe content\ndetection model in Safety Detector achieves comparable performance with OpenAI\nAPI, though trained on a small dataset constructed with several public\ndatasets. Meanwhile, the lightweight wrappers can address malicious URLs in\nmodel outputs in 1.06s per query with 100% accuracy without costly model calls.\nMoreover, the hallucination fixing model demonstrates effectiveness in reducing\nhallucinations with an accuracy of 80.7%."
                },
                "authors": [
                    {
                        "name": "Shanshan Han"
                    },
                    {
                        "name": "Salman Avestimehr"
                    },
                    {
                        "name": "Chaoyang He"
                    }
                ],
                "author_detail": {
                    "name": "Chaoyang He"
                },
                "author": "Chaoyang He",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2406.10847",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08141v1",
                "updated": "2025-02-12T05:48:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    48,
                    26,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T05:48:26Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    48,
                    26,
                    2,
                    43,
                    0
                ],
                "title": "LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits"
                },
                "summary": "Fine-tuning large language models (LLMs) is increasingly costly as models\nscale to hundreds of billions of parameters, and even parameter-efficient\nfine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce\nLowRA, the first framework to enable LoRA fine-tuning below 2 bits per\nparameter with minimal performance loss. LowRA optimizes fine-grained\nquantization - mapping, threshold selection, and precision assignment - while\nleveraging efficient CUDA kernels for scalable deployment. Extensive\nevaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior\nperformance-precision trade-off above 2 bits and remains accurate down to 1.15\nbits, reducing memory usage by up to 50%. Our results highlight the potential\nof ultra-low-bit LoRA fine-tuning for resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is increasingly costly as models\nscale to hundreds of billions of parameters, and even parameter-efficient\nfine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce\nLowRA, the first framework to enable LoRA fine-tuning below 2 bits per\nparameter with minimal performance loss. LowRA optimizes fine-grained\nquantization - mapping, threshold selection, and precision assignment - while\nleveraging efficient CUDA kernels for scalable deployment. Extensive\nevaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior\nperformance-precision trade-off above 2 bits and remains accurate down to 1.15\nbits, reducing memory usage by up to 50%. Our results highlight the potential\nof ultra-low-bit LoRA fine-tuning for resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Zikai Zhou"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Hermann Kumbong"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15504v2",
                "updated": "2025-02-12T05:45:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    45,
                    21,
                    2,
                    43,
                    0
                ],
                "published": "2024-12-20T02:35:39Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    2,
                    35,
                    39,
                    4,
                    355,
                    0
                ],
                "title": "Mitigating Social Bias in Large Language Models: A Multi-Objective\n  Approach within a Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Social Bias in Large Language Models: A Multi-Objective\n  Approach within a Multi-Agent Framework"
                },
                "summary": "Natural language processing (NLP) has seen remarkable advancements with the\ndevelopment of large language models (LLMs). Despite these advancements, LLMs\noften produce socially biased outputs. Recent studies have mainly addressed\nthis problem by prompting LLMs to behave ethically, but this approach results\nin unacceptable performance degradation. In this paper, we propose a\nmulti-objective approach within a multi-agent framework (MOMA) to mitigate\nsocial bias in LLMs without significantly compromising their performance. The\nkey idea of MOMA involves deploying multiple agents to perform causal\ninterventions on bias-related contents of the input questions, breaking the\nshortcut connection between these contents and the corresponding answers.\nUnlike traditional debiasing techniques leading to performance degradation,\nMOMA substantially reduces bias while maintaining accuracy in downstream tasks.\nOur experiments conducted on two datasets and two models demonstrate that MOMA\nreduces bias scores by up to 87.7%, with only a marginal performance\ndegradation of up to 6.8% in the BBQ dataset. Additionally, it significantly\nenhances the multi-objective metric icat in the StereoSet dataset by up to\n58.1%. Code will be made available at https://github.com/Cortantse/MOMA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing (NLP) has seen remarkable advancements with the\ndevelopment of large language models (LLMs). Despite these advancements, LLMs\noften produce socially biased outputs. Recent studies have mainly addressed\nthis problem by prompting LLMs to behave ethically, but this approach results\nin unacceptable performance degradation. In this paper, we propose a\nmulti-objective approach within a multi-agent framework (MOMA) to mitigate\nsocial bias in LLMs without significantly compromising their performance. The\nkey idea of MOMA involves deploying multiple agents to perform causal\ninterventions on bias-related contents of the input questions, breaking the\nshortcut connection between these contents and the corresponding answers.\nUnlike traditional debiasing techniques leading to performance degradation,\nMOMA substantially reduces bias while maintaining accuracy in downstream tasks.\nOur experiments conducted on two datasets and two models demonstrate that MOMA\nreduces bias scores by up to 87.7%, with only a marginal performance\ndegradation of up to 6.8% in the BBQ dataset. Additionally, it significantly\nenhances the multi-objective metric icat in the StereoSet dataset by up to\n58.1%. Code will be made available at https://github.com/Cortantse/MOMA."
                },
                "authors": [
                    {
                        "name": "Zhenjie Xu"
                    },
                    {
                        "name": "Wenqing Chen"
                    },
                    {
                        "name": "Yi Tang"
                    },
                    {
                        "name": "Xuanying Li"
                    },
                    {
                        "name": "Cheng Hu"
                    },
                    {
                        "name": "Zhixuan Chu"
                    },
                    {
                        "name": "Kui Ren"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Zhichao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Lu"
                },
                "arxiv_affiliation": "Department of Computer Science, City University of Hong Kong",
                "author": "Zhichao Lu",
                "arxiv_comment": "This work has been accepted at The 39th Annual AAAI Conference on\n  Artificial Intelligence (AAAI-2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18418v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18418v3",
                "updated": "2025-02-12T05:42:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    42,
                    9,
                    2,
                    43,
                    0
                ],
                "published": "2024-07-25T22:31:50Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    22,
                    31,
                    50,
                    3,
                    207,
                    0
                ],
                "title": "Know Your Limits: A Survey of Abstention in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know Your Limits: A Survey of Abstention in Large Language Models"
                },
                "summary": "Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future research, such as\nwhether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, and opportunities to optimize abstention abilities\nin specific contexts. In doing so, we aim to broaden the scope and impact of\nabstention methodologies in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future research, such as\nwhether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, and opportunities to optimize abstention abilities\nin specific contexts. In doing so, we aim to broaden the scope and impact of\nabstention methodologies in AI systems."
                },
                "authors": [
                    {
                        "name": "Bingbing Wen"
                    },
                    {
                        "name": "Jihan Yao"
                    },
                    {
                        "name": "Shangbin Feng"
                    },
                    {
                        "name": "Chenjun Xu"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Bill Howe"
                    },
                    {
                        "name": "Lucy Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lucy Lu Wang"
                },
                "author": "Lucy Lu Wang",
                "arxiv_comment": "TACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18418v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18418v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14249v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14249v3",
                "updated": "2025-02-12T05:39:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    39,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-24T05:27:46Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    27,
                    46,
                    4,
                    24,
                    0
                ],
                "title": "Humanity's Last Exam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanity's Last Exam"
                },
                "summary": "Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 3,000\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 3,000\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai."
                },
                "authors": [
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Alice Gatti"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Nathaniel Li"
                    },
                    {
                        "name": "Josephina Hu"
                    },
                    {
                        "name": "Hugh Zhang"
                    },
                    {
                        "name": "Chen Bo Calvin Zhang"
                    },
                    {
                        "name": "Mohamed Shaaban"
                    },
                    {
                        "name": "John Ling"
                    },
                    {
                        "name": "Sean Shi"
                    },
                    {
                        "name": "Michael Choi"
                    },
                    {
                        "name": "Anish Agrawal"
                    },
                    {
                        "name": "Arnav Chopra"
                    },
                    {
                        "name": "Adam Khoja"
                    },
                    {
                        "name": "Ryan Kim"
                    },
                    {
                        "name": "Richard Ren"
                    },
                    {
                        "name": "Jason Hausenloy"
                    },
                    {
                        "name": "Oliver Zhang"
                    },
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Tung Nguyen"
                    },
                    {
                        "name": "Daron Anderson"
                    },
                    {
                        "name": "Imad Ali Shah"
                    },
                    {
                        "name": "Mikhail Doroshenko"
                    },
                    {
                        "name": "Alun Cennyth Stokes"
                    },
                    {
                        "name": "Mobeen Mahmood"
                    },
                    {
                        "name": "Jaeho Lee"
                    },
                    {
                        "name": "Oleksandr Pokutnyi"
                    },
                    {
                        "name": "Oleg Iskra"
                    },
                    {
                        "name": "Jessica P. Wang"
                    },
                    {
                        "name": "Robert Gerbicz"
                    },
                    {
                        "name": "John-Clark Levin"
                    },
                    {
                        "name": "Serguei Popov"
                    },
                    {
                        "name": "Fiona Feng"
                    },
                    {
                        "name": "Steven Y. Feng"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Michael Yu"
                    },
                    {
                        "name": "Varun Gangal"
                    },
                    {
                        "name": "Chelsea Zou"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Mstyslav Kazakov"
                    },
                    {
                        "name": "Geoff Galgon"
                    },
                    {
                        "name": "Johannes Schmitt"
                    },
                    {
                        "name": "Alvaro Sanchez"
                    },
                    {
                        "name": "Yongki Lee"
                    },
                    {
                        "name": "Will Yeadon"
                    },
                    {
                        "name": "Scott Sauers"
                    },
                    {
                        "name": "Marc Roth"
                    },
                    {
                        "name": "Chidozie Agu"
                    },
                    {
                        "name": "Søren Riis"
                    },
                    {
                        "name": "Fabian Giska"
                    },
                    {
                        "name": "Saiteja Utpala"
                    },
                    {
                        "name": "Antrell Cheatom"
                    },
                    {
                        "name": "Zachary Giboney"
                    },
                    {
                        "name": "Gashaw M. Goshu"
                    },
                    {
                        "name": "Sarah-Jane Crowson"
                    },
                    {
                        "name": "Mohinder Maheshbhai Naiya"
                    },
                    {
                        "name": "Noah Burns"
                    },
                    {
                        "name": "Lennart Finke"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Hyunwoo Park"
                    },
                    {
                        "name": "Francesco Fournier-Facio"
                    },
                    {
                        "name": "Jennifer Zampese"
                    },
                    {
                        "name": "John Wydallis"
                    },
                    {
                        "name": "John B. Wydallis"
                    },
                    {
                        "name": "Ryan G. Hoerr"
                    },
                    {
                        "name": "Mark Nandor"
                    },
                    {
                        "name": "Tim Gehrunger"
                    },
                    {
                        "name": "Jiaqi Cai"
                    },
                    {
                        "name": "Ben McCarty"
                    },
                    {
                        "name": "Jungbae Nam"
                    },
                    {
                        "name": "Edwin Taylor"
                    },
                    {
                        "name": "Jun Jin"
                    },
                    {
                        "name": "Gautier Abou Loume"
                    },
                    {
                        "name": "Hangrui Cao"
                    },
                    {
                        "name": "Alexis C Garretson"
                    },
                    {
                        "name": "Damien Sileo"
                    },
                    {
                        "name": "Qiuyu Ren"
                    },
                    {
                        "name": "Doru Cojoc"
                    },
                    {
                        "name": "Pavel Arkhipov"
                    },
                    {
                        "name": "Usman Qazi"
                    },
                    {
                        "name": "Aras Bacho"
                    },
                    {
                        "name": "Lianghui Li"
                    },
                    {
                        "name": "Sumeet Motwani"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Alexei Kopylov"
                    },
                    {
                        "name": "Johannes Veith"
                    },
                    {
                        "name": "Eric Singer"
                    },
                    {
                        "name": "Paolo Rissone"
                    },
                    {
                        "name": "Jaehyeok Jin"
                    },
                    {
                        "name": "Jack Wei Lun Shi"
                    },
                    {
                        "name": "Chris G. Willcocks"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Longke Tang"
                    },
                    {
                        "name": "Kevin Zhou"
                    },
                    {
                        "name": "Emily de Oliveira Santos"
                    },
                    {
                        "name": "Andrey Pupasov Maksimov"
                    },
                    {
                        "name": "Edward Vendrow"
                    },
                    {
                        "name": "Kengo Zenitani"
                    },
                    {
                        "name": "Joshua Robinson"
                    },
                    {
                        "name": "Aleksandar Mikov"
                    },
                    {
                        "name": "Julien Guillod"
                    },
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Ben Pageler"
                    },
                    {
                        "name": "Joshua Vendrow"
                    },
                    {
                        "name": "Vladyslav Kuchkin"
                    },
                    {
                        "name": "Pierre Marion"
                    },
                    {
                        "name": "Denis Efremov"
                    },
                    {
                        "name": "Jayson Lynch"
                    },
                    {
                        "name": "Kaiqu Liang"
                    },
                    {
                        "name": "Andrew Gritsevskiy"
                    },
                    {
                        "name": "Dakotah Martinez"
                    },
                    {
                        "name": "Nick Crispino"
                    },
                    {
                        "name": "Dimitri Zvonkine"
                    },
                    {
                        "name": "Natanael Wildner Fraga"
                    },
                    {
                        "name": "Saeed Soori"
                    },
                    {
                        "name": "Ori Press"
                    },
                    {
                        "name": "Henry Tang"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Sean R. Green"
                    },
                    {
                        "name": "Lina Brüssel"
                    },
                    {
                        "name": "Moon Twayana"
                    },
                    {
                        "name": "Aymeric Dieuleveut"
                    },
                    {
                        "name": "T. Ryan Rogers"
                    },
                    {
                        "name": "Wenjin Zhang"
                    },
                    {
                        "name": "Ross Finocchio"
                    },
                    {
                        "name": "Bikun Li"
                    },
                    {
                        "name": "Jinzhou Yang"
                    },
                    {
                        "name": "Arun Rao"
                    },
                    {
                        "name": "Gabriel Loiseau"
                    },
                    {
                        "name": "Mikhail Kalinin"
                    },
                    {
                        "name": "Marco Lukas"
                    },
                    {
                        "name": "Ciprian Manolescu"
                    },
                    {
                        "name": "Nate Stambaugh"
                    },
                    {
                        "name": "Subrata Mishra"
                    },
                    {
                        "name": "Ariel Ghislain Kemogne Kamdoum"
                    },
                    {
                        "name": "Tad Hogg"
                    },
                    {
                        "name": "Alvin Jin"
                    },
                    {
                        "name": "Carlo Bosio"
                    },
                    {
                        "name": "Gongbo Sun"
                    },
                    {
                        "name": "Brian P Coppola"
                    },
                    {
                        "name": "Haline Heidinger"
                    },
                    {
                        "name": "Rafael Sayous"
                    },
                    {
                        "name": "Stefan Ivanov"
                    },
                    {
                        "name": "Joseph M Cavanagh"
                    },
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Joseph Marvin Imperial"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Shaipranesh Senthilkuma"
                    },
                    {
                        "name": "Andres M Bran"
                    },
                    {
                        "name": "Andres Algaba"
                    },
                    {
                        "name": "Brecht Verbeken"
                    },
                    {
                        "name": "Kelsey Van den Houte"
                    },
                    {
                        "name": "Lynn Van Der Sypt"
                    },
                    {
                        "name": "David Noever"
                    },
                    {
                        "name": "Lisa Schut"
                    },
                    {
                        "name": "Ilia Sucholutsky"
                    },
                    {
                        "name": "Evgenii Zheltonozhskii"
                    },
                    {
                        "name": "Qiaochu Yuan"
                    },
                    {
                        "name": "Derek Lim"
                    },
                    {
                        "name": "Richard Stanley"
                    },
                    {
                        "name": "Shankar Sivarajan"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "John Maar"
                    },
                    {
                        "name": "Julian Wykowski"
                    },
                    {
                        "name": "Martí Oller"
                    },
                    {
                        "name": "Jennifer Sandlin"
                    },
                    {
                        "name": "Anmol Sahu"
                    },
                    {
                        "name": "Cesare Giulio Ardito"
                    },
                    {
                        "name": "Yuzheng Hu"
                    },
                    {
                        "name": "Felipe Meneguitti Dias"
                    },
                    {
                        "name": "Tobias Kreiman"
                    },
                    {
                        "name": "Kaivalya Rawal"
                    },
                    {
                        "name": "Tobias Garcia Vilchis"
                    },
                    {
                        "name": "Yuexuan Zu"
                    },
                    {
                        "name": "Martin Lackner"
                    },
                    {
                        "name": "James Koppel"
                    },
                    {
                        "name": "Jeremy Nguyen"
                    },
                    {
                        "name": "Daniil S. Antonenko"
                    },
                    {
                        "name": "Steffi Chern"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Pierrot Arsene"
                    },
                    {
                        "name": "Sergey Ivanov"
                    },
                    {
                        "name": "Rafał Poświata"
                    },
                    {
                        "name": "Chenguang Wang"
                    },
                    {
                        "name": "Daofeng Li"
                    },
                    {
                        "name": "Donato Crisostomi"
                    },
                    {
                        "name": "Ali Dehghan"
                    },
                    {
                        "name": "Andrea Achilleos"
                    },
                    {
                        "name": "John Arnold Ambay"
                    },
                    {
                        "name": "Benjamin Myklebust"
                    },
                    {
                        "name": "Archan Sen"
                    },
                    {
                        "name": "David Perrella"
                    },
                    {
                        "name": "Nurdin Kaparov"
                    },
                    {
                        "name": "Mark H Inlow"
                    },
                    {
                        "name": "Allen Zang"
                    },
                    {
                        "name": "Kalyan Ramakrishnan"
                    },
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Vladislav Poritski"
                    },
                    {
                        "name": "Shalev Ben-David"
                    },
                    {
                        "name": "Zachary Berger"
                    },
                    {
                        "name": "Parker Whitfill"
                    },
                    {
                        "name": "Michael Foster"
                    },
                    {
                        "name": "Daniel Munro"
                    },
                    {
                        "name": "Linh Ho"
                    },
                    {
                        "name": "Dan Bar Hava"
                    },
                    {
                        "name": "Aleksey Kuchkin"
                    },
                    {
                        "name": "Robert Lauff"
                    },
                    {
                        "name": "David Holmes"
                    },
                    {
                        "name": "Frank Sommerhage"
                    },
                    {
                        "name": "Anji Zhang"
                    },
                    {
                        "name": "Richard Moat"
                    },
                    {
                        "name": "Keith Schneider"
                    },
                    {
                        "name": "Daniel Pyda"
                    },
                    {
                        "name": "Zakayo Kazibwe"
                    },
                    {
                        "name": "Mukhwinder Singh"
                    },
                    {
                        "name": "Don Clarke"
                    },
                    {
                        "name": "Dae Hyun Kim"
                    },
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Veit Elser"
                    },
                    {
                        "name": "Victor Efren Guadarrama Vilchis"
                    },
                    {
                        "name": "Immo Klose"
                    },
                    {
                        "name": "Christoph Demian"
                    },
                    {
                        "name": "Ujjwala Anantheswaran"
                    },
                    {
                        "name": "Adam Zweiger"
                    },
                    {
                        "name": "Guglielmo Albani"
                    },
                    {
                        "name": "Jeffery Li"
                    },
                    {
                        "name": "Nicolas Daans"
                    },
                    {
                        "name": "Maksim Radionov"
                    },
                    {
                        "name": "Václav Rozhoň"
                    },
                    {
                        "name": "Vincent Ginis"
                    },
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Christian Stump"
                    },
                    {
                        "name": "Jacob Platnick"
                    },
                    {
                        "name": "Volodymyr Nevirkovets"
                    },
                    {
                        "name": "Luke Basler"
                    },
                    {
                        "name": "Marco Piccardo"
                    },
                    {
                        "name": "Niv Cohen"
                    },
                    {
                        "name": "Virendra Singh"
                    },
                    {
                        "name": "Josef Tkadlec"
                    },
                    {
                        "name": "Paul Rosu"
                    },
                    {
                        "name": "Alan Goldfarb"
                    },
                    {
                        "name": "Piotr Padlewski"
                    },
                    {
                        "name": "Stanislaw Barzowski"
                    },
                    {
                        "name": "Kyle Montgomery"
                    },
                    {
                        "name": "Aline Menezes"
                    },
                    {
                        "name": "Arkil Patel"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jamie Tucker-Foltz"
                    },
                    {
                        "name": "Jack Stade"
                    },
                    {
                        "name": "Declan Grabb"
                    },
                    {
                        "name": "Tom Goertzen"
                    },
                    {
                        "name": "Fereshteh Kazemi"
                    },
                    {
                        "name": "Jeremiah Milbauer"
                    },
                    {
                        "name": "Abhishek Shukla"
                    },
                    {
                        "name": "Hossam Elgnainy"
                    },
                    {
                        "name": "Yan Carlos Leyva Labrador"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Ling Zhang"
                    },
                    {
                        "name": "Alan Givré"
                    },
                    {
                        "name": "Hew Wolff"
                    },
                    {
                        "name": "Gözdenur Demir"
                    },
                    {
                        "name": "Muhammad Fayez Aziz"
                    },
                    {
                        "name": "Younesse Kaddar"
                    },
                    {
                        "name": "Ivar Ängquist"
                    },
                    {
                        "name": "Yanxu Chen"
                    },
                    {
                        "name": "Elliott Thornley"
                    },
                    {
                        "name": "Robin Zhang"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Antonio Terpin"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Hailey Schoelkopf"
                    },
                    {
                        "name": "Eric Zheng"
                    },
                    {
                        "name": "Avishy Carmi"
                    },
                    {
                        "name": "Jainam Shah"
                    },
                    {
                        "name": "Ethan D. L. Brown"
                    },
                    {
                        "name": "Kelin Zhu"
                    },
                    {
                        "name": "Max Bartolo"
                    },
                    {
                        "name": "Richard Wheeler"
                    },
                    {
                        "name": "Andrew Ho"
                    },
                    {
                        "name": "Shaul Barkan"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Martin Stehberger"
                    },
                    {
                        "name": "Egor Kretov"
                    },
                    {
                        "name": "Peter Bradshaw"
                    },
                    {
                        "name": "JP Heimonen"
                    },
                    {
                        "name": "Kaustubh Sridhar"
                    },
                    {
                        "name": "Zaki Hossain"
                    },
                    {
                        "name": "Ido Akov"
                    },
                    {
                        "name": "Yury Makarychev"
                    },
                    {
                        "name": "Joanna Tam"
                    },
                    {
                        "name": "Hieu Hoang"
                    },
                    {
                        "name": "David M. Cunningham"
                    },
                    {
                        "name": "Vladimir Goryachev"
                    },
                    {
                        "name": "Demosthenes Patramanis"
                    },
                    {
                        "name": "Michael Krause"
                    },
                    {
                        "name": "Andrew Redenti"
                    },
                    {
                        "name": "David Aldous"
                    },
                    {
                        "name": "Jesyin Lai"
                    },
                    {
                        "name": "Shannon Coleman"
                    },
                    {
                        "name": "Jiangnan Xu"
                    },
                    {
                        "name": "Sangwon Lee"
                    },
                    {
                        "name": "Ilias Magoulas"
                    },
                    {
                        "name": "Sandy Zhao"
                    },
                    {
                        "name": "Ning Tang"
                    },
                    {
                        "name": "Michael K. Cohen"
                    },
                    {
                        "name": "Micah Carroll"
                    },
                    {
                        "name": "Orr Paradise"
                    },
                    {
                        "name": "Jan Hendrik Kirchner"
                    },
                    {
                        "name": "Stefan Steinerberger"
                    },
                    {
                        "name": "Maksym Ovchynnikov"
                    },
                    {
                        "name": "Jason O. Matos"
                    },
                    {
                        "name": "Adithya Shenoy"
                    },
                    {
                        "name": "Michael Wang"
                    },
                    {
                        "name": "Yuzhou Nie"
                    },
                    {
                        "name": "Paolo Giordano"
                    },
                    {
                        "name": "Philipp Petersen"
                    },
                    {
                        "name": "Anna Sztyber-Betley"
                    },
                    {
                        "name": "Paolo Faraboschi"
                    },
                    {
                        "name": "Robin Riblet"
                    },
                    {
                        "name": "Jonathan Crozier"
                    },
                    {
                        "name": "Shiv Halasyamani"
                    },
                    {
                        "name": "Antonella Pinto"
                    },
                    {
                        "name": "Shreyas Verma"
                    },
                    {
                        "name": "Prashant Joshi"
                    },
                    {
                        "name": "Eli Meril"
                    },
                    {
                        "name": "Zheng-Xin Yong"
                    },
                    {
                        "name": "Allison Tee"
                    },
                    {
                        "name": "Jérémy Andréoletti"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Gang Zhang"
                    },
                    {
                        "name": "Alexander Ivanov"
                    },
                    {
                        "name": "Seri Khoury"
                    },
                    {
                        "name": "Nils Gustafsson"
                    },
                    {
                        "name": "Hamid Mostaghimi"
                    },
                    {
                        "name": "Kunvar Thaman"
                    },
                    {
                        "name": "Qijia Chen"
                    },
                    {
                        "name": "Tran Quoc Khánh"
                    },
                    {
                        "name": "Jacob Loader"
                    },
                    {
                        "name": "Stefano Cavalleri"
                    },
                    {
                        "name": "Hannah Szlyk"
                    },
                    {
                        "name": "Zachary Brown"
                    },
                    {
                        "name": "Himanshu Narayan"
                    },
                    {
                        "name": "Jonathan Roberts"
                    },
                    {
                        "name": "William Alley"
                    },
                    {
                        "name": "Kunyang Sun"
                    },
                    {
                        "name": "Ryan Stendall"
                    },
                    {
                        "name": "Max Lamparth"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Ting Wang"
                    },
                    {
                        "name": "Hanmeng Xu"
                    },
                    {
                        "name": "Pablo Hernández-Cámara"
                    },
                    {
                        "name": "Freddie Martin"
                    },
                    {
                        "name": "Thomas Preu"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Marcus Abramovitch"
                    },
                    {
                        "name": "Dominic Williamson"
                    },
                    {
                        "name": "Ida Bosio"
                    },
                    {
                        "name": "Ziye Chen"
                    },
                    {
                        "name": "Biró Bálint"
                    },
                    {
                        "name": "Eve J. Y. Lo"
                    },
                    {
                        "name": "Maria Inês S. Nunes"
                    },
                    {
                        "name": "Yibo Jiang"
                    },
                    {
                        "name": "M Saiful Bari"
                    },
                    {
                        "name": "Peyman Kassani"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Behzad Ansarinejad"
                    },
                    {
                        "name": "Yewen Sun"
                    },
                    {
                        "name": "Stephane Durand"
                    },
                    {
                        "name": "Guillaume Douville"
                    },
                    {
                        "name": "Daniel Tordera"
                    },
                    {
                        "name": "George Balabanian"
                    },
                    {
                        "name": "Earth Anderson"
                    },
                    {
                        "name": "Lynna Kvistad"
                    },
                    {
                        "name": "Alejandro José Moyano"
                    },
                    {
                        "name": "Hsiaoyun Milliron"
                    },
                    {
                        "name": "Ahmad Sakor"
                    },
                    {
                        "name": "Murat Eron"
                    },
                    {
                        "name": "Isaac C. McAlister"
                    },
                    {
                        "name": "Andrew Favre D. O."
                    },
                    {
                        "name": "Shailesh Shah"
                    },
                    {
                        "name": "Xiaoxiang Zhou"
                    },
                    {
                        "name": "Firuz Kamalov"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Sherwin Abdoli"
                    },
                    {
                        "name": "Tim Santens"
                    },
                    {
                        "name": "Harrison K Wang"
                    },
                    {
                        "name": "Evan Chen"
                    },
                    {
                        "name": "Alessandro Tomasiello"
                    },
                    {
                        "name": "G. Bruno De Luca"
                    },
                    {
                        "name": "Shi-Zhuo Looi"
                    },
                    {
                        "name": "Vinh-Kha Le"
                    },
                    {
                        "name": "Noam Kolt"
                    },
                    {
                        "name": "Niels Mündler"
                    },
                    {
                        "name": "Avi Semler"
                    },
                    {
                        "name": "Emma Rodman"
                    },
                    {
                        "name": "Jacob Drori"
                    },
                    {
                        "name": "Carl J Fossum"
                    },
                    {
                        "name": "Luk Gloor"
                    },
                    {
                        "name": "Milind Jagota"
                    },
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Honglu Fan"
                    },
                    {
                        "name": "Tej Shah"
                    },
                    {
                        "name": "Jonathan Eicher"
                    },
                    {
                        "name": "Michael Chen"
                    },
                    {
                        "name": "Kushal Thaman"
                    },
                    {
                        "name": "William Merrill"
                    },
                    {
                        "name": "Moritz Firsching"
                    },
                    {
                        "name": "Carter Harris"
                    },
                    {
                        "name": "Stefan Ciobâcă"
                    },
                    {
                        "name": "Jason Gross"
                    },
                    {
                        "name": "Rohan Pandey"
                    },
                    {
                        "name": "Ilya Gusev"
                    },
                    {
                        "name": "Adam Jones"
                    },
                    {
                        "name": "Shashank Agnihotri"
                    },
                    {
                        "name": "Pavel Zhelnov"
                    },
                    {
                        "name": "Siranut Usawasutsakorn"
                    },
                    {
                        "name": "Mohammadreza Mofayezi"
                    },
                    {
                        "name": "Alexander Piperski"
                    },
                    {
                        "name": "Marc Carauleanu"
                    },
                    {
                        "name": "David K. Zhang"
                    },
                    {
                        "name": "Kostiantyn Dobarskyi"
                    },
                    {
                        "name": "Dylan Ler"
                    },
                    {
                        "name": "Roman Leventov"
                    },
                    {
                        "name": "Ignat Soroko"
                    },
                    {
                        "name": "Thorben Jansen"
                    },
                    {
                        "name": "Scott Creighton"
                    },
                    {
                        "name": "Pascal Lauer"
                    },
                    {
                        "name": "Joshua Duersch"
                    },
                    {
                        "name": "Vage Taamazyan"
                    },
                    {
                        "name": "Dario Bezzi"
                    },
                    {
                        "name": "Wiktor Morak"
                    },
                    {
                        "name": "Wenjie Ma"
                    },
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Tran Đuc Huy"
                    },
                    {
                        "name": "Ruicheng Xian"
                    },
                    {
                        "name": "Armel Randy Zebaze"
                    },
                    {
                        "name": "Mohanad Mohamed"
                    },
                    {
                        "name": "Julian Noah Leser"
                    },
                    {
                        "name": "Michelle X Yuan"
                    },
                    {
                        "name": "Laila Yacar"
                    },
                    {
                        "name": "Johannes Lengler"
                    },
                    {
                        "name": "Katarzyna Olszewska"
                    },
                    {
                        "name": "Hossein Shahrtash"
                    },
                    {
                        "name": "Edson Oliveira"
                    },
                    {
                        "name": "Joseph W. Jackson"
                    },
                    {
                        "name": "Daniel Espinosa Gonzalez"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Muthu Chidambaram"
                    },
                    {
                        "name": "Timothy Manik"
                    },
                    {
                        "name": "Hector Haffenden"
                    },
                    {
                        "name": "Dashiell Stander"
                    },
                    {
                        "name": "Ali Dasouqi"
                    },
                    {
                        "name": "Alexander Shen"
                    },
                    {
                        "name": "Emilien Duc"
                    },
                    {
                        "name": "Bita Golshani"
                    },
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Mikalai Uzhou"
                    },
                    {
                        "name": "Alina Borisovna Zhidkovskaya"
                    },
                    {
                        "name": "Lukas Lewark"
                    },
                    {
                        "name": "Miguel Orbegozo Rodriguez"
                    },
                    {
                        "name": "Mátyás Vincze"
                    },
                    {
                        "name": "Dustin Wehr"
                    },
                    {
                        "name": "Colin Tang"
                    },
                    {
                        "name": "Shaun Phillips"
                    },
                    {
                        "name": "Fortuna Samuele"
                    },
                    {
                        "name": "Jiang Muzhen"
                    },
                    {
                        "name": "Fredrik Ekström"
                    },
                    {
                        "name": "Angela Hammon"
                    },
                    {
                        "name": "Oam Patel"
                    },
                    {
                        "name": "Faraz Farhidi"
                    },
                    {
                        "name": "George Medley"
                    },
                    {
                        "name": "Forough Mohammadzadeh"
                    },
                    {
                        "name": "Madellene Peñaflor"
                    },
                    {
                        "name": "Haile Kassahun"
                    },
                    {
                        "name": "Alena Friedrich"
                    },
                    {
                        "name": "Claire Sparrow"
                    },
                    {
                        "name": "Rayner Hernandez Perez"
                    },
                    {
                        "name": "Taom Sakal"
                    },
                    {
                        "name": "Omkar Dhamane"
                    },
                    {
                        "name": "Ali Khajegili Mirabadi"
                    },
                    {
                        "name": "Eric Hallman"
                    },
                    {
                        "name": "Kenchi Okutsu"
                    },
                    {
                        "name": "Mike Battaglia"
                    },
                    {
                        "name": "Mohammad Maghsoudimehrabani"
                    },
                    {
                        "name": "Alon Amit"
                    },
                    {
                        "name": "Dave Hulbert"
                    },
                    {
                        "name": "Roberto Pereira"
                    },
                    {
                        "name": "Simon Weber"
                    },
                    {
                        "name": "Handoko"
                    },
                    {
                        "name": "Anton Peristyy"
                    },
                    {
                        "name": "Stephen Malina"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Will Cai"
                    },
                    {
                        "name": "Mustafa Mehkary"
                    },
                    {
                        "name": "Rami Aly"
                    },
                    {
                        "name": "Frank Reidegeld"
                    },
                    {
                        "name": "Anna-Katharina Dick"
                    },
                    {
                        "name": "Cary Friday"
                    },
                    {
                        "name": "Jasdeep Sidhu"
                    },
                    {
                        "name": "Hassan Shapourian"
                    },
                    {
                        "name": "Wanyoung Kim"
                    },
                    {
                        "name": "Mariana Costa"
                    },
                    {
                        "name": "Hubeyb Gurdogan"
                    },
                    {
                        "name": "Brian Weber"
                    },
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "Tong Jiang"
                    },
                    {
                        "name": "Arunim Agarwal"
                    },
                    {
                        "name": "Chiara Ceconello"
                    },
                    {
                        "name": "Warren S. Vaz"
                    },
                    {
                        "name": "Chao Zhuang"
                    },
                    {
                        "name": "Haon Park"
                    },
                    {
                        "name": "Andrew R. Tawfeek"
                    },
                    {
                        "name": "Daattavya Aggarwal"
                    },
                    {
                        "name": "Michael Kirchhof"
                    },
                    {
                        "name": "Linjie Dai"
                    },
                    {
                        "name": "Evan Kim"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Yuzhou Wang"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Krzysztof Burdzy"
                    },
                    {
                        "name": "Lixin Zhang"
                    },
                    {
                        "name": "Antonio Franca"
                    },
                    {
                        "name": "Diana T. Pham"
                    },
                    {
                        "name": "Kang Yong Loh"
                    },
                    {
                        "name": "Joshua Robinson"
                    },
                    {
                        "name": "Abram Jackson"
                    },
                    {
                        "name": "Shreen Gul"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Zhehang Du"
                    },
                    {
                        "name": "Adrian Cosma"
                    },
                    {
                        "name": "Jesus Colino"
                    },
                    {
                        "name": "Colin White"
                    },
                    {
                        "name": "Jacob Votava"
                    },
                    {
                        "name": "Vladimir Vinnikov"
                    },
                    {
                        "name": "Ethan Delaney"
                    },
                    {
                        "name": "Petr Spelda"
                    },
                    {
                        "name": "Vit Stritecky"
                    },
                    {
                        "name": "Syed M. Shahid"
                    },
                    {
                        "name": "Jean-Christophe Mourrat"
                    },
                    {
                        "name": "Lavr Vetoshkin"
                    },
                    {
                        "name": "Koen Sponselee"
                    },
                    {
                        "name": "Renas Bacho"
                    },
                    {
                        "name": "Florencia de la Rosa"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Guillaume Malod"
                    },
                    {
                        "name": "Leon Lang"
                    },
                    {
                        "name": "Julien Laurendeau"
                    },
                    {
                        "name": "Dmitry Kazakov"
                    },
                    {
                        "name": "Fatimah Adesanya"
                    },
                    {
                        "name": "Julien Portier"
                    },
                    {
                        "name": "Lawrence Hollom"
                    },
                    {
                        "name": "Victor Souza"
                    },
                    {
                        "name": "Yuchen Anna Zhou"
                    },
                    {
                        "name": "Julien Degorre"
                    },
                    {
                        "name": "Yiğit Yalın"
                    },
                    {
                        "name": "Gbenga Daniel Obikoya"
                    },
                    {
                        "name": "Luca Arnaboldi"
                    },
                    {
                        "name": "Rai"
                    },
                    {
                        "name": "Filippo Bigi"
                    },
                    {
                        "name": "M. C. Boscá"
                    },
                    {
                        "name": "Oleg Shumar"
                    },
                    {
                        "name": "Kaniuar Bacho"
                    },
                    {
                        "name": "Pierre Clavier"
                    },
                    {
                        "name": "Gabriel Recchia"
                    },
                    {
                        "name": "Mara Popescu"
                    },
                    {
                        "name": "Nikita Shulga"
                    },
                    {
                        "name": "Ngefor Mildred Tanwie"
                    },
                    {
                        "name": "Denis Peskoff"
                    },
                    {
                        "name": "Thomas C. H. Lux"
                    },
                    {
                        "name": "Ben Rank"
                    },
                    {
                        "name": "Colin Ni"
                    },
                    {
                        "name": "Matthew Brooks"
                    },
                    {
                        "name": "Alesia Yakimchyk"
                    },
                    {
                        "name": "Huanxu"
                    },
                    {
                        "name": "Liu"
                    },
                    {
                        "name": "Olle Häggström"
                    },
                    {
                        "name": "Emil Verkama"
                    },
                    {
                        "name": "Hans Gundlach"
                    },
                    {
                        "name": "Leonor Brito-Santana"
                    },
                    {
                        "name": "Brian Amaro"
                    },
                    {
                        "name": "Vivek Vajipey"
                    },
                    {
                        "name": "Rynaa Grover"
                    },
                    {
                        "name": "Yiyang Fan"
                    },
                    {
                        "name": "Gabriel Poesia Reis e Silva"
                    },
                    {
                        "name": "Linwei Xin"
                    },
                    {
                        "name": "Yosi Kratish"
                    },
                    {
                        "name": "Jakub Łucki"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Sivakanth Gopi"
                    },
                    {
                        "name": "Andrea Caciolai"
                    },
                    {
                        "name": "Justin Xu"
                    },
                    {
                        "name": "Kevin Joseph Scaria"
                    },
                    {
                        "name": "Freddie Vargus"
                    },
                    {
                        "name": "Farzad Habibi"
                    },
                    {
                        "name": "Long"
                    },
                    {
                        "name": "Lian"
                    },
                    {
                        "name": "Emanuele Rodolà"
                    },
                    {
                        "name": "Jules Robins"
                    },
                    {
                        "name": "Vincent Cheng"
                    },
                    {
                        "name": "Tony Fruhauff"
                    },
                    {
                        "name": "Brad Raynor"
                    },
                    {
                        "name": "Hao Qi"
                    },
                    {
                        "name": "Xi Jiang"
                    },
                    {
                        "name": "Ben Segev"
                    },
                    {
                        "name": "Jingxuan Fan"
                    },
                    {
                        "name": "Sarah Martinson"
                    },
                    {
                        "name": "Erik Y. Wang"
                    },
                    {
                        "name": "Kaylie Hausknecht"
                    },
                    {
                        "name": "Michael P. Brenner"
                    },
                    {
                        "name": "Mao Mao"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "David Avagian"
                    },
                    {
                        "name": "Eshawn Jessica Scipio"
                    },
                    {
                        "name": "Alon Ragoler"
                    },
                    {
                        "name": "Justin Tan"
                    },
                    {
                        "name": "Blake Sims"
                    },
                    {
                        "name": "Rebeka Plecnik"
                    },
                    {
                        "name": "Aaron Kirtland"
                    },
                    {
                        "name": "Omer Faruk Bodur"
                    },
                    {
                        "name": "D. P. Shinde"
                    },
                    {
                        "name": "Zahra Adoul"
                    },
                    {
                        "name": "Mohamed Zekry"
                    },
                    {
                        "name": "Ali Karakoc"
                    },
                    {
                        "name": "Tania C. B. Santos"
                    },
                    {
                        "name": "Samir Shamseldeen"
                    },
                    {
                        "name": "Loukmane Karim"
                    },
                    {
                        "name": "Anna Liakhovitskaia"
                    },
                    {
                        "name": "Nate Resman"
                    },
                    {
                        "name": "Nicholas Farina"
                    },
                    {
                        "name": "Juan Carlos Gonzalez"
                    },
                    {
                        "name": "Gabe Maayan"
                    },
                    {
                        "name": "Sarah Hoback"
                    },
                    {
                        "name": "Rodrigo De Oliveira Pena"
                    },
                    {
                        "name": "Glen Sherman"
                    },
                    {
                        "name": "Elizabeth Kelley"
                    },
                    {
                        "name": "Hodjat Mariji"
                    },
                    {
                        "name": "Rasoul Pouriamanesh"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Sandra Mendoza"
                    },
                    {
                        "name": "Ismail Alarab"
                    },
                    {
                        "name": "Joshua Cole"
                    },
                    {
                        "name": "Danyelle Ferreira"
                    },
                    {
                        "name": "Bryan Johnson"
                    },
                    {
                        "name": "Mohammad Safdari"
                    },
                    {
                        "name": "Liangti Dai"
                    },
                    {
                        "name": "Siriphan Arthornthurasuk"
                    },
                    {
                        "name": "Alexey Pronin"
                    },
                    {
                        "name": "Jing Fan"
                    },
                    {
                        "name": "Angel Ramirez-Trinidad"
                    },
                    {
                        "name": "Ashley Cartwright"
                    },
                    {
                        "name": "Daphiny Pottmaier"
                    },
                    {
                        "name": "Omid Taheri"
                    },
                    {
                        "name": "David Outevsky"
                    },
                    {
                        "name": "Stanley Stepanic"
                    },
                    {
                        "name": "Samuel Perry"
                    },
                    {
                        "name": "Luke Askew"
                    },
                    {
                        "name": "Raúl Adrián Huerta Rodríguez"
                    },
                    {
                        "name": "Ali M. R. Minissi"
                    },
                    {
                        "name": "Sam Ali"
                    },
                    {
                        "name": "Ricardo Lorena"
                    },
                    {
                        "name": "Krishnamurthy Iyer"
                    },
                    {
                        "name": "Arshad Anil Fasiludeen"
                    },
                    {
                        "name": "Sk Md Salauddin"
                    },
                    {
                        "name": "Murat Islam"
                    },
                    {
                        "name": "Juan Gonzalez"
                    },
                    {
                        "name": "Josh Ducey"
                    },
                    {
                        "name": "Maja Somrak"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Eric Vergo"
                    },
                    {
                        "name": "Juehang Qin"
                    },
                    {
                        "name": "Benjámin Borbás"
                    },
                    {
                        "name": "Eric Chu"
                    },
                    {
                        "name": "Jack Lindsey"
                    },
                    {
                        "name": "Anil Radhakrishnan"
                    },
                    {
                        "name": "Antoine Jallon"
                    },
                    {
                        "name": "I. M. J. McInnis"
                    },
                    {
                        "name": "Pawan Kumar"
                    },
                    {
                        "name": "Laxman Prasad Goswami"
                    },
                    {
                        "name": "Daniel Bugas"
                    },
                    {
                        "name": "Nasser Heydari"
                    },
                    {
                        "name": "Ferenc Jeanplong"
                    },
                    {
                        "name": "Archimedes Apronti"
                    },
                    {
                        "name": "Abdallah Galal"
                    },
                    {
                        "name": "Ng Ze-An"
                    },
                    {
                        "name": "Ankit Singh"
                    },
                    {
                        "name": "Joan of Arc Xavier"
                    },
                    {
                        "name": "Kanu Priya Agarwal"
                    },
                    {
                        "name": "Mohammed Berkani"
                    },
                    {
                        "name": "Benedito Alves de Oliveira Junior"
                    },
                    {
                        "name": "Dmitry Malishev"
                    },
                    {
                        "name": "Nicolas Remy"
                    },
                    {
                        "name": "Taylor D. Hartman"
                    },
                    {
                        "name": "Tim Tarver"
                    },
                    {
                        "name": "Stephen Mensah"
                    },
                    {
                        "name": "Javier Gimenez"
                    },
                    {
                        "name": "Roselynn Grace Montecillo"
                    },
                    {
                        "name": "Russell Campbell"
                    },
                    {
                        "name": "Asankhaya Sharma"
                    },
                    {
                        "name": "Khalida Meer"
                    },
                    {
                        "name": "Xavier Alapont"
                    },
                    {
                        "name": "Deepakkumar Patil"
                    },
                    {
                        "name": "Rajat Maheshwari"
                    },
                    {
                        "name": "Abdelkader Dendane"
                    },
                    {
                        "name": "Priti Shukla"
                    },
                    {
                        "name": "Sergei Bogdanov"
                    },
                    {
                        "name": "Sören Möller"
                    },
                    {
                        "name": "Muhammad Rehan Siddiqi"
                    },
                    {
                        "name": "Prajvi Saxena"
                    },
                    {
                        "name": "Himanshu Gupta"
                    },
                    {
                        "name": "Innocent Enyekwe"
                    },
                    {
                        "name": "Ragavendran P V"
                    },
                    {
                        "name": "Zienab EL-Wasif"
                    },
                    {
                        "name": "Aleksandr Maksapetyan"
                    },
                    {
                        "name": "Vivien Rossbach"
                    },
                    {
                        "name": "Chris Harjadi"
                    },
                    {
                        "name": "Mohsen Bahaloohoreh"
                    },
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "John Lai"
                    },
                    {
                        "name": "Justine Leon Uro"
                    },
                    {
                        "name": "Greg Bateman"
                    },
                    {
                        "name": "Mohamed Sayed"
                    },
                    {
                        "name": "Ahmed Menshawy"
                    },
                    {
                        "name": "Darling Duclosel"
                    },
                    {
                        "name": "Yashaswini Jain"
                    },
                    {
                        "name": "Ashley Aaron"
                    },
                    {
                        "name": "Murat Tiryakioglu"
                    },
                    {
                        "name": "Sheeshram Siddh"
                    },
                    {
                        "name": "Keith Krenek"
                    },
                    {
                        "name": "Alex Hoover"
                    },
                    {
                        "name": "Joseph McGowan"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    },
                    {
                        "name": "Summer Yue"
                    },
                    {
                        "name": "Alexandr Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "arxiv_affiliation": "Tony",
                "author": "Dan Hendrycks",
                "arxiv_comment": "26 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14249v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14249v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14654v2",
                "updated": "2025-02-12T05:32:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    32,
                    7,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-24T17:21:01Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    21,
                    1,
                    4,
                    24,
                    0
                ],
                "title": "MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical\n  LLM Agents"
                },
                "summary": "Recent large language models (LLMs) have demonstrated significant\nadvancements, particularly in their ability to serve as agents thereby\nsurpassing their traditional role as chatbots. These agents can leverage their\nplanning and tool utilization capabilities to address tasks specified at a high\nlevel. However, a standardized dataset to benchmark the agent capabilities of\nLLMs in medical applications is currently lacking, making the evaluation of\nLLMs on complex tasks in interactive healthcare environments challenging. To\naddress this gap, we introduce MedAgentBench, a broad evaluation suite designed\nto assess the agent capabilities of large language models within medical\nrecords contexts. MedAgentBench encompasses 300 patient-specific\nclinically-derived tasks from 10 categories written by human physicians,\nrealistic profiles of 100 patients with over 700,000 data elements, a\nFHIR-compliant interactive environment, and an accompanying codebase. The\nenvironment uses the standard APIs and communication infrastructure used in\nmodern EMR systems, so it can be easily migrated into live EMR systems.\nMedAgentBench presents an unsaturated agent-oriented benchmark that current\nstate-of-the-art LLMs exhibit some ability to succeed at. The best model\n(Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is\nstill substantial space for improvement which gives the community a next\ndirection to optimize. Furthermore, there is significant variation in\nperformance across task categories. MedAgentBench establishes this and is\npublicly available at https://github.com/stanfordmlgroup/MedAgentBench ,\noffering a valuable framework for model developers to track progress and drive\ncontinuous improvements in the agent capabilities of large language models\nwithin the medical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have demonstrated significant\nadvancements, particularly in their ability to serve as agents thereby\nsurpassing their traditional role as chatbots. These agents can leverage their\nplanning and tool utilization capabilities to address tasks specified at a high\nlevel. However, a standardized dataset to benchmark the agent capabilities of\nLLMs in medical applications is currently lacking, making the evaluation of\nLLMs on complex tasks in interactive healthcare environments challenging. To\naddress this gap, we introduce MedAgentBench, a broad evaluation suite designed\nto assess the agent capabilities of large language models within medical\nrecords contexts. MedAgentBench encompasses 300 patient-specific\nclinically-derived tasks from 10 categories written by human physicians,\nrealistic profiles of 100 patients with over 700,000 data elements, a\nFHIR-compliant interactive environment, and an accompanying codebase. The\nenvironment uses the standard APIs and communication infrastructure used in\nmodern EMR systems, so it can be easily migrated into live EMR systems.\nMedAgentBench presents an unsaturated agent-oriented benchmark that current\nstate-of-the-art LLMs exhibit some ability to succeed at. The best model\n(Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is\nstill substantial space for improvement which gives the community a next\ndirection to optimize. Furthermore, there is significant variation in\nperformance across task categories. MedAgentBench establishes this and is\npublicly available at https://github.com/stanfordmlgroup/MedAgentBench ,\noffering a valuable framework for model developers to track progress and drive\ncontinuous improvements in the agent capabilities of large language models\nwithin the medical domain."
                },
                "authors": [
                    {
                        "name": "Yixing Jiang"
                    },
                    {
                        "name": "Kameron C. Black"
                    },
                    {
                        "name": "Gloria Geng"
                    },
                    {
                        "name": "Danny Park"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Andrew Y. Ng"
                    },
                    {
                        "name": "Jonathan H. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan H. Chen"
                },
                "author": "Jonathan H. Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06832v2",
                "updated": "2025-02-12T05:30:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    30,
                    33,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-05T20:45:52Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    45,
                    52,
                    2,
                    36,
                    0
                ],
                "title": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model\n  Approach"
                },
                "summary": "Mixture of Experts (MoE) have shown remarkable success in leveraging\nspecialized expert networks for complex machine learning tasks. However, their\nsusceptibility to adversarial attacks presents a critical challenge for\ndeployment in robust applications. This paper addresses the critical question\nof how to incorporate robustness into MoEs while maintaining high natural\naccuracy. We begin by analyzing the vulnerability of MoE components, finding\nthat expert networks are notably more susceptible to adversarial attacks than\nthe router. Based on this insight, we propose a targeted robust training\ntechnique that integrates a novel loss function to enhance the adversarial\nrobustness of MoE, requiring only the robustification of one additional expert\nwithout compromising training or inference efficiency. Building on this, we\nintroduce a dual-model strategy that linearly combines a standard MoE model\nwith our robustified MoE model using a smoothing parameter. This approach\nallows for flexible control over the robustness-accuracy trade-off. We further\nprovide theoretical foundations by deriving certified robustness bounds for\nboth the single MoE and the dual-model. To push the boundaries of robustness\nand accuracy, we propose a novel joint training strategy JTDMoE for the\ndual-model. This joint training enhances both robustness and accuracy beyond\nwhat is achievable with separate models. Experimental results on CIFAR-10 and\nTinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures\ndemonstrate the effectiveness of our proposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) have shown remarkable success in leveraging\nspecialized expert networks for complex machine learning tasks. However, their\nsusceptibility to adversarial attacks presents a critical challenge for\ndeployment in robust applications. This paper addresses the critical question\nof how to incorporate robustness into MoEs while maintaining high natural\naccuracy. We begin by analyzing the vulnerability of MoE components, finding\nthat expert networks are notably more susceptible to adversarial attacks than\nthe router. Based on this insight, we propose a targeted robust training\ntechnique that integrates a novel loss function to enhance the adversarial\nrobustness of MoE, requiring only the robustification of one additional expert\nwithout compromising training or inference efficiency. Building on this, we\nintroduce a dual-model strategy that linearly combines a standard MoE model\nwith our robustified MoE model using a smoothing parameter. This approach\nallows for flexible control over the robustness-accuracy trade-off. We further\nprovide theoretical foundations by deriving certified robustness bounds for\nboth the single MoE and the dual-model. To push the boundaries of robustness\nand accuracy, we propose a novel joint training strategy JTDMoE for the\ndual-model. This joint training enhances both robustness and accuracy beyond\nwhat is achievable with separate models. Experimental results on CIFAR-10 and\nTinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures\ndemonstrate the effectiveness of our proposed methods."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Ziqing Hu"
                    },
                    {
                        "name": "Ren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ren Wang"
                },
                "author": "Ren Wang",
                "arxiv_comment": "10 pages, 3 figures, submitted to ICML 2025 (under review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08130v1",
                "updated": "2025-02-12T05:24:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    24,
                    21,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T05:24:21Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    24,
                    21,
                    2,
                    43,
                    0
                ],
                "title": "Selective Self-to-Supervised Fine-Tuning for Generalization in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Self-to-Supervised Fine-Tuning for Generalization in Large\n  Language Models"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common\npractice to improve performance on target tasks. However, this performance gain\noften leads to overfitting, where the model becomes too specialized in either\nthe task or the characteristics of the training data, resulting in a loss of\ngeneralization. This paper introduces Selective Self-to-Supervised Fine-Tuning\n(S3FT), a fine-tuning approach that achieves better performance than the\nstandard supervised fine-tuning (SFT) while improving generalization. S3FT\nleverages the existence of multiple valid responses to a query. By utilizing\nthe model's correct responses, S3FT reduces model specialization during the\nfine-tuning stage. S3FT first identifies the correct model responses from the\ntraining set by deploying an appropriate judge. Then, it fine-tunes the model\nusing the correct model responses and the gold response (or its paraphrase) for\nthe remaining samples. The effectiveness of S3FT is demonstrated through\nexperiments on mathematical reasoning, Python programming and reading\ncomprehension tasks. The results show that standard SFT can lead to an average\nperformance drop of up to $4.4$ on multiple benchmarks, such as MMLU and\nTruthfulQA. In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating\nbetter generalization capabilities than SFT while performing significantly\nbetter on the fine-tuning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common\npractice to improve performance on target tasks. However, this performance gain\noften leads to overfitting, where the model becomes too specialized in either\nthe task or the characteristics of the training data, resulting in a loss of\ngeneralization. This paper introduces Selective Self-to-Supervised Fine-Tuning\n(S3FT), a fine-tuning approach that achieves better performance than the\nstandard supervised fine-tuning (SFT) while improving generalization. S3FT\nleverages the existence of multiple valid responses to a query. By utilizing\nthe model's correct responses, S3FT reduces model specialization during the\nfine-tuning stage. S3FT first identifies the correct model responses from the\ntraining set by deploying an appropriate judge. Then, it fine-tunes the model\nusing the correct model responses and the gold response (or its paraphrase) for\nthe remaining samples. The effectiveness of S3FT is demonstrated through\nexperiments on mathematical reasoning, Python programming and reading\ncomprehension tasks. The results show that standard SFT can lead to an average\nperformance drop of up to $4.4$ on multiple benchmarks, such as MMLU and\nTruthfulQA. In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating\nbetter generalization capabilities than SFT while performing significantly\nbetter on the fine-tuning tasks."
                },
                "authors": [
                    {
                        "name": "Sonam Gupta"
                    },
                    {
                        "name": "Yatin Nandwani"
                    },
                    {
                        "name": "Asaf Yehudai"
                    },
                    {
                        "name": "Dinesh Khandelwal"
                    },
                    {
                        "name": "Dinesh Raghu"
                    },
                    {
                        "name": "Sachindra Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Sachindra Joshi"
                },
                "author": "Sachindra Joshi",
                "arxiv_comment": "10 pages, Accepted to NAACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07644v2",
                "updated": "2025-02-12T05:18:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    18,
                    48,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-11T15:34:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    34,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "SymGPT: Auditing Smart Contracts via Combining Symbolic Execution with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymGPT: Auditing Smart Contracts via Combining Symbolic Execution with\n  Large Language Models"
                },
                "summary": "To govern smart contracts running on Ethereum, multiple Ethereum Request for\nComment (ERC) standards have been developed, each having a set of rules to\nguide the behaviors of smart contracts. Violating the ERC rules could cause\nserious security issues and financial loss, signifying the importance of\nverifying smart contracts follow ERCs. Today's practices of such verification\nare to manually audit each single contract, use expert-developed\nprogram-analysis tools, or use large language models (LLMs), all of which are\nfar from effective in identifying ERC rule violations. This paper introduces\nSymGPT, a tool that combines the natural language understanding of large\nlanguage models (LLMs) with the formal guarantees of symbolic execution to\nautomatically verify smart contracts' compliance with ERC rules. To develop\nSymGPT, we conduct an empirical study of 132 ERC rules from three widely used\nERC standards, examining their content, security implications, and natural\nlanguage descriptions. Based on this study, we design SymGPT by first\ninstructing an LLM to translate ERC rules into a defined EBNF grammar. We then\nsynthesize constraints from the formalized rules to represent scenarios where\nviolations may occur and use symbolic execution to detect them. Our evaluation\nshows that SymGPT identifies 5,783 ERC rule violations in 4,000 real-world\ncontracts, including 1,375 violations with clear attack paths for stealing\nfinancial assets, demonstrating its effectiveness. Furthermore, SymGPT\noutperforms six automated techniques and a security-expert auditing service,\nunderscoring its superiority over current smart contract analysis methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To govern smart contracts running on Ethereum, multiple Ethereum Request for\nComment (ERC) standards have been developed, each having a set of rules to\nguide the behaviors of smart contracts. Violating the ERC rules could cause\nserious security issues and financial loss, signifying the importance of\nverifying smart contracts follow ERCs. Today's practices of such verification\nare to manually audit each single contract, use expert-developed\nprogram-analysis tools, or use large language models (LLMs), all of which are\nfar from effective in identifying ERC rule violations. This paper introduces\nSymGPT, a tool that combines the natural language understanding of large\nlanguage models (LLMs) with the formal guarantees of symbolic execution to\nautomatically verify smart contracts' compliance with ERC rules. To develop\nSymGPT, we conduct an empirical study of 132 ERC rules from three widely used\nERC standards, examining their content, security implications, and natural\nlanguage descriptions. Based on this study, we design SymGPT by first\ninstructing an LLM to translate ERC rules into a defined EBNF grammar. We then\nsynthesize constraints from the formalized rules to represent scenarios where\nviolations may occur and use symbolic execution to detect them. Our evaluation\nshows that SymGPT identifies 5,783 ERC rule violations in 4,000 real-world\ncontracts, including 1,375 violations with clear attack paths for stealing\nfinancial assets, demonstrating its effectiveness. Furthermore, SymGPT\noutperforms six automated techniques and a security-expert auditing service,\nunderscoring its superiority over current smart contract analysis methods."
                },
                "authors": [
                    {
                        "name": "Shihao Xia"
                    },
                    {
                        "name": "Mengting He"
                    },
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Tingting Yu"
                    },
                    {
                        "name": "Yiying Zhang"
                    },
                    {
                        "name": "Linhai Song"
                    }
                ],
                "author_detail": {
                    "name": "Linhai Song"
                },
                "author": "Linhai Song",
                "arxiv_comment": "16 pages. arXiv admin note: text overlap with arXiv:2404.04306",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07072v2",
                "updated": "2025-02-12T05:14:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    14,
                    41,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-10T22:07:02Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    22,
                    7,
                    2,
                    0,
                    41,
                    0
                ],
                "title": "IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models"
                },
                "summary": "Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair."
                },
                "authors": [
                    {
                        "name": "Sayem Mohammad Imtiaz"
                    },
                    {
                        "name": "Astha Singh"
                    },
                    {
                        "name": "Fraol Batole"
                    },
                    {
                        "name": "Hridesh Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Hridesh Rajan"
                },
                "author": "Hridesh Rajan",
                "arxiv_comment": "Accepted as full research paper at FSE'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08127v1",
                "updated": "2025-02-12T05:13:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    13,
                    4,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T05:13:04Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    5,
                    13,
                    4,
                    2,
                    43,
                    0
                ],
                "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown strong general\nreasoning abilities, yet their effectiveness in financial reasoning remains\nunderexplored. In this study, we comprehensively evaluate 16 powerful reasoning\nand general LLMs on three complex financial tasks involving financial text,\ntabular data, and equations, assessing numerical reasoning, tabular\ninterpretation, financial terminology comprehension, long-context processing,\nand equation-based problem solving. Our results show that while better datasets\nand pretraining improve financial reasoning, general enhancements like CoT\nfine-tuning do not always yield consistent gains. Moreover, all reasoning\nstrategies face challenges in improving performance on long-context and\nmulti-table tasks. To address these limitations, we develop a financial\nreasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and\nreinforcement learning with domain-specific reasoning paths. Even with simple\nfine-tuning with one financial dataset, our model achieves a consistent 10%\nperformance improvement across tasks, surpassing all 8B models and even\nLlama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight\nthe need for domain-specific adaptations in financial tasks, emphasizing future\ndirections such as multi-table reasoning, long-context processing, and\nfinancial terminology comprehension. All our datasets, models, and codes are\npublicly available. Furthermore, we introduce a leaderboard for benchmarking\nfuture datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown strong general\nreasoning abilities, yet their effectiveness in financial reasoning remains\nunderexplored. In this study, we comprehensively evaluate 16 powerful reasoning\nand general LLMs on three complex financial tasks involving financial text,\ntabular data, and equations, assessing numerical reasoning, tabular\ninterpretation, financial terminology comprehension, long-context processing,\nand equation-based problem solving. Our results show that while better datasets\nand pretraining improve financial reasoning, general enhancements like CoT\nfine-tuning do not always yield consistent gains. Moreover, all reasoning\nstrategies face challenges in improving performance on long-context and\nmulti-table tasks. To address these limitations, we develop a financial\nreasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and\nreinforcement learning with domain-specific reasoning paths. Even with simple\nfine-tuning with one financial dataset, our model achieves a consistent 10%\nperformance improvement across tasks, surpassing all 8B models and even\nLlama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight\nthe need for domain-specific adaptations in financial tasks, emphasizing future\ndirections such as multi-table reasoning, long-context processing, and\nfinancial terminology comprehension. All our datasets, models, and codes are\npublicly available. Furthermore, we introduce a leaderboard for benchmarking\nfuture datasets and models."
                },
                "authors": [
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Weipeng Zhou"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Qianqian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qianqian Xie"
                },
                "author": "Qianqian Xie",
                "arxiv_comment": "Ongoing work, 13 pages, 2 figures, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02913v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02913v4",
                "updated": "2025-02-12T04:59:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    4,
                    59,
                    16,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-05T06:20:20Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    20,
                    20,
                    2,
                    36,
                    0
                ],
                "title": "Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient\n  Leakage"
                },
                "summary": "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications."
                },
                "authors": [
                    {
                        "name": "Jiayang Meng"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xin Shi"
                    },
                    {
                        "name": "Qingyu Huang"
                    },
                    {
                        "name": "Chen Hou"
                    }
                ],
                "author_detail": {
                    "name": "Chen Hou"
                },
                "author": "Chen Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02913v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02913v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02629v2",
                "updated": "2025-02-12T04:55:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    4,
                    55,
                    19,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-05T19:06:03Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    19,
                    6,
                    3,
                    6,
                    5,
                    0
                ],
                "title": "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for\n  Jailbreak Attack Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for\n  Jailbreak Attack Defense"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in diverse\napplications, including chatbot assistants and code generation, aligning their\nbehavior with safety and ethical standards has become paramount. However,\njailbreak attacks, which exploit vulnerabilities to elicit unintended or\nharmful outputs, threaten LLMs' safety significantly. In this paper, we\nintroduce Layer-AdvPatcher, a novel methodology designed to defend against\njailbreak attacks by utilizing an unlearning strategy to patch specific layers\nwithin LLMs through self-augmented datasets. Our insight is that certain\nlayer(s), tend to produce affirmative tokens when faced with harmful prompts.\nBy identifying these layers and adversarially exposing them to generate more\nharmful data, one can understand their inherent and diverse vulnerabilities to\nattacks. With these exposures, we then \"unlearn\" these issues, reducing the\nimpact of affirmative tokens and hence minimizing jailbreak risks while keeping\nthe model's responses to safe queries intact. We conduct extensive experiments\non two models, four benchmark datasets, and multiple state-of-the-art jailbreak\nattacks to demonstrate the efficacy of our approach. Results indicate that our\nframework reduces the harmfulness and attack success rate of jailbreak attacks\nwithout compromising utility for benign queries compared to recent defense\nmethods. Our code is publicly available at:\nhttps://github.com/oyy2000/LayerAdvPatcher",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in diverse\napplications, including chatbot assistants and code generation, aligning their\nbehavior with safety and ethical standards has become paramount. However,\njailbreak attacks, which exploit vulnerabilities to elicit unintended or\nharmful outputs, threaten LLMs' safety significantly. In this paper, we\nintroduce Layer-AdvPatcher, a novel methodology designed to defend against\njailbreak attacks by utilizing an unlearning strategy to patch specific layers\nwithin LLMs through self-augmented datasets. Our insight is that certain\nlayer(s), tend to produce affirmative tokens when faced with harmful prompts.\nBy identifying these layers and adversarially exposing them to generate more\nharmful data, one can understand their inherent and diverse vulnerabilities to\nattacks. With these exposures, we then \"unlearn\" these issues, reducing the\nimpact of affirmative tokens and hence minimizing jailbreak risks while keeping\nthe model's responses to safe queries intact. We conduct extensive experiments\non two models, four benchmark datasets, and multiple state-of-the-art jailbreak\nattacks to demonstrate the efficacy of our approach. Results indicate that our\nframework reduces the harmfulness and attack success rate of jailbreak attacks\nwithout compromising utility for benign queries compared to recent defense\nmethods. Our code is publicly available at:\nhttps://github.com/oyy2000/LayerAdvPatcher"
                },
                "authors": [
                    {
                        "name": "Yang Ouyang"
                    },
                    {
                        "name": "Hengrui Gu"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Meijun Gao"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kaixiong Zhou"
                },
                "author": "Kaixiong Zhou",
                "arxiv_comment": "14 pages, 4 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04961v2",
                "updated": "2025-02-12T04:52:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    4,
                    52,
                    8,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-09T04:26:15Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    4,
                    26,
                    15,
                    3,
                    9,
                    0
                ],
                "title": "Demystifying Domain-adaptive Post-training for Financial LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Domain-adaptive Post-training for Financial LLMs"
                },
                "summary": "Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain adaptive post-training of LLMs for the finance\ndomain. Our approach consists of four key components: FinCap, which defines the\ncore capabilities required for the target domain; FinRec, an effective training\nrecipe that jointly optimizes continual pre-training and instruction-following,\nalong with a novel preference data distillation method leveraging process\nsignals from a generative reward model; FinTrain, a curated set of training\ndatasets supporting FinRec; and FinEval, a comprehensive evaluation suite\naligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art\nperformance across a wide range of financial tasks. Our analysis also\nhighlights how each post-training stage contributes to distinct capabilities,\nuncovering specific challenges and effective solutions, providing valuable\ninsights for domain adaptation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain adaptive post-training of LLMs for the finance\ndomain. Our approach consists of four key components: FinCap, which defines the\ncore capabilities required for the target domain; FinRec, an effective training\nrecipe that jointly optimizes continual pre-training and instruction-following,\nalong with a novel preference data distillation method leveraging process\nsignals from a generative reward model; FinTrain, a curated set of training\ndatasets supporting FinRec; and FinEval, a comprehensive evaluation suite\naligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art\nperformance across a wide range of financial tasks. Our analysis also\nhighlights how each post-training stage contributes to distinct capabilities,\nuncovering specific challenges and effective solutions, providing valuable\ninsights for domain adaptation of LLMs."
                },
                "authors": [
                    {
                        "name": "Zixuan Ke"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08119v1",
                "updated": "2025-02-12T04:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    4,
                    42,
                    59,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T04:42:59Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    4,
                    42,
                    59,
                    2,
                    43,
                    0
                ],
                "title": "Generative AI-Enhanced Cooperative MEC of UAVs and Ground Stations for\n  Unmanned Surface Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-Enhanced Cooperative MEC of UAVs and Ground Stations for\n  Unmanned Surface Vehicles"
                },
                "summary": "The increasing deployment of unmanned surface vehicles (USVs) require\ncomputational support and coverage in applications such as maritime search and\nrescue. Unmanned aerial vehicles (UAVs) can offer low-cost, flexible aerial\nservices, and ground stations (GSs) can provide powerful supports, which can\ncooperate to help the USVs in complex scenarios. However, the collaboration\nbetween UAVs and GSs for USVs faces challenges of task uncertainties, USVs\ntrajectory uncertainties, heterogeneities, and limited computational resources.\nTo address these issues, we propose a cooperative UAV and GS based robust\nmulti-access edge computing framework to assist USVs in completing\ncomputational tasks. Specifically, we formulate the optimization problem of\njoint task offloading and UAV trajectory to minimize the total execution time,\nwhich is in the form of mixed integer nonlinear programming and NP-hard to\ntackle. Therefore, we propose the algorithm of generative artificial\nintelligence-enhanced heterogeneous agent proximal policy optimization\n(GAI-HAPPO). The proposed algorithm integrates GAI models to enhance the actor\nnetwork ability to model complex environments and extract high-level features,\nthereby allowing the algorithm to predict uncertainties and adapt to dynamic\nconditions. Additionally, GAI stabilizes the critic network, addressing the\ninstability of multi-agent reinforcement learning approaches. Finally,\nextensive simulations demonstrate that the proposed algorithm outperforms the\nexisting benchmark methods, thus highlighting the potentials in tackling\nintricate, cross-domain issues in the considered scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing deployment of unmanned surface vehicles (USVs) require\ncomputational support and coverage in applications such as maritime search and\nrescue. Unmanned aerial vehicles (UAVs) can offer low-cost, flexible aerial\nservices, and ground stations (GSs) can provide powerful supports, which can\ncooperate to help the USVs in complex scenarios. However, the collaboration\nbetween UAVs and GSs for USVs faces challenges of task uncertainties, USVs\ntrajectory uncertainties, heterogeneities, and limited computational resources.\nTo address these issues, we propose a cooperative UAV and GS based robust\nmulti-access edge computing framework to assist USVs in completing\ncomputational tasks. Specifically, we formulate the optimization problem of\njoint task offloading and UAV trajectory to minimize the total execution time,\nwhich is in the form of mixed integer nonlinear programming and NP-hard to\ntackle. Therefore, we propose the algorithm of generative artificial\nintelligence-enhanced heterogeneous agent proximal policy optimization\n(GAI-HAPPO). The proposed algorithm integrates GAI models to enhance the actor\nnetwork ability to model complex environments and extract high-level features,\nthereby allowing the algorithm to predict uncertainties and adapt to dynamic\nconditions. Additionally, GAI stabilizes the critic network, addressing the\ninstability of multi-agent reinforcement learning approaches. Finally,\nextensive simulations demonstrate that the proposed algorithm outperforms the\nexisting benchmark methods, thus highlighting the potentials in tackling\nintricate, cross-domain issues in the considered scenarios."
                },
                "authors": [
                    {
                        "name": "Jiahao You"
                    },
                    {
                        "name": "Ziye Jia"
                    },
                    {
                        "name": "Chao Dong"
                    },
                    {
                        "name": "Qihui Wu"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]