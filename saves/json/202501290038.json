[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaquín Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gómez-Albarrán"
                    },
                    {
                        "name": "José-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "José-Luis Sierra"
                },
                "author": "José-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v4",
                "updated": "2025-01-26T07:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    7,
                    29,
                    6,
                    6,
                    26,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v2",
                "updated": "2025-01-26T01:43:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    1,
                    43,
                    46,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15348v1",
                "updated": "2025-01-25T23:16:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T23:16:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "ReInc: Scaling Training of Dynamic Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReInc: Scaling Training of Dynamic Graph Neural Networks"
                },
                "summary": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets."
                },
                "authors": [
                    {
                        "name": "Mingyu Guan"
                    },
                    {
                        "name": "Saumia Singhal"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v4",
                "updated": "2025-01-25T20:50:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    20,
                    50,
                    22,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v2",
                "updated": "2025-01-25T12:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    17,
                    41,
                    5,
                    25,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v2",
                "updated": "2025-01-25T10:38:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    10,
                    38,
                    11,
                    5,
                    25,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15126v1",
                "updated": "2025-01-25T08:27:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T08:27:26Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "title": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs"
                },
                "summary": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x."
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15113v1",
                "updated": "2025-01-25T07:28:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T07:28:13Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads"
                },
                "summary": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Shaowei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Chen"
                },
                "author": "Shaowei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v2",
                "updated": "2025-01-25T04:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    4,
                    21,
                    57,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15021v1",
                "updated": "2025-01-25T02:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T02:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v2",
                "updated": "2025-01-24T19:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    13,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Bürgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jiménez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. García-León"
                    },
                    {
                        "name": "R. García-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqué"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v1",
                "updated": "2025-01-23T02:20:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring"
                },
                "summary": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elsässer"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Anaïs Dréau"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Gérard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Gérard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v1",
                "updated": "2025-01-18T19:10:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or \"phantom\" antenna users, bridging the performance gains of the min-G\nand Grouping schemes. These strategies jointly optimize the number of users,\n$\\Omega$, and the parallel streams decoded by each user, $\\beta_k$, ensuring\nlinear decodability for all target users. Analytical and numerical results\nconfirm that the proposed schemes achieve significant DoF improvements across\nvarious system configurations, demonstrating the potential of content-aware\nMIMO-CC strategies for enhancing wireless network performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or \"phantom\" antenna users, bridging the performance gains of the min-G\nand Grouping schemes. These strategies jointly optimize the number of users,\n$\\Omega$, and the parallel streams decoded by each user, $\\beta_k$, ensuring\nlinear decodability for all target users. Analytical and numerical results\nconfirm that the proposed schemes achieve significant DoF improvements across\nvarious system configurations, demonstrating the potential of content-aware\nMIMO-CC strategies for enhancing wireless network performance."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14795v1",
                "updated": "2025-01-11T12:22:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    12,
                    22,
                    51,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T12:22:51Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    12,
                    22,
                    51,
                    5,
                    11,
                    0
                ],
                "title": "Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL"
                },
                "summary": "Due to its flexible architecture, FPGAs support unique, deep hardware\npipeline implementations for accelerating HPC applications. However, these\ndevices are quite new in the HPC space, and thus, have been scarcely explored\noutside some specific scientific domain, such as machine learning or biological\nsequence alignment. The objective of this thesis is to characterize the\nFPGA-based solution for accelerating particle-mesh algorithms, in which the\nforce applied to each particle is computed based on the fields deposited in a\nfinite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator\ncalled ZPIC that has the same core algorithm and functionalities as OSIRIS. To\ncreate an efficient hardware design, the program keeps the particles strictly\nsorted by tiles (a group of cells) and uses the local memory as an explicitly\nmanaged cache. We also create multiple copies of the local current buffer to\nsolve dependencies during the deposition phase. The resulting pipeline was\nreplicated multiple times to explore data parallelism and increase its\nthroughput. We then compare our hardware solution against similar\nimplementations on GPU and multicore CPUs, showing promising results in term of\npower efficiency and performance.\n  Keywords: FPGA, OpenCL, Kinetic Plasma Simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to its flexible architecture, FPGAs support unique, deep hardware\npipeline implementations for accelerating HPC applications. However, these\ndevices are quite new in the HPC space, and thus, have been scarcely explored\noutside some specific scientific domain, such as machine learning or biological\nsequence alignment. The objective of this thesis is to characterize the\nFPGA-based solution for accelerating particle-mesh algorithms, in which the\nforce applied to each particle is computed based on the fields deposited in a\nfinite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator\ncalled ZPIC that has the same core algorithm and functionalities as OSIRIS. To\ncreate an efficient hardware design, the program keeps the particles strictly\nsorted by tiles (a group of cells) and uses the local memory as an explicitly\nmanaged cache. We also create multiple copies of the local current buffer to\nsolve dependencies during the deposition phase. The resulting pipeline was\nreplicated multiple times to explore data parallelism and increase its\nthroughput. We then compare our hardware solution against similar\nimplementations on GPU and multicore CPUs, showing promising results in term of\npower efficiency and performance.\n  Keywords: FPGA, OpenCL, Kinetic Plasma Simulation."
                },
                "authors": [
                    {
                        "name": "Nicolas Lee Guidotti"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Lee Guidotti"
                },
                "author": "Nicolas Lee Guidotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_doi": "10.3847/1538-4365/ad9b8d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4365/ad9b8d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "arxiv_journal_ref": "Astrophys. j., suppl. ser. 276 (2025) 40",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.16329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16329v1",
                "updated": "2025-01-27T18:59:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    59,
                    55,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:59:55Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    59,
                    55,
                    0,
                    27,
                    0
                ],
                "title": "sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for\n  Automatic Sleep Staging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for\n  Automatic Sleep Staging"
                },
                "summary": "Automatic sleep staging based on electroencephalography (EEG) and\nelectromyography (EMG) signals is an important aspect of sleep-related\nresearch. Current sleep staging methods suffer from two major drawbacks. First,\nthere are limited information interactions between modalities in the existing\nmethods. Second, current methods do not develop unified models that can handle\ndifferent sources of input. To address these issues, we propose a novel sleep\nstage scoring model sDREAMER, which emphasizes cross-modality interaction and\nper-channel performance. Specifically, we develop a mixture-of-modality-expert\n(MoME) model with three pathways for EEG, EMG, and mixed signals with partially\nshared weights. We further propose a self-distillation training scheme for\nfurther information interaction across modalities. Our model is trained with\nmulti-channel inputs and can make classifications on either single-channel or\nmulti-channel inputs. Experiments demonstrate that our model outperforms the\nexisting transformer-based sleep scoring methods for multi-channel inference.\nFor single-channel inference, our model also outperforms the transformer-based\nmodels trained with single-channel signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic sleep staging based on electroencephalography (EEG) and\nelectromyography (EMG) signals is an important aspect of sleep-related\nresearch. Current sleep staging methods suffer from two major drawbacks. First,\nthere are limited information interactions between modalities in the existing\nmethods. Second, current methods do not develop unified models that can handle\ndifferent sources of input. To address these issues, we propose a novel sleep\nstage scoring model sDREAMER, which emphasizes cross-modality interaction and\nper-channel performance. Specifically, we develop a mixture-of-modality-expert\n(MoME) model with three pathways for EEG, EMG, and mixed signals with partially\nshared weights. We further propose a self-distillation training scheme for\nfurther information interaction across modalities. Our model is trained with\nmulti-channel inputs and can make classifications on either single-channel or\nmulti-channel inputs. Experiments demonstrate that our model outperforms the\nexisting transformer-based sleep scoring methods for multi-channel inference.\nFor single-channel inference, our model also outperforms the transformer-based\nmodels trained with single-channel signals."
                },
                "authors": [
                    {
                        "name": "Jingyuan Chen"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Mie Anderson"
                    },
                    {
                        "name": "Natalie Hauglund"
                    },
                    {
                        "name": "Celia Kjaerby"
                    },
                    {
                        "name": "Verena Untiet"
                    },
                    {
                        "name": "Maiken Nedergaard"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "arxiv_doi": "10.1109/ICDH60066.2023.00028",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDH60066.2023.00028",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.16329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13896v2",
                "updated": "2025-01-27T18:58:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    58,
                    42,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-23T18:16:21Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    18,
                    16,
                    21,
                    3,
                    23,
                    0
                ],
                "title": "GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous\n  Exploration"
                },
                "summary": "Graphical User Interface (GUI) action grounding is a critical step in GUI\nautomation that maps language instructions to actionable elements on GUI\nscreens. Most recent works of GUI action grounding leverage large GUI datasets\nto fine-tune MLLMs. However, the fine-tuning data always covers limited GUI\nenvironments, and we find the performance of the resulting model deteriorates\nin novel environments. We argue that the GUI grounding models should be further\naligned to the novel environments to reveal their full potential, when the\ninference is known to involve novel environments, i.e., environments not used\nduring the previous fine-tuning. To realize this, we first propose GUI-Bee, an\nMLLM-based autonomous agent, to collect high-quality, environment-specific data\nthrough exploration and then continuously fine-tune GUI grounding models with\nthe collected data. Our agent leverages a novel Q-value-Incentive In-Context\nReinforcement Learning (Q-ICRL) method to optimize exploration efficiency and\ndata quality. Additionally, we introduce NovelScreenSpot, a benchmark for\ntesting how well the data can help align GUI action grounding models to novel\nenvironments and demonstrate the effectiveness of data collected by GUI-Bee in\nthe experiments. Furthermore, we conduct an ablation study to validate the\nQ-ICRL method in enhancing the efficiency of GUI-Bee. Project page:\nhttps://gui-bee.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical User Interface (GUI) action grounding is a critical step in GUI\nautomation that maps language instructions to actionable elements on GUI\nscreens. Most recent works of GUI action grounding leverage large GUI datasets\nto fine-tune MLLMs. However, the fine-tuning data always covers limited GUI\nenvironments, and we find the performance of the resulting model deteriorates\nin novel environments. We argue that the GUI grounding models should be further\naligned to the novel environments to reveal their full potential, when the\ninference is known to involve novel environments, i.e., environments not used\nduring the previous fine-tuning. To realize this, we first propose GUI-Bee, an\nMLLM-based autonomous agent, to collect high-quality, environment-specific data\nthrough exploration and then continuously fine-tune GUI grounding models with\nthe collected data. Our agent leverages a novel Q-value-Incentive In-Context\nReinforcement Learning (Q-ICRL) method to optimize exploration efficiency and\ndata quality. Additionally, we introduce NovelScreenSpot, a benchmark for\ntesting how well the data can help align GUI action grounding models to novel\nenvironments and demonstrate the effectiveness of data collected by GUI-Bee in\nthe experiments. Furthermore, we conduct an ablation study to validate the\nQ-ICRL method in enhancing the efficiency of GUI-Bee. Project page:\nhttps://gui-bee.github.io"
                },
                "authors": [
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Handong Zhao"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Yu Shen"
                    },
                    {
                        "name": "Xin Eric Wang"
                    },
                    {
                        "name": "Gang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wu"
                },
                "author": "Gang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16317v1",
                "updated": "2025-01-27T18:54:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    54,
                    57,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:54:57Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    54,
                    57,
                    0,
                    27,
                    0
                ],
                "title": "A MARVEL-ous study of how well galaxy shapes reflect Dark Matter halo\n  shapes in Cold Dark Matter Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A MARVEL-ous study of how well galaxy shapes reflect Dark Matter halo\n  shapes in Cold Dark Matter Simulations"
                },
                "summary": "We present a 3D shape analysis of both dark matter (DM) and stellar matter\n(SM) in simulated dwarf galaxies to determine whether stellar shape traces DM\nshape. Using 80 central and satellite galaxies from three simulation suites\n(Marvelous Massive Dwarfs, Marvelous Dwarfs, and DC Justice League) spanning\nstellar masses of $10^6$--$10^{10}$ M$_\\odot$, we measure 3D shapes through the\nmoment of inertia tensor at two times the effective radius to derive axis\nratios ($C/A$, $B/A$) and triaxiality. We find that stellar shape does indeed\nfollow DM halo shape for our dwarf galaxies. However, the presence of a stellar\ndisk in more massive dwarfs (M$_* \\gtrsim 10^{7.5}$ M$_\\odot$) pulls the\ndistribution of stellar $C/A$ ratios to lower values, while in lower mass\ngalaxies the gravitational potential remains predominantly shaped by DM.\nSimilarly, stellar triaxiality generally tracks dark matter halo triaxiality,\nwith this relationship being particularly strong for non-disky galaxies though\nweaker in disky systems. This correlation is reinforced by strong alignment\nbetween SM and DM axes, particularly in disk galaxies. Further, we find no\ndetectable difference in either SM or DM shape comparing two different SNe\nfeedback implementations, demonstrating that shape measurements may be robust\nto different implementations of baryonic feedback in dwarf galaxies. We also\nobserve that a dwarf galaxy's shape is largely unperturbed by recent mergers\n(with merger ratios $>4$). This comprehensive study demonstrates that stellar\nshape measurements can serve as a reliable tool for inferring DM shapes in\ndwarf galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a 3D shape analysis of both dark matter (DM) and stellar matter\n(SM) in simulated dwarf galaxies to determine whether stellar shape traces DM\nshape. Using 80 central and satellite galaxies from three simulation suites\n(Marvelous Massive Dwarfs, Marvelous Dwarfs, and DC Justice League) spanning\nstellar masses of $10^6$--$10^{10}$ M$_\\odot$, we measure 3D shapes through the\nmoment of inertia tensor at two times the effective radius to derive axis\nratios ($C/A$, $B/A$) and triaxiality. We find that stellar shape does indeed\nfollow DM halo shape for our dwarf galaxies. However, the presence of a stellar\ndisk in more massive dwarfs (M$_* \\gtrsim 10^{7.5}$ M$_\\odot$) pulls the\ndistribution of stellar $C/A$ ratios to lower values, while in lower mass\ngalaxies the gravitational potential remains predominantly shaped by DM.\nSimilarly, stellar triaxiality generally tracks dark matter halo triaxiality,\nwith this relationship being particularly strong for non-disky galaxies though\nweaker in disky systems. This correlation is reinforced by strong alignment\nbetween SM and DM axes, particularly in disk galaxies. Further, we find no\ndetectable difference in either SM or DM shape comparing two different SNe\nfeedback implementations, demonstrating that shape measurements may be robust\nto different implementations of baryonic feedback in dwarf galaxies. We also\nobserve that a dwarf galaxy's shape is largely unperturbed by recent mergers\n(with merger ratios $>4$). This comprehensive study demonstrates that stellar\nshape measurements can serve as a reliable tool for inferring DM shapes in\ndwarf galaxies."
                },
                "authors": [
                    {
                        "name": "Blake Keith"
                    },
                    {
                        "name": "Ferah Munshi"
                    },
                    {
                        "name": "Alyson M. Brooks"
                    },
                    {
                        "name": "Jordan Van Nest"
                    },
                    {
                        "name": "Anna Engelhardt"
                    },
                    {
                        "name": "Akaxia Cruz"
                    },
                    {
                        "name": "Ben Keller"
                    },
                    {
                        "name": "Thomas Quinn"
                    },
                    {
                        "name": "James Wadsley"
                    }
                ],
                "author_detail": {
                    "name": "James Wadsley"
                },
                "author": "James Wadsley",
                "arxiv_comment": "22 pages, 14 figures, Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18644v2",
                "updated": "2025-01-27T18:54:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    54,
                    42,
                    0,
                    27,
                    0
                ],
                "published": "2024-12-24T16:06:53Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    6,
                    53,
                    1,
                    359,
                    0
                ],
                "title": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation"
                },
                "summary": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation."
                },
                "authors": [
                    {
                        "name": "Karishma Thakrar"
                    }
                ],
                "author_detail": {
                    "name": "Karishma Thakrar"
                },
                "author": "Karishma Thakrar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16315v1",
                "updated": "2025-01-27T18:52:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    52,
                    1,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:52:01Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    52,
                    1,
                    0,
                    27,
                    0
                ],
                "title": "A varifold-type estimation for data sampled on a rectifiable set",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A varifold-type estimation for data sampled on a rectifiable set"
                },
                "summary": "We investigate the inference of varifold structures in a statistical\nframework: assuming that we have access to i.i.d. samples in $\\mathbb{R}^n$\nobtained from an underlying $d$--dimensional shape $S$ endowed with a possibly\nnon uniform density $\\theta$, we propose and analyse an estimator of the\nvarifold structure associated to $S$. The shape $S$ is assumed to be piecewise\n$C^{1,a}$ in a sense that allows for a singular set whose small enlargements\nare of small $d$--dimensional measure. The estimators are kernel--based both\nfor infering the density and the tangent spaces and the convergence result\nholds for the bounded Lipschitz distance between varifolds, in expectation and\nin a noiseless model. The mean convergence rate involves the dimension $d$ of\n$S$, its regularity through $a \\in (0, 1]$ and the regularity of the density\n$\\theta$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the inference of varifold structures in a statistical\nframework: assuming that we have access to i.i.d. samples in $\\mathbb{R}^n$\nobtained from an underlying $d$--dimensional shape $S$ endowed with a possibly\nnon uniform density $\\theta$, we propose and analyse an estimator of the\nvarifold structure associated to $S$. The shape $S$ is assumed to be piecewise\n$C^{1,a}$ in a sense that allows for a singular set whose small enlargements\nare of small $d$--dimensional measure. The estimators are kernel--based both\nfor infering the density and the tangent spaces and the convergence result\nholds for the bounded Lipschitz distance between varifolds, in expectation and\nin a noiseless model. The mean convergence rate involves the dimension $d$ of\n$S$, its regularity through $a \\in (0, 1]$ and the regularity of the density\n$\\theta$."
                },
                "authors": [
                    {
                        "name": "Blanche Buet"
                    },
                    {
                        "name": "Charly Boricaud"
                    }
                ],
                "author_detail": {
                    "name": "Charly Boricaud"
                },
                "author": "Charly Boricaud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.CA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.CA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01553v2",
                "updated": "2025-01-27T18:51:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    51,
                    36,
                    0,
                    27,
                    0
                ],
                "published": "2024-03-16T03:12:45Z",
                "published_parsed": [
                    2024,
                    3,
                    16,
                    3,
                    12,
                    45,
                    5,
                    76,
                    0
                ],
                "title": "Empirical Studies of Parameter Efficient Methods for Large Language\n  Models of Code and Knowledge Transfer to R",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Studies of Parameter Efficient Methods for Large Language\n  Models of Code and Knowledge Transfer to R"
                },
                "summary": "Parameter Efficient Fine-Tuning (PEFT) methods are proposed as an alternative\nfine-tuning approach for Large Language Models (LLM) to minimize high training\ncosts. While prior research demonstrates the effectiveness of PEFT methods in\nknowledge transfer using smaller language models, their application to larger\nLLMs, particularly in low-resource and unseen programming languages such as R,\nremains under-explored. In this work, we evaluate PEFT methods, LoRA,\nCompacter, and IA^3 on LLMs for code summarization and generation, with a\nparticular emphasis on knowledge transfer to R as an unseen under-explored\ntarget language. Our experiments reveal that LoRA consistently outperforms\nCompacter and IA^3 in all settings, while Compacter offers significant resource\nefficiency with minimal performance trade-offs. Additionally, we find that the\nnumber of trainable parameters has a greater influence on the functional\naccuracy of the generated code than PEFT architecture. Our study can direct\nfuture research in developing code intelligent tasks for unseen languages\nincluding R, as well as the choice of PEFT methods for knowledge transfer,\nespecially when balancing the computational cost and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter Efficient Fine-Tuning (PEFT) methods are proposed as an alternative\nfine-tuning approach for Large Language Models (LLM) to minimize high training\ncosts. While prior research demonstrates the effectiveness of PEFT methods in\nknowledge transfer using smaller language models, their application to larger\nLLMs, particularly in low-resource and unseen programming languages such as R,\nremains under-explored. In this work, we evaluate PEFT methods, LoRA,\nCompacter, and IA^3 on LLMs for code summarization and generation, with a\nparticular emphasis on knowledge transfer to R as an unseen under-explored\ntarget language. Our experiments reveal that LoRA consistently outperforms\nCompacter and IA^3 in all settings, while Compacter offers significant resource\nefficiency with minimal performance trade-offs. Additionally, we find that the\nnumber of trainable parameters has a greater influence on the functional\naccuracy of the generated code than PEFT architecture. Our study can direct\nfuture research in developing code intelligent tasks for unseen languages\nincluding R, as well as the choice of PEFT methods for knowledge transfer,\nespecially when balancing the computational cost and performance."
                },
                "authors": [
                    {
                        "name": "Amirreza Esmaeili"
                    },
                    {
                        "name": "Iman Saberi"
                    },
                    {
                        "name": "Fatemeh H. Fard"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh H. Fard"
                },
                "author": "Fatemeh H. Fard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16309v1",
                "updated": "2025-01-27T18:47:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    47,
                    58,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:47:58Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    47,
                    58,
                    0,
                    27,
                    0
                ],
                "title": "Evaluating The Performance of Using Large Language Models to Automate\n  Summarization of CT Simulation Orders in Radiation Oncology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating The Performance of Using Large Language Models to Automate\n  Summarization of CT Simulation Orders in Radiation Oncology"
                },
                "summary": "Purpose: This study aims to use a large language model (LLM) to automate the\ngeneration of summaries from the CT simulation orders and evaluate its\nperformance.\n  Materials and Methods: A total of 607 CT simulation orders for patients were\ncollected from the Aria database at our institution. A locally hosted Llama 3.1\n405B model, accessed via the Application Programming Interface (API) service,\nwas used to extract keywords from the CT simulation orders and generate\nsummaries. The downloaded CT simulation orders were categorized into seven\ngroups based on treatment modalities and disease sites. For each group, a\ncustomized instruction prompt was developed collaboratively with therapists to\nguide the Llama 3.1 405B model in generating summaries. The ground truth for\nthe corresponding summaries was manually derived by carefully reviewing each CT\nsimulation order and subsequently verified by therapists. The accuracy of the\nLLM-generated summaries was evaluated by therapists using the verified ground\ntruth as a reference.\n  Results: About 98% of the LLM-generated summaries aligned with the manually\ngenerated ground truth in terms of accuracy. Our evaluations showed an improved\nconsistency in format and enhanced readability of the LLM-generated summaries\ncompared to the corresponding therapists-generated summaries. This automated\napproach demonstrated a consistent performance across all groups, regardless of\nmodality or disease site.\n  Conclusions: This study demonstrated the high precision and consistency of\nthe Llama 3.1 405B model in extracting keywords and summarizing CT simulation\norders, suggesting that LLMs have great potential to help with this task,\nreduce the workload of therapists and improve workflow efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: This study aims to use a large language model (LLM) to automate the\ngeneration of summaries from the CT simulation orders and evaluate its\nperformance.\n  Materials and Methods: A total of 607 CT simulation orders for patients were\ncollected from the Aria database at our institution. A locally hosted Llama 3.1\n405B model, accessed via the Application Programming Interface (API) service,\nwas used to extract keywords from the CT simulation orders and generate\nsummaries. The downloaded CT simulation orders were categorized into seven\ngroups based on treatment modalities and disease sites. For each group, a\ncustomized instruction prompt was developed collaboratively with therapists to\nguide the Llama 3.1 405B model in generating summaries. The ground truth for\nthe corresponding summaries was manually derived by carefully reviewing each CT\nsimulation order and subsequently verified by therapists. The accuracy of the\nLLM-generated summaries was evaluated by therapists using the verified ground\ntruth as a reference.\n  Results: About 98% of the LLM-generated summaries aligned with the manually\ngenerated ground truth in terms of accuracy. Our evaluations showed an improved\nconsistency in format and enhanced readability of the LLM-generated summaries\ncompared to the corresponding therapists-generated summaries. This automated\napproach demonstrated a consistent performance across all groups, regardless of\nmodality or disease site.\n  Conclusions: This study demonstrated the high precision and consistency of\nthe Llama 3.1 405B model in extracting keywords and summarizing CT simulation\norders, suggesting that LLMs have great potential to help with this task,\nreduce the workload of therapists and improve workflow efficiency."
                },
                "authors": [
                    {
                        "name": "Meiyun Cao"
                    },
                    {
                        "name": "Shaw Hu"
                    },
                    {
                        "name": "Jason Sharp"
                    },
                    {
                        "name": "Edward Clouser"
                    },
                    {
                        "name": "Jason Holmes"
                    },
                    {
                        "name": "Linda L. Lam"
                    },
                    {
                        "name": "Xiaoning Ding"
                    },
                    {
                        "name": "Diego Santos Toesca"
                    },
                    {
                        "name": "Wendy S. Lindholm"
                    },
                    {
                        "name": "Samir H. Patel"
                    },
                    {
                        "name": "Sujay A. Vora"
                    },
                    {
                        "name": "Peilong Wang"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16303v1",
                "updated": "2025-01-27T18:45:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    45,
                    7,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:45:07Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    45,
                    7,
                    0,
                    27,
                    0
                ],
                "title": "RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based\n  Video Event Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based\n  Video Event Retrieval"
                },
                "summary": "Retrieving events from videos using text queries has become increasingly\nchallenging due to the rapid growth of multimedia content. Existing methods for\ntext-based video event retrieval often focus heavily on object-level\ndescriptions, overlooking the crucial role of contextual information. This\nlimitation is especially apparent when queries lack sufficient context, such as\nmissing location details or ambiguous background elements. To address these\nchallenges, we propose a novel system called RAPID (Retrieval-Augmented\nParallel Inference Drafting), which leverages advancements in Large Language\nModels (LLMs) and prompt-based learning to semantically correct and enrich user\nqueries with relevant contextual information. These enriched queries are then\nprocessed through parallel retrieval, followed by an evaluation step to select\nthe most relevant results based on their alignment with the original query.\nThrough extensive experiments on our custom-developed dataset, we demonstrate\nthat RAPID significantly outperforms traditional retrieval methods,\nparticularly for contextually incomplete queries. Our system was validated for\nboth speed and accuracy through participation in the Ho Chi Minh City AI\nChallenge 2024, where it successfully retrieved events from over 300 hours of\nvideo. Further evaluation comparing RAPID with the baseline proposed by the\ncompetition organizers demonstrated its superior effectiveness, highlighting\nthe strength and robustness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving events from videos using text queries has become increasingly\nchallenging due to the rapid growth of multimedia content. Existing methods for\ntext-based video event retrieval often focus heavily on object-level\ndescriptions, overlooking the crucial role of contextual information. This\nlimitation is especially apparent when queries lack sufficient context, such as\nmissing location details or ambiguous background elements. To address these\nchallenges, we propose a novel system called RAPID (Retrieval-Augmented\nParallel Inference Drafting), which leverages advancements in Large Language\nModels (LLMs) and prompt-based learning to semantically correct and enrich user\nqueries with relevant contextual information. These enriched queries are then\nprocessed through parallel retrieval, followed by an evaluation step to select\nthe most relevant results based on their alignment with the original query.\nThrough extensive experiments on our custom-developed dataset, we demonstrate\nthat RAPID significantly outperforms traditional retrieval methods,\nparticularly for contextually incomplete queries. Our system was validated for\nboth speed and accuracy through participation in the Ho Chi Minh City AI\nChallenge 2024, where it successfully retrieved events from over 300 hours of\nvideo. Further evaluation comparing RAPID with the baseline proposed by the\ncompetition organizers demonstrated its superior effectiveness, highlighting\nthe strength and robustness of our approach."
                },
                "authors": [
                    {
                        "name": "Long Nguyen"
                    },
                    {
                        "name": "Huy Nguyen"
                    },
                    {
                        "name": "Bao Khuu"
                    },
                    {
                        "name": "Huy Luu"
                    },
                    {
                        "name": "Huy Le"
                    },
                    {
                        "name": "Tuan Nguyen"
                    },
                    {
                        "name": "Tho Quan"
                    }
                ],
                "author_detail": {
                    "name": "Tho Quan"
                },
                "author": "Tho Quan",
                "arxiv_comment": "Under review at SoICT'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16302v1",
                "updated": "2025-01-27T18:42:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    42,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:42:48Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    42,
                    48,
                    0,
                    27,
                    0
                ],
                "title": "Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With\n  Configurable Depth and Width",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With\n  Configurable Depth and Width"
                },
                "summary": "Large language models (LLMs) provide powerful foundations to perform\nfine-grained text re-ranking. However, they are often prohibitive in reality\ndue to constraints on computation bandwidth. In this work, we propose a\n\\textbf{flexible} architecture called \\textbf{Matroyshka Re-Ranker}, which is\ndesigned to facilitate \\textbf{runtime customization} of model layers and\nsequence lengths at each layer based on users' configurations. Consequently,\nthe LLM-based re-rankers can be made applicable across various real-world\nsituations. The increased flexibility may come at the cost of precision loss.\nTo address this problem, we introduce a suite of techniques to optimize the\nperformance. First, we propose \\textbf{cascaded self-distillation}, where each\nsub-architecture learns to preserve a precise re-ranking performance from its\nsuper components, whose predictions can be exploited as smooth and informative\nteacher signals. Second, we design a \\textbf{factorized compensation\nmechanism}, where two collaborative Low-Rank Adaptation modules, vertical and\nhorizontal, are jointly employed to compensate for the precision loss resulted\nfrom arbitrary combinations of layer and sequence compression. We perform\ncomprehensive experiments based on the passage and document retrieval datasets\nfrom MSMARCO, along with all public datasets from BEIR benchmark. In our\nexperiments, Matryoshka Re-Ranker substantially outperforms the existing\nmethods, while effectively preserving its superior performance across various\nforms of compression and different application scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) provide powerful foundations to perform\nfine-grained text re-ranking. However, they are often prohibitive in reality\ndue to constraints on computation bandwidth. In this work, we propose a\n\\textbf{flexible} architecture called \\textbf{Matroyshka Re-Ranker}, which is\ndesigned to facilitate \\textbf{runtime customization} of model layers and\nsequence lengths at each layer based on users' configurations. Consequently,\nthe LLM-based re-rankers can be made applicable across various real-world\nsituations. The increased flexibility may come at the cost of precision loss.\nTo address this problem, we introduce a suite of techniques to optimize the\nperformance. First, we propose \\textbf{cascaded self-distillation}, where each\nsub-architecture learns to preserve a precise re-ranking performance from its\nsuper components, whose predictions can be exploited as smooth and informative\nteacher signals. Second, we design a \\textbf{factorized compensation\nmechanism}, where two collaborative Low-Rank Adaptation modules, vertical and\nhorizontal, are jointly employed to compensate for the precision loss resulted\nfrom arbitrary combinations of layer and sequence compression. We perform\ncomprehensive experiments based on the passage and document retrieval datasets\nfrom MSMARCO, along with all public datasets from BEIR benchmark. In our\nexperiments, Matryoshka Re-Ranker substantially outperforms the existing\nmethods, while effectively preserving its superior performance across various\nforms of compression and different application scenarios."
                },
                "authors": [
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Chaozhuo Li"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Yingxia Shao"
                    }
                ],
                "author_detail": {
                    "name": "Yingxia Shao"
                },
                "author": "Yingxia Shao",
                "arxiv_comment": "The Web Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16300v1",
                "updated": "2025-01-27T18:38:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    38,
                    36,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:38:36Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    38,
                    36,
                    0,
                    27,
                    0
                ],
                "title": "Large Models in Dialogue for Active Perception and Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Models in Dialogue for Active Perception and Anomaly Detection"
                },
                "summary": "Autonomous aerial monitoring is an important task aimed at gathering\ninformation from areas that may not be easily accessible by humans. At the same\ntime, this task often requires recognizing anomalies from a significant\ndistance or not previously encountered in the past. In this paper, we propose a\nnovel framework that leverages the advanced capabilities provided by Large\nLanguage Models (LLMs) to actively collect information and perform anomaly\ndetection in novel scenes. To this end, we propose an LLM based model dialogue\napproach, in which two deep learning models engage in a dialogue to actively\ncontrol a drone to increase perception and anomaly detection accuracy. We\nconduct our experiments in a high fidelity simulation environment where an LLM\nis provided with a predetermined set of natural language movement commands\nmapped into executable code functions. Additionally, we deploy a multimodal\nVisual Question Answering (VQA) model charged with the task of visual question\nanswering and captioning. By engaging the two models in conversation, the LLM\nasks exploratory questions while simultaneously flying a drone into different\nparts of the scene, providing a novel way to implement active perception. By\nleveraging LLMs reasoning ability, we output an improved detailed description\nof the scene going beyond existing static perception approaches. In addition to\ninformation gathering, our approach is utilized for anomaly detection and our\nresults demonstrate the proposed methods effectiveness in informing and\nalerting about potential hazards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous aerial monitoring is an important task aimed at gathering\ninformation from areas that may not be easily accessible by humans. At the same\ntime, this task often requires recognizing anomalies from a significant\ndistance or not previously encountered in the past. In this paper, we propose a\nnovel framework that leverages the advanced capabilities provided by Large\nLanguage Models (LLMs) to actively collect information and perform anomaly\ndetection in novel scenes. To this end, we propose an LLM based model dialogue\napproach, in which two deep learning models engage in a dialogue to actively\ncontrol a drone to increase perception and anomaly detection accuracy. We\nconduct our experiments in a high fidelity simulation environment where an LLM\nis provided with a predetermined set of natural language movement commands\nmapped into executable code functions. Additionally, we deploy a multimodal\nVisual Question Answering (VQA) model charged with the task of visual question\nanswering and captioning. By engaging the two models in conversation, the LLM\nasks exploratory questions while simultaneously flying a drone into different\nparts of the scene, providing a novel way to implement active perception. By\nleveraging LLMs reasoning ability, we output an improved detailed description\nof the scene going beyond existing static perception approaches. In addition to\ninformation gathering, our approach is utilized for anomaly detection and our\nresults demonstrate the proposed methods effectiveness in informing and\nalerting about potential hazards."
                },
                "authors": [
                    {
                        "name": "Tzoulio Chamiti"
                    },
                    {
                        "name": "Nikolaos Passalis"
                    },
                    {
                        "name": "Anastasios Tefas"
                    }
                ],
                "author_detail": {
                    "name": "Anastasios Tefas"
                },
                "author": "Anastasios Tefas",
                "arxiv_comment": "Accepted to International Conference of Pattern Recognition (ICPR\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03471v2",
                "updated": "2025-01-27T18:27:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    27,
                    32,
                    0,
                    27,
                    0
                ],
                "published": "2024-11-05T19:52:58Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    19,
                    52,
                    58,
                    1,
                    310,
                    0
                ],
                "title": "MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs"
                },
                "summary": "Large Language Models (LLMs) have been applied to various hardware design\ntasks, including Verilog code generation, EDA tool scripting, and RTL bug\nfixing. Despite this extensive exploration, LLMs are yet to be used for the\ntask of post-synthesis metric reasoning and estimation of HDL designs. In this\npaper, we assess the ability of LLMs to reason about post-synthesis metrics of\nVerilog designs. We introduce MetRex, a large-scale dataset comprising 25,868\nVerilog HDL designs and their corresponding post-synthesis metrics, namely\narea, delay, and static power. MetRex incorporates a Chain of Thought (CoT)\ntemplate to enhance LLMs' reasoning about these metrics. Extensive experiments\nshow that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities\non average by 37.0\\%, 25.3\\%, and 25.7\\% on the area, delay, and static power,\nrespectively. While SFT improves performance on our benchmark, it remains far\nfrom achieving optimal results, especially on complex problems. Comparing to\nstate-of-the-art regression models, our approach delivers accurate\npost-synthesis predictions for 17.4\\% more designs (within a 5\\% error margin),\nin addition to offering a 1.7x speedup by eliminating the need for\npre-processing. This work lays the groundwork for advancing LLM-based Verilog\ncode metric reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been applied to various hardware design\ntasks, including Verilog code generation, EDA tool scripting, and RTL bug\nfixing. Despite this extensive exploration, LLMs are yet to be used for the\ntask of post-synthesis metric reasoning and estimation of HDL designs. In this\npaper, we assess the ability of LLMs to reason about post-synthesis metrics of\nVerilog designs. We introduce MetRex, a large-scale dataset comprising 25,868\nVerilog HDL designs and their corresponding post-synthesis metrics, namely\narea, delay, and static power. MetRex incorporates a Chain of Thought (CoT)\ntemplate to enhance LLMs' reasoning about these metrics. Extensive experiments\nshow that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities\non average by 37.0\\%, 25.3\\%, and 25.7\\% on the area, delay, and static power,\nrespectively. While SFT improves performance on our benchmark, it remains far\nfrom achieving optimal results, especially on complex problems. Comparing to\nstate-of-the-art regression models, our approach delivers accurate\npost-synthesis predictions for 17.4\\% more designs (within a 5\\% error margin),\nin addition to offering a 1.7x speedup by eliminating the need for\npre-processing. This work lays the groundwork for advancing LLM-based Verilog\ncode metric reasoning."
                },
                "authors": [
                    {
                        "name": "Manar Abdelatty"
                    },
                    {
                        "name": "Jingxiao Ma"
                    },
                    {
                        "name": "Sherief Reda"
                    }
                ],
                "author_detail": {
                    "name": "Sherief Reda"
                },
                "author": "Sherief Reda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16287v1",
                "updated": "2025-01-27T18:23:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    23,
                    59,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:23:59Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    23,
                    59,
                    0,
                    27,
                    0
                ],
                "title": "A Unified Representation of Density-Power-Based Divergences Reducible to\n  M-Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Representation of Density-Power-Based Divergences Reducible to\n  M-Estimation"
                },
                "summary": "Density-power-based divergences are known to provide robust inference\nprocedures against outliers, and their extensions have been widely studied. A\ncharacteristic of successful divergences is that the estimation problem can be\nreduced to M-estimation. In this paper, we define a norm-based Bregman density\npower divergence (NB-DPD) -- density-power-based divergence with functional\nflexibility within the framework of Bregman divergences that can be reduced to\nM-estimation. We show that, by specifying the function $\\phi_\\gamma$, NB-DPD\nreduces to well-known divergences, such as the density power divergence and the\n$\\gamma$-divergence. Furthermore, by examining the combinations of functions\n$\\phi_\\gamma$ corresponding to existing divergences, we show that a new\ndivergence connecting these existing divergences can be derived. Finally, we\nshow that the redescending property, one of the key indicators of robustness,\nholds only for the $\\gamma$-divergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Density-power-based divergences are known to provide robust inference\nprocedures against outliers, and their extensions have been widely studied. A\ncharacteristic of successful divergences is that the estimation problem can be\nreduced to M-estimation. In this paper, we define a norm-based Bregman density\npower divergence (NB-DPD) -- density-power-based divergence with functional\nflexibility within the framework of Bregman divergences that can be reduced to\nM-estimation. We show that, by specifying the function $\\phi_\\gamma$, NB-DPD\nreduces to well-known divergences, such as the density power divergence and the\n$\\gamma$-divergence. Furthermore, by examining the combinations of functions\n$\\phi_\\gamma$ corresponding to existing divergences, we show that a new\ndivergence connecting these existing divergences can be derived. Finally, we\nshow that the redescending property, one of the key indicators of robustness,\nholds only for the $\\gamma$-divergence."
                },
                "authors": [
                    {
                        "name": "Masahiro Kobayashi"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Kobayashi"
                },
                "author": "Masahiro Kobayashi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00958v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00958v3",
                "updated": "2025-01-27T18:17:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    17,
                    26,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-01T21:29:37Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    29,
                    37,
                    2,
                    1,
                    0
                ],
                "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining"
                },
                "summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook."
                },
                "authors": [
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jiashuo Sun"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00958v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00958v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16277v1",
                "updated": "2025-01-27T18:11:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    11,
                    6,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:11:06Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    11,
                    6,
                    0,
                    27,
                    0
                ],
                "title": "Do LLMs Have Visualization Literacy? An Evaluation on Modified\n  Visualizations to Test Generalization in Data Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Have Visualization Literacy? An Evaluation on Modified\n  Visualizations to Test Generalization in Data Interpretation"
                },
                "summary": "In this paper, we assess the visualization literacy of two prominent Large\nLanguage Models (LLMs): OpenAI's Generative Pretrained Transformers (GPT), the\nbackend of ChatGPT, and Google's Gemini, previously known as Bard, to establish\nbenchmarks for assessing their visualization capabilities. While LLMs have\nshown promise in generating chart descriptions, captions, and design\nsuggestions, their potential for evaluating visualizations remains\nunder-explored. Collecting data from humans for evaluations has been a\nbottleneck for visualization research in terms of both time and money, and if\nLLMs were able to serve, even in some limited role, as evaluators, they could\nbe a significant resource. To investigate the feasibility of using LLMs in the\nvisualization evaluation process, we explore the extent to which LLMs possess\nvisualization literacy -- a crucial factor for their effective utility in the\nfield. We conducted a series of experiments using a modified 53-item\nVisualization Literacy Assessment Test (VLAT) for GPT-4 and Gemini. Our\nfindings indicate that the LLMs we explored currently fail to achieve the same\nlevels of visualization literacy when compared to data from the general public\nreported in VLAT, and LLMs heavily relied on their pre-existing knowledge to\nanswer questions instead of utilizing the information provided by the\nvisualization when answering questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we assess the visualization literacy of two prominent Large\nLanguage Models (LLMs): OpenAI's Generative Pretrained Transformers (GPT), the\nbackend of ChatGPT, and Google's Gemini, previously known as Bard, to establish\nbenchmarks for assessing their visualization capabilities. While LLMs have\nshown promise in generating chart descriptions, captions, and design\nsuggestions, their potential for evaluating visualizations remains\nunder-explored. Collecting data from humans for evaluations has been a\nbottleneck for visualization research in terms of both time and money, and if\nLLMs were able to serve, even in some limited role, as evaluators, they could\nbe a significant resource. To investigate the feasibility of using LLMs in the\nvisualization evaluation process, we explore the extent to which LLMs possess\nvisualization literacy -- a crucial factor for their effective utility in the\nfield. We conducted a series of experiments using a modified 53-item\nVisualization Literacy Assessment Test (VLAT) for GPT-4 and Gemini. Our\nfindings indicate that the LLMs we explored currently fail to achieve the same\nlevels of visualization literacy when compared to data from the general public\nreported in VLAT, and LLMs heavily relied on their pre-existing knowledge to\nanswer questions instead of utilizing the information provided by the\nvisualization when answering questions."
                },
                "authors": [
                    {
                        "name": "Jiayi Hong"
                    },
                    {
                        "name": "Christian Seto"
                    },
                    {
                        "name": "Arlen Fan"
                    },
                    {
                        "name": "Ross Maciejewski"
                    }
                ],
                "author_detail": {
                    "name": "Ross Maciejewski"
                },
                "author": "Ross Maciejewski",
                "arxiv_comment": "This is the author's version of the article that has been accepted in\n  IEEE Transactions on Visualization and Computer Graphics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16276v1",
                "updated": "2025-01-27T18:10:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    10,
                    34,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:10:34Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    10,
                    34,
                    0,
                    27,
                    0
                ],
                "title": "URAG: Implementing a Unified Hybrid RAG for Precise Answers in\n  University Admission Chatbots -- A Case Study at HCMUT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URAG: Implementing a Unified Hybrid RAG for Precise Answers in\n  University Admission Chatbots -- A Case Study at HCMUT"
                },
                "summary": "With the rapid advancement of Artificial Intelligence, particularly in\nNatural Language Processing, Large Language Models (LLMs) have become pivotal\nin educational question-answering systems, especially university admission\nchatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other\nadvanced techniques have been developed to enhance these systems by integrating\nspecific university data, enabling LLMs to provide informed responses on\nadmissions and academic counseling. However, these enhanced RAG techniques\noften involve high operational costs and require the training of complex,\nspecialized modules, which poses challenges for practical deployment.\nAdditionally, in the educational context, it is crucial to provide accurate\nanswers to prevent misinformation, a task that LLM-based systems find\nchallenging without appropriate strategies and methods. In this paper, we\nintroduce the Unified RAG (URAG) Framework, a hybrid approach that\nsignificantly improves the accuracy of responses, particularly for critical\nqueries. Experimental results demonstrate that URAG enhances our in-house,\nlightweight model to perform comparably to state-of-the-art commercial models.\nMoreover, to validate its practical applicability, we conducted a case study at\nour educational institution, which received positive feedback and acclaim. This\nstudy not only proves the effectiveness of URAG but also highlights its\nfeasibility for real-world implementation in educational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Artificial Intelligence, particularly in\nNatural Language Processing, Large Language Models (LLMs) have become pivotal\nin educational question-answering systems, especially university admission\nchatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other\nadvanced techniques have been developed to enhance these systems by integrating\nspecific university data, enabling LLMs to provide informed responses on\nadmissions and academic counseling. However, these enhanced RAG techniques\noften involve high operational costs and require the training of complex,\nspecialized modules, which poses challenges for practical deployment.\nAdditionally, in the educational context, it is crucial to provide accurate\nanswers to prevent misinformation, a task that LLM-based systems find\nchallenging without appropriate strategies and methods. In this paper, we\nintroduce the Unified RAG (URAG) Framework, a hybrid approach that\nsignificantly improves the accuracy of responses, particularly for critical\nqueries. Experimental results demonstrate that URAG enhances our in-house,\nlightweight model to perform comparably to state-of-the-art commercial models.\nMoreover, to validate its practical applicability, we conducted a case study at\nour educational institution, which received positive feedback and acclaim. This\nstudy not only proves the effectiveness of URAG but also highlights its\nfeasibility for real-world implementation in educational settings."
                },
                "authors": [
                    {
                        "name": "Long Nguyen"
                    },
                    {
                        "name": "Tho Quan"
                    }
                ],
                "author_detail": {
                    "name": "Tho Quan"
                },
                "author": "Tho Quan",
                "arxiv_comment": "Under review at SoICT'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16273v1",
                "updated": "2025-01-27T18:06:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    6,
                    36,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:06:36Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    6,
                    36,
                    0,
                    27,
                    0
                ],
                "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs"
                },
                "summary": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount."
                },
                "authors": [
                    {
                        "name": "Mohamed Elfeki"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Chad Voegele"
                    }
                ],
                "author_detail": {
                    "name": "Chad Voegele"
                },
                "author": "Chad Voegele",
                "arxiv_comment": "13 pages, 5 figures. LLMs/SLMs, encoder-decoder and decoder-only",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16255v1",
                "updated": "2025-01-27T17:55:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    55,
                    37,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:55:37Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    55,
                    37,
                    0,
                    27,
                    0
                ],
                "title": "A foundation model for human-AI collaboration in medical literature\n  mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A foundation model for human-AI collaboration in medical literature\n  mining"
                },
                "summary": "Systematic literature review is essential for evidence-based medicine,\nrequiring comprehensive analysis of clinical trial publications. However, the\napplication of artificial intelligence (AI) models for medical literature\nmining has been limited by insufficient training and evaluation across broad\ntherapeutic areas and diverse tasks. Here, we present LEADS, an AI foundation\nmodel for study search, screening, and data extraction from medical literature.\nThe model is trained on 633,759 instruction data points in LEADSInstruct,\ncurated from 21,335 systematic reviews, 453,625 clinical trial publications,\nand 27,015 clinical trial registries. We showed that LEADS demonstrates\nconsistent improvements over four cutting-edge generic large language models\n(LLMs) on six tasks. Furthermore, LEADS enhances expert workflows by providing\nsupportive references following expert requests, streamlining processes while\nmaintaining high-quality results. A study with 16 clinicians and medical\nresearchers from 14 different institutions revealed that experts collaborating\nwith LEADS achieved a recall of 0.81 compared to 0.77 experts working alone in\nstudy selection, with a time savings of 22.6%. In data extraction tasks,\nexperts using LEADS achieved an accuracy of 0.85 versus 0.80 without using\nLEADS, alongside a 26.9% time savings. These findings highlight the potential\nof specialized medical literature foundation models to outperform generic\nmodels, delivering significant quality and efficiency benefits when integrated\ninto expert workflows for medical literature mining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic literature review is essential for evidence-based medicine,\nrequiring comprehensive analysis of clinical trial publications. However, the\napplication of artificial intelligence (AI) models for medical literature\nmining has been limited by insufficient training and evaluation across broad\ntherapeutic areas and diverse tasks. Here, we present LEADS, an AI foundation\nmodel for study search, screening, and data extraction from medical literature.\nThe model is trained on 633,759 instruction data points in LEADSInstruct,\ncurated from 21,335 systematic reviews, 453,625 clinical trial publications,\nand 27,015 clinical trial registries. We showed that LEADS demonstrates\nconsistent improvements over four cutting-edge generic large language models\n(LLMs) on six tasks. Furthermore, LEADS enhances expert workflows by providing\nsupportive references following expert requests, streamlining processes while\nmaintaining high-quality results. A study with 16 clinicians and medical\nresearchers from 14 different institutions revealed that experts collaborating\nwith LEADS achieved a recall of 0.81 compared to 0.77 experts working alone in\nstudy selection, with a time savings of 22.6%. In data extraction tasks,\nexperts using LEADS achieved an accuracy of 0.85 versus 0.80 without using\nLEADS, alongside a 26.9% time savings. These findings highlight the potential\nof specialized medical literature foundation models to outperform generic\nmodels, delivering significant quality and efficiency benefits when integrated\ninto expert workflows for medical literature mining."
                },
                "authors": [
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Joey Chan"
                    },
                    {
                        "name": "Nicholas Wan"
                    },
                    {
                        "name": "Behdad Afzali"
                    },
                    {
                        "name": "Hyun-Jin Cho"
                    },
                    {
                        "name": "Chang-In Choi"
                    },
                    {
                        "name": "Mehdi Emamverdi"
                    },
                    {
                        "name": "Manjot K. Gill"
                    },
                    {
                        "name": "Sun-Hyung Kim"
                    },
                    {
                        "name": "Yijia Li"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Hanley Ong"
                    },
                    {
                        "name": "Justin Rousseau"
                    },
                    {
                        "name": "Irfan Sheikh"
                    },
                    {
                        "name": "Jenny J. Wei"
                    },
                    {
                        "name": "Ziyang Xu"
                    },
                    {
                        "name": "Christopher M. Zallek"
                    },
                    {
                        "name": "Kyungsang Kim"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16254v1",
                "updated": "2025-01-27T17:54:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    54,
                    31,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:54:31Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    54,
                    31,
                    0,
                    27,
                    0
                ],
                "title": "Multi-Agent Geospatial Copilots for Remote Sensing Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Geospatial Copilots for Remote Sensing Workflows"
                },
                "summary": "We present GeoLLM-Squad, a geospatial Copilot that introduces the novel\nmulti-agent paradigm to remote sensing (RS) workflows. Unlike existing\nsingle-agent approaches that rely on monolithic large language models (LLM),\nGeoLLM-Squad separates agentic orchestration from geospatial task-solving, by\ndelegating RS tasks to specialized sub-agents. Built on the open-source AutoGen\nand GeoLLM-Engine frameworks, our work enables the modular integration of\ndiverse applications, spanning urban monitoring, forestry protection, climate\nanalysis, and agriculture studies. Our results demonstrate that while\nsingle-agent systems struggle to scale with increasing RS task complexity,\nGeoLLM-Squad maintains robust performance, achieving a 17% improvement in\nagentic correctness over state-of-the-art baselines. Our findings highlight the\npotential of multi-agent AI in advancing RS workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GeoLLM-Squad, a geospatial Copilot that introduces the novel\nmulti-agent paradigm to remote sensing (RS) workflows. Unlike existing\nsingle-agent approaches that rely on monolithic large language models (LLM),\nGeoLLM-Squad separates agentic orchestration from geospatial task-solving, by\ndelegating RS tasks to specialized sub-agents. Built on the open-source AutoGen\nand GeoLLM-Engine frameworks, our work enables the modular integration of\ndiverse applications, spanning urban monitoring, forestry protection, climate\nanalysis, and agriculture studies. Our results demonstrate that while\nsingle-agent systems struggle to scale with increasing RS task complexity,\nGeoLLM-Squad maintains robust performance, achieving a 17% improvement in\nagentic correctness over state-of-the-art baselines. Our findings highlight the\npotential of multi-agent AI in advancing RS workflows."
                },
                "authors": [
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Varatheepan Paramanayakam"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Heming Liao"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Ruopu Li"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16247v1",
                "updated": "2025-01-27T17:48:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    48,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:48:48Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    48,
                    48,
                    0,
                    27,
                    0
                ],
                "title": "Zero-Shot Decision Tree Construction via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Decision Tree Construction via Large Language Models"
                },
                "summary": "This paper introduces a novel algorithm for constructing decision trees using\nlarge language models (LLMs) in a zero-shot manner based on Classification and\nRegression Trees (CART) principles. Traditional decision tree induction methods\nrely heavily on labeled data to recursively partition data using criteria such\nas information gain or the Gini index. In contrast, we propose a method that\nuses the pre-trained knowledge embedded in LLMs to build decision trees without\nrequiring training data. Our approach leverages LLMs to perform operations\nessential for decision tree construction, including attribute discretization,\nprobability calculation, and Gini index computation based on the probabilities.\nWe show that these zero-shot decision trees can outperform baseline zero-shot\nmethods and achieve competitive performance compared to supervised data-driven\ndecision trees on tabular datasets. The decision trees constructed via this\nmethod provide transparent and interpretable models, addressing data scarcity\nwhile preserving interpretability. This work establishes a new baseline in\nlow-data machine learning, offering a principled, knowledge-driven alternative\nto data-driven tree construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel algorithm for constructing decision trees using\nlarge language models (LLMs) in a zero-shot manner based on Classification and\nRegression Trees (CART) principles. Traditional decision tree induction methods\nrely heavily on labeled data to recursively partition data using criteria such\nas information gain or the Gini index. In contrast, we propose a method that\nuses the pre-trained knowledge embedded in LLMs to build decision trees without\nrequiring training data. Our approach leverages LLMs to perform operations\nessential for decision tree construction, including attribute discretization,\nprobability calculation, and Gini index computation based on the probabilities.\nWe show that these zero-shot decision trees can outperform baseline zero-shot\nmethods and achieve competitive performance compared to supervised data-driven\ndecision trees on tabular datasets. The decision trees constructed via this\nmethod provide transparent and interpretable models, addressing data scarcity\nwhile preserving interpretability. This work establishes a new baseline in\nlow-data machine learning, offering a principled, knowledge-driven alternative\nto data-driven tree construction."
                },
                "authors": [
                    {
                        "name": "Lucas Carrasco"
                    },
                    {
                        "name": "Felipe Urrutia"
                    },
                    {
                        "name": "Andrés Abeliuk"
                    }
                ],
                "author_detail": {
                    "name": "Andrés Abeliuk"
                },
                "author": "Andrés Abeliuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16246v1",
                "updated": "2025-01-27T17:43:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    43,
                    51,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:43:51Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    43,
                    51,
                    0,
                    27,
                    0
                ],
                "title": "CLISC: Bridging clip and sam by enhanced cam for unsupervised brain\n  tumor segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLISC: Bridging clip and sam by enhanced cam for unsupervised brain\n  tumor segmentation"
                },
                "summary": "Brain tumor segmentation is important for diagnosis of the tumor, and current\ndeep-learning methods rely on a large set of annotated images for training,\nwith high annotation costs. Unsupervised segmentation is promising to avoid\nhuman annotations while the performance is often limited. In this study, we\npresent a novel unsupervised segmentation approach that leverages the\ncapabilities of foundation models, and it consists of three main steps: (1) A\nvision-language model (i.e., CLIP) is employed to obtain image-level\npseudo-labels for training a classification network. Class Activation Mapping\n(CAM) is then employed to extract Regions of Interest (ROIs), where an adaptive\nmasking-based data augmentation is used to enhance ROI identification.(2) The\nROIs are used to generate bounding box and point prompts for the Segment\nAnything Model (SAM) to obtain segmentation pseudo-labels. (3) A 3D\nsegmentation network is trained with the SAM-derived pseudo-labels, where\nlow-quality pseudo-labels are filtered out in a self-learning process based on\nthe similarity between the SAM's output and the network's prediction.\nEvaluation on the BraTS2020 dataset demonstrates that our approach obtained an\naverage Dice Similarity Score (DSC) of 85.60%, outperforming five\nstate-of-the-art unsupervised segmentation methods by more than 10 percentage\npoints. Besides, our approach outperforms directly using SAM for zero-shot\ninference, and its performance is close to fully supervised learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain tumor segmentation is important for diagnosis of the tumor, and current\ndeep-learning methods rely on a large set of annotated images for training,\nwith high annotation costs. Unsupervised segmentation is promising to avoid\nhuman annotations while the performance is often limited. In this study, we\npresent a novel unsupervised segmentation approach that leverages the\ncapabilities of foundation models, and it consists of three main steps: (1) A\nvision-language model (i.e., CLIP) is employed to obtain image-level\npseudo-labels for training a classification network. Class Activation Mapping\n(CAM) is then employed to extract Regions of Interest (ROIs), where an adaptive\nmasking-based data augmentation is used to enhance ROI identification.(2) The\nROIs are used to generate bounding box and point prompts for the Segment\nAnything Model (SAM) to obtain segmentation pseudo-labels. (3) A 3D\nsegmentation network is trained with the SAM-derived pseudo-labels, where\nlow-quality pseudo-labels are filtered out in a self-learning process based on\nthe similarity between the SAM's output and the network's prediction.\nEvaluation on the BraTS2020 dataset demonstrates that our approach obtained an\naverage Dice Similarity Score (DSC) of 85.60%, outperforming five\nstate-of-the-art unsupervised segmentation methods by more than 10 percentage\npoints. Besides, our approach outperforms directly using SAM for zero-shot\ninference, and its performance is close to fully supervised learning."
                },
                "authors": [
                    {
                        "name": "Xiaochuan Ma"
                    },
                    {
                        "name": "Jia Fu"
                    },
                    {
                        "name": "Wenjun Liao"
                    },
                    {
                        "name": "Shichuan Zhang"
                    },
                    {
                        "name": "Guotai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guotai Wang"
                },
                "author": "Guotai Wang",
                "arxiv_comment": "22st IEEE International Symposium on Biomedical Imaging (ISBI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16241v1",
                "updated": "2025-01-27T17:36:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    36,
                    6,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:36:06Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    36,
                    6,
                    0,
                    27,
                    0
                ],
                "title": "Phase Transitions in Large Language Models and the $O(N)$ Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phase Transitions in Large Language Models and the $O(N)$ Model"
                },
                "summary": "Large language models (LLMs) exhibit unprecedentedly rich scaling behaviors.\nIn physics, scaling behavior is closely related to phase transitions, critical\nphenomena, and field theory. To investigate the phase transition phenomena in\nLLMs, we reformulated the Transformer architecture as an $O(N)$ model. Our\nstudy reveals two distinct phase transitions corresponding to the temperature\nused in text generation and the model's parameter size, respectively. The first\nphase transition enables us to estimate the internal dimension of the model,\nwhile the second phase transition is of \\textit{higher-depth} and signals the\nemergence of new capabilities. As an application, the energy of the $O(N)$\nmodel can be used to evaluate whether an LLM's parameters are sufficient to\nlearn the training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit unprecedentedly rich scaling behaviors.\nIn physics, scaling behavior is closely related to phase transitions, critical\nphenomena, and field theory. To investigate the phase transition phenomena in\nLLMs, we reformulated the Transformer architecture as an $O(N)$ model. Our\nstudy reveals two distinct phase transitions corresponding to the temperature\nused in text generation and the model's parameter size, respectively. The first\nphase transition enables us to estimate the internal dimension of the model,\nwhile the second phase transition is of \\textit{higher-depth} and signals the\nemergence of new capabilities. As an application, the energy of the $O(N)$\nmodel can be used to evaluate whether an LLM's parameters are sufficient to\nlearn the training data."
                },
                "authors": [
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Babak Haghighat"
                    }
                ],
                "author_detail": {
                    "name": "Babak Haghighat"
                },
                "author": "Babak Haghighat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16239v1",
                "updated": "2025-01-27T17:35:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    35,
                    39,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:35:39Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    35,
                    39,
                    0,
                    27,
                    0
                ],
                "title": "Distilling foundation models for robust and efficient models in digital\n  pathology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling foundation models for robust and efficient models in digital\n  pathology"
                },
                "summary": "In recent years, the advent of foundation models (FM) for digital pathology\nhas relied heavily on scaling the pre-training datasets and the model size,\nyielding large and powerful models. While it resulted in improving the\nperformance on diverse downstream tasks, it also introduced increased\ncomputational cost and inference time. In this work, we explore the\ndistillation of a large foundation model into a smaller one, reducing the\nnumber of parameters by several orders of magnitude. Leveraging distillation\ntechniques, our distilled model, H0-mini, achieves nearly comparable\nperformance to large FMs at a significantly reduced inference cost. It is\nevaluated on several public benchmarks, achieving 3rd place on the HEST\nbenchmark and 5th place on the EVA benchmark. Additionally, a robustness\nanalysis conducted on the PLISM dataset demonstrates that our distilled model\nreaches excellent robustness to variations in staining and scanning conditions,\nsignificantly outperforming other state-of-the art models. This opens new\nperspectives to design lightweight and robust models for digital pathology,\nwithout compromising on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advent of foundation models (FM) for digital pathology\nhas relied heavily on scaling the pre-training datasets and the model size,\nyielding large and powerful models. While it resulted in improving the\nperformance on diverse downstream tasks, it also introduced increased\ncomputational cost and inference time. In this work, we explore the\ndistillation of a large foundation model into a smaller one, reducing the\nnumber of parameters by several orders of magnitude. Leveraging distillation\ntechniques, our distilled model, H0-mini, achieves nearly comparable\nperformance to large FMs at a significantly reduced inference cost. It is\nevaluated on several public benchmarks, achieving 3rd place on the HEST\nbenchmark and 5th place on the EVA benchmark. Additionally, a robustness\nanalysis conducted on the PLISM dataset demonstrates that our distilled model\nreaches excellent robustness to variations in staining and scanning conditions,\nsignificantly outperforming other state-of-the art models. This opens new\nperspectives to design lightweight and robust models for digital pathology,\nwithout compromising on performance."
                },
                "authors": [
                    {
                        "name": "Alexandre Filiot"
                    },
                    {
                        "name": "Nicolas Dop"
                    },
                    {
                        "name": "Oussama Tchita"
                    },
                    {
                        "name": "Auriane Riou"
                    },
                    {
                        "name": "Thomas Peeters"
                    },
                    {
                        "name": "Daria Valter"
                    },
                    {
                        "name": "Marin Scalbert"
                    },
                    {
                        "name": "Charlie Saillard"
                    },
                    {
                        "name": "Geneviève Robin"
                    },
                    {
                        "name": "Antoine Olivier"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Olivier"
                },
                "author": "Antoine Olivier",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16237v1",
                "updated": "2025-01-27T17:34:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    34,
                    4,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:34:04Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    34,
                    4,
                    0,
                    27,
                    0
                ],
                "title": "Application of Structured State Space Models to High energy physics with\n  locality-sensitive hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application of Structured State Space Models to High energy physics with\n  locality-sensitive hashing"
                },
                "summary": "Modern high-energy physics (HEP) experiments are increasingly challenged by\nthe vast size and complexity of their datasets, particularly regarding\nlarge-scale point cloud processing and long sequences. In this study, to\naddress these challenges, we explore the application of structured state space\nmodels (SSMs), proposing one of the first trials to integrate local-sensitive\nhashing into either a hybrid or pure Mamba Model. Our results demonstrate that\npure SSMs could serve as powerful backbones for HEP problems involving tasks\nfor long sequence data with local inductive bias. By integrating\nlocality-sensitive hashing into Mamba blocks, we achieve significant\nimprovements over traditional backbones in key HEP tasks, surpassing them in\ninference speed and physics metrics while reducing computational overhead. In\nkey tests, our approach demonstrated promising results, presenting a viable\nalternative to traditional transformer backbones by significantly reducing\nFLOPS while maintaining robust performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern high-energy physics (HEP) experiments are increasingly challenged by\nthe vast size and complexity of their datasets, particularly regarding\nlarge-scale point cloud processing and long sequences. In this study, to\naddress these challenges, we explore the application of structured state space\nmodels (SSMs), proposing one of the first trials to integrate local-sensitive\nhashing into either a hybrid or pure Mamba Model. Our results demonstrate that\npure SSMs could serve as powerful backbones for HEP problems involving tasks\nfor long sequence data with local inductive bias. By integrating\nlocality-sensitive hashing into Mamba blocks, we achieve significant\nimprovements over traditional backbones in key HEP tasks, surpassing them in\ninference speed and physics metrics while reducing computational overhead. In\nkey tests, our approach demonstrated promising results, presenting a viable\nalternative to traditional transformer backbones by significantly reducing\nFLOPS while maintaining robust performance."
                },
                "authors": [
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Sitian Qian"
                    }
                ],
                "author_detail": {
                    "name": "Sitian Qian"
                },
                "author": "Sitian Qian",
                "arxiv_comment": "6 figures, accepted by AISTATS 2025 as poster, camera ready versions\n  to be updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16224v1",
                "updated": "2025-01-27T17:20:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    20,
                    4,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:20:04Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    20,
                    4,
                    0,
                    27,
                    0
                ],
                "title": "Language-Based Bayesian Optimization Research Assistant (BORA)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Based Bayesian Optimization Research Assistant (BORA)"
                },
                "summary": "Many important scientific problems involve multivariate optimization coupled\nwith slow and laborious experimental measurements. These complex,\nhigh-dimensional searches can be defined by non-convex optimization landscapes\nthat resemble needle-in-a-haystack surfaces, leading to entrapment in local\nminima. Contextualizing optimizers with human domain knowledge is a powerful\napproach to guide searches to localized fruitful regions. However, this\napproach is susceptible to human confirmation bias and it is also challenging\nfor domain experts to keep track of the rapidly expanding scientific\nliterature. Here, we propose the use of Large Language Models (LLMs) for\ncontextualizing Bayesian optimization (BO) via a hybrid optimization framework\nthat intelligently and economically blends stochastic inference with domain\nknowledge-based insights from the LLM, which is used to suggest new,\nbetter-performing areas of the search space for exploration. Our method fosters\nuser engagement by offering real-time commentary on the optimization progress,\nexplaining the reasoning behind the search strategies. We validate the\neffectiveness of our approach on synthetic benchmarks with up to 15 independent\nvariables and demonstrate the ability of LLMs to reason in four real-world\nexperimental tasks where context-aware suggestions boost optimization\nperformance substantially.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many important scientific problems involve multivariate optimization coupled\nwith slow and laborious experimental measurements. These complex,\nhigh-dimensional searches can be defined by non-convex optimization landscapes\nthat resemble needle-in-a-haystack surfaces, leading to entrapment in local\nminima. Contextualizing optimizers with human domain knowledge is a powerful\napproach to guide searches to localized fruitful regions. However, this\napproach is susceptible to human confirmation bias and it is also challenging\nfor domain experts to keep track of the rapidly expanding scientific\nliterature. Here, we propose the use of Large Language Models (LLMs) for\ncontextualizing Bayesian optimization (BO) via a hybrid optimization framework\nthat intelligently and economically blends stochastic inference with domain\nknowledge-based insights from the LLM, which is used to suggest new,\nbetter-performing areas of the search space for exploration. Our method fosters\nuser engagement by offering real-time commentary on the optimization progress,\nexplaining the reasoning behind the search strategies. We validate the\neffectiveness of our approach on synthetic benchmarks with up to 15 independent\nvariables and demonstrate the ability of LLMs to reason in four real-world\nexperimental tasks where context-aware suggestions boost optimization\nperformance substantially."
                },
                "authors": [
                    {
                        "name": "Abdoulatif Cissé"
                    },
                    {
                        "name": "Xenophon Evangelopoulos"
                    },
                    {
                        "name": "Vladimir V. Gusev"
                    },
                    {
                        "name": "Andrew I. Cooper"
                    }
                ],
                "author_detail": {
                    "name": "Andrew I. Cooper"
                },
                "author": "Andrew I. Cooper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17737v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17737v5",
                "updated": "2025-01-27T17:14:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    14,
                    45,
                    0,
                    27,
                    0
                ],
                "published": "2024-12-23T17:36:51Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    17,
                    36,
                    51,
                    0,
                    358,
                    0
                ],
                "title": "Contextual Feedback Loops: Amplifying Deep Reasoning with Iterative\n  Top-Down Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Feedback Loops: Amplifying Deep Reasoning with Iterative\n  Top-Down Feedback"
                },
                "summary": "Conventional deep networks rely on one-way backpropagation that overlooks\nreconciling high-level predictions with lower-level representations. We propose\n\\emph{Contextual Feedback Loops} (CFLs), a lightweight mechanism that\nre-injects top-down context into earlier layers for iterative refinement.\nConcretely, CFLs map the network's prediction to a compact \\emph{context\nvector}, which is fused back into each layer via gating adapters. Unrolled over\nmultiple feedback steps, CFLs unify feed-forward and feedback-driven inference,\nletting top-level outputs continually refine lower-level features. Despite\nminimal overhead, CFLs yield consistent gains on tasks including CIFAR-10,\nImageNet-1k, SpeechCommands, and GLUE SST-2. Moreover, by a Banach Fixed Point\nargument under mild Lipschitz conditions, these updates converge stably.\nOverall, CFLs show that even modest top-down feedback can substantially improve\ndeep models, aligning with cognitive theories of iterative perception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional deep networks rely on one-way backpropagation that overlooks\nreconciling high-level predictions with lower-level representations. We propose\n\\emph{Contextual Feedback Loops} (CFLs), a lightweight mechanism that\nre-injects top-down context into earlier layers for iterative refinement.\nConcretely, CFLs map the network's prediction to a compact \\emph{context\nvector}, which is fused back into each layer via gating adapters. Unrolled over\nmultiple feedback steps, CFLs unify feed-forward and feedback-driven inference,\nletting top-level outputs continually refine lower-level features. Despite\nminimal overhead, CFLs yield consistent gains on tasks including CIFAR-10,\nImageNet-1k, SpeechCommands, and GLUE SST-2. Moreover, by a Banach Fixed Point\nargument under mild Lipschitz conditions, these updates converge stably.\nOverall, CFLs show that even modest top-down feedback can substantially improve\ndeep models, aligning with cognitive theories of iterative perception."
                },
                "authors": [
                    {
                        "name": "Jacob Fein-Ashley"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17737v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17737v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16223v1",
                "updated": "2025-01-27T17:14:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    14,
                    35,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:14:35Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    14,
                    35,
                    0,
                    27,
                    0
                ],
                "title": "Statistical Inference for Low-Rank Tensor Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Low-Rank Tensor Models"
                },
                "summary": "Statistical inference for tensors has emerged as a critical challenge in\nanalyzing high-dimensional data in modern data science. This paper introduces a\nunified framework for inferring general and low-Tucker-rank linear functionals\nof low-Tucker-rank signal tensors for several low-rank tensor models. Our\nmethodology tackles two primary goals: achieving asymptotic normality and\nconstructing minimax-optimal confidence intervals. By leveraging a debiasing\nstrategy and projecting onto the tangent space of the low-Tucker-rank manifold,\nwe enable inference for general and structured linear functionals, extending\nfar beyond the scope of traditional entrywise inference. Specifically, in the\nlow-Tucker-rank tensor regression or PCA model, we establish the computational\nand statistical efficiency of our approach, achieving near-optimal sample size\nrequirements (in regression model) and signal-to-noise ratio (SNR) conditions\n(in PCA model) for general linear functionals without requiring sparsity in the\nloading tensor. Our framework also attains both computationally and\nstatistically optimal sample size and SNR thresholds for low-Tucker-rank linear\nfunctionals. Numerical experiments validate our theoretical results, showcasing\nthe framework's utility in diverse applications. This work addresses\nsignificant methodological gaps in statistical inference, advancing tensor\nanalysis for complex and high-dimensional data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference for tensors has emerged as a critical challenge in\nanalyzing high-dimensional data in modern data science. This paper introduces a\nunified framework for inferring general and low-Tucker-rank linear functionals\nof low-Tucker-rank signal tensors for several low-rank tensor models. Our\nmethodology tackles two primary goals: achieving asymptotic normality and\nconstructing minimax-optimal confidence intervals. By leveraging a debiasing\nstrategy and projecting onto the tangent space of the low-Tucker-rank manifold,\nwe enable inference for general and structured linear functionals, extending\nfar beyond the scope of traditional entrywise inference. Specifically, in the\nlow-Tucker-rank tensor regression or PCA model, we establish the computational\nand statistical efficiency of our approach, achieving near-optimal sample size\nrequirements (in regression model) and signal-to-noise ratio (SNR) conditions\n(in PCA model) for general linear functionals without requiring sparsity in the\nloading tensor. Our framework also attains both computationally and\nstatistically optimal sample size and SNR thresholds for low-Tucker-rank linear\nfunctionals. Numerical experiments validate our theoretical results, showcasing\nthe framework's utility in diverse applications. This work addresses\nsignificant methodological gaps in statistical inference, advancing tensor\nanalysis for complex and high-dimensional data environments."
                },
                "authors": [
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Elynn Chen"
                    },
                    {
                        "name": "Yuefeng Han"
                    }
                ],
                "author_detail": {
                    "name": "Yuefeng Han"
                },
                "author": "Yuefeng Han",
                "arxiv_comment": "118 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H10 (Primary) 62H25, 62F12 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16220v1",
                "updated": "2025-01-27T17:09:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    9,
                    47,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:09:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    9,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "DBRouting: Routing End User Queries to Databases for Answerability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBRouting: Routing End User Queries to Databases for Answerability"
                },
                "summary": "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task."
                },
                "authors": [
                    {
                        "name": "Priyangshu Mandal"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Mayur Patidar"
                    },
                    {
                        "name": "Lovekesh Vig"
                    }
                ],
                "author_detail": {
                    "name": "Lovekesh Vig"
                },
                "author": "Lovekesh Vig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16215v1",
                "updated": "2025-01-27T17:07:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    7,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:07:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    7,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "Enhancing Visual Inspection Capability of Multi-Modal Large Language\n  Models on Medical Time Series with Supportive Conformalized and Interpretable\n  Small Specialized Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Visual Inspection Capability of Multi-Modal Large Language\n  Models on Medical Time Series with Supportive Conformalized and Interpretable\n  Small Specialized Models"
                },
                "summary": "Large language models (LLMs) exhibit remarkable capabilities in visual\ninspection of medical time-series data, achieving proficiency comparable to\nhuman clinicians. However, their broad scope limits domain-specific precision,\nand proprietary weights hinder fine-tuning for specialized datasets. In\ncontrast, small specialized models (SSMs) excel in targeted tasks but lack the\ncontextual reasoning required for complex clinical decision-making. To address\nthese challenges, we propose ConMIL (Conformalized Multiple Instance Learning),\na decision-support SSM that integrates seamlessly with LLMs. By using Multiple\nInstance Learning (MIL) to identify clinically significant signal segments and\nconformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'\ninterpretative capabilities for medical time-series analysis. Experimental\nresults demonstrate that ConMIL significantly improves the performance of\nstate-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,\n\\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for\nconfident samples in arrhythmia detection and sleep staging, compared to\nstandalone LLM accuracy of 46.13% and 13.16%. These findings highlight the\npotential of ConMIL to bridge task-specific precision and broader contextual\nreasoning, enabling more reliable and interpretable AI-driven clinical decision\nsupport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable capabilities in visual\ninspection of medical time-series data, achieving proficiency comparable to\nhuman clinicians. However, their broad scope limits domain-specific precision,\nand proprietary weights hinder fine-tuning for specialized datasets. In\ncontrast, small specialized models (SSMs) excel in targeted tasks but lack the\ncontextual reasoning required for complex clinical decision-making. To address\nthese challenges, we propose ConMIL (Conformalized Multiple Instance Learning),\na decision-support SSM that integrates seamlessly with LLMs. By using Multiple\nInstance Learning (MIL) to identify clinically significant signal segments and\nconformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'\ninterpretative capabilities for medical time-series analysis. Experimental\nresults demonstrate that ConMIL significantly improves the performance of\nstate-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,\n\\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for\nconfident samples in arrhythmia detection and sleep staging, compared to\nstandalone LLM accuracy of 46.13% and 13.16%. These findings highlight the\npotential of ConMIL to bridge task-specific precision and broader contextual\nreasoning, enabling more reliable and interpretable AI-driven clinical decision\nsupport."
                },
                "authors": [
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "Xiwen Chen"
                    },
                    {
                        "name": "Ci Zhang"
                    },
                    {
                        "name": "Stuart F. Quan"
                    },
                    {
                        "name": "William D. S. Killgore"
                    },
                    {
                        "name": "Shu-Fen Wung"
                    },
                    {
                        "name": "Chen X. Chen"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Ao Li"
                    }
                ],
                "author_detail": {
                    "name": "Ao Li"
                },
                "author": "Ao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16214v1",
                "updated": "2025-01-27T17:06:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    6,
                    56,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:06:56Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    6,
                    56,
                    0,
                    27,
                    0
                ],
                "title": "Provence: efficient and robust context pruning for retrieval-augmented\n  generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provence: efficient and robust context pruning for retrieval-augmented\n  generation"
                },
                "summary": "Retrieval-augmented generation improves various aspects of large language\nmodels (LLMs) generation, but suffers from computational overhead caused by\nlong contexts as well as the propagation of irrelevant retrieved information\ninto generated responses. Context pruning deals with both aspects, by removing\nirrelevant parts of retrieved contexts before LLM generation. Existing context\npruning approaches are however limited, and do not provide a universal model\nthat would be both efficient and robust in a wide range of scenarios, e.g.,\nwhen contexts contain a variable amount of relevant information or vary in\nlength, or when evaluated on various domains. In this work, we close this gap\nand introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts),\nan efficient and robust context pruner for Question Answering, which\ndynamically detects the needed amount of pruning for a given context and can be\nused out-of-the-box for various domains. The three key ingredients of Provence\nare formulating the context pruning task as sequence labeling, unifying context\npruning capabilities with context reranking, and training on diverse data. Our\nexperimental results show that Provence enables context pruning with negligible\nto no drop in performance, in various domains and settings, at almost no cost\nin a standard RAG pipeline. We also conduct a deeper analysis alongside various\nablations to provide insights into training context pruners for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation improves various aspects of large language\nmodels (LLMs) generation, but suffers from computational overhead caused by\nlong contexts as well as the propagation of irrelevant retrieved information\ninto generated responses. Context pruning deals with both aspects, by removing\nirrelevant parts of retrieved contexts before LLM generation. Existing context\npruning approaches are however limited, and do not provide a universal model\nthat would be both efficient and robust in a wide range of scenarios, e.g.,\nwhen contexts contain a variable amount of relevant information or vary in\nlength, or when evaluated on various domains. In this work, we close this gap\nand introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts),\nan efficient and robust context pruner for Question Answering, which\ndynamically detects the needed amount of pruning for a given context and can be\nused out-of-the-box for various domains. The three key ingredients of Provence\nare formulating the context pruning task as sequence labeling, unifying context\npruning capabilities with context reranking, and training on diverse data. Our\nexperimental results show that Provence enables context pruning with negligible\nto no drop in performance, in various domains and settings, at almost no cost\nin a standard RAG pipeline. We also conduct a deeper analysis alongside various\nablations to provide insights into training context pruners for future work."
                },
                "authors": [
                    {
                        "name": "Nadezhda Chirkova"
                    },
                    {
                        "name": "Thibault Formal"
                    },
                    {
                        "name": "Vassilina Nikoulina"
                    },
                    {
                        "name": "Stéphane Clinchant"
                    }
                ],
                "author_detail": {
                    "name": "Stéphane Clinchant"
                },
                "author": "Stéphane Clinchant",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16882v2",
                "updated": "2025-01-27T17:06:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    6,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-22T10:36:15Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    10,
                    36,
                    15,
                    1,
                    296,
                    0
                ],
                "title": "Large Language Model-based Augmentation for Imbalanced Node\n  Classification on Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Augmentation for Imbalanced Node\n  Classification on Text-Attributed Graphs"
                },
                "summary": "Node classification on graphs often suffers from class imbalance, leading to\nbiased predictions and significant risks in real-world applications. While\ndata-centric solutions have been explored, they largely overlook\nText-Attributed Graphs (TAGs) and the potential of using rich textual semantics\nto improve the classification of minority nodes. Given this gap, we propose\nLarge Language Model-based Augmentation on Text-Attributed Graphs (LA-TAG), a\nnovel framework that leverages Large Language Models (LLMs) to handle\nimbalanced node classification. Specifically, we develop prompting strategies\ninspired by interpolation to synthesize textual node attributes. Additionally,\nto effectively integrate synthetic nodes into the graph structure, we introduce\na textual link predictor that connects the generated nodes to the original\ngraph, preserving structural and contextual information. Experiments across\nvarious datasets and evaluation metrics demonstrate that LA-TAG outperforms\nexisting textual augmentation and graph imbalance learning methods, emphasizing\nthe efficacy of our approach in addressing class imbalance in TAGs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Node classification on graphs often suffers from class imbalance, leading to\nbiased predictions and significant risks in real-world applications. While\ndata-centric solutions have been explored, they largely overlook\nText-Attributed Graphs (TAGs) and the potential of using rich textual semantics\nto improve the classification of minority nodes. Given this gap, we propose\nLarge Language Model-based Augmentation on Text-Attributed Graphs (LA-TAG), a\nnovel framework that leverages Large Language Models (LLMs) to handle\nimbalanced node classification. Specifically, we develop prompting strategies\ninspired by interpolation to synthesize textual node attributes. Additionally,\nto effectively integrate synthetic nodes into the graph structure, we introduce\na textual link predictor that connects the generated nodes to the original\ngraph, preserving structural and contextual information. Experiments across\nvarious datasets and evaluation metrics demonstrate that LA-TAG outperforms\nexisting textual augmentation and graph imbalance learning methods, emphasizing\nthe efficacy of our approach in addressing class imbalance in TAGs."
                },
                "authors": [
                    {
                        "name": "Leyao Wang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Bo Ni"
                    },
                    {
                        "name": "Yuying Zhao"
                    },
                    {
                        "name": "Tyler Derr"
                    }
                ],
                "author_detail": {
                    "name": "Tyler Derr"
                },
                "author": "Tyler Derr",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20791v2",
                "updated": "2025-01-27T17:05:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    5,
                    55,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-28T07:16:00Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    16,
                    0,
                    0,
                    302,
                    0
                ],
                "title": "From Cool Demos to Production-Ready FMware: Core Challenges and a\n  Technology Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cool Demos to Production-Ready FMware: Core Challenges and a\n  Technology Roadmap"
                },
                "summary": "The rapid expansion of foundation models (FMs), such as large language models\n(LLMs), has given rise to FMware--software systems that integrate FMs as core\ncomponents. While building demonstration-level FMware is relatively\nstraightforward, transitioning to production-ready systems presents numerous\nchallenges, including reliability, high implementation costs, scalability, and\ncompliance with privacy regulations. Our paper conducts a semi-structured\nthematic synthesis to identify the key challenges in productionizing FMware\nacross diverse data sources including our own industry experience in developing\nFMArts--a FMware lifecycle engineering platform and integrating it into Huawei\ncloud, grey literature, academic publications, hands-on involvement in the Open\nPlatform for Enterprise AI (OPEA), organizing the AIware conference and\nBootcamp, and co-leading the ISO SPDX SBOM working group on AI and datasets. We\nidentify critical issues in FM selection, data and model alignment, prompt\nengineering, agent orchestration, system testing, and deployment, alongside\ncross-cutting concerns such as memory management, observability, and feedback\nintegration. We discuss needed technologies and strategies to address these\nchallenges and offer guidance on how to enable the transition from\ndemonstration systems to scalable, production-ready FMware solutions. Our\nfindings underscore the importance of continued research and multi-industry\ncollaboration to advance the development of production-ready FMware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of foundation models (FMs), such as large language models\n(LLMs), has given rise to FMware--software systems that integrate FMs as core\ncomponents. While building demonstration-level FMware is relatively\nstraightforward, transitioning to production-ready systems presents numerous\nchallenges, including reliability, high implementation costs, scalability, and\ncompliance with privacy regulations. Our paper conducts a semi-structured\nthematic synthesis to identify the key challenges in productionizing FMware\nacross diverse data sources including our own industry experience in developing\nFMArts--a FMware lifecycle engineering platform and integrating it into Huawei\ncloud, grey literature, academic publications, hands-on involvement in the Open\nPlatform for Enterprise AI (OPEA), organizing the AIware conference and\nBootcamp, and co-leading the ISO SPDX SBOM working group on AI and datasets. We\nidentify critical issues in FM selection, data and model alignment, prompt\nengineering, agent orchestration, system testing, and deployment, alongside\ncross-cutting concerns such as memory management, observability, and feedback\nintegration. We discuss needed technologies and strategies to address these\nchallenges and offer guidance on how to enable the transition from\ndemonstration systems to scalable, production-ready FMware solutions. Our\nfindings underscore the importance of continued research and multi-industry\ncollaboration to advance the development of production-ready FMware."
                },
                "authors": [
                    {
                        "name": "Gopi Krishnan Rajbahadur"
                    },
                    {
                        "name": "Gustavo A. Oliva"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16207v1",
                "updated": "2025-01-27T17:00:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    0,
                    56,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:00:56Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    0,
                    56,
                    0,
                    27,
                    0
                ],
                "title": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs"
                },
                "summary": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO, showing significant progress. However, these studies\nintertwined multiple skills simultaneously, i.e., problem-solving, reasoning,\nand writing formal specifications, making it hard to precisely identify the\nLLMs' strengths and weaknesses in each task. This paper focuses on formal\nverification, an immediate application scenario of formal reasoning, and\ndecomposes it into six sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five mainstream formal specification\nlanguages (Coq, Lean4, Dafny, ACSL, and TLA+) in six\nformal-verification-related tasks by distilling GPT-4o. They are split into a\n14k+ fine-tuning dataset FM-alpaca and a 4k benchmark FM-Bench. We found that\nLLMs are good at writing proof segments when given either the code, or the\ndetailed description of proof steps. Also, the fine-tuning brought about a\nnearly threefold improvement at most. Interestingly, we observed that\nfine-tuning with formal data also enhances mathematics, reasoning, and coding\nabilities. We hope our findings inspire further research. Fine-tuned models are\nreleased to facilitate subsequent studies",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO, showing significant progress. However, these studies\nintertwined multiple skills simultaneously, i.e., problem-solving, reasoning,\nand writing formal specifications, making it hard to precisely identify the\nLLMs' strengths and weaknesses in each task. This paper focuses on formal\nverification, an immediate application scenario of formal reasoning, and\ndecomposes it into six sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five mainstream formal specification\nlanguages (Coq, Lean4, Dafny, ACSL, and TLA+) in six\nformal-verification-related tasks by distilling GPT-4o. They are split into a\n14k+ fine-tuning dataset FM-alpaca and a 4k benchmark FM-Bench. We found that\nLLMs are good at writing proof segments when given either the code, or the\ndetailed description of proof steps. Also, the fine-tuning brought about a\nnearly threefold improvement at most. Interestingly, we observed that\nfine-tuning with formal data also enhances mathematics, reasoning, and coding\nabilities. We hope our findings inspire further research. Fine-tuned models are\nreleased to facilitate subsequent studies"
                },
                "authors": [
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Meiziniu Li"
                    },
                    {
                        "name": "Haoyang Ma"
                    },
                    {
                        "name": "Haokun Li"
                    },
                    {
                        "name": "Mengda He"
                    },
                    {
                        "name": "Cheng Wen"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Shengchao Qin"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Cong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Cong Tian"
                },
                "author": "Cong Tian",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19442v3",
                "updated": "2025-01-27T16:55:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    55,
                    57,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T10:45:40Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    10,
                    45,
                    40,
                    1,
                    121,
                    0
                ],
                "title": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs"
                },
                "summary": "Nigeria is a multilingual country with 500+ languages. Naija is a\nNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed\nlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has\nmainly been a spoken language until recently, there are now various platforms\npublishing exclusively in Naija such as Naija Wikipedia. However, it is hard to\ndistinguish by non-native from a larger pidgin languages spoken across West\nAfrica known as West African Pidgin English (WAPE) -- which is more simplied\nand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news\nplatform publishes exclusively in WAPE to cater for several countries in West\nAfrica. In our paper, we show through statistical analyses and Machine\nTranslation experiments that these two creole varieties do not represent each\nother (i.e., there are linguistic differences in word order and vocabulary) and\nGenerative AI operates only based on WAPE. In other words, Naija is\nunder-represented in Generative AI, and it is hard to teach LLMs with few\nexamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nigeria is a multilingual country with 500+ languages. Naija is a\nNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed\nlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has\nmainly been a spoken language until recently, there are now various platforms\npublishing exclusively in Naija such as Naija Wikipedia. However, it is hard to\ndistinguish by non-native from a larger pidgin languages spoken across West\nAfrica known as West African Pidgin English (WAPE) -- which is more simplied\nand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news\nplatform publishes exclusively in WAPE to cater for several countries in West\nAfrica. In our paper, we show through statistical analyses and Machine\nTranslation experiments that these two creole varieties do not represent each\nother (i.e., there are linguistic differences in word order and vocabulary) and\nGenerative AI operates only based on WAPE. In other words, Naija is\nunder-represented in Generative AI, and it is hard to teach LLMs with few\nexamples."
                },
                "authors": [
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "A. Seza Doğruöz"
                    },
                    {
                        "name": "Iyanuoluwa Shode"
                    },
                    {
                        "name": "Anuoluwapo Aremu"
                    }
                ],
                "author_detail": {
                    "name": "Anuoluwapo Aremu"
                },
                "author": "Anuoluwapo Aremu",
                "arxiv_comment": "Accepted to NAACL 2025 (findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16201v1",
                "updated": "2025-01-27T16:55:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    55,
                    38,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T16:55:38Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    55,
                    38,
                    0,
                    27,
                    0
                ],
                "title": "Enhancing and Exploring Mild Cognitive Impairment Detection with\n  W2V-BERT-2.0",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing and Exploring Mild Cognitive Impairment Detection with\n  W2V-BERT-2.0"
                },
                "summary": "This study explores a multi-lingual audio self-supervised learning model for\ndetecting mild cognitive impairment (MCI) using the TAUKADIAL cross-lingual\ndataset. While speech transcription-based detection with BERT models is\neffective, limitations exist due to a lack of transcriptions and temporal\ninformation. To address these issues, the study utilizes features directly from\nspeech utterances with W2V-BERT-2.0. We propose a visualization method to\ndetect essential layers of the model for MCI classification and design a\nspecific inference logic considering the characteristics of MCI. The experiment\nshows competitive results, and the proposed inference logic significantly\ncontributes to the improvements from the baseline. We also conduct detailed\nanalysis which reveals the challenges related to speaker bias in the features\nand the sensitivity of MCI classification accuracy to the data split, providing\nvaluable insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores a multi-lingual audio self-supervised learning model for\ndetecting mild cognitive impairment (MCI) using the TAUKADIAL cross-lingual\ndataset. While speech transcription-based detection with BERT models is\neffective, limitations exist due to a lack of transcriptions and temporal\ninformation. To address these issues, the study utilizes features directly from\nspeech utterances with W2V-BERT-2.0. We propose a visualization method to\ndetect essential layers of the model for MCI classification and design a\nspecific inference logic considering the characteristics of MCI. The experiment\nshows competitive results, and the proposed inference logic significantly\ncontributes to the improvements from the baseline. We also conduct detailed\nanalysis which reveals the challenges related to speaker bias in the features\nand the sensitivity of MCI classification accuracy to the data split, providing\nvaluable insights for future research."
                },
                "authors": [
                    {
                        "name": "Yueguan Wang"
                    },
                    {
                        "name": "Tatsunari Matsushima"
                    },
                    {
                        "name": "Soichiro Matsushima"
                    },
                    {
                        "name": "Toshimitsu Sakai"
                    }
                ],
                "author_detail": {
                    "name": "Toshimitsu Sakai"
                },
                "author": "Toshimitsu Sakai",
                "arxiv_comment": "Submitted to ICASSP-SPADE workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16191v1",
                "updated": "2025-01-27T16:45:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    45,
                    34,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T16:45:34Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    45,
                    34,
                    0,
                    27,
                    0
                ],
                "title": "Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python\n  using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python\n  using LLMs"
                },
                "summary": "Fixing Python dependency issues is a tedious and error-prone task for\ndevelopers, who must manually identify and resolve environment dependencies and\nversion constraints of third-party modules and Python interpreters. Researchers\nhave attempted to automate this process by relying on large knowledge graphs\nand database lookup tables. However, these traditional approaches face\nlimitations due to the variety of dependency error types, large sets of\npossible module versions, and conflicts among transitive dependencies. This\nstudy explores the potential of using large language models (LLMs) to\nautomatically fix dependency issues in Python programs. We introduce PLLM\n(pronounced \"plum\"), a novel technique that employs retrieval-augmented\ngeneration (RAG) to help an LLM infer Python versions and required modules for\na given Python file. PLLM builds a testing environment that iteratively (1)\nprompts the LLM for module combinations, (2) tests the suggested changes, and\n(3) provides feedback (error messages) to the LLM to refine the fix. This\nfeedback cycle leverages natural language processing (NLP) to intelligently\nparse and interpret build error messages. We benchmark PLLM on the Gistable\nHG2.9K dataset, a collection of challenging single-file Python gists. We\ncompare PLLM against two state-of-the-art automatic dependency inference\napproaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency\nissues. Our results indicate that PLLM can fix more dependency issues than the\ntwo baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%)\nover PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial\nfor projects with many dependencies and for specific third-party numerical and\nmachine-learning modules. Our findings demonstrate the potential of LLM-based\napproaches to iteratively resolve Python dependency issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixing Python dependency issues is a tedious and error-prone task for\ndevelopers, who must manually identify and resolve environment dependencies and\nversion constraints of third-party modules and Python interpreters. Researchers\nhave attempted to automate this process by relying on large knowledge graphs\nand database lookup tables. However, these traditional approaches face\nlimitations due to the variety of dependency error types, large sets of\npossible module versions, and conflicts among transitive dependencies. This\nstudy explores the potential of using large language models (LLMs) to\nautomatically fix dependency issues in Python programs. We introduce PLLM\n(pronounced \"plum\"), a novel technique that employs retrieval-augmented\ngeneration (RAG) to help an LLM infer Python versions and required modules for\na given Python file. PLLM builds a testing environment that iteratively (1)\nprompts the LLM for module combinations, (2) tests the suggested changes, and\n(3) provides feedback (error messages) to the LLM to refine the fix. This\nfeedback cycle leverages natural language processing (NLP) to intelligently\nparse and interpret build error messages. We benchmark PLLM on the Gistable\nHG2.9K dataset, a collection of challenging single-file Python gists. We\ncompare PLLM against two state-of-the-art automatic dependency inference\napproaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency\nissues. Our results indicate that PLLM can fix more dependency issues than the\ntwo baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%)\nover PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial\nfor projects with many dependencies and for specific third-party numerical and\nmachine-learning modules. Our findings demonstrate the potential of LLM-based\napproaches to iteratively resolve Python dependency issues."
                },
                "authors": [
                    {
                        "name": "Antony Bartlett"
                    },
                    {
                        "name": "Cynthia Liem"
                    },
                    {
                        "name": "Annibale Panichella"
                    }
                ],
                "author_detail": {
                    "name": "Annibale Panichella"
                },
                "author": "Annibale Panichella",
                "arxiv_comment": "Under submission to TOSEM, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04637v3",
                "updated": "2025-01-27T16:38:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    38,
                    30,
                    0,
                    27,
                    0
                ],
                "published": "2024-11-07T11:51:14Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    51,
                    14,
                    3,
                    312,
                    0
                ],
                "title": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop"
                },
                "summary": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Dominik Schlechtweg"
                    },
                    {
                        "name": "Natalia Fedorova"
                    },
                    {
                        "name": "Konstantin Chernyshev"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Boris Obmoroshev"
                    }
                ],
                "author_detail": {
                    "name": "Boris Obmoroshev"
                },
                "author": "Boris Obmoroshev",
                "arxiv_comment": "To be presented at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01834v3",
                "updated": "2025-01-27T16:34:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    34,
                    59,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-03T14:38:01Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    38,
                    1,
                    4,
                    3,
                    0
                ],
                "title": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning"
                },
                "summary": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we call MoColl, designed to effectively\nintegrate domain-specific and general knowledge. Specifically, our approach is\nto decompose complex image captioning tasks into a series of interconnected\nquestion-answer subtasks. A trainable visual question answering (VQA) model is\nemployed as a specialized tool to focus on domain-specific visual analysis,\nanswering task-specific questions based on image content. Concurrently, an\nLLM-based agent with general knowledge formulates these questions and\nsynthesizes the resulting question-answer pairs into coherent captions. Beyond\nits role in leveraging the VQA model, the agent further guides its training to\nenhance its domain-specific capabilities. Experimental results on radiology\nreport generation validate the effectiveness of the proposed framework,\ndemonstrating significant improvements in the quality of generated reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we call MoColl, designed to effectively\nintegrate domain-specific and general knowledge. Specifically, our approach is\nto decompose complex image captioning tasks into a series of interconnected\nquestion-answer subtasks. A trainable visual question answering (VQA) model is\nemployed as a specialized tool to focus on domain-specific visual analysis,\nanswering task-specific questions based on image content. Concurrently, an\nLLM-based agent with general knowledge formulates these questions and\nsynthesizes the resulting question-answer pairs into coherent captions. Beyond\nits role in leveraging the VQA model, the agent further guides its training to\nenhance its domain-specific capabilities. Experimental results on radiology\nreport generation validate the effectiveness of the proposed framework,\ndemonstrating significant improvements in the quality of generated reports."
                },
                "authors": [
                    {
                        "name": "Pu Yang"
                    },
                    {
                        "name": "Bin Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dong"
                },
                "author": "Bin Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16178v1",
                "updated": "2025-01-27T16:26:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    26,
                    7,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T16:26:07Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    26,
                    7,
                    0,
                    27,
                    0
                ],
                "title": "SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time\n  Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time\n  Series Forecasting"
                },
                "summary": "In recent work on time-series prediction, Transformers and even large\nlanguage models have garnered significant attention due to their strong\ncapabilities in sequence modeling. However, in practical deployments,\ntime-series prediction often requires operation in resource-constrained\nenvironments, such as edge devices, which are unable to handle the\ncomputational overhead of large models. To address such scenarios, some\nlightweight models have been proposed, but they exhibit poor performance on\nnon-stationary sequences. In this paper, we propose $\\textit{SWIFT}$, a\nlightweight model that is not only powerful, but also efficient in deployment\nand inference for Long-term Time Series Forecasting (LTSF). Our model is based\non three key points: (i) Utilizing wavelet transform to perform lossless\ndownsampling of time series. (ii) Achieving cross-band information fusion with\na learnable filter. (iii) Using only one shared linear layer or one shallow MLP\nfor sub-series' mapping. We conduct comprehensive experiments, and the results\nshow that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on\nmultiple datasets, offering a promising method for edge computing and\ndeployment in this task. Moreover, it is noteworthy that the number of\nparameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a\nsingle-layer linear model for time-domain prediction. Our code is available at\nhttps://github.com/LancelotXWX/SWIFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent work on time-series prediction, Transformers and even large\nlanguage models have garnered significant attention due to their strong\ncapabilities in sequence modeling. However, in practical deployments,\ntime-series prediction often requires operation in resource-constrained\nenvironments, such as edge devices, which are unable to handle the\ncomputational overhead of large models. To address such scenarios, some\nlightweight models have been proposed, but they exhibit poor performance on\nnon-stationary sequences. In this paper, we propose $\\textit{SWIFT}$, a\nlightweight model that is not only powerful, but also efficient in deployment\nand inference for Long-term Time Series Forecasting (LTSF). Our model is based\non three key points: (i) Utilizing wavelet transform to perform lossless\ndownsampling of time series. (ii) Achieving cross-band information fusion with\na learnable filter. (iii) Using only one shared linear layer or one shallow MLP\nfor sub-series' mapping. We conduct comprehensive experiments, and the results\nshow that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on\nmultiple datasets, offering a promising method for edge computing and\ndeployment in this task. Moreover, it is noteworthy that the number of\nparameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a\nsingle-layer linear model for time-domain prediction. Our code is available at\nhttps://github.com/LancelotXWX/SWIFT."
                },
                "authors": [
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Fanpu Cao"
                    }
                ],
                "author_detail": {
                    "name": "Fanpu Cao"
                },
                "author": "Fanpu Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07615v2",
                "updated": "2025-01-27T16:24:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    24,
                    47,
                    0,
                    27,
                    0
                ],
                "published": "2024-09-11T20:55:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    55,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "MOSAIC: Multiple Observers Spotting AI Content, a Robust Approach to\n  Machine-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSAIC: Multiple Observers Spotting AI Content, a Robust Approach to\n  Machine-Generated Text Detection"
                },
                "summary": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities has vastly increased the\nthreats posed by generative AI technologies by reducing the cost of producing\nharmful, toxic, faked or forged content. In response, various proposals have\nbeen made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a classification problem.\nMost approaches evaluate an input document by a well-chosen detector LLM,\nassuming that low-perplexity scores reliably signal machine-made content. As\nusing one single detector can induce brittleness of performance, we instead\nconsider several and derive a new, theoretically grounded approach to combine\ntheir respective strengths. Our experiments, using a variety of generator LLMs,\nsuggest that our method effectively leads to robust detection performances. An\nearly version of the code is available at\nhttps://github.com/BaggerOfWords/MOSAIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities has vastly increased the\nthreats posed by generative AI technologies by reducing the cost of producing\nharmful, toxic, faked or forged content. In response, various proposals have\nbeen made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a classification problem.\nMost approaches evaluate an input document by a well-chosen detector LLM,\nassuming that low-perplexity scores reliably signal machine-made content. As\nusing one single detector can induce brittleness of performance, we instead\nconsider several and derive a new, theoretically grounded approach to combine\ntheir respective strengths. Our experiments, using a variety of generator LLMs,\nsuggest that our method effectively leads to robust detection performances. An\nearly version of the code is available at\nhttps://github.com/BaggerOfWords/MOSAIC."
                },
                "authors": [
                    {
                        "name": "Matthieu Dubois"
                    },
                    {
                        "name": "François Yvon"
                    },
                    {
                        "name": "Pablo Piantanida"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Piantanida"
                },
                "author": "Pablo Piantanida",
                "arxiv_comment": "Still a work in progress, early version of the code can be found here\n  :https://github.com/BaggerOfWords/MOSAIC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10259v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10259v4",
                "updated": "2025-01-27T16:15:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    15,
                    26,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-16T03:26:43Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    3,
                    26,
                    43,
                    1,
                    107,
                    0
                ],
                "title": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy"
                },
                "summary": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events."
                },
                "authors": [
                    {
                        "name": "Tunazzina Islam"
                    },
                    {
                        "name": "Dan Goldwasser"
                    }
                ],
                "author_detail": {
                    "name": "Dan Goldwasser"
                },
                "author": "Dan Goldwasser",
                "arxiv_comment": "Accepted at the Findings of 2025 Annual Conference of the Nations of\n  the Americas Chapter of the ACL (NAACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10259v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10259v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16173v1",
                "updated": "2025-01-27T16:14:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    14,
                    33,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T16:14:33Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    14,
                    33,
                    0,
                    27,
                    0
                ],
                "title": "Will Systems of LLM Agents Cooperate: An Investigation into a Social\n  Dilemma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will Systems of LLM Agents Cooperate: An Investigation into a Social\n  Dilemma"
                },
                "summary": "As autonomous agents become more prevalent, understanding their collective\nbehaviour in strategic interactions is crucial. This study investigates the\nemergent cooperative tendencies of systems of Large Language Model (LLM) agents\nin a social dilemma. Unlike previous research where LLMs output individual\nactions, we prompt state-of-the-art LLMs to generate complete strategies for\niterated Prisoner's Dilemma. Using evolutionary game theory, we simulate\npopulations of agents with different strategic dispositions (aggressive,\ncooperative, or neutral) and observe their evolutionary dynamics. Our findings\nreveal that different LLMs exhibit distinct biases affecting the relative\nsuccess of aggressive versus cooperative strategies. This research provides\ninsights into the potential long-term behaviour of systems of deployed\nLLM-based autonomous agents and highlights the importance of carefully\nconsidering the strategic environments in which they operate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous agents become more prevalent, understanding their collective\nbehaviour in strategic interactions is crucial. This study investigates the\nemergent cooperative tendencies of systems of Large Language Model (LLM) agents\nin a social dilemma. Unlike previous research where LLMs output individual\nactions, we prompt state-of-the-art LLMs to generate complete strategies for\niterated Prisoner's Dilemma. Using evolutionary game theory, we simulate\npopulations of agents with different strategic dispositions (aggressive,\ncooperative, or neutral) and observe their evolutionary dynamics. Our findings\nreveal that different LLMs exhibit distinct biases affecting the relative\nsuccess of aggressive versus cooperative strategies. This research provides\ninsights into the potential long-term behaviour of systems of deployed\nLLM-based autonomous agents and highlights the importance of carefully\nconsidering the strategic environments in which they operate."
                },
                "authors": [
                    {
                        "name": "Richard Willis"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Joel Z Leibo"
                    },
                    {
                        "name": "Michael Luck"
                    }
                ],
                "author_detail": {
                    "name": "Michael Luck"
                },
                "author": "Michael Luck",
                "arxiv_comment": "7 pages (10 including references), 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11675v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11675v5",
                "updated": "2025-01-27T16:00:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    0,
                    59,
                    0,
                    27,
                    0
                ],
                "published": "2024-06-17T15:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    55,
                    38,
                    0,
                    169,
                    0
                ],
                "title": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) often suffer from overconfidence during\ninference, particularly when adapted to downstream domain-specific tasks with\nlimited data. Previous work addresses this issue by employing approximate\nBayesian estimation after the LLMs are trained, enabling them to quantify\nuncertainty. However, such post-training approaches' performance is severely\nlimited by the parameters learned during training. In this paper, we go beyond\npost-training Bayesianization and propose Bayesian Low-Rank Adaptation by\nBackpropagation (BLoB), an algorithm that continuously and jointly adjusts both\nthe mean and covariance of LLM parameters throughout the whole fine-tuning\nprocess. Our empirical results verify the effectiveness of BLoB in terms of\ngeneralization and uncertainty estimation, when evaluated on both\nin-distribution and out-of-distribution data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often suffer from overconfidence during\ninference, particularly when adapted to downstream domain-specific tasks with\nlimited data. Previous work addresses this issue by employing approximate\nBayesian estimation after the LLMs are trained, enabling them to quantify\nuncertainty. However, such post-training approaches' performance is severely\nlimited by the parameters learned during training. In this paper, we go beyond\npost-training Bayesianization and propose Bayesian Low-Rank Adaptation by\nBackpropagation (BLoB), an algorithm that continuously and jointly adjusts both\nthe mean and covariance of LLM parameters throughout the whole fine-tuning\nprocess. Our empirical results verify the effectiveness of BLoB in terms of\ngeneralization and uncertainty estimation, when evaluated on both\nin-distribution and out-of-distribution data."
                },
                "authors": [
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Dimitris Metaxas"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024. Additional experiments have been included\n  in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11675v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11675v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16164v1",
                "updated": "2025-01-27T15:59:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    59,
                    58,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:59:58Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    59,
                    58,
                    0,
                    27,
                    0
                ],
                "title": "MetaDecorator: Generating Immersive Virtual Tours through Multimodality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaDecorator: Generating Immersive Virtual Tours through Multimodality"
                },
                "summary": "MetaDecorator, is a framework that empowers users to personalize virtual\nspaces. By leveraging text-driven prompts and image synthesis techniques,\nMetaDecorator adorns static panoramas captured by 360{\\deg} imaging devices,\ntransforming them into uniquely styled and visually appealing environments.\nThis significantly enhances the realism and engagement of virtual tours\ncompared to traditional offerings. Beyond the core framework, we also discuss\nthe integration of Large Language Models (LLMs) and haptics in the VR\napplication to provide a more immersive experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaDecorator, is a framework that empowers users to personalize virtual\nspaces. By leveraging text-driven prompts and image synthesis techniques,\nMetaDecorator adorns static panoramas captured by 360{\\deg} imaging devices,\ntransforming them into uniquely styled and visually appealing environments.\nThis significantly enhances the realism and engagement of virtual tours\ncompared to traditional offerings. Beyond the core framework, we also discuss\nthe integration of Large Language Models (LLMs) and haptics in the VR\napplication to provide a more immersive experience."
                },
                "authors": [
                    {
                        "name": "Shuang Xie"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jeannie S. A. Lee"
                    },
                    {
                        "name": "Haiwei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Haiwei Dong"
                },
                "author": "Haiwei Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.16991v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.16991v3",
                "updated": "2025-01-27T15:54:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    54,
                    17,
                    0,
                    27,
                    0
                ],
                "published": "2022-10-31T00:16:55Z",
                "published_parsed": [
                    2022,
                    10,
                    31,
                    0,
                    16,
                    55,
                    0,
                    304,
                    0
                ],
                "title": "Non-Robustness of the Cluster-Robust Inference: with a Proposal of a New\n  Robust Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Robustness of the Cluster-Robust Inference: with a Proposal of a New\n  Robust Method"
                },
                "summary": "The conventional cluster-robust (CR) standard errors may not be robust. They\nare vulnerable to data that contain a small number of large clusters. When a\nresearcher uses the 51 states in the U.S. as clusters, the largest cluster\n(California) consists of about 10% of the total sample. Such a case in fact\nviolates the assumptions under which the widely used CR methods are guaranteed\nto work. We formally show that the conventional CR methods fail if the\ndistribution of cluster sizes follows a power law with exponent less than two.\nBesides the example of 51 state clusters, some examples are drawn from a list\nof recent original research articles published in a top journal. In light of\nthese negative results about the existing CR methods, we propose a weighted CR\n(WCR) method as a simple fix. Simulation studies support our arguments that the\nWCR method is robust while the conventional CR methods are not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conventional cluster-robust (CR) standard errors may not be robust. They\nare vulnerable to data that contain a small number of large clusters. When a\nresearcher uses the 51 states in the U.S. as clusters, the largest cluster\n(California) consists of about 10% of the total sample. Such a case in fact\nviolates the assumptions under which the widely used CR methods are guaranteed\nto work. We formally show that the conventional CR methods fail if the\ndistribution of cluster sizes follows a power law with exponent less than two.\nBesides the example of 51 state clusters, some examples are drawn from a list\nof recent original research articles published in a top journal. In light of\nthese negative results about the existing CR methods, we propose a weighted CR\n(WCR) method as a simple fix. Simulation studies support our arguments that the\nWCR method is robust while the conventional CR methods are not."
                },
                "authors": [
                    {
                        "name": "Yuya Sasaki"
                    },
                    {
                        "name": "Yulong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Wang"
                },
                "author": "Yulong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.16991v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.16991v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16156v1",
                "updated": "2025-01-27T15:50:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    50,
                    28,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:50:28Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    50,
                    28,
                    0,
                    27,
                    0
                ],
                "title": "Moving toward best practice when using propensity score weighting in\n  survey observational studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moving toward best practice when using propensity score weighting in\n  survey observational studies"
                },
                "summary": "Propensity score weighting is a common method for estimating treatment\neffects with survey data. The method is applied to minimize confounding using\nmeasured covariates that are often different between individuals in treatment\nand control. However, existing literature does not reach a consensus on the\noptimal use of survey weights for population-level inference in the propensity\nscore weighting analysis. Under the balancing weights framework, we provided a\nunified solution for incorporating survey weights in both the propensity score\nof estimation and the outcome regression model. We derived estimators for\ndifferent target populations, including the combined, treated, controlled, and\noverlap populations. We provide a unified expression of the sandwich variance\nestimator and demonstrate that the survey-weighted estimator is asymptotically\nnormal, as established through the theory of M-estimators. Through an extensive\nseries of simulation studies, we examined the performance of our derived\nestimators and compared the results to those of alternative methods. We further\ncarried out two case studies to illustrate the application of the different\nmethods of propensity score analysis with complex survey data. We concluded\nwith a discussion of our findings and provided practical guidelines for\npropensity score weighting analysis of observational data from complex surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propensity score weighting is a common method for estimating treatment\neffects with survey data. The method is applied to minimize confounding using\nmeasured covariates that are often different between individuals in treatment\nand control. However, existing literature does not reach a consensus on the\noptimal use of survey weights for population-level inference in the propensity\nscore weighting analysis. Under the balancing weights framework, we provided a\nunified solution for incorporating survey weights in both the propensity score\nof estimation and the outcome regression model. We derived estimators for\ndifferent target populations, including the combined, treated, controlled, and\noverlap populations. We provide a unified expression of the sandwich variance\nestimator and demonstrate that the survey-weighted estimator is asymptotically\nnormal, as established through the theory of M-estimators. Through an extensive\nseries of simulation studies, we examined the performance of our derived\nestimators and compared the results to those of alternative methods. We further\ncarried out two case studies to illustrate the application of the different\nmethods of propensity score analysis with complex survey data. We concluded\nwith a discussion of our findings and provided practical guidelines for\npropensity score weighting analysis of observational data from complex surveys."
                },
                "authors": [
                    {
                        "name": "Yukang Zeng"
                    },
                    {
                        "name": "Fan Li"
                    },
                    {
                        "name": "Guangyu Tong"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Tong"
                },
                "author": "Guangyu Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16155v1",
                "updated": "2025-01-27T15:49:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    49,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:49:24Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    49,
                    24,
                    0,
                    27,
                    0
                ],
                "title": "CITYWALK: Enhancing LLM-Based C++ Unit Test Generation via\n  Project-Dependency Awareness and Language-Specific Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITYWALK: Enhancing LLM-Based C++ Unit Test Generation via\n  Project-Dependency Awareness and Language-Specific Knowledge"
                },
                "summary": "Unit testing plays a pivotal role in the software development lifecycle, as\nit ensures code quality. However, writing high-quality unit tests remains a\ntime-consuming task for developers in practice. More recently, the application\nof large language models (LLMs) in automated unit test generation has\ndemonstrated promising results. Existing approaches primarily focus on\ninterpreted programming languages (e.g., Java), while mature solutions tailored\nto compiled programming languages like C++ are yet to be explored. The\nintricate language features of C++, such as pointers, templates, and virtual\nfunctions, pose particular challenges for LLMs in generating both executable\nand high-coverage unit tests. To tackle the aforementioned problems, this paper\nintroduces CITYWALK, a novel LLM-based framework for C++ unit test generation.\nCITYWALK enhances LLMs by providing a comprehensive understanding of the\ndependency relationships within the project under test via program analysis.\nFurthermore, CITYWALK incorporates language-specific knowledge about C++\nderived from project documentation and empirical observations, significantly\nimproving the correctness of the LLM-generated unit tests. We implement\nCITYWALK by employing the widely popular LLM GPT-4o. The experimental results\nshow that CITYWALK outperforms current state-of-the-art approaches on a\ncollection of eight popular C++ projects. Our findings demonstrate the\neffectiveness of CITYWALK in generating high-quality C++ unit tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing plays a pivotal role in the software development lifecycle, as\nit ensures code quality. However, writing high-quality unit tests remains a\ntime-consuming task for developers in practice. More recently, the application\nof large language models (LLMs) in automated unit test generation has\ndemonstrated promising results. Existing approaches primarily focus on\ninterpreted programming languages (e.g., Java), while mature solutions tailored\nto compiled programming languages like C++ are yet to be explored. The\nintricate language features of C++, such as pointers, templates, and virtual\nfunctions, pose particular challenges for LLMs in generating both executable\nand high-coverage unit tests. To tackle the aforementioned problems, this paper\nintroduces CITYWALK, a novel LLM-based framework for C++ unit test generation.\nCITYWALK enhances LLMs by providing a comprehensive understanding of the\ndependency relationships within the project under test via program analysis.\nFurthermore, CITYWALK incorporates language-specific knowledge about C++\nderived from project documentation and empirical observations, significantly\nimproving the correctness of the LLM-generated unit tests. We implement\nCITYWALK by employing the widely popular LLM GPT-4o. The experimental results\nshow that CITYWALK outperforms current state-of-the-art approaches on a\ncollection of eight popular C++ projects. Our findings demonstrate the\neffectiveness of CITYWALK in generating high-quality C++ unit tests."
                },
                "authors": [
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Qingyuan Lu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Wensheng Dou"
                    },
                    {
                        "name": "Jiaxin Zhu"
                    },
                    {
                        "name": "Li Qian"
                    },
                    {
                        "name": "Chunxi Zhang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Jun Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wei"
                },
                "author": "Jun Wei",
                "arxiv_comment": "13 tables, 12 figures. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16154v1",
                "updated": "2025-01-27T15:48:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    48,
                    57,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:48:57Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    48,
                    57,
                    0,
                    27,
                    0
                ],
                "title": "AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive\n  Chain-of-Thought"
                },
                "summary": "Large language models (LLMs) have shown impressive multilingual capabilities\nthrough pretraining on diverse corpora. While these models show strong\nreasoning abilities, their performance varies significantly across languages\ndue to uneven training data distribution. Existing approaches using machine\ntranslation, and extensive multilingual pretraining and cross-lingual tuning\nface scalability challenges and often fail to capture nuanced reasoning\nprocesses across languages. In this paper, we introduce AdaCoT (Adaptive\nChain-of-Thought), a framework that enhances multilingual reasoning by\ndynamically routing thought processes through intermediary \"thinking languages\"\nbefore generating target-language responses. AdaCoT leverages a\nlanguage-agnostic core and incorporates an adaptive, reward-based mechanism for\nselecting optimal reasoning pathways without requiring additional pretraining.\nOur comprehensive evaluation across multiple benchmarks demonstrates\nsubstantial improvements in both factual reasoning quality and cross-lingual\nconsistency, with particularly strong performance gains in low-resource\nlanguage settings. The results suggest that adaptive reasoning paths can\neffectively bridge the performance gap between high and low-resource languages\nwhile maintaining cultural and linguistic nuances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive multilingual capabilities\nthrough pretraining on diverse corpora. While these models show strong\nreasoning abilities, their performance varies significantly across languages\ndue to uneven training data distribution. Existing approaches using machine\ntranslation, and extensive multilingual pretraining and cross-lingual tuning\nface scalability challenges and often fail to capture nuanced reasoning\nprocesses across languages. In this paper, we introduce AdaCoT (Adaptive\nChain-of-Thought), a framework that enhances multilingual reasoning by\ndynamically routing thought processes through intermediary \"thinking languages\"\nbefore generating target-language responses. AdaCoT leverages a\nlanguage-agnostic core and incorporates an adaptive, reward-based mechanism for\nselecting optimal reasoning pathways without requiring additional pretraining.\nOur comprehensive evaluation across multiple benchmarks demonstrates\nsubstantial improvements in both factual reasoning quality and cross-lingual\nconsistency, with particularly strong performance gains in low-resource\nlanguage settings. The results suggest that adaptive reasoning paths can\neffectively bridge the performance gap between high and low-resource languages\nwhile maintaining cultural and linguistic nuances."
                },
                "authors": [
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Tarun Kumar Vangani"
                    },
                    {
                        "name": "Zhengyuan Liu"
                    },
                    {
                        "name": "Bowei Zou"
                    },
                    {
                        "name": "Ai Ti Aw"
                    }
                ],
                "author_detail": {
                    "name": "Ai Ti Aw"
                },
                "author": "Ai Ti Aw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16151v1",
                "updated": "2025-01-27T15:45:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    45,
                    47,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:45:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    45,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "Breaking the degeneracy in stellar spectral classification from single\n  wide-band images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the degeneracy in stellar spectral classification from single\n  wide-band images"
                },
                "summary": "The spectral energy distribution (SED) of observed stars in wide-field images\nis crucial for chromatic point spread function (PSF) modelling methods, which\nuse unresolved stars as integrated spectral samples of the PSF across the field\nof view. This is particularly important for weak gravitational lensing studies,\nwhere precise PSF modelling is essential to get accurate shear measurements.\nPrevious research has demonstrated that the SED of stars can be inferred from\nlow-resolution observations using machine-learning classification algorithms.\nHowever, a degeneracy exists between the PSF size, which can vary significantly\nacross the field of view, and the spectral type of stars, leading to strong\nlimitations of such methods. We propose a new SED classification method that\nincorporates stellar spectral information by using a preliminary PSF model,\nthereby breaking this degeneracy and enhancing the classification accuracy. Our\nmethod involves calculating a set of similarity features between an observed\nstar and a preliminary PSF model at different wavelengths and applying a\nsupport vector machine to these similarity features to classify the observed\nstar into a specific stellar class. The proposed approach achieves a 91\\%\ntop-two accuracy, surpassing machine-learning methods that do not consider the\nspectral variation of the PSF. Additionally, we examined the impact of PSF\nmodelling errors on the spectral classification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spectral energy distribution (SED) of observed stars in wide-field images\nis crucial for chromatic point spread function (PSF) modelling methods, which\nuse unresolved stars as integrated spectral samples of the PSF across the field\nof view. This is particularly important for weak gravitational lensing studies,\nwhere precise PSF modelling is essential to get accurate shear measurements.\nPrevious research has demonstrated that the SED of stars can be inferred from\nlow-resolution observations using machine-learning classification algorithms.\nHowever, a degeneracy exists between the PSF size, which can vary significantly\nacross the field of view, and the spectral type of stars, leading to strong\nlimitations of such methods. We propose a new SED classification method that\nincorporates stellar spectral information by using a preliminary PSF model,\nthereby breaking this degeneracy and enhancing the classification accuracy. Our\nmethod involves calculating a set of similarity features between an observed\nstar and a preliminary PSF model at different wavelengths and applying a\nsupport vector machine to these similarity features to classify the observed\nstar into a specific stellar class. The proposed approach achieves a 91\\%\ntop-two accuracy, surpassing machine-learning methods that do not consider the\nspectral variation of the PSF. Additionally, we examined the impact of PSF\nmodelling errors on the spectral classification accuracy."
                },
                "authors": [
                    {
                        "name": "Ezequiel Centofanti"
                    },
                    {
                        "name": "Samuel Farrens"
                    },
                    {
                        "name": "Jean-Luc Starck"
                    },
                    {
                        "name": "Tobias Liaudat"
                    },
                    {
                        "name": "Alex Szapiro"
                    },
                    {
                        "name": "Jennifer Pollack"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Pollack"
                },
                "author": "Jennifer Pollack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16150v1",
                "updated": "2025-01-27T15:44:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    44,
                    2,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:44:02Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    44,
                    2,
                    0,
                    27,
                    0
                ],
                "title": "AI Agents for Computer Use: A Review of Instruction-based Computer\n  Control, GUI Automation, and Operator Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents for Computer Use: A Review of Instruction-based Computer\n  Control, GUI Automation, and Operator Assistants"
                },
                "summary": "Instruction-based computer control agents (CCAs) execute complex action\nsequences on personal computers or mobile devices to fulfill tasks using the\nsame graphical user interfaces as a human user would, provided instructions in\nnatural language. This review offers a comprehensive overview of the emerging\nfield of instruction-based computer control, examining available agents --\ntheir taxonomy, development, and respective resources -- and emphasizing the\nshift from manually designed, specialized agents to leveraging foundation\nmodels such as large language models (LLMs) and vision-language models (VLMs).\nWe formalize the problem and establish a taxonomy of the field to analyze\nagents from three perspectives: (a) the environment perspective, analyzing\ncomputer environments; (b) the interaction perspective, describing observations\nspaces (e.g., screenshots, HTML) and action spaces (e.g., mouse and keyboard\nactions, executable code); and (c) the agent perspective, focusing on the core\nprinciple of how an agent acts and learns to act. Our framework encompasses\nboth specialized and foundation agents, facilitating their comparative analysis\nand revealing how prior solutions in specialized agents, such as an environment\nlearning step, can guide the development of more capable foundation agents.\nAdditionally, we review current CCA datasets and CCA evaluation methods and\noutline the challenges to deploying such agents in a productive setting. In\ntotal, we review and classify 86 CCAs and 33 related datasets. By highlighting\ntrends, limitations, and future research directions, this work presents a\ncomprehensive foundation to obtain a broad understanding of the field and push\nits future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-based computer control agents (CCAs) execute complex action\nsequences on personal computers or mobile devices to fulfill tasks using the\nsame graphical user interfaces as a human user would, provided instructions in\nnatural language. This review offers a comprehensive overview of the emerging\nfield of instruction-based computer control, examining available agents --\ntheir taxonomy, development, and respective resources -- and emphasizing the\nshift from manually designed, specialized agents to leveraging foundation\nmodels such as large language models (LLMs) and vision-language models (VLMs).\nWe formalize the problem and establish a taxonomy of the field to analyze\nagents from three perspectives: (a) the environment perspective, analyzing\ncomputer environments; (b) the interaction perspective, describing observations\nspaces (e.g., screenshots, HTML) and action spaces (e.g., mouse and keyboard\nactions, executable code); and (c) the agent perspective, focusing on the core\nprinciple of how an agent acts and learns to act. Our framework encompasses\nboth specialized and foundation agents, facilitating their comparative analysis\nand revealing how prior solutions in specialized agents, such as an environment\nlearning step, can guide the development of more capable foundation agents.\nAdditionally, we review current CCA datasets and CCA evaluation methods and\noutline the challenges to deploying such agents in a productive setting. In\ntotal, we review and classify 86 CCAs and 33 related datasets. By highlighting\ntrends, limitations, and future research directions, this work presents a\ncomprehensive foundation to obtain a broad understanding of the field and push\nits future development."
                },
                "authors": [
                    {
                        "name": "Pascal J. Sager"
                    },
                    {
                        "name": "Benjamin Meyer"
                    },
                    {
                        "name": "Peng Yan"
                    },
                    {
                        "name": "Rebekka von Wartburg-Kottler"
                    },
                    {
                        "name": "Layan Etaiwi"
                    },
                    {
                        "name": "Aref Enayati"
                    },
                    {
                        "name": "Gabriel Nobel"
                    },
                    {
                        "name": "Ahmed Abdulkadir"
                    },
                    {
                        "name": "Benjamin F. Grewe"
                    },
                    {
                        "name": "Thilo Stadelmann"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Stadelmann"
                },
                "author": "Thilo Stadelmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16149v1",
                "updated": "2025-01-27T15:43:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    43,
                    4,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:43:04Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    43,
                    4,
                    0,
                    27,
                    0
                ],
                "title": "PATCH: Empowering Large Language Model with Programmer-Intent Guidance\n  and Collaborative-Behavior Simulation for Automatic Bug Fixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCH: Empowering Large Language Model with Programmer-Intent Guidance\n  and Collaborative-Behavior Simulation for Automatic Bug Fixing"
                },
                "summary": "Bug fixing holds significant importance in software development and\nmaintenance. Recent research has made substantial strides in exploring the\npotential of large language models (LLMs) for automatically resolving software\nbugs. However, a noticeable gap in existing approaches lies in the oversight of\ncollaborative facets intrinsic to bug resolution, treating the process as a\nsingle-stage endeavor. Moreover, most approaches solely take the buggy code\nsnippet as input for LLMs during the patch generation stage. To mitigate the\naforementioned limitations, we introduce a novel stage-wise framework named\nPATCH. Specifically, we first augment the buggy code snippet with corresponding\ndependence context and intent information to better guide LLMs in generating\nthe correct candidate patches. Additionally, by taking inspiration from bug\nmanagement practices, we decompose the bug-fixing task into four distinct\nstages: bug reporting, bug diagnosis, patch generation, and patch verification.\nThese stages are performed interactively by LLMs, aiming to simulate the\ncollaborative behavior of programmers during the resolution of software bugs.\nBy harnessing these collective contributions, PATCH effectively enhances the\nbug-fixing capability of LLMs. We implement PATCH by employing the powerful\ndialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing\nbenchmark BFP demonstrates that PATCH has achieved better performance than\nstate-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bug fixing holds significant importance in software development and\nmaintenance. Recent research has made substantial strides in exploring the\npotential of large language models (LLMs) for automatically resolving software\nbugs. However, a noticeable gap in existing approaches lies in the oversight of\ncollaborative facets intrinsic to bug resolution, treating the process as a\nsingle-stage endeavor. Moreover, most approaches solely take the buggy code\nsnippet as input for LLMs during the patch generation stage. To mitigate the\naforementioned limitations, we introduce a novel stage-wise framework named\nPATCH. Specifically, we first augment the buggy code snippet with corresponding\ndependence context and intent information to better guide LLMs in generating\nthe correct candidate patches. Additionally, by taking inspiration from bug\nmanagement practices, we decompose the bug-fixing task into four distinct\nstages: bug reporting, bug diagnosis, patch generation, and patch verification.\nThese stages are performed interactively by LLMs, aiming to simulate the\ncollaborative behavior of programmers during the resolution of software bugs.\nBy harnessing these collective contributions, PATCH effectively enhances the\nbug-fixing capability of LLMs. We implement PATCH by employing the powerful\ndialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing\nbenchmark BFP demonstrates that PATCH has achieved better performance than\nstate-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Ying Xing"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Jiaxin Zhu"
                    },
                    {
                        "name": "Wensheng Dou"
                    },
                    {
                        "name": "Jun Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wei"
                },
                "author": "Jun Wei",
                "arxiv_comment": "8 tables, 18 figures. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16146v1",
                "updated": "2025-01-27T15:39:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    39,
                    39,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:39:39Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    39,
                    39,
                    0,
                    27,
                    0
                ],
                "title": "Toward Efficient Generalization in 3D Human Pose Estimation via a\n  Canonical Domain Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Efficient Generalization in 3D Human Pose Estimation via a\n  Canonical Domain Approach"
                },
                "summary": "Recent advancements in deep learning methods have significantly improved the\nperformance of 3D Human Pose Estimation (HPE). However, performance degradation\ncaused by domain gaps between source and target domains remains a major\nchallenge to generalization, necessitating extensive data augmentation and/or\nfine-tuning for each specific target domain. To address this issue more\nefficiently, we propose a novel canonical domain approach that maps both the\nsource and target domains into a unified canonical domain, alleviating the need\nfor additional fine-tuning in the target domain. To construct the canonical\ndomain, we introduce a canonicalization process to generate a novel canonical\n2D-3D pose mapping that ensures 2D-3D pose consistency and simplifies 2D-3D\npose patterns, enabling more efficient training of lifting networks. The\ncanonicalization of both domains is achieved through the following steps: (1)\nin the source domain, the lifting network is trained within the canonical\ndomain; (2) in the target domain, input 2D poses are canonicalized prior to\ninference by leveraging the properties of perspective projection and known\ncamera intrinsics. Consequently, the trained network can be directly applied to\nthe target domain without requiring additional fine-tuning. Experiments\nconducted with various lifting networks and publicly available datasets (e.g.,\nHuman3.6M, Fit3D, MPI-INF-3DHP) demonstrate that the proposed method\nsubstantially improves generalization capability across datasets while using\nthe same data volume.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in deep learning methods have significantly improved the\nperformance of 3D Human Pose Estimation (HPE). However, performance degradation\ncaused by domain gaps between source and target domains remains a major\nchallenge to generalization, necessitating extensive data augmentation and/or\nfine-tuning for each specific target domain. To address this issue more\nefficiently, we propose a novel canonical domain approach that maps both the\nsource and target domains into a unified canonical domain, alleviating the need\nfor additional fine-tuning in the target domain. To construct the canonical\ndomain, we introduce a canonicalization process to generate a novel canonical\n2D-3D pose mapping that ensures 2D-3D pose consistency and simplifies 2D-3D\npose patterns, enabling more efficient training of lifting networks. The\ncanonicalization of both domains is achieved through the following steps: (1)\nin the source domain, the lifting network is trained within the canonical\ndomain; (2) in the target domain, input 2D poses are canonicalized prior to\ninference by leveraging the properties of perspective projection and known\ncamera intrinsics. Consequently, the trained network can be directly applied to\nthe target domain without requiring additional fine-tuning. Experiments\nconducted with various lifting networks and publicly available datasets (e.g.,\nHuman3.6M, Fit3D, MPI-INF-3DHP) demonstrate that the proposed method\nsubstantially improves generalization capability across datasets while using\nthe same data volume."
                },
                "authors": [
                    {
                        "name": "Hoosang Lee"
                    },
                    {
                        "name": "Jeha Ryu"
                    }
                ],
                "author_detail": {
                    "name": "Jeha Ryu"
                },
                "author": "Jeha Ryu",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07921v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07921v3",
                "updated": "2025-01-27T15:39:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    39,
                    26,
                    0,
                    27,
                    0
                ],
                "published": "2024-02-28T03:20:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    3,
                    20,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Merino: Entropy-driven Design for Generative Language Models on IoT\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merino: Entropy-driven Design for Generative Language Models on IoT\n  Devices"
                },
                "summary": "Generative Large Language Models (LLMs) stand as a revolutionary advancement\nin the modern era of artificial intelligence (AI). However, scaling down LLMs\nfor resource-constrained hardware, such as Internet-of-Things (IoT) devices\nrequires non-trivial efforts and domain knowledge. In this paper, we propose a\nnovel information-entropy framework for designing mobile-friendly generative\nlanguage models. The whole design procedure involves solving a mathematical\nprogramming (MP) problem, which can be done on the CPU within minutes, making\nit nearly zero-cost. We evaluate our designed models, termed MeRino, across\nfourteen NLP downstream tasks, showing their competitive performance against\nthe state-of-the-art autoregressive transformer models under the mobile\nsetting. Notably, MeRino achieves similar or better performance on both\nlanguage modeling and zero-shot learning tasks, compared to the 350M parameter\nOPT while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) stand as a revolutionary advancement\nin the modern era of artificial intelligence (AI). However, scaling down LLMs\nfor resource-constrained hardware, such as Internet-of-Things (IoT) devices\nrequires non-trivial efforts and domain knowledge. In this paper, we propose a\nnovel information-entropy framework for designing mobile-friendly generative\nlanguage models. The whole design procedure involves solving a mathematical\nprogramming (MP) problem, which can be done on the CPU within minutes, making\nit nearly zero-cost. We evaluate our designed models, termed MeRino, across\nfourteen NLP downstream tasks, showing their competitive performance against\nthe state-of-the-art autoregressive transformer models under the mobile\nsetting. Notably, MeRino achieves similar or better performance on both\nlanguage modeling and zero-shot learning tasks, compared to the 350M parameter\nOPT while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model\nsize."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Ming Lin"
                    },
                    {
                        "name": "Huadong Tang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07921v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07921v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12239v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12239v3",
                "updated": "2025-01-27T15:30:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    30,
                    57,
                    0,
                    27,
                    0
                ],
                "published": "2024-08-22T09:16:36Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    9,
                    16,
                    36,
                    3,
                    235,
                    0
                ],
                "title": "Fast Burst-Sparsity Learning Approach for Massive MIMO-OTFS Channel\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Burst-Sparsity Learning Approach for Massive MIMO-OTFS Channel\n  Estimation"
                },
                "summary": "Accurate channel estimation in orthogonal time frequency space (OTFS) systems\nwith massive multiple-input multiple-output (MIMO) configurations is\nchallenging due to high-dimensional sparse representation (SR). Existing\nmethods often face performance degradation and/or high computational\ncomplexity. To address these issues and exploit intricate channel sparsity\nstructure, this letter first leverages a novel hybrid burst-sparsity prior to\ncapture the burst/common sparse structure in the angle/delay domain, and then\nutilizes an independent variational Bayesian inference (VBI) factorization\ntechnique to efficiently solve the high-dimensional SR problem. Additionally,\nan angle/Doppler refinement approach is incorporated into the proposed method\nto automatically mitigate off-grid mismatches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate channel estimation in orthogonal time frequency space (OTFS) systems\nwith massive multiple-input multiple-output (MIMO) configurations is\nchallenging due to high-dimensional sparse representation (SR). Existing\nmethods often face performance degradation and/or high computational\ncomplexity. To address these issues and exploit intricate channel sparsity\nstructure, this letter first leverages a novel hybrid burst-sparsity prior to\ncapture the burst/common sparse structure in the angle/delay domain, and then\nutilizes an independent variational Bayesian inference (VBI) factorization\ntechnique to efficiently solve the high-dimensional SR problem. Additionally,\nan angle/Doppler refinement approach is incorporated into the proposed method\nto automatically mitigate off-grid mismatches."
                },
                "authors": [
                    {
                        "name": "Ming Ma"
                    },
                    {
                        "name": "Jisheng Dai"
                    },
                    {
                        "name": "Xue-Qin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xue-Qin Jiang"
                },
                "author": "Xue-Qin Jiang",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12239v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12239v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16126v1",
                "updated": "2025-01-27T15:15:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    15,
                    17,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:15:17Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    15,
                    17,
                    0,
                    27,
                    0
                ],
                "title": "E-INSPIRE -- I. Bridging the gap with the local Universe: Stellar\n  population of a statistical sample of ultra-compact massive galaxies at\n  $z<0.3$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-INSPIRE -- I. Bridging the gap with the local Universe: Stellar\n  population of a statistical sample of ultra-compact massive galaxies at\n  $z<0.3$"
                },
                "summary": "This paper presents the first effort to Extend the Investigation of Stellar\nPopulations In RElics (E-INSPIRE). We present a catalogue of 430\nspectroscopically-confirmed ultra-compact massive galaxies (UCMGs) from the\nSloan Digital Sky Survey at redshifts $0.01<z<0.3$. This increases the original\nINSPIRE sample eightfold, bridging the gap with the local Universe. For each\nobject, we compute integrated stellar velocity dispersion, age, metallicity,\nand [Mg/Fe] through spectroscopic stellar population analysis. We infer star\nformation histories (SFHs), metallicity evolution histories (MEHs) and compute\nthe Degree of Relicness (DoR) of each object. The UCMGs, covering a wide range\nof DoR from 0.05 to 0.88, can be divided into three groups, according to how\nextreme their SFH was. The first group consists of 81 extreme relics\n($\\text{DoR}\\gtrsim0.6$) that have formed the totality of their stellar mass by\n$z\\sim2$ and have super-solar metallicities at all cosmic epochs. The second\ngroup ($0.3\\lesssim\\text{DoR}\\lesssim0.6$) contains 293 objects also\ncharacterised by peaked SFHs but with a small percentage of later-formed stars\nand with a variety of MEHs. The third group ($\\text{DoR}\\lesssim0.3$), has 56\nobjects that cannot be considered relics since they have extended SFHs and\nformed a non-negligible fraction ($>25\\%$) of their stellar mass at $z<2$. We\nconfirm that an efficient method of finding relics is to select UCMGs with\nlarge velocity dispersion values but we believe that the most efficient way is\nto select high velocity dispersion objects that also have super-solar\nmetallicities and high [Mg/Fe].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the first effort to Extend the Investigation of Stellar\nPopulations In RElics (E-INSPIRE). We present a catalogue of 430\nspectroscopically-confirmed ultra-compact massive galaxies (UCMGs) from the\nSloan Digital Sky Survey at redshifts $0.01<z<0.3$. This increases the original\nINSPIRE sample eightfold, bridging the gap with the local Universe. For each\nobject, we compute integrated stellar velocity dispersion, age, metallicity,\nand [Mg/Fe] through spectroscopic stellar population analysis. We infer star\nformation histories (SFHs), metallicity evolution histories (MEHs) and compute\nthe Degree of Relicness (DoR) of each object. The UCMGs, covering a wide range\nof DoR from 0.05 to 0.88, can be divided into three groups, according to how\nextreme their SFH was. The first group consists of 81 extreme relics\n($\\text{DoR}\\gtrsim0.6$) that have formed the totality of their stellar mass by\n$z\\sim2$ and have super-solar metallicities at all cosmic epochs. The second\ngroup ($0.3\\lesssim\\text{DoR}\\lesssim0.6$) contains 293 objects also\ncharacterised by peaked SFHs but with a small percentage of later-formed stars\nand with a variety of MEHs. The third group ($\\text{DoR}\\lesssim0.3$), has 56\nobjects that cannot be considered relics since they have extended SFHs and\nformed a non-negligible fraction ($>25\\%$) of their stellar mass at $z<2$. We\nconfirm that an efficient method of finding relics is to select UCMGs with\nlarge velocity dispersion values but we believe that the most efficient way is\nto select high velocity dispersion objects that also have super-solar\nmetallicities and high [Mg/Fe]."
                },
                "authors": [
                    {
                        "name": "John Mills"
                    },
                    {
                        "name": "Chiara Spiniello"
                    },
                    {
                        "name": "Alexey Sergeyev"
                    },
                    {
                        "name": "Crescenzo Tortora"
                    },
                    {
                        "name": "Vladyslav Khramtsov"
                    },
                    {
                        "name": "Giuseppe D'Ago"
                    },
                    {
                        "name": "Michalina Maksymowicz-Maciata"
                    },
                    {
                        "name": "João P. V. Benedetti"
                    },
                    {
                        "name": "Anna Ferré-Mateu"
                    },
                    {
                        "name": "Michele Cappellari"
                    },
                    {
                        "name": "Roger Davies"
                    },
                    {
                        "name": "Johanna Hartke"
                    },
                    {
                        "name": "Charles Rosen"
                    }
                ],
                "author_detail": {
                    "name": "Charles Rosen"
                },
                "author": "Charles Rosen",
                "arxiv_comment": "19 pages, 21 figures, submitted to MNRAS - minor revision requested\n  by the referee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16125v1",
                "updated": "2025-01-27T15:12:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    12,
                    27,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:12:27Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    12,
                    27,
                    0,
                    27,
                    0
                ],
                "title": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations"
                },
                "summary": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17890v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17890v3",
                "updated": "2025-01-27T15:12:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    12,
                    2,
                    0,
                    27,
                    0
                ],
                "published": "2024-05-28T07:12:06Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    7,
                    12,
                    6,
                    1,
                    149,
                    0
                ],
                "title": "SLMRec: Distilling Large Language Models into Small for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLMRec: Distilling Large Language Models into Small for Sequential\n  Recommendation"
                },
                "summary": "Sequential Recommendation (SR) task involves predicting the next item a user\nis likely to interact with, given their past interactions. The SR models\nexamine the sequence of a user's actions to discern more complex behavioral\npatterns and temporal dynamics. Recent research demonstrates the great impact\nof LLMs on sequential recommendation systems, either viewing sequential\nrecommendation as language modeling or serving as the backbone for user\nrepresentation. Although these methods deliver outstanding performance, there\nis scant evidence of the necessity of a large language model and how large the\nlanguage model is needed, especially in the sequential recommendation scene.\nMeanwhile, due to the huge size of LLMs, it is inefficient and impractical to\napply a LLM-based model in real-world platforms that often need to process\nbillions of traffic logs daily. In this paper, we explore the influence of\nLLMs' depth by conducting extensive experiments on large-scale industry\ndatasets. Surprisingly, our motivational experiments reveal that most\nintermediate layers of LLMs are redundant, indicating that pruning the\nremaining layers can still maintain strong performance. Motivated by this\ninsight, we empower small language models for SR, namely SLMRec, which adopt a\nsimple yet effective knowledge distillation method. Moreover, SLMRec is\northogonal to other post-training efficiency techniques, such as quantization\nand pruning, so that they can be leveraged in combination. Comprehensive\nexperimental results illustrate that the proposed SLMRec model attains the best\nperformance using only 13% of the parameters found in LLM-based recommendation\nmodels while simultaneously achieving up to 6.6x and 8.0x speedups in training\nand inference time costs, respectively. Besides, we provide a theoretical\njustification for why small language models can perform comparably to large\nlanguage models in SR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Recommendation (SR) task involves predicting the next item a user\nis likely to interact with, given their past interactions. The SR models\nexamine the sequence of a user's actions to discern more complex behavioral\npatterns and temporal dynamics. Recent research demonstrates the great impact\nof LLMs on sequential recommendation systems, either viewing sequential\nrecommendation as language modeling or serving as the backbone for user\nrepresentation. Although these methods deliver outstanding performance, there\nis scant evidence of the necessity of a large language model and how large the\nlanguage model is needed, especially in the sequential recommendation scene.\nMeanwhile, due to the huge size of LLMs, it is inefficient and impractical to\napply a LLM-based model in real-world platforms that often need to process\nbillions of traffic logs daily. In this paper, we explore the influence of\nLLMs' depth by conducting extensive experiments on large-scale industry\ndatasets. Surprisingly, our motivational experiments reveal that most\nintermediate layers of LLMs are redundant, indicating that pruning the\nremaining layers can still maintain strong performance. Motivated by this\ninsight, we empower small language models for SR, namely SLMRec, which adopt a\nsimple yet effective knowledge distillation method. Moreover, SLMRec is\northogonal to other post-training efficiency techniques, such as quantization\nand pruning, so that they can be leveraged in combination. Comprehensive\nexperimental results illustrate that the proposed SLMRec model attains the best\nperformance using only 13% of the parameters found in LLM-based recommendation\nmodels while simultaneously achieving up to 6.6x and 8.0x speedups in training\nand inference time costs, respectively. Besides, we provide a theoretical\njustification for why small language models can perform comparably to large\nlanguage models in SR."
                },
                "authors": [
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Qitian Wu"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Jiaojiao Han"
                    },
                    {
                        "name": "Xuying Ning"
                    },
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wenfang Lin"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "International Conference on Learning Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17890v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17890v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10994v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10994v3",
                "updated": "2025-01-27T15:00:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    0,
                    31,
                    0,
                    27,
                    0
                ],
                "published": "2024-06-24T12:09:34Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    12,
                    9,
                    34,
                    0,
                    176,
                    0
                ],
                "title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant"
                },
                "summary": "The availability of powerful open-source large language models (LLMs) opens\nexciting use cases, such as automated personal assistants that adapt to the\nuser's unique data and demands. Two key requirements for such assistants are\npersonalization - in the sense that the assistant should reflect the user's own\nwriting style - and privacy - users may prefer to always store their personal\ndata locally, on their own computing device. In this application paper, we\npresent a new design and evaluation for such an automated assistant, for the\nspecific use case of email generation, which we call Panza. Specifically, Panza\ncan be trained and deployed locally on commodity hardware, and is personalized\nto the user's writing style. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to better reflect a user's\nwriting style using limited data, while executing on extremely limited\nresources, e.g. on a free Google Colab instance. Our key methodological\ncontribution is what we believe to be the first detailed study of evaluation\nmetrics for this personalized writing task, and of how different choices of\nsystem components - e.g. the use of RAG and of different fine-tuning approaches\n- impact the system's performance. We also perform an ablation study showing\nthat less than 100 emails are generally sufficient to produce a credible Panza\nmodel. We are releasing the full Panza code as well as a new \"David\"\npersonalized email dataset licensed for research use, both available on\nhttps://github.com/IST-DASLab/PanzaMail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of powerful open-source large language models (LLMs) opens\nexciting use cases, such as automated personal assistants that adapt to the\nuser's unique data and demands. Two key requirements for such assistants are\npersonalization - in the sense that the assistant should reflect the user's own\nwriting style - and privacy - users may prefer to always store their personal\ndata locally, on their own computing device. In this application paper, we\npresent a new design and evaluation for such an automated assistant, for the\nspecific use case of email generation, which we call Panza. Specifically, Panza\ncan be trained and deployed locally on commodity hardware, and is personalized\nto the user's writing style. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to better reflect a user's\nwriting style using limited data, while executing on extremely limited\nresources, e.g. on a free Google Colab instance. Our key methodological\ncontribution is what we believe to be the first detailed study of evaluation\nmetrics for this personalized writing task, and of how different choices of\nsystem components - e.g. the use of RAG and of different fine-tuning approaches\n- impact the system's performance. We also perform an ablation study showing\nthat less than 100 emails are generally sufficient to produce a credible Panza\nmodel. We are releasing the full Panza code as well as a new \"David\"\npersonalized email dataset licensed for research use, both available on\nhttps://github.com/IST-DASLab/PanzaMail."
                },
                "authors": [
                    {
                        "name": "Armand Nicolicioiu"
                    },
                    {
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "name": "Andrej Jovanovic"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Ilia Markov"
                    },
                    {
                        "name": "Nir Shavit"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Panza is available at https://github.com/IST-DASLab/PanzaMail",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10994v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10994v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16103v1",
                "updated": "2025-01-27T14:50:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    50,
                    49,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:50:49Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    50,
                    49,
                    0,
                    27,
                    0
                ],
                "title": "Static Batching of Irregular Workloads on GPUs: Framework and\n  Application to Efficient MoE Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Batching of Irregular Workloads on GPUs: Framework and\n  Application to Efficient MoE Model Inference"
                },
                "summary": "It has long been a problem to arrange and execute irregular workloads on\nmassively parallel devices. We propose a general framework for statically\nbatching irregular workloads into a single kernel with a runtime task mapping\nmechanism on GPUs. We further apply this framework to Mixture-of-Experts (MoE)\nmodel inference and implement an optimized and efficient CUDA kernel. Our MoE\nkernel achieves up to 91% of the peak Tensor Core throughput on NVIDIA H800 GPU\nand 95% on NVIDIA H20 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has long been a problem to arrange and execute irregular workloads on\nmassively parallel devices. We propose a general framework for statically\nbatching irregular workloads into a single kernel with a runtime task mapping\nmechanism on GPUs. We further apply this framework to Mixture-of-Experts (MoE)\nmodel inference and implement an optimized and efficient CUDA kernel. Our MoE\nkernel achieves up to 91% of the peak Tensor Core throughput on NVIDIA H800 GPU\nand 95% on NVIDIA H20 GPU."
                },
                "authors": [
                    {
                        "name": "Yinghan Li"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Jiejing Zhang"
                    },
                    {
                        "name": "Bujiao Chen"
                    },
                    {
                        "name": "Xiaotong Chen"
                    },
                    {
                        "name": "Lian Duan"
                    },
                    {
                        "name": "Yejun Jin"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Xuanyu Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Wente Wang"
                    },
                    {
                        "name": "Yajie Wang"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Peiyang Zhang"
                    },
                    {
                        "name": "Laiwen Zheng"
                    },
                    {
                        "name": "Wenyuan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyuan Yu"
                },
                "author": "Wenyuan Yu",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.3; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14334v2",
                "updated": "2025-01-27T14:50:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    50,
                    32,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-24T08:58:49Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    58,
                    49,
                    4,
                    24,
                    0
                ],
                "title": "Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts"
                },
                "summary": "The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.\n  In this paper, we propose a methodology to estimate the environmental impact\nof a company's AI portfolio, providing actionable insights without\nnecessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results\nconfirm that large generative AI models consume up to 4600x more energy than\ntraditional models. Our modelling approach, which accounts for increased AI\nusage, hardware computing efficiency, and changes in electricity mix in line\nwith IPCC scenarios, forecasts AI electricity use up to 2030. Under a high\nadoption scenario, driven by widespread Generative AI and agents adoption\nassociated to increasingly complex models and frameworks, AI electricity use is\nprojected to rise by a factor of 24.4.\n  Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n\"Return on Environment\" metric to align AI development with net-zero goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.\n  In this paper, we propose a methodology to estimate the environmental impact\nof a company's AI portfolio, providing actionable insights without\nnecessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results\nconfirm that large generative AI models consume up to 4600x more energy than\ntraditional models. Our modelling approach, which accounts for increased AI\nusage, hardware computing efficiency, and changes in electricity mix in line\nwith IPCC scenarios, forecasts AI electricity use up to 2030. Under a high\nadoption scenario, driven by widespread Generative AI and agents adoption\nassociated to increasingly complex models and frameworks, AI electricity use is\nprojected to rise by a factor of 24.4.\n  Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n\"Return on Environment\" metric to align AI development with net-zero goals."
                },
                "authors": [
                    {
                        "name": "Clément Desroches"
                    },
                    {
                        "name": "Martin Chauvin"
                    },
                    {
                        "name": "Louis Ladan"
                    },
                    {
                        "name": "Caroline Vateau"
                    },
                    {
                        "name": "Simon Gosset"
                    },
                    {
                        "name": "Philippe Cordier"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Cordier"
                },
                "author": "Philippe Cordier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15100v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15100v3",
                "updated": "2025-01-27T14:49:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    49,
                    26,
                    0,
                    27,
                    0
                ],
                "published": "2024-07-21T09:32:34Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    9,
                    32,
                    34,
                    6,
                    203,
                    0
                ],
                "title": "A General Framework for Data-Use Auditing of ML Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Framework for Data-Use Auditing of ML Models"
                },
                "summary": "Auditing the use of data in training machine-learning (ML) models is an\nincreasingly pressing challenge, as myriad ML practitioners routinely leverage\nthe effort of content creators to train models without their permission. In\nthis paper, we propose a general method to audit an ML model for the use of a\ndata-owner's data in training, without prior knowledge of the ML task for which\nthe data might be used. Our method leverages any existing black-box membership\ninference method, together with a sequential hypothesis test of our own design,\nto detect data use with a quantifiable, tunable false-detection rate. We show\nthe effectiveness of our proposed framework by applying it to audit data use in\ntwo types of ML models, namely image classifiers and foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing the use of data in training machine-learning (ML) models is an\nincreasingly pressing challenge, as myriad ML practitioners routinely leverage\nthe effort of content creators to train models without their permission. In\nthis paper, we propose a general method to audit an ML model for the use of a\ndata-owner's data in training, without prior knowledge of the ML task for which\nthe data might be used. Our method leverages any existing black-box membership\ninference method, together with a sequential hypothesis test of our own design,\nto detect data use with a quantifiable, tunable false-detection rate. We show\nthe effectiveness of our proposed framework by applying it to audit data use in\ntwo types of ML models, namely image classifiers and foundation models."
                },
                "authors": [
                    {
                        "name": "Zonghao Huang"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    },
                    {
                        "name": "Michael K. Reiter"
                    }
                ],
                "author_detail": {
                    "name": "Michael K. Reiter"
                },
                "author": "Michael K. Reiter",
                "arxiv_doi": "10.1145/3658644.3690226",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690226",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.15100v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15100v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The full paper of \"A General Framework for Data-Use Auditing of ML\n  Models\" accepted by ACM CCS 2024",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16097v1",
                "updated": "2025-01-27T14:46:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    46,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:46:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    46,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Gamma rays from star clusters and implications for the origin of\n  Galactic cosmic rays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma rays from star clusters and implications for the origin of\n  Galactic cosmic rays"
                },
                "summary": "Context. Star clusters are often invoked as contributors to the flux of\nGalactic cosmic rays and as sources potentially able to accelerate particles to\n$\\sim$PeV energies. The gamma radiation with $E\\gtrsim$ TeV recently observed\nfrom selected star clusters has profound implications for the origin of\nGalactic cosmic rays.\n  Aims. We show that if the gamma rays observed from the Cygnus cocoon and\nWesterlund 1 are of hadronic origin, then the cosmic rays escaping the cluster\nat energies $\\gtrsim$ 10 TeV must cross a grammage inside the cluster that\nexceeds the Galactic grammage. At lower energies, depending on the model\nadopted to describe the production of gamma rays, such grammage may exceed or\nbe comparable with the grammage inferred from propagation on Galactic scales.\n  Methods. The flux of gamma rays is analytically computed for a few models of\ninjection of cosmic rays in star clusters, and compared with the flux measured\nfrom selected clusters.\n  Results. In all models considered here, comparing the inferred and observed\ngamma ray fluxes at $E\\gtrsim$ TeV, we conclude that CRs must traverse a large\ngrammage inside or around the cluster before escaping. Clearly these\nimplications would not apply to a scenario in which gamma rays are produced due\nto radiative losses of leptons in the cluster. Leptonic models typically\nrequire weaker magnetic fields, which in turn result in maximum energies of\naccelerated particles much below $\\sim$ PeV.\n  Conclusions. We conclude that if gamma ray emission in SCs is a generic\nphenomenon and if this radiation is due to hadronic interactions, either star\nclusters cannot contribute but a small fraction of the total cosmic ray flux at\nthe Earth, or their contribution to the grammage cannot be neglected and the\nparadigm of Galactic transport should be profoundly revisited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Star clusters are often invoked as contributors to the flux of\nGalactic cosmic rays and as sources potentially able to accelerate particles to\n$\\sim$PeV energies. The gamma radiation with $E\\gtrsim$ TeV recently observed\nfrom selected star clusters has profound implications for the origin of\nGalactic cosmic rays.\n  Aims. We show that if the gamma rays observed from the Cygnus cocoon and\nWesterlund 1 are of hadronic origin, then the cosmic rays escaping the cluster\nat energies $\\gtrsim$ 10 TeV must cross a grammage inside the cluster that\nexceeds the Galactic grammage. At lower energies, depending on the model\nadopted to describe the production of gamma rays, such grammage may exceed or\nbe comparable with the grammage inferred from propagation on Galactic scales.\n  Methods. The flux of gamma rays is analytically computed for a few models of\ninjection of cosmic rays in star clusters, and compared with the flux measured\nfrom selected clusters.\n  Results. In all models considered here, comparing the inferred and observed\ngamma ray fluxes at $E\\gtrsim$ TeV, we conclude that CRs must traverse a large\ngrammage inside or around the cluster before escaping. Clearly these\nimplications would not apply to a scenario in which gamma rays are produced due\nto radiative losses of leptons in the cluster. Leptonic models typically\nrequire weaker magnetic fields, which in turn result in maximum energies of\naccelerated particles much below $\\sim$ PeV.\n  Conclusions. We conclude that if gamma ray emission in SCs is a generic\nphenomenon and if this radiation is due to hadronic interactions, either star\nclusters cannot contribute but a small fraction of the total cosmic ray flux at\nthe Earth, or their contribution to the grammage cannot be neglected and the\nparadigm of Galactic transport should be profoundly revisited."
                },
                "authors": [
                    {
                        "name": "Pasquale Blasi"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Blasi"
                },
                "arxiv_affiliation": "GSSI",
                "author": "Pasquale Blasi",
                "arxiv_comment": "Accepted for Publication in A&A, 6 Pages, 1 Figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16093v1",
                "updated": "2025-01-27T14:41:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    41,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:41:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    41,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "STAR: Stepwise Task Augmentation and Relation Learning for Aspect\n  Sentiment Quad Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Stepwise Task Augmentation and Relation Learning for Aspect\n  Sentiment Quad Prediction"
                },
                "summary": "Aspect-based sentiment analysis (ABSA) aims to identify four sentiment\nelements, including aspect term, aspect category, opinion term, and sentiment\npolarity. These elements construct the complete picture of sentiments. The most\nchallenging task, aspect sentiment quad prediction (ASQP), predicts these\nelements simultaneously, hindered by difficulties in accurately coupling\ndifferent sentiment elements. A key challenge is insufficient annotated data\nthat limits the capability of models in semantic understanding and reasoning\nabout quad prediction. To address this, we propose stepwise task augmentation\nand relation learning (STAR), a strategy inspired by human reasoning. STAR\nconstructs auxiliary data to learn quadruple relationships incrementally by\naugmenting with pairwise and overall relation tasks derived from training data.\nBy encouraging the model to infer causal relationships among sentiment elements\nwithout requiring additional annotations, STAR effectively enhances quad\nprediction. Extensive experiments demonstrate the proposed STAR exhibits\nsuperior performance on four benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ABSA) aims to identify four sentiment\nelements, including aspect term, aspect category, opinion term, and sentiment\npolarity. These elements construct the complete picture of sentiments. The most\nchallenging task, aspect sentiment quad prediction (ASQP), predicts these\nelements simultaneously, hindered by difficulties in accurately coupling\ndifferent sentiment elements. A key challenge is insufficient annotated data\nthat limits the capability of models in semantic understanding and reasoning\nabout quad prediction. To address this, we propose stepwise task augmentation\nand relation learning (STAR), a strategy inspired by human reasoning. STAR\nconstructs auxiliary data to learn quadruple relationships incrementally by\naugmenting with pairwise and overall relation tasks derived from training data.\nBy encouraging the model to infer causal relationships among sentiment elements\nwithout requiring additional annotations, STAR effectively enhances quad\nprediction. Extensive experiments demonstrate the proposed STAR exhibits\nsuperior performance on four benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Wenna Lai"
                    },
                    {
                        "name": "Haoran Xie"
                    },
                    {
                        "name": "Guandong Xu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "8 pages, 2 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16084v1",
                "updated": "2025-01-27T14:33:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    33,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:33:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    33,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "The Shiny Scary Future of Automated Research Synthesis in HCI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Shiny Scary Future of Automated Research Synthesis in HCI"
                },
                "summary": "Automation and semi-automation through computational tools like LLMs are also\nmaking their way to deployment in research synthesis and secondary research,\nsuch as systematic reviews. In some steps of research synthesis, this has the\nopportunity to provide substantial benefits by saving time that previously was\nspent on repetitive tasks. The screening stages in particular may benefit from\ncarefully vetted computational support. However, this position paper argues for\nadditional caution when bringing in such tools to the analysis and synthesis\nphases, where human judgement and expertise should be paramount throughout the\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation and semi-automation through computational tools like LLMs are also\nmaking their way to deployment in research synthesis and secondary research,\nsuch as systematic reviews. In some steps of research synthesis, this has the\nopportunity to provide substantial benefits by saving time that previously was\nspent on repetitive tasks. The screening stages in particular may benefit from\ncarefully vetted computational support. However, this position paper argues for\nadditional caution when bringing in such tools to the analysis and synthesis\nphases, where human judgement and expertise should be paramount throughout the\nprocess."
                },
                "authors": [
                    {
                        "name": "Katja Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Katja Rogers"
                },
                "author": "Katja Rogers",
                "arxiv_comment": "Accepted at CHI 2024 workshop: \"LLMs as Research Tools: Applications\n  and Evaluations in HCI Data Work\" (https://doi.org/10.1145/3613905.3636301)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00651v2",
                "updated": "2025-01-27T14:32:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    32,
                    44,
                    0,
                    27,
                    0
                ],
                "published": "2024-02-01T15:13:13Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    15,
                    13,
                    13,
                    3,
                    32,
                    0
                ],
                "title": "Skew-elliptical copula based mixed models for non-Gaussian longitudinal\n  data with application to an HIV-AIDS study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-elliptical copula based mixed models for non-Gaussian longitudinal\n  data with application to an HIV-AIDS study"
                },
                "summary": "This study was sparked by an extensive longitudinal dataset focusing on HIV\nCD4 T$^+$ cell counts from Livingstone district, Zambia. Analysis of the\ncorresponding histogram plots reveals an absence of symmetry in the marginal\ndistributions, while pairwise scatter plots uncover non-elliptical dependence\npatterns. Traditional linear mixed models designed for longitudinal data fail\nto capture these complexities adequately. Therefore, it appears prudent to\nexplore a broader framework for modeling such data. In this article, we delve\ninto generalized linear mixed models (GLMM) for the marginals (e.g., the Gamma\nmixed model), and we address the temporal dependency of repeated measurements\nby utilizing copulas associated with skew-elliptical distributions (such as the\nskew-normal/skew-$t$). Our proposed class of copula-based mixed models\nsimultaneously accommodates asymmetry, between-subject variability, and\nnon-standard temporal dependence, thus offering extensions to the standard\nlinear mixed model based on multivariate normality. We estimate the model\nparameters using the IFM (inference function of margins) method and outline the\nprocess of obtaining standard errors for parameter estimates. Through extensive\nsimulation studies covering skewed and symmetric marginal distributions and\nvarious copula choices, we assess the finite sample performance of our\napproach. Finally, we apply these models to the HIV dataset and present our\nfindings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study was sparked by an extensive longitudinal dataset focusing on HIV\nCD4 T$^+$ cell counts from Livingstone district, Zambia. Analysis of the\ncorresponding histogram plots reveals an absence of symmetry in the marginal\ndistributions, while pairwise scatter plots uncover non-elliptical dependence\npatterns. Traditional linear mixed models designed for longitudinal data fail\nto capture these complexities adequately. Therefore, it appears prudent to\nexplore a broader framework for modeling such data. In this article, we delve\ninto generalized linear mixed models (GLMM) for the marginals (e.g., the Gamma\nmixed model), and we address the temporal dependency of repeated measurements\nby utilizing copulas associated with skew-elliptical distributions (such as the\nskew-normal/skew-$t$). Our proposed class of copula-based mixed models\nsimultaneously accommodates asymmetry, between-subject variability, and\nnon-standard temporal dependence, thus offering extensions to the standard\nlinear mixed model based on multivariate normality. We estimate the model\nparameters using the IFM (inference function of margins) method and outline the\nprocess of obtaining standard errors for parameter estimates. Through extensive\nsimulation studies covering skewed and symmetric marginal distributions and\nvarious copula choices, we assess the finite sample performance of our\napproach. Finally, we apply these models to the HIV dataset and present our\nfindings."
                },
                "authors": [
                    {
                        "name": "Subhajit Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Subhajit Chattopadhyay"
                },
                "author": "Subhajit Chattopadhyay",
                "arxiv_comment": "24 pages, 5 figures and 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16078v1",
                "updated": "2025-01-27T14:28:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    28,
                    1,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:28:01Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    28,
                    1,
                    0,
                    27,
                    0
                ],
                "title": "Integration of LLM Quality Assurance into an NLG System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of LLM Quality Assurance into an NLG System"
                },
                "summary": "In this paper, we present a system that uses a Large Language Model (LLM) to\nperform grammar and spelling correction as a component of Quality Assurance\n(QA) for texts generated by NLG systems, which is important for text production\nin real-world scenarios. Evaluating the results of the system on\nwork-in-progress sports news texts in three languages, we show that it is able\nto deliver acceptable corrections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a system that uses a Large Language Model (LLM) to\nperform grammar and spelling correction as a component of Quality Assurance\n(QA) for texts generated by NLG systems, which is important for text production\nin real-world scenarios. Evaluating the results of the system on\nwork-in-progress sports news texts in three languages, we show that it is able\nto deliver acceptable corrections."
                },
                "authors": [
                    {
                        "name": "Ching-Yi Chen"
                    },
                    {
                        "name": "Johanna Heininger"
                    },
                    {
                        "name": "Adela Schneider"
                    },
                    {
                        "name": "Christian Eckard"
                    },
                    {
                        "name": "Andreas Madsack"
                    },
                    {
                        "name": "Robert Weißgraeber"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weißgraeber"
                },
                "author": "Robert Weißgraeber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16075v1",
                "updated": "2025-01-27T14:26:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    26,
                    27,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:26:27Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    26,
                    27,
                    0,
                    27,
                    0
                ],
                "title": "PISCO: Pretty Simple Compression for Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PISCO: Pretty Simple Compression for Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models\n(LLMs) by retrieving relevant documents, but they face scalability issues due\nto high inference costs and limited context size. Document compression is a\npractical solution, but current soft compression methods suffer from accuracy\nlosses and require extensive pretraining. In this paper, we introduce PISCO, a\nnovel method that achieves a 16x compression rate with minimal accuracy loss\n(0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing\napproaches, PISCO requires no pretraining or annotated data, relying solely on\nsequence-level knowledge distillation from document-based questions. With the\nability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers\na highly efficient and scalable solution. We present comprehensive experiments\nshowing that PISCO outperforms existing compression models by 8% in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models\n(LLMs) by retrieving relevant documents, but they face scalability issues due\nto high inference costs and limited context size. Document compression is a\npractical solution, but current soft compression methods suffer from accuracy\nlosses and require extensive pretraining. In this paper, we introduce PISCO, a\nnovel method that achieves a 16x compression rate with minimal accuracy loss\n(0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing\napproaches, PISCO requires no pretraining or annotated data, relying solely on\nsequence-level knowledge distillation from document-based questions. With the\nability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers\na highly efficient and scalable solution. We present comprehensive experiments\nshowing that PISCO outperforms existing compression models by 8% in accuracy."
                },
                "authors": [
                    {
                        "name": "Maxime Louis"
                    },
                    {
                        "name": "Hervé Déjean"
                    },
                    {
                        "name": "Stéphane Clinchant"
                    }
                ],
                "author_detail": {
                    "name": "Stéphane Clinchant"
                },
                "author": "Stéphane Clinchant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16070v1",
                "updated": "2025-01-27T14:19:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    19,
                    56,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:19:56Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    19,
                    56,
                    0,
                    27,
                    0
                ],
                "title": "Generalizing Egocentric Temporal Neighborhoods to probe for spatial\n  correlations in temporal networks and infer their topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing Egocentric Temporal Neighborhoods to probe for spatial\n  correlations in temporal networks and infer their topology"
                },
                "summary": "Motifs are thought to be some fundamental components of social face-to-face\ninteraction temporal networks. However, the motifs previously considered are\neither limited to a handful of nodes and edges, or do not include triangles,\nwhich are thought to be of critical relevance to understand the dynamics of\nsocial systems. Thus, we introduce a new class of motifs, that include these\ntriangles, are not limited in their number of nodes or edges, and yet can be\nmined efficiently in any temporal network. Referring to these motifs as the\nedge-centered motifs, we show analytically how they subsume the Egocentric\nTemporal Neighborhoods motifs of [A. Longa, G. Cencetti, B. Lepri, and A.\nPasserini, An efficient procedure for mining egocentric temporal motifs, Data\nMining and Knowledge Discovery 36, 355 (2022)]. We also confirm in empirical\ndata that the edge-centered motifs bring relevant information with respect to\nthe Egocentric motifs by using a principle of maximum entropy. Then, we show\nhow mining for the edge-centered motifs in a network can be used to probe for\nspatial correlations in the underlying dynamics that have produced that\nnetwork. We deduce an approximate formula for the distribution of the\nedge-centered motifs in empirical networks of social face-to-face interactions.\nIn the last section of this paper, we explore how the statistics of the\nedge-centered motifs can be used to infer the complete topology of the network\nthey were sampled from. This leads to the needs of mathematical development,\nthat we inaugurate here under the name of graph tiling theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motifs are thought to be some fundamental components of social face-to-face\ninteraction temporal networks. However, the motifs previously considered are\neither limited to a handful of nodes and edges, or do not include triangles,\nwhich are thought to be of critical relevance to understand the dynamics of\nsocial systems. Thus, we introduce a new class of motifs, that include these\ntriangles, are not limited in their number of nodes or edges, and yet can be\nmined efficiently in any temporal network. Referring to these motifs as the\nedge-centered motifs, we show analytically how they subsume the Egocentric\nTemporal Neighborhoods motifs of [A. Longa, G. Cencetti, B. Lepri, and A.\nPasserini, An efficient procedure for mining egocentric temporal motifs, Data\nMining and Knowledge Discovery 36, 355 (2022)]. We also confirm in empirical\ndata that the edge-centered motifs bring relevant information with respect to\nthe Egocentric motifs by using a principle of maximum entropy. Then, we show\nhow mining for the edge-centered motifs in a network can be used to probe for\nspatial correlations in the underlying dynamics that have produced that\nnetwork. We deduce an approximate formula for the distribution of the\nedge-centered motifs in empirical networks of social face-to-face interactions.\nIn the last section of this paper, we explore how the statistics of the\nedge-centered motifs can be used to infer the complete topology of the network\nthey were sampled from. This leads to the needs of mathematical development,\nthat we inaugurate here under the name of graph tiling theory."
                },
                "authors": [
                    {
                        "name": "Didier Le Bail"
                    }
                ],
                "author_detail": {
                    "name": "Didier Le Bail"
                },
                "author": "Didier Le Bail",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10138v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10138v6",
                "updated": "2025-01-27T14:14:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    14,
                    59,
                    0,
                    27,
                    0
                ],
                "published": "2023-08-20T02:35:52Z",
                "published_parsed": [
                    2023,
                    8,
                    20,
                    2,
                    35,
                    52,
                    6,
                    232,
                    0
                ],
                "title": "Genuinely Robust Inference for Clustered Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genuinely Robust Inference for Clustered Data"
                },
                "summary": "Conventional cluster-robust inference methods are inconsistent when clusters\nare unignorably large. We derive a necessary and sufficient condition for\nconsistency, which is violated in 77% of empirical studies published in\nAmerican Economic Review and Econometrica (2020-2021). To address this, we\npropose two methods: (i) score subsampling, which retains the original\nestimator, and (ii) size-adjusted reweighting, which is easy to implement in\nsoftware like Stata and remains valid if the cluster size follows Zipf's law.\nSimulations confirm the reliability and uniform size control of these\napproaches, offering robust alternatives where conventional methods fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional cluster-robust inference methods are inconsistent when clusters\nare unignorably large. We derive a necessary and sufficient condition for\nconsistency, which is violated in 77% of empirical studies published in\nAmerican Economic Review and Econometrica (2020-2021). To address this, we\npropose two methods: (i) score subsampling, which retains the original\nestimator, and (ii) size-adjusted reweighting, which is easy to implement in\nsoftware like Stata and remains valid if the cluster size follows Zipf's law.\nSimulations confirm the reliability and uniform size control of these\napproaches, offering robust alternatives where conventional methods fail."
                },
                "authors": [
                    {
                        "name": "Harold D. Chiang"
                    },
                    {
                        "name": "Yuya Sasaki"
                    },
                    {
                        "name": "Yulong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Wang"
                },
                "author": "Yulong Wang",
                "arxiv_comment": "This paper supersedes the manuscripts previously circulated under the\n  titles \"On the Inconsistency of Cluster-Robust Inference and How Subsampling\n  Can Fix It\" and \"Non-Robustness of the Cluster-Robust Inference: with a\n  Proposal of a New Robust Method\" (arXiv:2210.16991)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10138v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10138v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16033v1",
                "updated": "2025-01-27T13:27:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    27,
                    4,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:27:04Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    27,
                    4,
                    0,
                    27,
                    0
                ],
                "title": "PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy\n  Assessment"
                },
                "summary": "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present PRISMe (Privacy Risk Information Scanner for Me), a novel Large\nLanguage Model (LLM)-driven privacy policy assessment tool, which helps users\nto understand the essence of a lengthy, complex privacy policy while browsing.\nThe tool, a browser extension, integrates a dashboard and an LLM chat. One\nmajor contribution is the first rigorous evaluation of such a tool. In a\nmixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,\nunderstandability of the provided information, and impacts on awareness. While\nour tool improves privacy awareness by providing a comprehensible quick\noverview and a quality chat for in-depth discussion, users note issues with\nconsistency and building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present PRISMe (Privacy Risk Information Scanner for Me), a novel Large\nLanguage Model (LLM)-driven privacy policy assessment tool, which helps users\nto understand the essence of a lengthy, complex privacy policy while browsing.\nThe tool, a browser extension, integrates a dashboard and an LLM chat. One\nmajor contribution is the first rigorous evaluation of such a tool. In a\nmixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,\nunderstandability of the provided information, and impacts on awareness. While\nour tool improves privacy awareness by providing a comprehensible quick\noverview and a quality chat for in-depth discussion, users note issues with\nconsistency and building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools."
                },
                "authors": [
                    {
                        "name": "Vincent Freiberger"
                    },
                    {
                        "name": "Arthur Fleig"
                    },
                    {
                        "name": "Erik Buchmann"
                    }
                ],
                "author_detail": {
                    "name": "Erik Buchmann"
                },
                "author": "Erik Buchmann",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.m; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20912v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20912v2",
                "updated": "2025-01-27T13:20:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    20,
                    18,
                    0,
                    27,
                    0
                ],
                "published": "2024-12-30T12:44:48Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    44,
                    48,
                    0,
                    365,
                    0
                ],
                "title": "Dynamics of Information Exchange in Zebrafish: The Role of U-Turns in\n  Visual Communication and Behavior Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamics of Information Exchange in Zebrafish: The Role of U-Turns in\n  Visual Communication and Behavior Modulation"
                },
                "summary": "Motions of visually coupled zebrafish pairs are studied to understand the\neffects of information exchange on their behavior as a function of their\nminimal separation ($d$). We find that when $d$ is small, the pair can display\na leader-follower relation (LFR) with trajectories of almost synchronized form.\nHowever, with larger $d$, although the same LFR is still maintained, the\noriginally similar trajectories turn into different forms. Detailed analysis of\ntheir motion trajectories suggests that the pair might be using U-turns (UTs)\nto exchange information and to maintain a LFR at the same time. A simulation\nmodel based on UTs with inferred and proposed rules is able to reproduce\nprominent features of observed trajectories; indicating that the transition of\ntrajectories can be understood as the result of a change in information\nexchange between the fish as $d$ increases. Our finding that UTs as important\nvisual signals is consistent with the fact that UTs can induce a large amount\nof firings in retinas of observing fish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motions of visually coupled zebrafish pairs are studied to understand the\neffects of information exchange on their behavior as a function of their\nminimal separation ($d$). We find that when $d$ is small, the pair can display\na leader-follower relation (LFR) with trajectories of almost synchronized form.\nHowever, with larger $d$, although the same LFR is still maintained, the\noriginally similar trajectories turn into different forms. Detailed analysis of\ntheir motion trajectories suggests that the pair might be using U-turns (UTs)\nto exchange information and to maintain a LFR at the same time. A simulation\nmodel based on UTs with inferred and proposed rules is able to reproduce\nprominent features of observed trajectories; indicating that the transition of\ntrajectories can be understood as the result of a change in information\nexchange between the fish as $d$ increases. Our finding that UTs as important\nvisual signals is consistent with the fact that UTs can induce a large amount\nof firings in retinas of observing fish."
                },
                "authors": [
                    {
                        "name": "C. K. Chan"
                    },
                    {
                        "name": "Hao-Yun Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Hao-Yun Hsu"
                },
                "author": "Hao-Yun Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20912v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20912v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16029v1",
                "updated": "2025-01-27T13:18:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    18,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:18:40Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    18,
                    40,
                    0,
                    27,
                    0
                ],
                "title": "FDLLM: A Text Fingerprint Detection Method for LLMs in Multi-Language,\n  Multi-Domain Black-Box Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FDLLM: A Text Fingerprint Detection Method for LLMs in Multi-Language,\n  Multi-Domain Black-Box Environments"
                },
                "summary": "Using large language models (LLMs) integration platforms without transparency\nabout which LLM is being invoked can lead to potential security risks.\nSpecifically, attackers may exploit this black-box scenario to deploy malicious\nmodels and embed viruses in the code provided to users. In this context, it is\nincreasingly urgent for users to clearly identify the LLM they are interacting\nwith, in order to avoid unknowingly becoming victims of malicious models.\nHowever, existing studies primarily focus on mixed classification of human and\nmachine-generated text, with limited attention to classifying texts generated\nsolely by different models. Current research also faces dual bottlenecks: poor\nquality of LLM-generated text (LLMGT) datasets and limited coverage of\ndetectable LLMs, resulting in poor detection performance for various LLMGT in\nblack-box scenarios. We propose the first LLMGT fingerprint detection model,\n\\textbf{FDLLM}, based on Qwen2.5-7B and fine-tuned using LoRA to address these\nchallenges. FDLLM can more efficiently handle detection tasks across\nmultilingual and multi-domain scenarios. Furthermore, we constructed a dataset\nnamed \\textbf{FD-Datasets}, consisting of 90,000 samples that span multiple\nlanguages and domains, covering 20 different LLMs. Experimental results\ndemonstrate that FDLLM achieves a macro F1 score 16.7\\% higher than the best\nbaseline method, LM-D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using large language models (LLMs) integration platforms without transparency\nabout which LLM is being invoked can lead to potential security risks.\nSpecifically, attackers may exploit this black-box scenario to deploy malicious\nmodels and embed viruses in the code provided to users. In this context, it is\nincreasingly urgent for users to clearly identify the LLM they are interacting\nwith, in order to avoid unknowingly becoming victims of malicious models.\nHowever, existing studies primarily focus on mixed classification of human and\nmachine-generated text, with limited attention to classifying texts generated\nsolely by different models. Current research also faces dual bottlenecks: poor\nquality of LLM-generated text (LLMGT) datasets and limited coverage of\ndetectable LLMs, resulting in poor detection performance for various LLMGT in\nblack-box scenarios. We propose the first LLMGT fingerprint detection model,\n\\textbf{FDLLM}, based on Qwen2.5-7B and fine-tuned using LoRA to address these\nchallenges. FDLLM can more efficiently handle detection tasks across\nmultilingual and multi-domain scenarios. Furthermore, we constructed a dataset\nnamed \\textbf{FD-Datasets}, consisting of 90,000 samples that span multiple\nlanguages and domains, covering 20 different LLMs. Experimental results\ndemonstrate that FDLLM achieves a macro F1 score 16.7\\% higher than the best\nbaseline method, LM-D."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fu"
                    },
                    {
                        "name": "Junfan Chen"
                    },
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Ting Yang"
                    },
                    {
                        "name": "Ruidong Li"
                    },
                    {
                        "name": "Yuqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Zhang"
                },
                "author": "Yuqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19058v2",
                "updated": "2025-01-27T13:09:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    9,
                    41,
                    0,
                    27,
                    0
                ],
                "published": "2024-11-28T11:17:30Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    11,
                    17,
                    30,
                    3,
                    333,
                    0
                ],
                "title": "Quality Time: Carbon-Aware Quality Adaptation for Energy-Intensive\n  Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Time: Carbon-Aware Quality Adaptation for Energy-Intensive\n  Services"
                },
                "summary": "The energy demand of modern cloud services, particularly those related to\ngenerative AI, is increasing at an unprecedented pace. While hyperscalers\ncollectively fail to meet their self-imposed emission reduction targets, they\nface increasing pressure from environmental sustainability reporting across\nmany jurisdictions. To date, carbon-aware computing strategies have primarily\nfocused on batch process scheduling or geo-distributed load balancing. However,\nsuch approaches are not applicable to services that require constant\navailability at specific locations due to latency, privacy, data, or\ninfrastructure constraints.\n  In this paper, we explore how the carbon footprint of energy-intensive\nservices can be reduced by adjusting the fraction of requests served by\ndifferent service quality tiers. We show that adapting this quality of\nresponses with respect to grid carbon intensity can lead to additional carbon\nsavings beyond resource and energy efficiency. Building on this, we introduce a\nforecast-based multi-horizon optimization that reaches close-to-optimal carbon\nsavings and is able to automatically adapt service quality for best-effort\nusers to stay within an annual carbon budget. Our approach can reduce the\nemissions of large-scale LLM services, which we estimate at multiple 10,000\ntons of CO2 annually, by up to 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The energy demand of modern cloud services, particularly those related to\ngenerative AI, is increasing at an unprecedented pace. While hyperscalers\ncollectively fail to meet their self-imposed emission reduction targets, they\nface increasing pressure from environmental sustainability reporting across\nmany jurisdictions. To date, carbon-aware computing strategies have primarily\nfocused on batch process scheduling or geo-distributed load balancing. However,\nsuch approaches are not applicable to services that require constant\navailability at specific locations due to latency, privacy, data, or\ninfrastructure constraints.\n  In this paper, we explore how the carbon footprint of energy-intensive\nservices can be reduced by adjusting the fraction of requests served by\ndifferent service quality tiers. We show that adapting this quality of\nresponses with respect to grid carbon intensity can lead to additional carbon\nsavings beyond resource and energy efficiency. Building on this, we introduce a\nforecast-based multi-horizon optimization that reaches close-to-optimal carbon\nsavings and is able to automatically adapt service quality for best-effort\nusers to stay within an annual carbon budget. Our approach can reduce the\nemissions of large-scale LLM services, which we estimate at multiple 10,000\ntons of CO2 annually, by up to 10%."
                },
                "authors": [
                    {
                        "name": "Philipp Wiesner"
                    },
                    {
                        "name": "Dennis Grinwald"
                    },
                    {
                        "name": "Philipp Weiß"
                    },
                    {
                        "name": "Patrick Wilhelm"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Odej Kao"
                    }
                ],
                "author_detail": {
                    "name": "Odej Kao"
                },
                "author": "Odej Kao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15044v2",
                "updated": "2025-01-27T12:47:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    47,
                    28,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-19T09:04:01Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    9,
                    4,
                    1,
                    5,
                    293,
                    0
                ],
                "title": "Adanonymizer: Interactively Navigating and Balancing the Duality of\n  Privacy and Output Performance in Human-LLM Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adanonymizer: Interactively Navigating and Balancing the Duality of\n  Privacy and Output Performance in Human-LLM Interaction"
                },
                "summary": "Current Large Language Models (LLMs) cannot support users to precisely\nbalance privacy protection and output performance during individual\nconsultations. We introduce Adanonymizer, an anonymization plug-in that allows\nusers to control this balance by navigating a trade-off curve. A survey (N=221)\nrevealed a privacy paradox, where users frequently disclosed sensitive\ninformation despite acknowledging privacy risks. The study further demonstrated\nthat privacy risks were not significantly correlated with model output\nperformance, highlighting the potential to navigate this trade-off.\nAdanonymizer normalizes privacy and utility ratings by type and automates the\npseudonymization of sensitive terms based on user preferences, significantly\nreducing user effort. Its 2D color palette interface visualizes the\nprivacy-utility trade-off, allowing users to adjust the balance by manipulating\na point. An evaluation (N=36) compared Adanonymizer with ablation methods and\ndifferential privacy techniques, where Adanonymizer significantly reduced\nmodification time, achieved better perceived model performance and overall user\npreference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) cannot support users to precisely\nbalance privacy protection and output performance during individual\nconsultations. We introduce Adanonymizer, an anonymization plug-in that allows\nusers to control this balance by navigating a trade-off curve. A survey (N=221)\nrevealed a privacy paradox, where users frequently disclosed sensitive\ninformation despite acknowledging privacy risks. The study further demonstrated\nthat privacy risks were not significantly correlated with model output\nperformance, highlighting the potential to navigate this trade-off.\nAdanonymizer normalizes privacy and utility ratings by type and automates the\npseudonymization of sensitive terms based on user preferences, significantly\nreducing user effort. Its 2D color palette interface visualizes the\nprivacy-utility trade-off, allowing users to adjust the balance by manipulating\na point. An evaluation (N=36) compared Adanonymizer with ablation methods and\ndifferential privacy techniques, where Adanonymizer significantly reduced\nmodification time, achieved better perceived model performance and overall user\npreference."
                },
                "authors": [
                    {
                        "name": "Shuning Zhang"
                    },
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Haobin Xing"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Yongquan Hu"
                    },
                    {
                        "name": "Hewu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hewu Li"
                },
                "author": "Hewu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16007v1",
                "updated": "2025-01-27T12:46:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    46,
                    45,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T12:46:45Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    46,
                    45,
                    0,
                    27,
                    0
                ],
                "title": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable\n  Inference"
                },
                "summary": "Large language models (LLMs) have proven to be very capable, but access to\nthe best models currently rely on inference providers which introduces trust\nchallenges -- how can we be sure that the provider is using the model\nconfiguration they claim? We propose TOPLOC, a novel method for verifiable\ninference that addresses this problem. TOPLOC leverages a compact locality\nsensitive hashing mechanism for intermediate activations which can detect\nunauthorized modifications to models, prompts, or precision with 100% accuracy,\nachieving no false positives or negatives in our empirical evaluations. Our\napproach is robust across diverse hardware configurations, GPU types, and\nalgebraic reorderings, which allows for validation speeds significantly faster\nthan the original inference. By introducing a polynomial encoding scheme,\nTOPLOC minimizes memory overhead of the generated commits by $1000\\times$,\nrequiring only 258 bytes of storage per 32 new tokens compared to the 262KB\nrequirement of storing the token embeddings directly for Llama-3.1-8B-Instruct.\nOur method empowers users to verify LLM inference computations efficiently,\nfostering greater trust and transparency in open ecosystems and lays a\nfoundation for decentralized and verifiable AI services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have proven to be very capable, but access to\nthe best models currently rely on inference providers which introduces trust\nchallenges -- how can we be sure that the provider is using the model\nconfiguration they claim? We propose TOPLOC, a novel method for verifiable\ninference that addresses this problem. TOPLOC leverages a compact locality\nsensitive hashing mechanism for intermediate activations which can detect\nunauthorized modifications to models, prompts, or precision with 100% accuracy,\nachieving no false positives or negatives in our empirical evaluations. Our\napproach is robust across diverse hardware configurations, GPU types, and\nalgebraic reorderings, which allows for validation speeds significantly faster\nthan the original inference. By introducing a polynomial encoding scheme,\nTOPLOC minimizes memory overhead of the generated commits by $1000\\times$,\nrequiring only 258 bytes of storage per 32 new tokens compared to the 262KB\nrequirement of storing the token embeddings directly for Llama-3.1-8B-Instruct.\nOur method empowers users to verify LLM inference computations efficiently,\nfostering greater trust and transparency in open ecosystems and lays a\nfoundation for decentralized and verifiable AI services."
                },
                "authors": [
                    {
                        "name": "Jack Min Ong"
                    },
                    {
                        "name": "Matthew Di Ferrante"
                    },
                    {
                        "name": "Aaron Pazdera"
                    },
                    {
                        "name": "Ryan Garner"
                    },
                    {
                        "name": "Sami Jaghouar"
                    },
                    {
                        "name": "Manveer Basra"
                    },
                    {
                        "name": "Johannes Hagemann"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Hagemann"
                },
                "author": "Johannes Hagemann",
                "arxiv_comment": "18 pages, 13 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07623v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07623v4",
                "updated": "2025-01-27T12:32:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    32,
                    8,
                    0,
                    27,
                    0
                ],
                "published": "2024-05-13T10:30:33Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    30,
                    33,
                    0,
                    134,
                    0
                ],
                "title": "COBias and Debias: Balancing Class Accuracies for Language Models in\n  Inference Time via Nonlinear Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COBias and Debias: Balancing Class Accuracies for Language Models in\n  Inference Time via Nonlinear Integer Programming"
                },
                "summary": "Large language models (LLMs) are good knowledge bases but struggle to perform\nequally well for all classes in text classification tasks. This paper\ninvestigates a fundamental inference-time problem in language models:\nimbalanced class accuracies. We find what's underneath the issue is a tendency\nto over-predict some classes while under-predicting some others. This class\naccuracy imbalance is difficult to solve from the root via better pre-training\nor fine-tuning strategies, but we show it can be effectively mitigated via\ninference-time combinatorial optimization. To this end, we conceptualize and\nquantify the over- and under-prediction issue as the Contextual Oddity Bias\n(COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP)\nmodel to correct in-context learned class probabilities based on minimizing\nCOBias and maximizing overall accuracy, without LLM parameter update.\nConsidering that the DNIP model implicitly contains non-differentiable\nelements, we therefore use the simulated annealing algorithm to solve it.\nExtensive evaluations on three LLMs across seven NLP classification tasks in\ndifferent prompting settings show that DNIP simultaneously achieves significant\nCOBias reduction (-27%) and accuracy improvement (+12%) over the conventional\nICL approach, suggesting that inference-time mitigation of class accuracy\nimbalance is a promising direction to push forward LLM performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are good knowledge bases but struggle to perform\nequally well for all classes in text classification tasks. This paper\ninvestigates a fundamental inference-time problem in language models:\nimbalanced class accuracies. We find what's underneath the issue is a tendency\nto over-predict some classes while under-predicting some others. This class\naccuracy imbalance is difficult to solve from the root via better pre-training\nor fine-tuning strategies, but we show it can be effectively mitigated via\ninference-time combinatorial optimization. To this end, we conceptualize and\nquantify the over- and under-prediction issue as the Contextual Oddity Bias\n(COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP)\nmodel to correct in-context learned class probabilities based on minimizing\nCOBias and maximizing overall accuracy, without LLM parameter update.\nConsidering that the DNIP model implicitly contains non-differentiable\nelements, we therefore use the simulated annealing algorithm to solve it.\nExtensive evaluations on three LLMs across seven NLP classification tasks in\ndifferent prompting settings show that DNIP simultaneously achieves significant\nCOBias reduction (-27%) and accuracy improvement (+12%) over the conventional\nICL approach, suggesting that inference-time mitigation of class accuracy\nimbalance is a promising direction to push forward LLM performances."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07623v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07623v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06664v2",
                "updated": "2025-01-27T12:18:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    18,
                    51,
                    0,
                    27,
                    0
                ],
                "published": "2024-07-09T08:41:13Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    8,
                    41,
                    13,
                    1,
                    191,
                    0
                ],
                "title": "PDEformer-1: A Foundation Model for One-Dimensional Partial Differential\n  Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDEformer-1: A Foundation Model for One-Dimensional Partial Differential\n  Equations"
                },
                "summary": "This paper introduces PDEformer-1, a versatile neural solver capable of\nsimultaneously addressing various partial differential equations (PDEs). With\nthe PDE represented as a computational graph, we facilitate the seamless\nintegration of symbolic and numeric information inherent in a PDE. A graph\nTransformer and an implicit neural representation (INR) are employed\nsubsequently to generate mesh-free predicted solutions. We generated a dataset\nwith up to three million samples involving diverse one-dimensional PDEs to\npretrain our model. Compared with baseline models trained specifically on\nbenchmark datasets, our pretrained model achieves comparable accuracy via\nzero-shot inference, and the advantage expands after finetuning. For PDEs new\nor unseen in the pretraining stage, our model can adapt quickly by finetuning\non a relatively small set of examples from the target equation. Additionally,\nPDEformer-1 demonstrates promising results in the inverse problem of PDE scalar\ncoefficient recovery and coefficient field recovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PDEformer-1, a versatile neural solver capable of\nsimultaneously addressing various partial differential equations (PDEs). With\nthe PDE represented as a computational graph, we facilitate the seamless\nintegration of symbolic and numeric information inherent in a PDE. A graph\nTransformer and an implicit neural representation (INR) are employed\nsubsequently to generate mesh-free predicted solutions. We generated a dataset\nwith up to three million samples involving diverse one-dimensional PDEs to\npretrain our model. Compared with baseline models trained specifically on\nbenchmark datasets, our pretrained model achieves comparable accuracy via\nzero-shot inference, and the advantage expands after finetuning. For PDEs new\nor unseen in the pretraining stage, our model can adapt quickly by finetuning\non a relatively small set of examples from the target equation. Additionally,\nPDEformer-1 demonstrates promising results in the inverse problem of PDE scalar\ncoefficient recovery and coefficient field recovery."
                },
                "authors": [
                    {
                        "name": "Zhanhong Ye"
                    },
                    {
                        "name": "Xiang Huang"
                    },
                    {
                        "name": "Leheng Chen"
                    },
                    {
                        "name": "Zining Liu"
                    },
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Hongsheng Liu"
                    },
                    {
                        "name": "Zidong Wang"
                    },
                    {
                        "name": "Bin Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dong"
                },
                "author": "Bin Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15980v1",
                "updated": "2025-01-27T12:07:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    7,
                    15,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T12:07:15Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    7,
                    15,
                    0,
                    27,
                    0
                ],
                "title": "A New Approach to Radiocarbon Summarisation: Rigorous Identification of\n  Variations/Changepoints in the Occurrence Rate of Radiocarbon Samples using a\n  Poisson Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Approach to Radiocarbon Summarisation: Rigorous Identification of\n  Variations/Changepoints in the Occurrence Rate of Radiocarbon Samples using a\n  Poisson Process"
                },
                "summary": "A commonly-used paradigm to estimate changes in the frequency of past events\nor the size of populations is to consider the occurrence rate of\narchaeological/environmental samples found at a site over time. The reliability\nof such a \"dates-as-data\" approach is highly dependent upon how the occurrence\nrates are estimated from the underlying samples, particularly when calendar age\ninformation for the samples is obtained from radiocarbon (14C). The most\nfrequently-used \"14C-dates-as-data\" approach of creating Summed Probability\nDistributions (SPDs) is not statistically valid or coherent and can provide\nhighly misleading inference. Here, we provide an alternative method with a\nrigorous statistical underpinning that also provides valuable additional\ninformation on potential changepoints in the rate of events. Our approach\nensures more reliable \"14C-dates-as-data\" analyses, allowing us to better\nassess and identify potential signals present. We model the occurrence of\nevents, each assumed to leave a radiocarbon sample in the\narchaeological/environmental record, as an inhomogeneous Poisson process. The\nvarying rate of samples over time is then estimated within a fully-Bayesian\nframework using reversible-jump Markov Chain Monte Carlo (RJ-MCMC). Given a set\nof radiocarbon samples, we reconstruct how their occurrence rate varies over\ncalendar time and identify if that rate contains statistically-significant\nchanges, i.e., specific times at which the rate of events abruptly changes. We\nillustrate our method with both a simulation study and a practical example\nconcerning late-Pleistocene megafaunal population changes in Alaska and Yukon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A commonly-used paradigm to estimate changes in the frequency of past events\nor the size of populations is to consider the occurrence rate of\narchaeological/environmental samples found at a site over time. The reliability\nof such a \"dates-as-data\" approach is highly dependent upon how the occurrence\nrates are estimated from the underlying samples, particularly when calendar age\ninformation for the samples is obtained from radiocarbon (14C). The most\nfrequently-used \"14C-dates-as-data\" approach of creating Summed Probability\nDistributions (SPDs) is not statistically valid or coherent and can provide\nhighly misleading inference. Here, we provide an alternative method with a\nrigorous statistical underpinning that also provides valuable additional\ninformation on potential changepoints in the rate of events. Our approach\nensures more reliable \"14C-dates-as-data\" analyses, allowing us to better\nassess and identify potential signals present. We model the occurrence of\nevents, each assumed to leave a radiocarbon sample in the\narchaeological/environmental record, as an inhomogeneous Poisson process. The\nvarying rate of samples over time is then estimated within a fully-Bayesian\nframework using reversible-jump Markov Chain Monte Carlo (RJ-MCMC). Given a set\nof radiocarbon samples, we reconstruct how their occurrence rate varies over\ncalendar time and identify if that rate contains statistically-significant\nchanges, i.e., specific times at which the rate of events abruptly changes. We\nillustrate our method with both a simulation study and a practical example\nconcerning late-Pleistocene megafaunal population changes in Alaska and Yukon."
                },
                "authors": [
                    {
                        "name": "Timothy J Heaton"
                    },
                    {
                        "name": "Sara Al-assam"
                    },
                    {
                        "name": "Edouard Bard"
                    }
                ],
                "author_detail": {
                    "name": "Edouard Bard"
                },
                "author": "Edouard Bard",
                "arxiv_comment": "27 pages (+ 5 for Suppl Info/Appendix), 9 Figures (+1 in Suppl.\n  Info/Appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13428v2",
                "updated": "2025-01-27T11:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    58,
                    15,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-23T07:21:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    21,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Softplus Attention with Re-weighting Boosts Length Extrapolation in\n  Large Language Models"
                },
                "summary": "Large language models have achieved remarkable success in recent years,\nprimarily due to the implementation of self-attention mechanisms. However,\ntraditional Softmax attention suffers from numerical instability and reduced\nperformance as the length of inference tokens increases. This paper addresses\nthese issues by decomposing the Softmax operation into a non-linear\ntransformation and the $l_1$-norm. We identify the latter as essential for\nmaintaining model performance. By replacing the non-linear transformation with\nthe Softplus activation function and introducing a dynamic scale factor for\ndifferent token lengths based on invariance entropy, we create a novel\nattention mechanism with performance better than conventional Softmax attention\nacross various inference lengths. To further improve the length extrapolation\nability of the proposed attention mechanism, we introduce a fine-tuning-free\nre-weighting mechanism that amplifies significant attention weights while\ndiminishing weaker ones, enabling the model to concentrate more effectively on\nrelevant tokens without requiring retraining. When combined with our proposed\nattention mechanism, this approach demonstrates significant promise in managing\nlonger sequences, maintaining nearly constant validation loss even at\n16$\\times$ the training token length while ensuring numerical stability. Our\ncode is available at: https://github.com/iminfine/freeatten.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved remarkable success in recent years,\nprimarily due to the implementation of self-attention mechanisms. However,\ntraditional Softmax attention suffers from numerical instability and reduced\nperformance as the length of inference tokens increases. This paper addresses\nthese issues by decomposing the Softmax operation into a non-linear\ntransformation and the $l_1$-norm. We identify the latter as essential for\nmaintaining model performance. By replacing the non-linear transformation with\nthe Softplus activation function and introducing a dynamic scale factor for\ndifferent token lengths based on invariance entropy, we create a novel\nattention mechanism with performance better than conventional Softmax attention\nacross various inference lengths. To further improve the length extrapolation\nability of the proposed attention mechanism, we introduce a fine-tuning-free\nre-weighting mechanism that amplifies significant attention weights while\ndiminishing weaker ones, enabling the model to concentrate more effectively on\nrelevant tokens without requiring retraining. When combined with our proposed\nattention mechanism, this approach demonstrates significant promise in managing\nlonger sequences, maintaining nearly constant validation loss even at\n16$\\times$ the training token length while ensuring numerical stability. Our\ncode is available at: https://github.com/iminfine/freeatten."
                },
                "authors": [
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Michael W. Spratling"
                    }
                ],
                "author_detail": {
                    "name": "Michael W. Spratling"
                },
                "author": "Michael W. Spratling",
                "arxiv_comment": "11 pages and 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23000v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23000v3",
                "updated": "2025-01-27T11:58:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    58,
                    0,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-30T13:29:36Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    29,
                    36,
                    2,
                    304,
                    0
                ],
                "title": "Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented\n  Generation with Key Point Recall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented\n  Generation with Key Point Recall"
                },
                "summary": "Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information."
                },
                "authors": [
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "Accepted to EMNLP'24 (Findings). Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23000v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23000v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02337v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02337v3",
                "updated": "2025-01-27T11:56:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    56,
                    15,
                    0,
                    27,
                    0
                ],
                "published": "2024-11-04T17:59:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems."
                },
                "authors": [
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Iat Long Iong"
                    },
                    {
                        "name": "Hanyu Lai"
                    },
                    {
                        "name": "Xueqiao Sun"
                    },
                    {
                        "name": "Wenyi Zhao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Xinyue Yang"
                    },
                    {
                        "name": "Jiadai Sun"
                    },
                    {
                        "name": "Shuntian Yao"
                    },
                    {
                        "name": "Tianjie Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02337v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02337v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15268v3",
                "updated": "2025-01-27T11:35:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    35,
                    4,
                    0,
                    27,
                    0
                ],
                "published": "2024-09-23T17:58:07Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    58,
                    7,
                    0,
                    267,
                    0
                ],
                "title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment\n  Benchmarking"
                },
                "summary": "The release of ChatGPT in November 2022 sparked an explosion of interest in\npost-training and an avalanche of new preference optimization (PO) methods.\nThese methods claim superior alignment by virtue of better correspondence with\nhuman pairwise preferences, often measured by LLM-judges. In this work, we\nattempt to answer the following question -- do LLM-judge preferences translate\nto progress on other, more concrete metrics for alignment, and if not, why not?\nWe define a concrete metric for alignment, and introduce SOS-Bench (Substance\nOutweighs Style Benchmark), which is to the best of our knowledge the largest\nstandardized, reproducible LLM meta-benchmark to date. We find that (1)\nLLM-judge preferences do not correlate with concrete measures of safety, world\nknowledge, and instruction following; (2) LLM-judges have powerful implicit\nbiases, prioritizing style over factuality and safety; and (3) the supervised\nfine-tuning (SFT) stage of post-training, and not the PO stage, has the\ngreatest impact on alignment, with data scaling and prompt diversity as the\ndriving factors. Our codebase and complete results can be found at\nhttps://github.com/penfever/sos-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The release of ChatGPT in November 2022 sparked an explosion of interest in\npost-training and an avalanche of new preference optimization (PO) methods.\nThese methods claim superior alignment by virtue of better correspondence with\nhuman pairwise preferences, often measured by LLM-judges. In this work, we\nattempt to answer the following question -- do LLM-judge preferences translate\nto progress on other, more concrete metrics for alignment, and if not, why not?\nWe define a concrete metric for alignment, and introduce SOS-Bench (Substance\nOutweighs Style Benchmark), which is to the best of our knowledge the largest\nstandardized, reproducible LLM meta-benchmark to date. We find that (1)\nLLM-judge preferences do not correlate with concrete measures of safety, world\nknowledge, and instruction following; (2) LLM-judges have powerful implicit\nbiases, prioritizing style over factuality and safety; and (3) the supervised\nfine-tuning (SFT) stage of post-training, and not the PO stage, has the\ngreatest impact on alignment, with data scaling and prompt diversity as the\ndriving factors. Our codebase and complete results can be found at\nhttps://github.com/penfever/sos-bench."
                },
                "authors": [
                    {
                        "name": "Benjamin Feuer"
                    },
                    {
                        "name": "Micah Goldblum"
                    },
                    {
                        "name": "Teresa Datta"
                    },
                    {
                        "name": "Sanjana Nambiar"
                    },
                    {
                        "name": "Raz Besaleli"
                    },
                    {
                        "name": "Samuel Dooley"
                    },
                    {
                        "name": "Max Cembalest"
                    },
                    {
                        "name": "John P. Dickerson"
                    }
                ],
                "author_detail": {
                    "name": "John P. Dickerson"
                },
                "author": "John P. Dickerson",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18193v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18193v3",
                "updated": "2025-01-27T11:32:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    32,
                    51,
                    0,
                    27,
                    0
                ],
                "published": "2024-09-26T18:10:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    10,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource\n  Languages Injected with Multilingual Graph Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource\n  Languages Injected with Multilingual Graph Knowledge"
                },
                "summary": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Using LLMs for such languages is often difficult due to a high\ncomputational cost; not only during training, but also during inference. Static\nword embeddings are much more resource-efficient (\"green\"), and thus still\nprovide value, particularly for very low-resource languages. There is, however,\na notable lack of comprehensive repositories with such embeddings for diverse\nlanguages. To address this gap, we present GrEmLIn, a centralized repository of\ngreen, static baseline embeddings for 87 mid- and low-resource languages. We\ncompute GrEmLIn embeddings with a novel method that enhances GloVe embeddings\nby integrating multilingual graph knowledge, which makes our static embeddings\ncompetitive with LLM representations, while being parameter-free at inference\ntime. Our experiments demonstrate that GrEmLIn embeddings outperform\nstate-of-the-art contextualized embeddings from E5 on the task of lexical\nsimilarity. They remain competitive in extrinsic evaluation tasks like\nsentiment analysis and natural language inference, with average performance\ngaps of just 5-10\\% or less compared to state-of-the-art models, given a\nsufficient vocabulary overlap with the target task, and underperform only on\ntopic classification. Our code and embeddings are publicly available at\nhttps://huggingface.co/DFKI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Using LLMs for such languages is often difficult due to a high\ncomputational cost; not only during training, but also during inference. Static\nword embeddings are much more resource-efficient (\"green\"), and thus still\nprovide value, particularly for very low-resource languages. There is, however,\na notable lack of comprehensive repositories with such embeddings for diverse\nlanguages. To address this gap, we present GrEmLIn, a centralized repository of\ngreen, static baseline embeddings for 87 mid- and low-resource languages. We\ncompute GrEmLIn embeddings with a novel method that enhances GloVe embeddings\nby integrating multilingual graph knowledge, which makes our static embeddings\ncompetitive with LLM representations, while being parameter-free at inference\ntime. Our experiments demonstrate that GrEmLIn embeddings outperform\nstate-of-the-art contextualized embeddings from E5 on the task of lexical\nsimilarity. They remain competitive in extrinsic evaluation tasks like\nsentiment analysis and natural language inference, with average performance\ngaps of just 5-10\\% or less compared to state-of-the-art models, given a\nsufficient vocabulary overlap with the target task, and underperform only on\ntopic classification. Our code and embeddings are publicly available at\nhttps://huggingface.co/DFKI."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Rishu Kumar"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Long paper, accepted to NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18193v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15969v1",
                "updated": "2025-01-27T11:26:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    26,
                    54,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T11:26:54Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    26,
                    54,
                    0,
                    27,
                    0
                ],
                "title": "An Explainable Disease Surveillance System for Early Prediction of\n  Multiple Chronic Diseases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Explainable Disease Surveillance System for Early Prediction of\n  Multiple Chronic Diseases"
                },
                "summary": "This study addresses a critical gap in the healthcare system by developing a\nclinically meaningful, practical, and explainable disease surveillance system\nfor multiple chronic diseases, utilizing routine EHR data from multiple U.S.\npractices integrated with CureMD's EMR/EHR system. Unlike traditional\nsystems--using AI models that rely on features from patients' labs--our\napproach focuses on routinely available data, such as medical history, vitals,\ndiagnoses, and medications, to preemptively assess the risks of chronic\ndiseases in the next year. We trained three distinct models for each chronic\ndisease: prediction models that forecast the risk of a disease 3, 6, and 12\nmonths before a potential diagnosis. We developed Random Forest models, which\nwere internally validated using F1 scores and AUROC as performance metrics and\nfurther evaluated by a panel of expert physicians for clinical relevance based\non inferences grounded in medical knowledge. Additionally, we discuss our\nimplementation of integrating these models into a practical EMR system. Beyond\nusing Shapley attributes and surrogate models for explainability, we also\nintroduce a new rule-engineering framework to enhance the intrinsic\nexplainability of Random Forests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses a critical gap in the healthcare system by developing a\nclinically meaningful, practical, and explainable disease surveillance system\nfor multiple chronic diseases, utilizing routine EHR data from multiple U.S.\npractices integrated with CureMD's EMR/EHR system. Unlike traditional\nsystems--using AI models that rely on features from patients' labs--our\napproach focuses on routinely available data, such as medical history, vitals,\ndiagnoses, and medications, to preemptively assess the risks of chronic\ndiseases in the next year. We trained three distinct models for each chronic\ndisease: prediction models that forecast the risk of a disease 3, 6, and 12\nmonths before a potential diagnosis. We developed Random Forest models, which\nwere internally validated using F1 scores and AUROC as performance metrics and\nfurther evaluated by a panel of expert physicians for clinical relevance based\non inferences grounded in medical knowledge. Additionally, we discuss our\nimplementation of integrating these models into a practical EMR system. Beyond\nusing Shapley attributes and surrogate models for explainability, we also\nintroduce a new rule-engineering framework to enhance the intrinsic\nexplainability of Random Forests."
                },
                "authors": [
                    {
                        "name": "Shaheer Ahmad Khan"
                    },
                    {
                        "name": "Muhammad Usamah Shahid"
                    },
                    {
                        "name": "Ahmad Abdullah"
                    },
                    {
                        "name": "Ibrahim Hashmat"
                    },
                    {
                        "name": "Muddassar Farooq"
                    }
                ],
                "author_detail": {
                    "name": "Muddassar Farooq"
                },
                "author": "Muddassar Farooq",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14908v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14908v4",
                "updated": "2025-01-27T11:25:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    25,
                    33,
                    0,
                    27,
                    0
                ],
                "published": "2024-05-23T09:44:02Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    44,
                    2,
                    3,
                    144,
                    0
                ],
                "title": "BiMix: A Bivariate Data Mixing Law for Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiMix: A Bivariate Data Mixing Law for Language Model Pretraining"
                },
                "summary": "Large language models have demonstrated remarkable capabilities across\nvarious tasks, primarily attributed to the utilization of diversely sourced\ndata. However, the impact of pretraining data composition on model performance\nremains poorly understood. This paper introduces $\\textbf{BiMix}$, a novel\nbivariate data mixing law that models the joint scaling behavior of domain\nproportions and data volume in LLM pretraining. $\\textbf{BiMix}$ provides a\nsystematic framework for understanding and optimizing data mixtures across\ndiverse domains. Through extensive experiments on two large-scale datasets, we\ndemonstrate $\\textbf{BiMix}$'s high accuracy in loss extrapolation (mean\nrelative error < 0.2%) and its generalization to unseen mixtures (R${}^{2}$ >\n0.97). Optimization of domain proportions yields superior model performance\ncompared to existing methods. Furthermore, we establish entropy-based measures\nas efficient proxies for data mixing, offering a computationally lightweight\nstrategy. Our work contributes both theoretical insights into data mixing\ndynamics and practical tools for enhancing LLM training efficiency, paving the\nway for more effective scaling strategies in language model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable capabilities across\nvarious tasks, primarily attributed to the utilization of diversely sourced\ndata. However, the impact of pretraining data composition on model performance\nremains poorly understood. This paper introduces $\\textbf{BiMix}$, a novel\nbivariate data mixing law that models the joint scaling behavior of domain\nproportions and data volume in LLM pretraining. $\\textbf{BiMix}$ provides a\nsystematic framework for understanding and optimizing data mixtures across\ndiverse domains. Through extensive experiments on two large-scale datasets, we\ndemonstrate $\\textbf{BiMix}$'s high accuracy in loss extrapolation (mean\nrelative error < 0.2%) and its generalization to unseen mixtures (R${}^{2}$ >\n0.97). Optimization of domain proportions yields superior model performance\ncompared to existing methods. Furthermore, we establish entropy-based measures\nas efficient proxies for data mixing, offering a computationally lightweight\nstrategy. Our work contributes both theoretical insights into data mixing\ndynamics and practical tools for enhancing LLM training efficiency, paving the\nway for more effective scaling strategies in language model development."
                },
                "authors": [
                    {
                        "name": "Ce Ge"
                    },
                    {
                        "name": "Zhijian Ma"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "arxiv_comment": "Clarify details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14908v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14908v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15953v1",
                "updated": "2025-01-27T10:57:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    57,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T10:57:24Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    57,
                    24,
                    0,
                    27,
                    0
                ],
                "title": "Understanding Long Videos via LLM-Powered Entity Relation Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Long Videos via LLM-Powered Entity Relation Graphs"
                },
                "summary": "The analysis of extended video content poses unique challenges in artificial\nintelligence, particularly when dealing with the complexity of tracking and\nunderstanding visual elements across time. Current methodologies that process\nvideo frames sequentially struggle to maintain coherent tracking of objects,\nespecially when these objects temporarily vanish and later reappear in the\nfootage. A critical limitation of these approaches is their inability to\neffectively identify crucial moments in the video, largely due to their limited\ngrasp of temporal relationships. To overcome these obstacles, we present\nGraphVideoAgent, a cutting-edge system that leverages the power of graph-based\nobject tracking in conjunction with large language model capabilities. At its\ncore, our framework employs a dynamic graph structure that maps and monitors\nthe evolving relationships between visual entities throughout the video\nsequence. This innovative approach enables more nuanced understanding of how\nobjects interact and transform over time, facilitating improved frame selection\nthrough comprehensive contextual awareness. Our approach demonstrates\nremarkable effectiveness when tested against industry benchmarks. In\nevaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2\nimprovement over existing methods while requiring analysis of only 8.2 frames\non average. Similarly, testing on the NExT-QA benchmark yielded a 2.0\nperformance increase with an average frame requirement of 8.1. These results\nunderscore the efficiency of our graph-guided methodology in enhancing both\naccuracy and computational performance in long-form video understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of extended video content poses unique challenges in artificial\nintelligence, particularly when dealing with the complexity of tracking and\nunderstanding visual elements across time. Current methodologies that process\nvideo frames sequentially struggle to maintain coherent tracking of objects,\nespecially when these objects temporarily vanish and later reappear in the\nfootage. A critical limitation of these approaches is their inability to\neffectively identify crucial moments in the video, largely due to their limited\ngrasp of temporal relationships. To overcome these obstacles, we present\nGraphVideoAgent, a cutting-edge system that leverages the power of graph-based\nobject tracking in conjunction with large language model capabilities. At its\ncore, our framework employs a dynamic graph structure that maps and monitors\nthe evolving relationships between visual entities throughout the video\nsequence. This innovative approach enables more nuanced understanding of how\nobjects interact and transform over time, facilitating improved frame selection\nthrough comprehensive contextual awareness. Our approach demonstrates\nremarkable effectiveness when tested against industry benchmarks. In\nevaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2\nimprovement over existing methods while requiring analysis of only 8.2 frames\non average. Similarly, testing on the NExT-QA benchmark yielded a 2.0\nperformance increase with an average frame requirement of 8.1. These results\nunderscore the efficiency of our graph-guided methodology in enhancing both\naccuracy and computational performance in long-form video understanding tasks."
                },
                "authors": [
                    {
                        "name": "Meng Chu"
                    },
                    {
                        "name": "Yicong Li"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15945v1",
                "updated": "2025-01-27T10:47:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    47,
                    2,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T10:47:02Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    47,
                    2,
                    0,
                    27,
                    0
                ],
                "title": "Isotropic randomization for one-sample testing in metric spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isotropic randomization for one-sample testing in metric spaces"
                },
                "summary": "We address the problem of testing hypotheses about a specific value of the\nFr\\'echet mean in metric spaces, extending classical mean testing from\nEuclidean spaces to more general settings. We extend an Euclidean testing\nprocedure progresively, starting with test construction in Riemannian\nmanifolds, leveraging their natural geometric structure through exponential and\nlogarithm maps, and then extend to general metric spaces through the\nintroduction of admissible randomization techniques. This approach preserves\nessential geometric properties required for valid statistical inference while\nmaintaining broad applicability. We establish theoretical guarantees for our\ntesting procedure and demonstrate its effectiveness through numerical\nexperiments across different metric spaces and distributional settings. The\npractical utility of our method is further illustrated through an application\nto wind data in western Denmark, showcasing its relevance for real-world\nstatistical analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of testing hypotheses about a specific value of the\nFr\\'echet mean in metric spaces, extending classical mean testing from\nEuclidean spaces to more general settings. We extend an Euclidean testing\nprocedure progresively, starting with test construction in Riemannian\nmanifolds, leveraging their natural geometric structure through exponential and\nlogarithm maps, and then extend to general metric spaces through the\nintroduction of admissible randomization techniques. This approach preserves\nessential geometric properties required for valid statistical inference while\nmaintaining broad applicability. We establish theoretical guarantees for our\ntesting procedure and demonstrate its effectiveness through numerical\nexperiments across different metric spaces and distributional settings. The\npractical utility of our method is further illustrated through an application\nto wind data in western Denmark, showcasing its relevance for real-world\nstatistical analysis."
                },
                "authors": [
                    {
                        "name": "Matthieu Bulté"
                    },
                    {
                        "name": "Helle Sørensen"
                    }
                ],
                "author_detail": {
                    "name": "Helle Sørensen"
                },
                "author": "Helle Sørensen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18721v2",
                "updated": "2025-01-27T10:41:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    41,
                    15,
                    0,
                    27,
                    0
                ],
                "published": "2024-12-25T00:41:43Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    0,
                    41,
                    43,
                    2,
                    360,
                    0
                ],
                "title": "LHC-friendly freeze-in dark matter via Higgs portal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LHC-friendly freeze-in dark matter via Higgs portal"
                },
                "summary": "It is known that single-field freeze-in dark matter barely leaves footprints\nin dark matter direct detection and collider experiments. This situation can be\naltered in two-field context. In this work we propose a two-field freeze-in\ndark matter model through Higgs portal. The observed dark matter relic\nabundance is obtained by a decay of scalar mediator thermalized in the early\nUniverse. While there is a lack of direct dark matter signals, the scalar\nmediator is in the reach of HL-LHC either through vector boson fusion or Mono-Z\nchannel. Within allowed scalar mass window of 10-50 GeV, we use improved cuts\nto derive both $2\\sigma$ exclusion and $5\\sigma$ discovery limits, depending on\nthe value of Higgs portal coupling. If verified, this scalar mediator signal\nallows us to infer the freeze-in dark matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is known that single-field freeze-in dark matter barely leaves footprints\nin dark matter direct detection and collider experiments. This situation can be\naltered in two-field context. In this work we propose a two-field freeze-in\ndark matter model through Higgs portal. The observed dark matter relic\nabundance is obtained by a decay of scalar mediator thermalized in the early\nUniverse. While there is a lack of direct dark matter signals, the scalar\nmediator is in the reach of HL-LHC either through vector boson fusion or Mono-Z\nchannel. Within allowed scalar mass window of 10-50 GeV, we use improved cuts\nto derive both $2\\sigma$ exclusion and $5\\sigma$ discovery limits, depending on\nthe value of Higgs portal coupling. If verified, this scalar mediator signal\nallows us to infer the freeze-in dark matter."
                },
                "authors": [
                    {
                        "name": "Xinyue Yin"
                    },
                    {
                        "name": "Shuai Xu"
                    },
                    {
                        "name": "Sibo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Sibo Zheng"
                },
                "author": "Sibo Zheng",
                "arxiv_comment": "16 pages, 8 figures; v2: 17 pp, minor corrections and references\n  added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12791v2",
                "updated": "2025-01-27T10:40:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    40,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-12-17T10:52:50Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    52,
                    50,
                    1,
                    352,
                    0
                ],
                "title": "Implicit Location-Caption Alignment via Complementary Masking for\n  Weakly-Supervised Dense Video Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Location-Caption Alignment via Complementary Masking for\n  Weakly-Supervised Dense Video Captioning"
                },
                "summary": "Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods."
                },
                "authors": [
                    {
                        "name": "Shiping Ge"
                    },
                    {
                        "name": "Qiang Chen"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Yafeng Yin"
                    },
                    {
                        "name": "Liu Qin"
                    },
                    {
                        "name": "Ziyao Chen"
                    },
                    {
                        "name": "Qing Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qing Gu"
                },
                "author": "Qing Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15925v1",
                "updated": "2025-01-27T10:22:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    22,
                    38,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T10:22:38Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    22,
                    38,
                    0,
                    27,
                    0
                ],
                "title": "Efficient Distillation of Deep Spiking Neural Networks for Full-Range\n  Timestep Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distillation of Deep Spiking Neural Networks for Full-Range\n  Timestep Deployment"
                },
                "summary": "Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative\nto traditional Artificial Neural Networks (ANNs), prized for their potential\nenergy efficiency on neuromorphic hardware. Despite this, SNNs often suffer\nfrom accuracy degradation compared to ANNs and face deployment challenges due\nto fixed inference timesteps, which require retraining for adjustments,\nlimiting operational flexibility. To address these issues, our work considers\nthe spatio-temporal property inherent in SNNs, and proposes a novel\ndistillation framework for deep SNNs that optimizes performance across\nfull-range timesteps without specific retraining, enhancing both efficacy and\ndeployment adaptability. We provide both theoretical analysis and empirical\nvalidations to illustrate that training guarantees the convergence of all\nimplicit models across full-range timesteps. Experimental results on CIFAR-10,\nCIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance\namong distillation-based SNNs training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative\nto traditional Artificial Neural Networks (ANNs), prized for their potential\nenergy efficiency on neuromorphic hardware. Despite this, SNNs often suffer\nfrom accuracy degradation compared to ANNs and face deployment challenges due\nto fixed inference timesteps, which require retraining for adjustments,\nlimiting operational flexibility. To address these issues, our work considers\nthe spatio-temporal property inherent in SNNs, and proposes a novel\ndistillation framework for deep SNNs that optimizes performance across\nfull-range timesteps without specific retraining, enhancing both efficacy and\ndeployment adaptability. We provide both theoretical analysis and empirical\nvalidations to illustrate that training guarantees the convergence of all\nimplicit models across full-range timesteps. Experimental results on CIFAR-10,\nCIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance\namong distillation-based SNNs training methods."
                },
                "authors": [
                    {
                        "name": "Chengting Yu"
                    },
                    {
                        "name": "Xiaochen Zhao"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Gaoang Wang"
                    },
                    {
                        "name": "Erping Li"
                    },
                    {
                        "name": "Aili Wang"
                    }
                ],
                "author_detail": {
                    "name": "Aili Wang"
                },
                "author": "Aili Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13187v2",
                "updated": "2025-01-27T10:19:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    19,
                    44,
                    0,
                    27,
                    0
                ],
                "published": "2024-03-19T22:56:53Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    22,
                    56,
                    53,
                    1,
                    79,
                    0
                ],
                "title": "Evolutionary Optimization of Model Merging Recipes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Optimization of Model Merging Recipes"
                },
                "summary": "Large language models (LLMs) have become increasingly capable, but their\ndevelopment often requires substantial computational resources. While model\nmerging has emerged as a cost-effective promising approach for creating new\nmodels by combining existing ones, it currently relies on human intuition and\ndomain knowledge, limiting its potential. Here, we propose an evolutionary\napproach that overcomes this limitation by automatically discovering effective\ncombinations of diverse open-source models, harnessing their collective\nintelligence without requiring extensive additional training data or compute.\nOur approach operates in both parameter space and data flow space, allowing for\noptimization beyond just the weights of the individual models. This approach\neven facilitates cross-domain merging, generating models like a Japanese LLM\nwith Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved\nstate-of-the-art performance on a variety of established Japanese LLM\nbenchmarks, even surpassing models with significantly more parameters, despite\nnot being explicitly trained for such tasks. Furthermore, a culturally-aware\nJapanese VLM generated through our approach demonstrates its effectiveness in\ndescribing Japanese culture-specific content, outperforming previous Japanese\nVLMs. This work not only contributes new state-of-the-art models back to the\nopen-source community, but also introduces a new paradigm for automated model\ncomposition, paving the way for exploring alternative, efficient approaches to\nfoundation model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become increasingly capable, but their\ndevelopment often requires substantial computational resources. While model\nmerging has emerged as a cost-effective promising approach for creating new\nmodels by combining existing ones, it currently relies on human intuition and\ndomain knowledge, limiting its potential. Here, we propose an evolutionary\napproach that overcomes this limitation by automatically discovering effective\ncombinations of diverse open-source models, harnessing their collective\nintelligence without requiring extensive additional training data or compute.\nOur approach operates in both parameter space and data flow space, allowing for\noptimization beyond just the weights of the individual models. This approach\neven facilitates cross-domain merging, generating models like a Japanese LLM\nwith Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved\nstate-of-the-art performance on a variety of established Japanese LLM\nbenchmarks, even surpassing models with significantly more parameters, despite\nnot being explicitly trained for such tasks. Furthermore, a culturally-aware\nJapanese VLM generated through our approach demonstrates its effectiveness in\ndescribing Japanese culture-specific content, outperforming previous Japanese\nVLMs. This work not only contributes new state-of-the-art models back to the\nopen-source community, but also introduces a new paradigm for automated model\ncomposition, paving the way for exploring alternative, efficient approaches to\nfoundation model development."
                },
                "authors": [
                    {
                        "name": "Takuya Akiba"
                    },
                    {
                        "name": "Makoto Shing"
                    },
                    {
                        "name": "Yujin Tang"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "David Ha"
                    }
                ],
                "author_detail": {
                    "name": "David Ha"
                },
                "author": "David Ha",
                "arxiv_doi": "10.1038/s42256-024-00975-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s42256-024-00975-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.13187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Authors' submitted version before final edits. Published in Nature\n  Machine Intelligence on January 27, 2025:\n  https://www.nature.com/articles/s42256-024-00975-8",
                "arxiv_journal_ref": "Nat Mach Intell (2025)",
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15922v1",
                "updated": "2025-01-27T10:17:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    17,
                    38,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T10:17:38Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    17,
                    38,
                    0,
                    27,
                    0
                ],
                "title": "SkillScope: A Tool to Predict Fine-Grained Skills Needed to Solve Issues\n  on GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkillScope: A Tool to Predict Fine-Grained Skills Needed to Solve Issues\n  on GitHub"
                },
                "summary": "New contributors often struggle to find tasks that they can tackle when\nonboarding onto a new Open Source Software (OSS) project. One reason for this\ndifficulty is that issue trackers lack explanations about the knowledge or\nskills needed to complete a given task successfully. These explanations can be\ncomplex and time-consuming to produce. Past research has partially addressed\nthis problem by labeling issues with issue types, issue difficulty level, and\nissue skills. However, current approaches are limited to a small set of labels\nand lack in-depth details about their semantics, which may not sufficiently\nhelp contributors identify suitable issues. To surmount this limitation, this\npaper explores large language models (LLMs) and Random Forest (RF) to predict\nthe multilevel skills required to solve the open issues. We introduce a novel\ntool, SkillScope, which retrieves current issues from Java projects hosted on\nGitHub and predicts the multilevel programming skills required to resolve these\nissues. In a case study, we demonstrate that SkillScope could predict 217\nmultilevel skills for tasks with 91% precision, 88% recall, and 89% F-measure\non average. Practitioners can use this tool to better delegate or choose tasks\nto solve in OSS projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New contributors often struggle to find tasks that they can tackle when\nonboarding onto a new Open Source Software (OSS) project. One reason for this\ndifficulty is that issue trackers lack explanations about the knowledge or\nskills needed to complete a given task successfully. These explanations can be\ncomplex and time-consuming to produce. Past research has partially addressed\nthis problem by labeling issues with issue types, issue difficulty level, and\nissue skills. However, current approaches are limited to a small set of labels\nand lack in-depth details about their semantics, which may not sufficiently\nhelp contributors identify suitable issues. To surmount this limitation, this\npaper explores large language models (LLMs) and Random Forest (RF) to predict\nthe multilevel skills required to solve the open issues. We introduce a novel\ntool, SkillScope, which retrieves current issues from Java projects hosted on\nGitHub and predicts the multilevel programming skills required to resolve these\nissues. In a case study, we demonstrate that SkillScope could predict 217\nmultilevel skills for tasks with 91% precision, 88% recall, and 89% F-measure\non average. Practitioners can use this tool to better delegate or choose tasks\nto solve in OSS projects."
                },
                "authors": [
                    {
                        "name": "Benjamin C. Carter"
                    },
                    {
                        "name": "Jonathan Rivas Contreras"
                    },
                    {
                        "name": "Carlos A. Llanes Villegas"
                    },
                    {
                        "name": "Pawan Acharya"
                    },
                    {
                        "name": "Jack Utzerath"
                    },
                    {
                        "name": "Adonijah O. Farner"
                    },
                    {
                        "name": "Hunter Jenkins"
                    },
                    {
                        "name": "Dylan Johnson"
                    },
                    {
                        "name": "Jacob Penney"
                    },
                    {
                        "name": "Igor Steinmacher"
                    },
                    {
                        "name": "Marco A. Gerosa"
                    },
                    {
                        "name": "Fabio Santos"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Santos"
                },
                "author": "Fabio Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15915v1",
                "updated": "2025-01-27T10:04:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    4,
                    49,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T10:04:49Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    4,
                    49,
                    0,
                    27,
                    0
                ],
                "title": "Parametric Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parametric Retrieval Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) techniques have emerged as a promising\nsolution to enhance the reliability of large language models (LLMs) by\naddressing issues like hallucinations, outdated knowledge, and domain\nadaptation. In particular, existing RAG methods append relevant documents\nretrieved from external corpus or databases to the input of LLMs to guide their\ngeneration process, which we refer to as the in-context knowledge injection\nmethod. While this approach is simple and often effective, it has inherent\nlimitations. Firstly, increasing the context length and number of relevant\ndocuments can lead to higher computational overhead and degraded performance,\nespecially in complex reasoning tasks. More importantly, in-context knowledge\ninjection operates primarily at the input level, but LLMs store their internal\nknowledge in their parameters. This gap fundamentally limits the capacity of\nin-context methods. To this end, we introduce Parametric retrieval-augmented\ngeneration (Parametric RAG), a new RAG paradigm that integrates external\nknowledge directly into the parameters of feed-forward networks (FFN) of an LLM\nthrough document parameterization. This approach not only saves online\ncomputational costs by eliminating the need to inject multiple documents into\nthe LLMs' input context, but also deepens the integration of external knowledge\ninto the parametric knowledge space of the LLM. Experimental results\ndemonstrate that Parametric RAG substantially enhances both the effectiveness\nand efficiency of knowledge augmentation in LLMs. Also, it can be combined with\nin-context RAG methods to achieve even better performance.\n  We have open-sourced all the code, data, and models in the following\nanonymized GitHub link: https://github.com/oneal2000/PRAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) techniques have emerged as a promising\nsolution to enhance the reliability of large language models (LLMs) by\naddressing issues like hallucinations, outdated knowledge, and domain\nadaptation. In particular, existing RAG methods append relevant documents\nretrieved from external corpus or databases to the input of LLMs to guide their\ngeneration process, which we refer to as the in-context knowledge injection\nmethod. While this approach is simple and often effective, it has inherent\nlimitations. Firstly, increasing the context length and number of relevant\ndocuments can lead to higher computational overhead and degraded performance,\nespecially in complex reasoning tasks. More importantly, in-context knowledge\ninjection operates primarily at the input level, but LLMs store their internal\nknowledge in their parameters. This gap fundamentally limits the capacity of\nin-context methods. To this end, we introduce Parametric retrieval-augmented\ngeneration (Parametric RAG), a new RAG paradigm that integrates external\nknowledge directly into the parameters of feed-forward networks (FFN) of an LLM\nthrough document parameterization. This approach not only saves online\ncomputational costs by eliminating the need to inject multiple documents into\nthe LLMs' input context, but also deepens the integration of external knowledge\ninto the parametric knowledge space of the LLM. Experimental results\ndemonstrate that Parametric RAG substantially enhances both the effectiveness\nand efficiency of knowledge augmentation in LLMs. Also, it can be combined with\nin-context RAG methods to achieve even better performance.\n  We have open-sourced all the code, data, and models in the following\nanonymized GitHub link: https://github.com/oneal2000/PRAG"
                },
                "authors": [
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Yichen Tang"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Junxi Yan"
                    },
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Ziyi Ye"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12030v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12030v3",
                "updated": "2025-01-27T10:02:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    2,
                    44,
                    0,
                    27,
                    0
                ],
                "published": "2024-02-19T10:37:29Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    10,
                    37,
                    29,
                    0,
                    50,
                    0
                ],
                "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation\n  Loss for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation\n  Loss for LLMs"
                },
                "summary": "Deploying large language models (LLMs) of several billion parameters can be\nimpractical in most industrial use cases due to constraints such as cost,\nlatency limitations, and hardware accessibility. Knowledge distillation (KD)\noffers a solution by compressing knowledge from resource-intensive large models\nto smaller ones. Various strategies exist, some relying on the text generated\nby the teacher model and optionally utilizing his logits to enhance learning.\nHowever, these methods based on logits often require both teacher and student\nmodels to share the same tokenizer, limiting their applicability across\ndifferent LLM families. In this paper, we introduce Universal Logit\nDistillation (ULD) loss, grounded in optimal transport, to address this\nlimitation. Our experimental results demonstrate the effectiveness of ULD loss\nin enabling distillation across models with different architectures and\ntokenizers, paving the way to a more widespread use of distillation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) of several billion parameters can be\nimpractical in most industrial use cases due to constraints such as cost,\nlatency limitations, and hardware accessibility. Knowledge distillation (KD)\noffers a solution by compressing knowledge from resource-intensive large models\nto smaller ones. Various strategies exist, some relying on the text generated\nby the teacher model and optionally utilizing his logits to enhance learning.\nHowever, these methods based on logits often require both teacher and student\nmodels to share the same tokenizer, limiting their applicability across\ndifferent LLM families. In this paper, we introduce Universal Logit\nDistillation (ULD) loss, grounded in optimal transport, to address this\nlimitation. Our experimental results demonstrate the effectiveness of ULD loss\nin enabling distillation across models with different architectures and\ntokenizers, paving the way to a more widespread use of distillation techniques."
                },
                "authors": [
                    {
                        "name": "Nicolas Boizard"
                    },
                    {
                        "name": "Kevin El Haddad"
                    },
                    {
                        "name": "Céline Hudelot"
                    },
                    {
                        "name": "Pierre Colombo"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Colombo"
                },
                "author": "Pierre Colombo",
                "arxiv_comment": "10 pages, 5 figures",
                "arxiv_journal_ref": "Transactions on Machine Learning Research, January 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12030v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12030v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15901v1",
                "updated": "2025-01-27T09:51:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    51,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T09:51:48Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    51,
                    48,
                    0,
                    27,
                    0
                ],
                "title": "Robust Mobile Robot Path Planning via LLM-Based Dynamic Waypoint\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Mobile Robot Path Planning via LLM-Based Dynamic Waypoint\n  Generation"
                },
                "summary": "Mobile robot path planning in complex environments remains a significant\nchallenge, especially in achieving efficient, safe and robust paths. The\ntraditional path planning techniques like DRL models typically trained for a\ngiven configuration of the starting point and target positions, these models\nonly perform well when these conditions are satisfied. In this paper, we\nproposed a novel path planning framework that embeds Large Language Models to\nempower mobile robots with the capability of dynamically interpreting natural\nlanguage commands and autonomously generating efficient, collision-free\nnavigation paths. The proposed framework uses LLMs to translate high-level user\ninputs into actionable waypoints while dynamically adjusting paths in response\nto obstacles. We experimentally evaluated our proposed LLM-based approach\nacross three different environments of progressive complexity, showing the\nrobustness of our approach with llama3.1 model that outperformed other LLM\nmodels in path planning time, waypoint generation success rate, and collision\navoidance. This underlines the promising contribution of LLMs for enhancing the\ncapability of mobile robots, especially when their operation involves complex\ndecisions in large and complex environments. Our framework has provided safer,\nmore reliable navigation systems and opened a new direction for the future\nresearch. The source code of this work is publicly available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile robot path planning in complex environments remains a significant\nchallenge, especially in achieving efficient, safe and robust paths. The\ntraditional path planning techniques like DRL models typically trained for a\ngiven configuration of the starting point and target positions, these models\nonly perform well when these conditions are satisfied. In this paper, we\nproposed a novel path planning framework that embeds Large Language Models to\nempower mobile robots with the capability of dynamically interpreting natural\nlanguage commands and autonomously generating efficient, collision-free\nnavigation paths. The proposed framework uses LLMs to translate high-level user\ninputs into actionable waypoints while dynamically adjusting paths in response\nto obstacles. We experimentally evaluated our proposed LLM-based approach\nacross three different environments of progressive complexity, showing the\nrobustness of our approach with llama3.1 model that outperformed other LLM\nmodels in path planning time, waypoint generation success rate, and collision\navoidance. This underlines the promising contribution of LLMs for enhancing the\ncapability of mobile robots, especially when their operation involves complex\ndecisions in large and complex environments. Our framework has provided safer,\nmore reliable navigation systems and opened a new direction for the future\nresearch. The source code of this work is publicly available on GitHub."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Tariq"
                    },
                    {
                        "name": "Congqing Wang"
                    },
                    {
                        "name": "Yasir Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Yasir Hussain"
                },
                "author": "Yasir Hussain",
                "arxiv_comment": "18 pages, 6 figures, submitted in Journal Expert Systems with\n  Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15896v1",
                "updated": "2025-01-27T09:42:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    42,
                    49,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T09:42:49Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    42,
                    49,
                    0,
                    27,
                    0
                ],
                "title": "A mirror descent approach to maximum likelihood estimation in latent\n  variable models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A mirror descent approach to maximum likelihood estimation in latent\n  variable models"
                },
                "summary": "We introduce an approach based on mirror descent and sequential Monte Carlo\n(SMC) to perform joint parameter inference and posterior estimation in latent\nvariable models. This approach is based on minimisation of a functional over\nthe parameter space and the space of probability distributions and, contrary to\nother popular approaches, can be implemented when the latent variable takes\nvalues in discrete spaces. We provide a detailed theoretical analysis of both\nthe mirror descent algorithm and its approximation via SMC. We experimentally\nshow that the proposed algorithm outperforms standard expectation maximisation\nalgorithms and is competitive with other popular methods for real-valued latent\nvariables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an approach based on mirror descent and sequential Monte Carlo\n(SMC) to perform joint parameter inference and posterior estimation in latent\nvariable models. This approach is based on minimisation of a functional over\nthe parameter space and the space of probability distributions and, contrary to\nother popular approaches, can be implemented when the latent variable takes\nvalues in discrete spaces. We provide a detailed theoretical analysis of both\nthe mirror descent algorithm and its approximation via SMC. We experimentally\nshow that the proposed algorithm outperforms standard expectation maximisation\nalgorithms and is competitive with other popular methods for real-valued latent\nvariables."
                },
                "authors": [
                    {
                        "name": "Francesca R. Crucinio"
                    }
                ],
                "author_detail": {
                    "name": "Francesca R. Crucinio"
                },
                "author": "Francesca R. Crucinio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13444v3",
                "updated": "2025-01-27T09:39:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    39,
                    49,
                    0,
                    27,
                    0
                ],
                "published": "2024-01-24T13:36:50Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    13,
                    36,
                    50,
                    2,
                    24,
                    0
                ],
                "title": "Fine-Grained Stateful Knowledge Exploration: A Novel Paradigm for\n  Integrating Knowledge Graphs with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Stateful Knowledge Exploration: A Novel Paradigm for\n  Integrating Knowledge Graphs with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown impressive capabilities, yet updating\ntheir knowledge remains a significant challenge, often leading to outdated or\ninaccurate responses. A proposed solution is the integration of external\nknowledge bases, such as knowledge graphs, with LLMs. Most existing methods use\na paradigm that treats the question as the objective, with relevant knowledge\nbeing incrementally retrieved from the knowledge graph. However, this strategy\nfrequently experiences an mismatch in the granularity of knowledge between the\ntarget question and the entities and relations being retrieved. As a result,\nthe information in the question cannot precisely correspond to the retrieved\nknowledge. This may cause redundant exploration or omission of vital knowledge,\nthereby leading to enhanced computational consumption and reduced retrieval\naccuracy. In this paper, we propose a novel paradigm of fine-grained stateful\nknowledge exploration, which addresses the `information granularity mismatch'\nissue. We extract fine-grained information from questions and explore the\nsemantic mapping between this information and the knowledge in graph. By\ndynamically updating the mapping records, we avoid redundant exploration and\nensure no pertinent information is overlooked, thereby reducing computational\noverhead and improving the accuracy of knowledge exploration. The use of\nfine-grained information also eliminates the need for a priori knowledge, a\ncommon requirement in existing methods. Experiments on multiple datasets\nrevealed that our paradigm surpasses current advanced methods in knowledge\nretrieval while significantly reducing the average number of LLM invocations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive capabilities, yet updating\ntheir knowledge remains a significant challenge, often leading to outdated or\ninaccurate responses. A proposed solution is the integration of external\nknowledge bases, such as knowledge graphs, with LLMs. Most existing methods use\na paradigm that treats the question as the objective, with relevant knowledge\nbeing incrementally retrieved from the knowledge graph. However, this strategy\nfrequently experiences an mismatch in the granularity of knowledge between the\ntarget question and the entities and relations being retrieved. As a result,\nthe information in the question cannot precisely correspond to the retrieved\nknowledge. This may cause redundant exploration or omission of vital knowledge,\nthereby leading to enhanced computational consumption and reduced retrieval\naccuracy. In this paper, we propose a novel paradigm of fine-grained stateful\nknowledge exploration, which addresses the `information granularity mismatch'\nissue. We extract fine-grained information from questions and explore the\nsemantic mapping between this information and the knowledge in graph. By\ndynamically updating the mapping records, we avoid redundant exploration and\nensure no pertinent information is overlooked, thereby reducing computational\noverhead and improving the accuracy of knowledge exploration. The use of\nfine-grained information also eliminates the need for a priori knowledge, a\ncommon requirement in existing methods. Experiments on multiple datasets\nrevealed that our paradigm surpasses current advanced methods in knowledge\nretrieval while significantly reducing the average number of LLM invocations."
                },
                "authors": [
                    {
                        "name": "Dehao Tao"
                    },
                    {
                        "name": "Congqi Wang"
                    },
                    {
                        "name": "Feng Huang"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Minghu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Minghu Jiang"
                },
                "author": "Minghu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12333v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12333v3",
                "updated": "2025-01-27T09:18:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    18,
                    7,
                    0,
                    27,
                    0
                ],
                "published": "2024-08-22T12:21:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    21,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "GRATR: Zero-Shot Evidence Graph Retrieval-Augmented Trustworthiness\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRATR: Zero-Shot Evidence Graph Retrieval-Augmented Trustworthiness\n  Reasoning"
                },
                "summary": "Trustworthiness reasoning aims to enable agents in multiplayer games with\nincomplete information to identify potential allies and adversaries, thereby\nenhancing decision-making. In this paper, we introduce the graph\nretrieval-augmented trustworthiness reasoning (GRATR) framework, which\nretrieves observable evidence from the game environment to inform\ndecision-making by large language models (LLMs) without requiring additional\ntraining, making it a zero-shot approach. Within the GRATR framework, agents\nfirst observe the actions of other players and evaluate the resulting shifts in\ninter-player trust, constructing a corresponding trustworthiness graph. During\ndecision-making, the agent performs multi-hop retrieval to evaluate\ntrustworthiness toward a specific target, where evidence chains are retrieved\nfrom multiple trusted sources to form a comprehensive assessment. Experiments\nin the multiplayer game \\emph{Werewolf} demonstrate that GRATR outperforms the\nalternatives, improving reasoning accuracy by 50.5\\% and reducing hallucination\nby 30.6\\% compared to the baseline method. Additionally, when tested on a\ndataset of Twitter tweets during the U.S. election period, GRATR surpasses the\nbaseline method by 10.4\\% in accuracy, highlighting its potential in real-world\napplications such as intent analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness reasoning aims to enable agents in multiplayer games with\nincomplete information to identify potential allies and adversaries, thereby\nenhancing decision-making. In this paper, we introduce the graph\nretrieval-augmented trustworthiness reasoning (GRATR) framework, which\nretrieves observable evidence from the game environment to inform\ndecision-making by large language models (LLMs) without requiring additional\ntraining, making it a zero-shot approach. Within the GRATR framework, agents\nfirst observe the actions of other players and evaluate the resulting shifts in\ninter-player trust, constructing a corresponding trustworthiness graph. During\ndecision-making, the agent performs multi-hop retrieval to evaluate\ntrustworthiness toward a specific target, where evidence chains are retrieved\nfrom multiple trusted sources to form a comprehensive assessment. Experiments\nin the multiplayer game \\emph{Werewolf} demonstrate that GRATR outperforms the\nalternatives, improving reasoning accuracy by 50.5\\% and reducing hallucination\nby 30.6\\% compared to the baseline method. Additionally, when tested on a\ndataset of Twitter tweets during the U.S. election period, GRATR surpasses the\nbaseline method by 10.4\\% in accuracy, highlighting its potential in real-world\napplications such as intent analysis."
                },
                "authors": [
                    {
                        "name": "Ying Zhu"
                    },
                    {
                        "name": "Shengchang Li"
                    },
                    {
                        "name": "Ziqian Kong"
                    },
                    {
                        "name": "Qiang Yang"
                    },
                    {
                        "name": "Peilan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Peilan Xu"
                },
                "author": "Peilan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12333v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12333v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03667v2",
                "updated": "2025-01-27T09:02:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    2,
                    46,
                    0,
                    27,
                    0
                ],
                "published": "2024-02-06T03:41:12Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    3,
                    41,
                    12,
                    1,
                    37,
                    0
                ],
                "title": "Large Language Models as an Indirect Reasoner: Contrapositive and\n  Contradiction for Automated Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as an Indirect Reasoner: Contrapositive and\n  Contradiction for Automated Reasoning"
                },
                "summary": "Recently, increasing attention has been focused on improving the ability of\nLarge Language Models (LLMs) to perform complex reasoning. Advanced methods,\nsuch as Chain-of-Thought (CoT) and its variants, are found to enhance their\nreasoning skills by designing suitable prompts or breaking down complex\nproblems into more manageable sub-problems. However, little concentration has\nbeen put on exploring the reasoning process, \\textit{i.e.}, we discovered that\nmost methods resort to Direct Reasoning (DR) and disregard Indirect Reasoning\n(IR). This can make LLMs difficult to solve IR tasks, which are often\nencountered in the real world. To address this issue, we propose a\nDirect-Indirect Reasoning (DIR) method, which considers DR and IR as multiple\nparallel reasoning paths that are merged to derive the final answer. We\nstimulate LLMs to implement IR by crafting prompt templates incorporating the\nprinciples of contrapositive and contradiction. These templates trigger LLMs to\nassume the negation of the conclusion as true, combine it with the premises to\ndeduce a conclusion, and utilize the logical equivalence of the contrapositive\nto enhance their comprehension of the rules used in the reasoning process. Our\nDIR method is simple yet effective and can be straightforwardly integrated with\nexisting variants of CoT methods. Experimental results on four datasets related\nto logical reasoning and mathematic proof demonstrate that our DIR method, when\ncombined with various baseline methods, significantly outperforms all the\noriginal methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, increasing attention has been focused on improving the ability of\nLarge Language Models (LLMs) to perform complex reasoning. Advanced methods,\nsuch as Chain-of-Thought (CoT) and its variants, are found to enhance their\nreasoning skills by designing suitable prompts or breaking down complex\nproblems into more manageable sub-problems. However, little concentration has\nbeen put on exploring the reasoning process, \\textit{i.e.}, we discovered that\nmost methods resort to Direct Reasoning (DR) and disregard Indirect Reasoning\n(IR). This can make LLMs difficult to solve IR tasks, which are often\nencountered in the real world. To address this issue, we propose a\nDirect-Indirect Reasoning (DIR) method, which considers DR and IR as multiple\nparallel reasoning paths that are merged to derive the final answer. We\nstimulate LLMs to implement IR by crafting prompt templates incorporating the\nprinciples of contrapositive and contradiction. These templates trigger LLMs to\nassume the negation of the conclusion as true, combine it with the premises to\ndeduce a conclusion, and utilize the logical equivalence of the contrapositive\nto enhance their comprehension of the rules used in the reasoning process. Our\nDIR method is simple yet effective and can be straightforwardly integrated with\nexisting variants of CoT methods. Experimental results on four datasets related\nto logical reasoning and mathematic proof demonstrate that our DIR method, when\ncombined with various baseline methods, significantly outperforms all the\noriginal methods."
                },
                "authors": [
                    {
                        "name": "Yanfang Zhang"
                    },
                    {
                        "name": "Yiliu Sun"
                    },
                    {
                        "name": "Yibing Zhan"
                    },
                    {
                        "name": "Dapeng Tao"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Chen Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chen Gong"
                },
                "author": "Chen Gong",
                "arxiv_comment": "Accepted by COLING 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15875v1",
                "updated": "2025-01-27T08:59:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    59,
                    10,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T08:59:10Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    59,
                    10,
                    0,
                    27,
                    0
                ],
                "title": "LCTG Bench: LLM Controlled Text Generation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCTG Bench: LLM Controlled Text Generation Benchmark"
                },
                "summary": "The rise of large language models (LLMs) has led to more diverse and\nhigher-quality machine-generated text. However, their high expressive power\nmakes it difficult to control outputs based on specific business instructions.\nIn response, benchmarks focusing on the controllability of LLMs have been\ndeveloped, but several issues remain: (1) They primarily cover major languages\nlike English and Chinese, neglecting low-resource languages like Japanese; (2)\nCurrent benchmarks employ task-specific evaluation metrics, lacking a unified\nframework for selecting models based on controllability across different use\ncases. To address these challenges, this research introduces LCTG Bench, the\nfirst Japanese benchmark for evaluating the controllability of LLMs. LCTG Bench\nprovides a unified framework for assessing control performance, enabling users\nto select the most suitable model for their use cases based on controllability.\nBy evaluating nine diverse Japanese-specific and multilingual LLMs like GPT-4,\nwe highlight the current state and challenges of controllability in Japanese\nLLMs and reveal the significant gap between multilingual models and\nJapanese-specific models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has led to more diverse and\nhigher-quality machine-generated text. However, their high expressive power\nmakes it difficult to control outputs based on specific business instructions.\nIn response, benchmarks focusing on the controllability of LLMs have been\ndeveloped, but several issues remain: (1) They primarily cover major languages\nlike English and Chinese, neglecting low-resource languages like Japanese; (2)\nCurrent benchmarks employ task-specific evaluation metrics, lacking a unified\nframework for selecting models based on controllability across different use\ncases. To address these challenges, this research introduces LCTG Bench, the\nfirst Japanese benchmark for evaluating the controllability of LLMs. LCTG Bench\nprovides a unified framework for assessing control performance, enabling users\nto select the most suitable model for their use cases based on controllability.\nBy evaluating nine diverse Japanese-specific and multilingual LLMs like GPT-4,\nwe highlight the current state and challenges of controllability in Japanese\nLLMs and reveal the significant gap between multilingual models and\nJapanese-specific models."
                },
                "authors": [
                    {
                        "name": "Kentaro Kurihara"
                    },
                    {
                        "name": "Masato Mita"
                    },
                    {
                        "name": "Peinan Zhang"
                    },
                    {
                        "name": "Shota Sasaki"
                    },
                    {
                        "name": "Ryosuke Ishigami"
                    },
                    {
                        "name": "Naoaki Okazaki"
                    }
                ],
                "author_detail": {
                    "name": "Naoaki Okazaki"
                },
                "author": "Naoaki Okazaki",
                "arxiv_comment": "15 pages, 11 figures. Project page: this\n  [URL](https://github.com/CyberAgentAILab/LCTG-Bench)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.18644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18644v2",
                "updated": "2025-01-27T18:54:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    54,
                    42,
                    0,
                    27,
                    0
                ],
                "published": "2024-12-24T16:06:53Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    6,
                    53,
                    1,
                    359,
                    0
                ],
                "title": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaGRAG | Exploring the Topology of Information for Advancing Language\n  Understanding and Generation in Graph Retrieval-Augmented Generation"
                },
                "summary": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to\nenhance language understanding and generation by leveraging external knowledge.\nHowever, effectively capturing and integrating the rich semantic information\npresent in textual and structured data remains a challenge. To address this, a\nnovel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),\nis proposed to focus on enhancing subgraph representation and diversity within\nthe knowledge graph. By improving graph density, capturing entity and relation\ninformation more effectively, and dynamically prioritizing relevant and diverse\nsubgraphs and information within them, the proposed approach enables a more\ncomprehensive understanding of the underlying semantic structure. This is\nachieved through a combination of de-duplication processes, two-step mean\npooling of embeddings, query-aware retrieval considering unique nodes, and a\nDynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph\nConvolutional Networks (GCNs) and Large Language Models (LLMs) through hard\nprompting further enhances the learning of rich node and edge representations\nwhile preserving the hierarchical subgraph structure. Experimental results\ndemonstrate the effectiveness of DynaGRAG, showcasing the significance of\nenhanced subgraph representation and diversity for improved language\nunderstanding and generation."
                },
                "authors": [
                    {
                        "name": "Karishma Thakrar"
                    }
                ],
                "author_detail": {
                    "name": "Karishma Thakrar"
                },
                "author": "Karishma Thakrar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01553v2",
                "updated": "2025-01-27T18:51:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    51,
                    36,
                    0,
                    27,
                    0
                ],
                "published": "2024-03-16T03:12:45Z",
                "published_parsed": [
                    2024,
                    3,
                    16,
                    3,
                    12,
                    45,
                    5,
                    76,
                    0
                ],
                "title": "Empirical Studies of Parameter Efficient Methods for Large Language\n  Models of Code and Knowledge Transfer to R",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Studies of Parameter Efficient Methods for Large Language\n  Models of Code and Knowledge Transfer to R"
                },
                "summary": "Parameter Efficient Fine-Tuning (PEFT) methods are proposed as an alternative\nfine-tuning approach for Large Language Models (LLM) to minimize high training\ncosts. While prior research demonstrates the effectiveness of PEFT methods in\nknowledge transfer using smaller language models, their application to larger\nLLMs, particularly in low-resource and unseen programming languages such as R,\nremains under-explored. In this work, we evaluate PEFT methods, LoRA,\nCompacter, and IA^3 on LLMs for code summarization and generation, with a\nparticular emphasis on knowledge transfer to R as an unseen under-explored\ntarget language. Our experiments reveal that LoRA consistently outperforms\nCompacter and IA^3 in all settings, while Compacter offers significant resource\nefficiency with minimal performance trade-offs. Additionally, we find that the\nnumber of trainable parameters has a greater influence on the functional\naccuracy of the generated code than PEFT architecture. Our study can direct\nfuture research in developing code intelligent tasks for unseen languages\nincluding R, as well as the choice of PEFT methods for knowledge transfer,\nespecially when balancing the computational cost and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter Efficient Fine-Tuning (PEFT) methods are proposed as an alternative\nfine-tuning approach for Large Language Models (LLM) to minimize high training\ncosts. While prior research demonstrates the effectiveness of PEFT methods in\nknowledge transfer using smaller language models, their application to larger\nLLMs, particularly in low-resource and unseen programming languages such as R,\nremains under-explored. In this work, we evaluate PEFT methods, LoRA,\nCompacter, and IA^3 on LLMs for code summarization and generation, with a\nparticular emphasis on knowledge transfer to R as an unseen under-explored\ntarget language. Our experiments reveal that LoRA consistently outperforms\nCompacter and IA^3 in all settings, while Compacter offers significant resource\nefficiency with minimal performance trade-offs. Additionally, we find that the\nnumber of trainable parameters has a greater influence on the functional\naccuracy of the generated code than PEFT architecture. Our study can direct\nfuture research in developing code intelligent tasks for unseen languages\nincluding R, as well as the choice of PEFT methods for knowledge transfer,\nespecially when balancing the computational cost and performance."
                },
                "authors": [
                    {
                        "name": "Amirreza Esmaeili"
                    },
                    {
                        "name": "Iman Saberi"
                    },
                    {
                        "name": "Fatemeh H. Fard"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh H. Fard"
                },
                "author": "Fatemeh H. Fard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16309v1",
                "updated": "2025-01-27T18:47:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    47,
                    58,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:47:58Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    47,
                    58,
                    0,
                    27,
                    0
                ],
                "title": "Evaluating The Performance of Using Large Language Models to Automate\n  Summarization of CT Simulation Orders in Radiation Oncology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating The Performance of Using Large Language Models to Automate\n  Summarization of CT Simulation Orders in Radiation Oncology"
                },
                "summary": "Purpose: This study aims to use a large language model (LLM) to automate the\ngeneration of summaries from the CT simulation orders and evaluate its\nperformance.\n  Materials and Methods: A total of 607 CT simulation orders for patients were\ncollected from the Aria database at our institution. A locally hosted Llama 3.1\n405B model, accessed via the Application Programming Interface (API) service,\nwas used to extract keywords from the CT simulation orders and generate\nsummaries. The downloaded CT simulation orders were categorized into seven\ngroups based on treatment modalities and disease sites. For each group, a\ncustomized instruction prompt was developed collaboratively with therapists to\nguide the Llama 3.1 405B model in generating summaries. The ground truth for\nthe corresponding summaries was manually derived by carefully reviewing each CT\nsimulation order and subsequently verified by therapists. The accuracy of the\nLLM-generated summaries was evaluated by therapists using the verified ground\ntruth as a reference.\n  Results: About 98% of the LLM-generated summaries aligned with the manually\ngenerated ground truth in terms of accuracy. Our evaluations showed an improved\nconsistency in format and enhanced readability of the LLM-generated summaries\ncompared to the corresponding therapists-generated summaries. This automated\napproach demonstrated a consistent performance across all groups, regardless of\nmodality or disease site.\n  Conclusions: This study demonstrated the high precision and consistency of\nthe Llama 3.1 405B model in extracting keywords and summarizing CT simulation\norders, suggesting that LLMs have great potential to help with this task,\nreduce the workload of therapists and improve workflow efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: This study aims to use a large language model (LLM) to automate the\ngeneration of summaries from the CT simulation orders and evaluate its\nperformance.\n  Materials and Methods: A total of 607 CT simulation orders for patients were\ncollected from the Aria database at our institution. A locally hosted Llama 3.1\n405B model, accessed via the Application Programming Interface (API) service,\nwas used to extract keywords from the CT simulation orders and generate\nsummaries. The downloaded CT simulation orders were categorized into seven\ngroups based on treatment modalities and disease sites. For each group, a\ncustomized instruction prompt was developed collaboratively with therapists to\nguide the Llama 3.1 405B model in generating summaries. The ground truth for\nthe corresponding summaries was manually derived by carefully reviewing each CT\nsimulation order and subsequently verified by therapists. The accuracy of the\nLLM-generated summaries was evaluated by therapists using the verified ground\ntruth as a reference.\n  Results: About 98% of the LLM-generated summaries aligned with the manually\ngenerated ground truth in terms of accuracy. Our evaluations showed an improved\nconsistency in format and enhanced readability of the LLM-generated summaries\ncompared to the corresponding therapists-generated summaries. This automated\napproach demonstrated a consistent performance across all groups, regardless of\nmodality or disease site.\n  Conclusions: This study demonstrated the high precision and consistency of\nthe Llama 3.1 405B model in extracting keywords and summarizing CT simulation\norders, suggesting that LLMs have great potential to help with this task,\nreduce the workload of therapists and improve workflow efficiency."
                },
                "authors": [
                    {
                        "name": "Meiyun Cao"
                    },
                    {
                        "name": "Shaw Hu"
                    },
                    {
                        "name": "Jason Sharp"
                    },
                    {
                        "name": "Edward Clouser"
                    },
                    {
                        "name": "Jason Holmes"
                    },
                    {
                        "name": "Linda L. Lam"
                    },
                    {
                        "name": "Xiaoning Ding"
                    },
                    {
                        "name": "Diego Santos Toesca"
                    },
                    {
                        "name": "Wendy S. Lindholm"
                    },
                    {
                        "name": "Samir H. Patel"
                    },
                    {
                        "name": "Sujay A. Vora"
                    },
                    {
                        "name": "Peilong Wang"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.09254v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.09254v4",
                "updated": "2025-01-27T18:45:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    45,
                    22,
                    0,
                    27,
                    0
                ],
                "published": "2023-07-18T13:36:24Z",
                "published_parsed": [
                    2023,
                    7,
                    18,
                    13,
                    36,
                    24,
                    1,
                    199,
                    0
                ],
                "title": "Selective Generation for Controllable Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Generation for Controllable Language Models"
                },
                "summary": "Trustworthiness of generative language models (GLMs) is crucial in their\ndeployment to critical decision making systems. Hence, certified risk control\nmethods such as selective prediction and conformal prediction have been applied\nto mitigating the hallucination problem in various supervised downstream tasks.\nHowever, the lack of appropriate correctness metric hinders applying such\nprincipled methods to language generation tasks. In this paper, we circumvent\nthis problem by leveraging the concept of textual entailment to evaluate the\ncorrectness of the generated sequence, and propose two selective generation\nalgorithms which control the false discovery rate with respect to the textual\nentailment relation (FDR-E) with a theoretical guarantee:\n$\\texttt{SGen}^{\\texttt{Sup}}$ and $\\texttt{SGen}^{\\texttt{Semi}}$.\n$\\texttt{SGen}^{\\texttt{Sup}}$, a direct modification of the selective\nprediction, is a supervised learning algorithm which exploits\nentailment-labeled data, annotated by humans. Since human annotation is costly,\nwe further propose a semi-supervised version, $\\texttt{SGen}^{\\texttt{Semi}}$,\nwhich fully utilizes the unlabeled data by pseudo-labeling, leveraging an\nentailment set function learned via conformal prediction. Furthermore,\n$\\texttt{SGen}^{\\texttt{Semi}}$ enables to use more general class of selection\nfunctions, neuro-selection functions, and provides users with an optimal\nselection function class given multiple candidates. Finally, we demonstrate the\nefficacy of the $\\texttt{SGen}$ family in achieving a desired FDR-E level with\ncomparable selection efficiency to those from baselines on both open and closed\nsource GLMs. Code and datasets are provided at\nhttps://github.com/ml-postech/selective-generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness of generative language models (GLMs) is crucial in their\ndeployment to critical decision making systems. Hence, certified risk control\nmethods such as selective prediction and conformal prediction have been applied\nto mitigating the hallucination problem in various supervised downstream tasks.\nHowever, the lack of appropriate correctness metric hinders applying such\nprincipled methods to language generation tasks. In this paper, we circumvent\nthis problem by leveraging the concept of textual entailment to evaluate the\ncorrectness of the generated sequence, and propose two selective generation\nalgorithms which control the false discovery rate with respect to the textual\nentailment relation (FDR-E) with a theoretical guarantee:\n$\\texttt{SGen}^{\\texttt{Sup}}$ and $\\texttt{SGen}^{\\texttt{Semi}}$.\n$\\texttt{SGen}^{\\texttt{Sup}}$, a direct modification of the selective\nprediction, is a supervised learning algorithm which exploits\nentailment-labeled data, annotated by humans. Since human annotation is costly,\nwe further propose a semi-supervised version, $\\texttt{SGen}^{\\texttt{Semi}}$,\nwhich fully utilizes the unlabeled data by pseudo-labeling, leveraging an\nentailment set function learned via conformal prediction. Furthermore,\n$\\texttt{SGen}^{\\texttt{Semi}}$ enables to use more general class of selection\nfunctions, neuro-selection functions, and provides users with an optimal\nselection function class given multiple candidates. Finally, we demonstrate the\nefficacy of the $\\texttt{SGen}$ family in achieving a desired FDR-E level with\ncomparable selection efficiency to those from baselines on both open and closed\nsource GLMs. Code and datasets are provided at\nhttps://github.com/ml-postech/selective-generation."
                },
                "authors": [
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Kyungmin Kim"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Sangdon Park"
                    }
                ],
                "author_detail": {
                    "name": "Sangdon Park"
                },
                "author": "Sangdon Park",
                "arxiv_comment": "Accepted to NeurIPS 2024 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.09254v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.09254v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16303v1",
                "updated": "2025-01-27T18:45:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    45,
                    7,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:45:07Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    45,
                    7,
                    0,
                    27,
                    0
                ],
                "title": "RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based\n  Video Event Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based\n  Video Event Retrieval"
                },
                "summary": "Retrieving events from videos using text queries has become increasingly\nchallenging due to the rapid growth of multimedia content. Existing methods for\ntext-based video event retrieval often focus heavily on object-level\ndescriptions, overlooking the crucial role of contextual information. This\nlimitation is especially apparent when queries lack sufficient context, such as\nmissing location details or ambiguous background elements. To address these\nchallenges, we propose a novel system called RAPID (Retrieval-Augmented\nParallel Inference Drafting), which leverages advancements in Large Language\nModels (LLMs) and prompt-based learning to semantically correct and enrich user\nqueries with relevant contextual information. These enriched queries are then\nprocessed through parallel retrieval, followed by an evaluation step to select\nthe most relevant results based on their alignment with the original query.\nThrough extensive experiments on our custom-developed dataset, we demonstrate\nthat RAPID significantly outperforms traditional retrieval methods,\nparticularly for contextually incomplete queries. Our system was validated for\nboth speed and accuracy through participation in the Ho Chi Minh City AI\nChallenge 2024, where it successfully retrieved events from over 300 hours of\nvideo. Further evaluation comparing RAPID with the baseline proposed by the\ncompetition organizers demonstrated its superior effectiveness, highlighting\nthe strength and robustness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving events from videos using text queries has become increasingly\nchallenging due to the rapid growth of multimedia content. Existing methods for\ntext-based video event retrieval often focus heavily on object-level\ndescriptions, overlooking the crucial role of contextual information. This\nlimitation is especially apparent when queries lack sufficient context, such as\nmissing location details or ambiguous background elements. To address these\nchallenges, we propose a novel system called RAPID (Retrieval-Augmented\nParallel Inference Drafting), which leverages advancements in Large Language\nModels (LLMs) and prompt-based learning to semantically correct and enrich user\nqueries with relevant contextual information. These enriched queries are then\nprocessed through parallel retrieval, followed by an evaluation step to select\nthe most relevant results based on their alignment with the original query.\nThrough extensive experiments on our custom-developed dataset, we demonstrate\nthat RAPID significantly outperforms traditional retrieval methods,\nparticularly for contextually incomplete queries. Our system was validated for\nboth speed and accuracy through participation in the Ho Chi Minh City AI\nChallenge 2024, where it successfully retrieved events from over 300 hours of\nvideo. Further evaluation comparing RAPID with the baseline proposed by the\ncompetition organizers demonstrated its superior effectiveness, highlighting\nthe strength and robustness of our approach."
                },
                "authors": [
                    {
                        "name": "Long Nguyen"
                    },
                    {
                        "name": "Huy Nguyen"
                    },
                    {
                        "name": "Bao Khuu"
                    },
                    {
                        "name": "Huy Luu"
                    },
                    {
                        "name": "Huy Le"
                    },
                    {
                        "name": "Tuan Nguyen"
                    },
                    {
                        "name": "Tho Quan"
                    }
                ],
                "author_detail": {
                    "name": "Tho Quan"
                },
                "author": "Tho Quan",
                "arxiv_comment": "Under review at SoICT'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16302v1",
                "updated": "2025-01-27T18:42:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    42,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:42:48Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    42,
                    48,
                    0,
                    27,
                    0
                ],
                "title": "Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With\n  Configurable Depth and Width",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With\n  Configurable Depth and Width"
                },
                "summary": "Large language models (LLMs) provide powerful foundations to perform\nfine-grained text re-ranking. However, they are often prohibitive in reality\ndue to constraints on computation bandwidth. In this work, we propose a\n\\textbf{flexible} architecture called \\textbf{Matroyshka Re-Ranker}, which is\ndesigned to facilitate \\textbf{runtime customization} of model layers and\nsequence lengths at each layer based on users' configurations. Consequently,\nthe LLM-based re-rankers can be made applicable across various real-world\nsituations. The increased flexibility may come at the cost of precision loss.\nTo address this problem, we introduce a suite of techniques to optimize the\nperformance. First, we propose \\textbf{cascaded self-distillation}, where each\nsub-architecture learns to preserve a precise re-ranking performance from its\nsuper components, whose predictions can be exploited as smooth and informative\nteacher signals. Second, we design a \\textbf{factorized compensation\nmechanism}, where two collaborative Low-Rank Adaptation modules, vertical and\nhorizontal, are jointly employed to compensate for the precision loss resulted\nfrom arbitrary combinations of layer and sequence compression. We perform\ncomprehensive experiments based on the passage and document retrieval datasets\nfrom MSMARCO, along with all public datasets from BEIR benchmark. In our\nexperiments, Matryoshka Re-Ranker substantially outperforms the existing\nmethods, while effectively preserving its superior performance across various\nforms of compression and different application scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) provide powerful foundations to perform\nfine-grained text re-ranking. However, they are often prohibitive in reality\ndue to constraints on computation bandwidth. In this work, we propose a\n\\textbf{flexible} architecture called \\textbf{Matroyshka Re-Ranker}, which is\ndesigned to facilitate \\textbf{runtime customization} of model layers and\nsequence lengths at each layer based on users' configurations. Consequently,\nthe LLM-based re-rankers can be made applicable across various real-world\nsituations. The increased flexibility may come at the cost of precision loss.\nTo address this problem, we introduce a suite of techniques to optimize the\nperformance. First, we propose \\textbf{cascaded self-distillation}, where each\nsub-architecture learns to preserve a precise re-ranking performance from its\nsuper components, whose predictions can be exploited as smooth and informative\nteacher signals. Second, we design a \\textbf{factorized compensation\nmechanism}, where two collaborative Low-Rank Adaptation modules, vertical and\nhorizontal, are jointly employed to compensate for the precision loss resulted\nfrom arbitrary combinations of layer and sequence compression. We perform\ncomprehensive experiments based on the passage and document retrieval datasets\nfrom MSMARCO, along with all public datasets from BEIR benchmark. In our\nexperiments, Matryoshka Re-Ranker substantially outperforms the existing\nmethods, while effectively preserving its superior performance across various\nforms of compression and different application scenarios."
                },
                "authors": [
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Chaofan Li"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Chaozhuo Li"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Yingxia Shao"
                    }
                ],
                "author_detail": {
                    "name": "Yingxia Shao"
                },
                "author": "Yingxia Shao",
                "arxiv_comment": "The Web Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16300v1",
                "updated": "2025-01-27T18:38:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    38,
                    36,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:38:36Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    38,
                    36,
                    0,
                    27,
                    0
                ],
                "title": "Large Models in Dialogue for Active Perception and Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Models in Dialogue for Active Perception and Anomaly Detection"
                },
                "summary": "Autonomous aerial monitoring is an important task aimed at gathering\ninformation from areas that may not be easily accessible by humans. At the same\ntime, this task often requires recognizing anomalies from a significant\ndistance or not previously encountered in the past. In this paper, we propose a\nnovel framework that leverages the advanced capabilities provided by Large\nLanguage Models (LLMs) to actively collect information and perform anomaly\ndetection in novel scenes. To this end, we propose an LLM based model dialogue\napproach, in which two deep learning models engage in a dialogue to actively\ncontrol a drone to increase perception and anomaly detection accuracy. We\nconduct our experiments in a high fidelity simulation environment where an LLM\nis provided with a predetermined set of natural language movement commands\nmapped into executable code functions. Additionally, we deploy a multimodal\nVisual Question Answering (VQA) model charged with the task of visual question\nanswering and captioning. By engaging the two models in conversation, the LLM\nasks exploratory questions while simultaneously flying a drone into different\nparts of the scene, providing a novel way to implement active perception. By\nleveraging LLMs reasoning ability, we output an improved detailed description\nof the scene going beyond existing static perception approaches. In addition to\ninformation gathering, our approach is utilized for anomaly detection and our\nresults demonstrate the proposed methods effectiveness in informing and\nalerting about potential hazards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous aerial monitoring is an important task aimed at gathering\ninformation from areas that may not be easily accessible by humans. At the same\ntime, this task often requires recognizing anomalies from a significant\ndistance or not previously encountered in the past. In this paper, we propose a\nnovel framework that leverages the advanced capabilities provided by Large\nLanguage Models (LLMs) to actively collect information and perform anomaly\ndetection in novel scenes. To this end, we propose an LLM based model dialogue\napproach, in which two deep learning models engage in a dialogue to actively\ncontrol a drone to increase perception and anomaly detection accuracy. We\nconduct our experiments in a high fidelity simulation environment where an LLM\nis provided with a predetermined set of natural language movement commands\nmapped into executable code functions. Additionally, we deploy a multimodal\nVisual Question Answering (VQA) model charged with the task of visual question\nanswering and captioning. By engaging the two models in conversation, the LLM\nasks exploratory questions while simultaneously flying a drone into different\nparts of the scene, providing a novel way to implement active perception. By\nleveraging LLMs reasoning ability, we output an improved detailed description\nof the scene going beyond existing static perception approaches. In addition to\ninformation gathering, our approach is utilized for anomaly detection and our\nresults demonstrate the proposed methods effectiveness in informing and\nalerting about potential hazards."
                },
                "authors": [
                    {
                        "name": "Tzoulio Chamiti"
                    },
                    {
                        "name": "Nikolaos Passalis"
                    },
                    {
                        "name": "Anastasios Tefas"
                    }
                ],
                "author_detail": {
                    "name": "Anastasios Tefas"
                },
                "author": "Anastasios Tefas",
                "arxiv_comment": "Accepted to International Conference of Pattern Recognition (ICPR\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03471v2",
                "updated": "2025-01-27T18:27:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    27,
                    32,
                    0,
                    27,
                    0
                ],
                "published": "2024-11-05T19:52:58Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    19,
                    52,
                    58,
                    1,
                    310,
                    0
                ],
                "title": "MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs"
                },
                "summary": "Large Language Models (LLMs) have been applied to various hardware design\ntasks, including Verilog code generation, EDA tool scripting, and RTL bug\nfixing. Despite this extensive exploration, LLMs are yet to be used for the\ntask of post-synthesis metric reasoning and estimation of HDL designs. In this\npaper, we assess the ability of LLMs to reason about post-synthesis metrics of\nVerilog designs. We introduce MetRex, a large-scale dataset comprising 25,868\nVerilog HDL designs and their corresponding post-synthesis metrics, namely\narea, delay, and static power. MetRex incorporates a Chain of Thought (CoT)\ntemplate to enhance LLMs' reasoning about these metrics. Extensive experiments\nshow that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities\non average by 37.0\\%, 25.3\\%, and 25.7\\% on the area, delay, and static power,\nrespectively. While SFT improves performance on our benchmark, it remains far\nfrom achieving optimal results, especially on complex problems. Comparing to\nstate-of-the-art regression models, our approach delivers accurate\npost-synthesis predictions for 17.4\\% more designs (within a 5\\% error margin),\nin addition to offering a 1.7x speedup by eliminating the need for\npre-processing. This work lays the groundwork for advancing LLM-based Verilog\ncode metric reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been applied to various hardware design\ntasks, including Verilog code generation, EDA tool scripting, and RTL bug\nfixing. Despite this extensive exploration, LLMs are yet to be used for the\ntask of post-synthesis metric reasoning and estimation of HDL designs. In this\npaper, we assess the ability of LLMs to reason about post-synthesis metrics of\nVerilog designs. We introduce MetRex, a large-scale dataset comprising 25,868\nVerilog HDL designs and their corresponding post-synthesis metrics, namely\narea, delay, and static power. MetRex incorporates a Chain of Thought (CoT)\ntemplate to enhance LLMs' reasoning about these metrics. Extensive experiments\nshow that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities\non average by 37.0\\%, 25.3\\%, and 25.7\\% on the area, delay, and static power,\nrespectively. While SFT improves performance on our benchmark, it remains far\nfrom achieving optimal results, especially on complex problems. Comparing to\nstate-of-the-art regression models, our approach delivers accurate\npost-synthesis predictions for 17.4\\% more designs (within a 5\\% error margin),\nin addition to offering a 1.7x speedup by eliminating the need for\npre-processing. This work lays the groundwork for advancing LLM-based Verilog\ncode metric reasoning."
                },
                "authors": [
                    {
                        "name": "Manar Abdelatty"
                    },
                    {
                        "name": "Jingxiao Ma"
                    },
                    {
                        "name": "Sherief Reda"
                    }
                ],
                "author_detail": {
                    "name": "Sherief Reda"
                },
                "author": "Sherief Reda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00958v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00958v3",
                "updated": "2025-01-27T18:17:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    17,
                    26,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-01T21:29:37Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    29,
                    37,
                    2,
                    1,
                    0
                ],
                "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining"
                },
                "summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving. Our code are available\nat https://github.com/DAMO-NLP-SG/multimodal_textbook."
                },
                "authors": [
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jiashuo Sun"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00958v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00958v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16277v1",
                "updated": "2025-01-27T18:11:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    11,
                    6,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:11:06Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    11,
                    6,
                    0,
                    27,
                    0
                ],
                "title": "Do LLMs Have Visualization Literacy? An Evaluation on Modified\n  Visualizations to Test Generalization in Data Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Have Visualization Literacy? An Evaluation on Modified\n  Visualizations to Test Generalization in Data Interpretation"
                },
                "summary": "In this paper, we assess the visualization literacy of two prominent Large\nLanguage Models (LLMs): OpenAI's Generative Pretrained Transformers (GPT), the\nbackend of ChatGPT, and Google's Gemini, previously known as Bard, to establish\nbenchmarks for assessing their visualization capabilities. While LLMs have\nshown promise in generating chart descriptions, captions, and design\nsuggestions, their potential for evaluating visualizations remains\nunder-explored. Collecting data from humans for evaluations has been a\nbottleneck for visualization research in terms of both time and money, and if\nLLMs were able to serve, even in some limited role, as evaluators, they could\nbe a significant resource. To investigate the feasibility of using LLMs in the\nvisualization evaluation process, we explore the extent to which LLMs possess\nvisualization literacy -- a crucial factor for their effective utility in the\nfield. We conducted a series of experiments using a modified 53-item\nVisualization Literacy Assessment Test (VLAT) for GPT-4 and Gemini. Our\nfindings indicate that the LLMs we explored currently fail to achieve the same\nlevels of visualization literacy when compared to data from the general public\nreported in VLAT, and LLMs heavily relied on their pre-existing knowledge to\nanswer questions instead of utilizing the information provided by the\nvisualization when answering questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we assess the visualization literacy of two prominent Large\nLanguage Models (LLMs): OpenAI's Generative Pretrained Transformers (GPT), the\nbackend of ChatGPT, and Google's Gemini, previously known as Bard, to establish\nbenchmarks for assessing their visualization capabilities. While LLMs have\nshown promise in generating chart descriptions, captions, and design\nsuggestions, their potential for evaluating visualizations remains\nunder-explored. Collecting data from humans for evaluations has been a\nbottleneck for visualization research in terms of both time and money, and if\nLLMs were able to serve, even in some limited role, as evaluators, they could\nbe a significant resource. To investigate the feasibility of using LLMs in the\nvisualization evaluation process, we explore the extent to which LLMs possess\nvisualization literacy -- a crucial factor for their effective utility in the\nfield. We conducted a series of experiments using a modified 53-item\nVisualization Literacy Assessment Test (VLAT) for GPT-4 and Gemini. Our\nfindings indicate that the LLMs we explored currently fail to achieve the same\nlevels of visualization literacy when compared to data from the general public\nreported in VLAT, and LLMs heavily relied on their pre-existing knowledge to\nanswer questions instead of utilizing the information provided by the\nvisualization when answering questions."
                },
                "authors": [
                    {
                        "name": "Jiayi Hong"
                    },
                    {
                        "name": "Christian Seto"
                    },
                    {
                        "name": "Arlen Fan"
                    },
                    {
                        "name": "Ross Maciejewski"
                    }
                ],
                "author_detail": {
                    "name": "Ross Maciejewski"
                },
                "author": "Ross Maciejewski",
                "arxiv_comment": "This is the author's version of the article that has been accepted in\n  IEEE Transactions on Visualization and Computer Graphics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16276v1",
                "updated": "2025-01-27T18:10:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    10,
                    34,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:10:34Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    10,
                    34,
                    0,
                    27,
                    0
                ],
                "title": "URAG: Implementing a Unified Hybrid RAG for Precise Answers in\n  University Admission Chatbots -- A Case Study at HCMUT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URAG: Implementing a Unified Hybrid RAG for Precise Answers in\n  University Admission Chatbots -- A Case Study at HCMUT"
                },
                "summary": "With the rapid advancement of Artificial Intelligence, particularly in\nNatural Language Processing, Large Language Models (LLMs) have become pivotal\nin educational question-answering systems, especially university admission\nchatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other\nadvanced techniques have been developed to enhance these systems by integrating\nspecific university data, enabling LLMs to provide informed responses on\nadmissions and academic counseling. However, these enhanced RAG techniques\noften involve high operational costs and require the training of complex,\nspecialized modules, which poses challenges for practical deployment.\nAdditionally, in the educational context, it is crucial to provide accurate\nanswers to prevent misinformation, a task that LLM-based systems find\nchallenging without appropriate strategies and methods. In this paper, we\nintroduce the Unified RAG (URAG) Framework, a hybrid approach that\nsignificantly improves the accuracy of responses, particularly for critical\nqueries. Experimental results demonstrate that URAG enhances our in-house,\nlightweight model to perform comparably to state-of-the-art commercial models.\nMoreover, to validate its practical applicability, we conducted a case study at\nour educational institution, which received positive feedback and acclaim. This\nstudy not only proves the effectiveness of URAG but also highlights its\nfeasibility for real-world implementation in educational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of Artificial Intelligence, particularly in\nNatural Language Processing, Large Language Models (LLMs) have become pivotal\nin educational question-answering systems, especially university admission\nchatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other\nadvanced techniques have been developed to enhance these systems by integrating\nspecific university data, enabling LLMs to provide informed responses on\nadmissions and academic counseling. However, these enhanced RAG techniques\noften involve high operational costs and require the training of complex,\nspecialized modules, which poses challenges for practical deployment.\nAdditionally, in the educational context, it is crucial to provide accurate\nanswers to prevent misinformation, a task that LLM-based systems find\nchallenging without appropriate strategies and methods. In this paper, we\nintroduce the Unified RAG (URAG) Framework, a hybrid approach that\nsignificantly improves the accuracy of responses, particularly for critical\nqueries. Experimental results demonstrate that URAG enhances our in-house,\nlightweight model to perform comparably to state-of-the-art commercial models.\nMoreover, to validate its practical applicability, we conducted a case study at\nour educational institution, which received positive feedback and acclaim. This\nstudy not only proves the effectiveness of URAG but also highlights its\nfeasibility for real-world implementation in educational settings."
                },
                "authors": [
                    {
                        "name": "Long Nguyen"
                    },
                    {
                        "name": "Tho Quan"
                    }
                ],
                "author_detail": {
                    "name": "Tho Quan"
                },
                "author": "Tho Quan",
                "arxiv_comment": "Under review at SoICT'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16273v1",
                "updated": "2025-01-27T18:06:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    6,
                    36,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T18:06:36Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    18,
                    6,
                    36,
                    0,
                    27,
                    0
                ],
                "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs"
                },
                "summary": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount."
                },
                "authors": [
                    {
                        "name": "Mohamed Elfeki"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Chad Voegele"
                    }
                ],
                "author_detail": {
                    "name": "Chad Voegele"
                },
                "author": "Chad Voegele",
                "arxiv_comment": "13 pages, 5 figures. LLMs/SLMs, encoder-decoder and decoder-only",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16255v1",
                "updated": "2025-01-27T17:55:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    55,
                    37,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:55:37Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    55,
                    37,
                    0,
                    27,
                    0
                ],
                "title": "A foundation model for human-AI collaboration in medical literature\n  mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A foundation model for human-AI collaboration in medical literature\n  mining"
                },
                "summary": "Systematic literature review is essential for evidence-based medicine,\nrequiring comprehensive analysis of clinical trial publications. However, the\napplication of artificial intelligence (AI) models for medical literature\nmining has been limited by insufficient training and evaluation across broad\ntherapeutic areas and diverse tasks. Here, we present LEADS, an AI foundation\nmodel for study search, screening, and data extraction from medical literature.\nThe model is trained on 633,759 instruction data points in LEADSInstruct,\ncurated from 21,335 systematic reviews, 453,625 clinical trial publications,\nand 27,015 clinical trial registries. We showed that LEADS demonstrates\nconsistent improvements over four cutting-edge generic large language models\n(LLMs) on six tasks. Furthermore, LEADS enhances expert workflows by providing\nsupportive references following expert requests, streamlining processes while\nmaintaining high-quality results. A study with 16 clinicians and medical\nresearchers from 14 different institutions revealed that experts collaborating\nwith LEADS achieved a recall of 0.81 compared to 0.77 experts working alone in\nstudy selection, with a time savings of 22.6%. In data extraction tasks,\nexperts using LEADS achieved an accuracy of 0.85 versus 0.80 without using\nLEADS, alongside a 26.9% time savings. These findings highlight the potential\nof specialized medical literature foundation models to outperform generic\nmodels, delivering significant quality and efficiency benefits when integrated\ninto expert workflows for medical literature mining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic literature review is essential for evidence-based medicine,\nrequiring comprehensive analysis of clinical trial publications. However, the\napplication of artificial intelligence (AI) models for medical literature\nmining has been limited by insufficient training and evaluation across broad\ntherapeutic areas and diverse tasks. Here, we present LEADS, an AI foundation\nmodel for study search, screening, and data extraction from medical literature.\nThe model is trained on 633,759 instruction data points in LEADSInstruct,\ncurated from 21,335 systematic reviews, 453,625 clinical trial publications,\nand 27,015 clinical trial registries. We showed that LEADS demonstrates\nconsistent improvements over four cutting-edge generic large language models\n(LLMs) on six tasks. Furthermore, LEADS enhances expert workflows by providing\nsupportive references following expert requests, streamlining processes while\nmaintaining high-quality results. A study with 16 clinicians and medical\nresearchers from 14 different institutions revealed that experts collaborating\nwith LEADS achieved a recall of 0.81 compared to 0.77 experts working alone in\nstudy selection, with a time savings of 22.6%. In data extraction tasks,\nexperts using LEADS achieved an accuracy of 0.85 versus 0.80 without using\nLEADS, alongside a 26.9% time savings. These findings highlight the potential\nof specialized medical literature foundation models to outperform generic\nmodels, delivering significant quality and efficiency benefits when integrated\ninto expert workflows for medical literature mining."
                },
                "authors": [
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Joey Chan"
                    },
                    {
                        "name": "Nicholas Wan"
                    },
                    {
                        "name": "Behdad Afzali"
                    },
                    {
                        "name": "Hyun-Jin Cho"
                    },
                    {
                        "name": "Chang-In Choi"
                    },
                    {
                        "name": "Mehdi Emamverdi"
                    },
                    {
                        "name": "Manjot K. Gill"
                    },
                    {
                        "name": "Sun-Hyung Kim"
                    },
                    {
                        "name": "Yijia Li"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Hanley Ong"
                    },
                    {
                        "name": "Justin Rousseau"
                    },
                    {
                        "name": "Irfan Sheikh"
                    },
                    {
                        "name": "Jenny J. Wei"
                    },
                    {
                        "name": "Ziyang Xu"
                    },
                    {
                        "name": "Christopher M. Zallek"
                    },
                    {
                        "name": "Kyungsang Kim"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16254v1",
                "updated": "2025-01-27T17:54:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    54,
                    31,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:54:31Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    54,
                    31,
                    0,
                    27,
                    0
                ],
                "title": "Multi-Agent Geospatial Copilots for Remote Sensing Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Geospatial Copilots for Remote Sensing Workflows"
                },
                "summary": "We present GeoLLM-Squad, a geospatial Copilot that introduces the novel\nmulti-agent paradigm to remote sensing (RS) workflows. Unlike existing\nsingle-agent approaches that rely on monolithic large language models (LLM),\nGeoLLM-Squad separates agentic orchestration from geospatial task-solving, by\ndelegating RS tasks to specialized sub-agents. Built on the open-source AutoGen\nand GeoLLM-Engine frameworks, our work enables the modular integration of\ndiverse applications, spanning urban monitoring, forestry protection, climate\nanalysis, and agriculture studies. Our results demonstrate that while\nsingle-agent systems struggle to scale with increasing RS task complexity,\nGeoLLM-Squad maintains robust performance, achieving a 17% improvement in\nagentic correctness over state-of-the-art baselines. Our findings highlight the\npotential of multi-agent AI in advancing RS workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GeoLLM-Squad, a geospatial Copilot that introduces the novel\nmulti-agent paradigm to remote sensing (RS) workflows. Unlike existing\nsingle-agent approaches that rely on monolithic large language models (LLM),\nGeoLLM-Squad separates agentic orchestration from geospatial task-solving, by\ndelegating RS tasks to specialized sub-agents. Built on the open-source AutoGen\nand GeoLLM-Engine frameworks, our work enables the modular integration of\ndiverse applications, spanning urban monitoring, forestry protection, climate\nanalysis, and agriculture studies. Our results demonstrate that while\nsingle-agent systems struggle to scale with increasing RS task complexity,\nGeoLLM-Squad maintains robust performance, achieving a 17% improvement in\nagentic correctness over state-of-the-art baselines. Our findings highlight the\npotential of multi-agent AI in advancing RS workflows."
                },
                "authors": [
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Varatheepan Paramanayakam"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Heming Liao"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Ruopu Li"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16249v1",
                "updated": "2025-01-27T17:51:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    51,
                    29,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:51:29Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    51,
                    29,
                    0,
                    27,
                    0
                ],
                "title": "Lightweight Weighted Average Ensemble Model for Pneumonia Detection in\n  Chest X-Ray Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Weighted Average Ensemble Model for Pneumonia Detection in\n  Chest X-Ray Images"
                },
                "summary": "Pneumonia is a leading cause of illness and death in children, underscoring\nthe need for early and accurate detection. In this study, we propose a novel\nlightweight ensemble model for detecting pneumonia in children using chest\nX-ray images. This ensemble model integrates two pre-trained convolutional\nneural networks (CNNs), MobileNetV2 and NASNetMobile, selected for their\nbalance of computational efficiency and accuracy. These models were fine-tuned\non a pediatric chest X-ray dataset and combined to enhance classification\nperformance. Our proposed ensemble model achieved a classification accuracy of\n98.63%, significantly outperforming individual models such as MobileNetV2\n(97.10%) and NASNetMobile(96.25%) in terms of accuracy, precision, recall, and\nF1 score. Moreover, the ensemble model outperformed state-of-the-art\narchitectures, including ResNet50, InceptionV3, and DenseNet201, while\nmaintaining computational efficiency. The proposed lightweight ensemble model\npresents a highly effective and resource-efficient solution for pneumonia\ndetection, making it particularly suitable for deployment in\nresource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pneumonia is a leading cause of illness and death in children, underscoring\nthe need for early and accurate detection. In this study, we propose a novel\nlightweight ensemble model for detecting pneumonia in children using chest\nX-ray images. This ensemble model integrates two pre-trained convolutional\nneural networks (CNNs), MobileNetV2 and NASNetMobile, selected for their\nbalance of computational efficiency and accuracy. These models were fine-tuned\non a pediatric chest X-ray dataset and combined to enhance classification\nperformance. Our proposed ensemble model achieved a classification accuracy of\n98.63%, significantly outperforming individual models such as MobileNetV2\n(97.10%) and NASNetMobile(96.25%) in terms of accuracy, precision, recall, and\nF1 score. Moreover, the ensemble model outperformed state-of-the-art\narchitectures, including ResNet50, InceptionV3, and DenseNet201, while\nmaintaining computational efficiency. The proposed lightweight ensemble model\npresents a highly effective and resource-efficient solution for pneumonia\ndetection, making it particularly suitable for deployment in\nresource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Suresh Babu Nettur"
                    },
                    {
                        "name": "Shanthi Karpurapu"
                    },
                    {
                        "name": "Unnati Nettur"
                    },
                    {
                        "name": "Likhit Sagar Gajja"
                    },
                    {
                        "name": "Sravanthy Myneni"
                    },
                    {
                        "name": "Akhil Dusi"
                    },
                    {
                        "name": "Lalithya Posham"
                    }
                ],
                "author_detail": {
                    "name": "Lalithya Posham"
                },
                "author": "Lalithya Posham",
                "arxiv_comment": "Corresponding authors: Shanthi Karpurapu\n  (shanthi.karpurapu@gmail.com), Suresh Babu Nettur (nettursuresh@gmail.com)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16247v1",
                "updated": "2025-01-27T17:48:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    48,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:48:48Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    48,
                    48,
                    0,
                    27,
                    0
                ],
                "title": "Zero-Shot Decision Tree Construction via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Decision Tree Construction via Large Language Models"
                },
                "summary": "This paper introduces a novel algorithm for constructing decision trees using\nlarge language models (LLMs) in a zero-shot manner based on Classification and\nRegression Trees (CART) principles. Traditional decision tree induction methods\nrely heavily on labeled data to recursively partition data using criteria such\nas information gain or the Gini index. In contrast, we propose a method that\nuses the pre-trained knowledge embedded in LLMs to build decision trees without\nrequiring training data. Our approach leverages LLMs to perform operations\nessential for decision tree construction, including attribute discretization,\nprobability calculation, and Gini index computation based on the probabilities.\nWe show that these zero-shot decision trees can outperform baseline zero-shot\nmethods and achieve competitive performance compared to supervised data-driven\ndecision trees on tabular datasets. The decision trees constructed via this\nmethod provide transparent and interpretable models, addressing data scarcity\nwhile preserving interpretability. This work establishes a new baseline in\nlow-data machine learning, offering a principled, knowledge-driven alternative\nto data-driven tree construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel algorithm for constructing decision trees using\nlarge language models (LLMs) in a zero-shot manner based on Classification and\nRegression Trees (CART) principles. Traditional decision tree induction methods\nrely heavily on labeled data to recursively partition data using criteria such\nas information gain or the Gini index. In contrast, we propose a method that\nuses the pre-trained knowledge embedded in LLMs to build decision trees without\nrequiring training data. Our approach leverages LLMs to perform operations\nessential for decision tree construction, including attribute discretization,\nprobability calculation, and Gini index computation based on the probabilities.\nWe show that these zero-shot decision trees can outperform baseline zero-shot\nmethods and achieve competitive performance compared to supervised data-driven\ndecision trees on tabular datasets. The decision trees constructed via this\nmethod provide transparent and interpretable models, addressing data scarcity\nwhile preserving interpretability. This work establishes a new baseline in\nlow-data machine learning, offering a principled, knowledge-driven alternative\nto data-driven tree construction."
                },
                "authors": [
                    {
                        "name": "Lucas Carrasco"
                    },
                    {
                        "name": "Felipe Urrutia"
                    },
                    {
                        "name": "Andrés Abeliuk"
                    }
                ],
                "author_detail": {
                    "name": "Andrés Abeliuk"
                },
                "author": "Andrés Abeliuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16241v1",
                "updated": "2025-01-27T17:36:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    36,
                    6,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:36:06Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    36,
                    6,
                    0,
                    27,
                    0
                ],
                "title": "Phase Transitions in Large Language Models and the $O(N)$ Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phase Transitions in Large Language Models and the $O(N)$ Model"
                },
                "summary": "Large language models (LLMs) exhibit unprecedentedly rich scaling behaviors.\nIn physics, scaling behavior is closely related to phase transitions, critical\nphenomena, and field theory. To investigate the phase transition phenomena in\nLLMs, we reformulated the Transformer architecture as an $O(N)$ model. Our\nstudy reveals two distinct phase transitions corresponding to the temperature\nused in text generation and the model's parameter size, respectively. The first\nphase transition enables us to estimate the internal dimension of the model,\nwhile the second phase transition is of \\textit{higher-depth} and signals the\nemergence of new capabilities. As an application, the energy of the $O(N)$\nmodel can be used to evaluate whether an LLM's parameters are sufficient to\nlearn the training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit unprecedentedly rich scaling behaviors.\nIn physics, scaling behavior is closely related to phase transitions, critical\nphenomena, and field theory. To investigate the phase transition phenomena in\nLLMs, we reformulated the Transformer architecture as an $O(N)$ model. Our\nstudy reveals two distinct phase transitions corresponding to the temperature\nused in text generation and the model's parameter size, respectively. The first\nphase transition enables us to estimate the internal dimension of the model,\nwhile the second phase transition is of \\textit{higher-depth} and signals the\nemergence of new capabilities. As an application, the energy of the $O(N)$\nmodel can be used to evaluate whether an LLM's parameters are sufficient to\nlearn the training data."
                },
                "authors": [
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Babak Haghighat"
                    }
                ],
                "author_detail": {
                    "name": "Babak Haghighat"
                },
                "author": "Babak Haghighat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16224v1",
                "updated": "2025-01-27T17:20:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    20,
                    4,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:20:04Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    20,
                    4,
                    0,
                    27,
                    0
                ],
                "title": "Language-Based Bayesian Optimization Research Assistant (BORA)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Based Bayesian Optimization Research Assistant (BORA)"
                },
                "summary": "Many important scientific problems involve multivariate optimization coupled\nwith slow and laborious experimental measurements. These complex,\nhigh-dimensional searches can be defined by non-convex optimization landscapes\nthat resemble needle-in-a-haystack surfaces, leading to entrapment in local\nminima. Contextualizing optimizers with human domain knowledge is a powerful\napproach to guide searches to localized fruitful regions. However, this\napproach is susceptible to human confirmation bias and it is also challenging\nfor domain experts to keep track of the rapidly expanding scientific\nliterature. Here, we propose the use of Large Language Models (LLMs) for\ncontextualizing Bayesian optimization (BO) via a hybrid optimization framework\nthat intelligently and economically blends stochastic inference with domain\nknowledge-based insights from the LLM, which is used to suggest new,\nbetter-performing areas of the search space for exploration. Our method fosters\nuser engagement by offering real-time commentary on the optimization progress,\nexplaining the reasoning behind the search strategies. We validate the\neffectiveness of our approach on synthetic benchmarks with up to 15 independent\nvariables and demonstrate the ability of LLMs to reason in four real-world\nexperimental tasks where context-aware suggestions boost optimization\nperformance substantially.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many important scientific problems involve multivariate optimization coupled\nwith slow and laborious experimental measurements. These complex,\nhigh-dimensional searches can be defined by non-convex optimization landscapes\nthat resemble needle-in-a-haystack surfaces, leading to entrapment in local\nminima. Contextualizing optimizers with human domain knowledge is a powerful\napproach to guide searches to localized fruitful regions. However, this\napproach is susceptible to human confirmation bias and it is also challenging\nfor domain experts to keep track of the rapidly expanding scientific\nliterature. Here, we propose the use of Large Language Models (LLMs) for\ncontextualizing Bayesian optimization (BO) via a hybrid optimization framework\nthat intelligently and economically blends stochastic inference with domain\nknowledge-based insights from the LLM, which is used to suggest new,\nbetter-performing areas of the search space for exploration. Our method fosters\nuser engagement by offering real-time commentary on the optimization progress,\nexplaining the reasoning behind the search strategies. We validate the\neffectiveness of our approach on synthetic benchmarks with up to 15 independent\nvariables and demonstrate the ability of LLMs to reason in four real-world\nexperimental tasks where context-aware suggestions boost optimization\nperformance substantially."
                },
                "authors": [
                    {
                        "name": "Abdoulatif Cissé"
                    },
                    {
                        "name": "Xenophon Evangelopoulos"
                    },
                    {
                        "name": "Vladimir V. Gusev"
                    },
                    {
                        "name": "Andrew I. Cooper"
                    }
                ],
                "author_detail": {
                    "name": "Andrew I. Cooper"
                },
                "author": "Andrew I. Cooper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16220v1",
                "updated": "2025-01-27T17:09:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    9,
                    47,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:09:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    9,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "DBRouting: Routing End User Queries to Databases for Answerability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBRouting: Routing End User Queries to Databases for Answerability"
                },
                "summary": "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task."
                },
                "authors": [
                    {
                        "name": "Priyangshu Mandal"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Mayur Patidar"
                    },
                    {
                        "name": "Lovekesh Vig"
                    }
                ],
                "author_detail": {
                    "name": "Lovekesh Vig"
                },
                "author": "Lovekesh Vig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16215v1",
                "updated": "2025-01-27T17:07:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    7,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:07:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    7,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "Enhancing Visual Inspection Capability of Multi-Modal Large Language\n  Models on Medical Time Series with Supportive Conformalized and Interpretable\n  Small Specialized Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Visual Inspection Capability of Multi-Modal Large Language\n  Models on Medical Time Series with Supportive Conformalized and Interpretable\n  Small Specialized Models"
                },
                "summary": "Large language models (LLMs) exhibit remarkable capabilities in visual\ninspection of medical time-series data, achieving proficiency comparable to\nhuman clinicians. However, their broad scope limits domain-specific precision,\nand proprietary weights hinder fine-tuning for specialized datasets. In\ncontrast, small specialized models (SSMs) excel in targeted tasks but lack the\ncontextual reasoning required for complex clinical decision-making. To address\nthese challenges, we propose ConMIL (Conformalized Multiple Instance Learning),\na decision-support SSM that integrates seamlessly with LLMs. By using Multiple\nInstance Learning (MIL) to identify clinically significant signal segments and\nconformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'\ninterpretative capabilities for medical time-series analysis. Experimental\nresults demonstrate that ConMIL significantly improves the performance of\nstate-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,\n\\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for\nconfident samples in arrhythmia detection and sleep staging, compared to\nstandalone LLM accuracy of 46.13% and 13.16%. These findings highlight the\npotential of ConMIL to bridge task-specific precision and broader contextual\nreasoning, enabling more reliable and interpretable AI-driven clinical decision\nsupport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable capabilities in visual\ninspection of medical time-series data, achieving proficiency comparable to\nhuman clinicians. However, their broad scope limits domain-specific precision,\nand proprietary weights hinder fine-tuning for specialized datasets. In\ncontrast, small specialized models (SSMs) excel in targeted tasks but lack the\ncontextual reasoning required for complex clinical decision-making. To address\nthese challenges, we propose ConMIL (Conformalized Multiple Instance Learning),\na decision-support SSM that integrates seamlessly with LLMs. By using Multiple\nInstance Learning (MIL) to identify clinically significant signal segments and\nconformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs'\ninterpretative capabilities for medical time-series analysis. Experimental\nresults demonstrate that ConMIL significantly improves the performance of\nstate-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically,\n\\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for\nconfident samples in arrhythmia detection and sleep staging, compared to\nstandalone LLM accuracy of 46.13% and 13.16%. These findings highlight the\npotential of ConMIL to bridge task-specific precision and broader contextual\nreasoning, enabling more reliable and interpretable AI-driven clinical decision\nsupport."
                },
                "authors": [
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "Xiwen Chen"
                    },
                    {
                        "name": "Ci Zhang"
                    },
                    {
                        "name": "Stuart F. Quan"
                    },
                    {
                        "name": "William D. S. Killgore"
                    },
                    {
                        "name": "Shu-Fen Wung"
                    },
                    {
                        "name": "Chen X. Chen"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Ao Li"
                    }
                ],
                "author_detail": {
                    "name": "Ao Li"
                },
                "author": "Ao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16214v1",
                "updated": "2025-01-27T17:06:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    6,
                    56,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:06:56Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    6,
                    56,
                    0,
                    27,
                    0
                ],
                "title": "Provence: efficient and robust context pruning for retrieval-augmented\n  generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provence: efficient and robust context pruning for retrieval-augmented\n  generation"
                },
                "summary": "Retrieval-augmented generation improves various aspects of large language\nmodels (LLMs) generation, but suffers from computational overhead caused by\nlong contexts as well as the propagation of irrelevant retrieved information\ninto generated responses. Context pruning deals with both aspects, by removing\nirrelevant parts of retrieved contexts before LLM generation. Existing context\npruning approaches are however limited, and do not provide a universal model\nthat would be both efficient and robust in a wide range of scenarios, e.g.,\nwhen contexts contain a variable amount of relevant information or vary in\nlength, or when evaluated on various domains. In this work, we close this gap\nand introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts),\nan efficient and robust context pruner for Question Answering, which\ndynamically detects the needed amount of pruning for a given context and can be\nused out-of-the-box for various domains. The three key ingredients of Provence\nare formulating the context pruning task as sequence labeling, unifying context\npruning capabilities with context reranking, and training on diverse data. Our\nexperimental results show that Provence enables context pruning with negligible\nto no drop in performance, in various domains and settings, at almost no cost\nin a standard RAG pipeline. We also conduct a deeper analysis alongside various\nablations to provide insights into training context pruners for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation improves various aspects of large language\nmodels (LLMs) generation, but suffers from computational overhead caused by\nlong contexts as well as the propagation of irrelevant retrieved information\ninto generated responses. Context pruning deals with both aspects, by removing\nirrelevant parts of retrieved contexts before LLM generation. Existing context\npruning approaches are however limited, and do not provide a universal model\nthat would be both efficient and robust in a wide range of scenarios, e.g.,\nwhen contexts contain a variable amount of relevant information or vary in\nlength, or when evaluated on various domains. In this work, we close this gap\nand introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts),\nan efficient and robust context pruner for Question Answering, which\ndynamically detects the needed amount of pruning for a given context and can be\nused out-of-the-box for various domains. The three key ingredients of Provence\nare formulating the context pruning task as sequence labeling, unifying context\npruning capabilities with context reranking, and training on diverse data. Our\nexperimental results show that Provence enables context pruning with negligible\nto no drop in performance, in various domains and settings, at almost no cost\nin a standard RAG pipeline. We also conduct a deeper analysis alongside various\nablations to provide insights into training context pruners for future work."
                },
                "authors": [
                    {
                        "name": "Nadezhda Chirkova"
                    },
                    {
                        "name": "Thibault Formal"
                    },
                    {
                        "name": "Vassilina Nikoulina"
                    },
                    {
                        "name": "Stéphane Clinchant"
                    }
                ],
                "author_detail": {
                    "name": "Stéphane Clinchant"
                },
                "author": "Stéphane Clinchant",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16882v2",
                "updated": "2025-01-27T17:06:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    6,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-22T10:36:15Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    10,
                    36,
                    15,
                    1,
                    296,
                    0
                ],
                "title": "Large Language Model-based Augmentation for Imbalanced Node\n  Classification on Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based Augmentation for Imbalanced Node\n  Classification on Text-Attributed Graphs"
                },
                "summary": "Node classification on graphs often suffers from class imbalance, leading to\nbiased predictions and significant risks in real-world applications. While\ndata-centric solutions have been explored, they largely overlook\nText-Attributed Graphs (TAGs) and the potential of using rich textual semantics\nto improve the classification of minority nodes. Given this gap, we propose\nLarge Language Model-based Augmentation on Text-Attributed Graphs (LA-TAG), a\nnovel framework that leverages Large Language Models (LLMs) to handle\nimbalanced node classification. Specifically, we develop prompting strategies\ninspired by interpolation to synthesize textual node attributes. Additionally,\nto effectively integrate synthetic nodes into the graph structure, we introduce\na textual link predictor that connects the generated nodes to the original\ngraph, preserving structural and contextual information. Experiments across\nvarious datasets and evaluation metrics demonstrate that LA-TAG outperforms\nexisting textual augmentation and graph imbalance learning methods, emphasizing\nthe efficacy of our approach in addressing class imbalance in TAGs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Node classification on graphs often suffers from class imbalance, leading to\nbiased predictions and significant risks in real-world applications. While\ndata-centric solutions have been explored, they largely overlook\nText-Attributed Graphs (TAGs) and the potential of using rich textual semantics\nto improve the classification of minority nodes. Given this gap, we propose\nLarge Language Model-based Augmentation on Text-Attributed Graphs (LA-TAG), a\nnovel framework that leverages Large Language Models (LLMs) to handle\nimbalanced node classification. Specifically, we develop prompting strategies\ninspired by interpolation to synthesize textual node attributes. Additionally,\nto effectively integrate synthetic nodes into the graph structure, we introduce\na textual link predictor that connects the generated nodes to the original\ngraph, preserving structural and contextual information. Experiments across\nvarious datasets and evaluation metrics demonstrate that LA-TAG outperforms\nexisting textual augmentation and graph imbalance learning methods, emphasizing\nthe efficacy of our approach in addressing class imbalance in TAGs."
                },
                "authors": [
                    {
                        "name": "Leyao Wang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Bo Ni"
                    },
                    {
                        "name": "Yuying Zhao"
                    },
                    {
                        "name": "Tyler Derr"
                    }
                ],
                "author_detail": {
                    "name": "Tyler Derr"
                },
                "author": "Tyler Derr",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20791v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20791v2",
                "updated": "2025-01-27T17:05:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    5,
                    55,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-28T07:16:00Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    16,
                    0,
                    0,
                    302,
                    0
                ],
                "title": "From Cool Demos to Production-Ready FMware: Core Challenges and a\n  Technology Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cool Demos to Production-Ready FMware: Core Challenges and a\n  Technology Roadmap"
                },
                "summary": "The rapid expansion of foundation models (FMs), such as large language models\n(LLMs), has given rise to FMware--software systems that integrate FMs as core\ncomponents. While building demonstration-level FMware is relatively\nstraightforward, transitioning to production-ready systems presents numerous\nchallenges, including reliability, high implementation costs, scalability, and\ncompliance with privacy regulations. Our paper conducts a semi-structured\nthematic synthesis to identify the key challenges in productionizing FMware\nacross diverse data sources including our own industry experience in developing\nFMArts--a FMware lifecycle engineering platform and integrating it into Huawei\ncloud, grey literature, academic publications, hands-on involvement in the Open\nPlatform for Enterprise AI (OPEA), organizing the AIware conference and\nBootcamp, and co-leading the ISO SPDX SBOM working group on AI and datasets. We\nidentify critical issues in FM selection, data and model alignment, prompt\nengineering, agent orchestration, system testing, and deployment, alongside\ncross-cutting concerns such as memory management, observability, and feedback\nintegration. We discuss needed technologies and strategies to address these\nchallenges and offer guidance on how to enable the transition from\ndemonstration systems to scalable, production-ready FMware solutions. Our\nfindings underscore the importance of continued research and multi-industry\ncollaboration to advance the development of production-ready FMware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of foundation models (FMs), such as large language models\n(LLMs), has given rise to FMware--software systems that integrate FMs as core\ncomponents. While building demonstration-level FMware is relatively\nstraightforward, transitioning to production-ready systems presents numerous\nchallenges, including reliability, high implementation costs, scalability, and\ncompliance with privacy regulations. Our paper conducts a semi-structured\nthematic synthesis to identify the key challenges in productionizing FMware\nacross diverse data sources including our own industry experience in developing\nFMArts--a FMware lifecycle engineering platform and integrating it into Huawei\ncloud, grey literature, academic publications, hands-on involvement in the Open\nPlatform for Enterprise AI (OPEA), organizing the AIware conference and\nBootcamp, and co-leading the ISO SPDX SBOM working group on AI and datasets. We\nidentify critical issues in FM selection, data and model alignment, prompt\nengineering, agent orchestration, system testing, and deployment, alongside\ncross-cutting concerns such as memory management, observability, and feedback\nintegration. We discuss needed technologies and strategies to address these\nchallenges and offer guidance on how to enable the transition from\ndemonstration systems to scalable, production-ready FMware solutions. Our\nfindings underscore the importance of continued research and multi-industry\ncollaboration to advance the development of production-ready FMware."
                },
                "authors": [
                    {
                        "name": "Gopi Krishnan Rajbahadur"
                    },
                    {
                        "name": "Gustavo A. Oliva"
                    },
                    {
                        "name": "Dayi Lin"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20791v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20791v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13548v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13548v4",
                "updated": "2025-01-27T17:04:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    4,
                    37,
                    0,
                    27,
                    0
                ],
                "published": "2024-12-18T06:49:46Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    6,
                    49,
                    46,
                    2,
                    353,
                    0
                ],
                "title": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness"
                },
                "summary": "Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website https://nus-lins-lab.github.io/telepreview/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website https://nus-lins-lab.github.io/telepreview/."
                },
                "authors": [
                    {
                        "name": "Jingxiang Guo"
                    },
                    {
                        "name": "Jiayu Luo"
                    },
                    {
                        "name": "Zhenyu Wei"
                    },
                    {
                        "name": "Yiwen Hou"
                    },
                    {
                        "name": "Zhixuan Xu"
                    },
                    {
                        "name": "Xiaoyi Lin"
                    },
                    {
                        "name": "Chongkai Gao"
                    },
                    {
                        "name": "Lin Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shao"
                },
                "author": "Lin Shao",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13548v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13548v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16207v1",
                "updated": "2025-01-27T17:00:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    0,
                    56,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:00:56Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    0,
                    56,
                    0,
                    27,
                    0
                ],
                "title": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs"
                },
                "summary": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO, showing significant progress. However, these studies\nintertwined multiple skills simultaneously, i.e., problem-solving, reasoning,\nand writing formal specifications, making it hard to precisely identify the\nLLMs' strengths and weaknesses in each task. This paper focuses on formal\nverification, an immediate application scenario of formal reasoning, and\ndecomposes it into six sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five mainstream formal specification\nlanguages (Coq, Lean4, Dafny, ACSL, and TLA+) in six\nformal-verification-related tasks by distilling GPT-4o. They are split into a\n14k+ fine-tuning dataset FM-alpaca and a 4k benchmark FM-Bench. We found that\nLLMs are good at writing proof segments when given either the code, or the\ndetailed description of proof steps. Also, the fine-tuning brought about a\nnearly threefold improvement at most. Interestingly, we observed that\nfine-tuning with formal data also enhances mathematics, reasoning, and coding\nabilities. We hope our findings inspire further research. Fine-tuned models are\nreleased to facilitate subsequent studies",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO, showing significant progress. However, these studies\nintertwined multiple skills simultaneously, i.e., problem-solving, reasoning,\nand writing formal specifications, making it hard to precisely identify the\nLLMs' strengths and weaknesses in each task. This paper focuses on formal\nverification, an immediate application scenario of formal reasoning, and\ndecomposes it into six sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five mainstream formal specification\nlanguages (Coq, Lean4, Dafny, ACSL, and TLA+) in six\nformal-verification-related tasks by distilling GPT-4o. They are split into a\n14k+ fine-tuning dataset FM-alpaca and a 4k benchmark FM-Bench. We found that\nLLMs are good at writing proof segments when given either the code, or the\ndetailed description of proof steps. Also, the fine-tuning brought about a\nnearly threefold improvement at most. Interestingly, we observed that\nfine-tuning with formal data also enhances mathematics, reasoning, and coding\nabilities. We hope our findings inspire further research. Fine-tuned models are\nreleased to facilitate subsequent studies"
                },
                "authors": [
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Meiziniu Li"
                    },
                    {
                        "name": "Haoyang Ma"
                    },
                    {
                        "name": "Haokun Li"
                    },
                    {
                        "name": "Mengda He"
                    },
                    {
                        "name": "Cheng Wen"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Shengchao Qin"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Cong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Cong Tian"
                },
                "author": "Cong Tian",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19442v3",
                "updated": "2025-01-27T16:55:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    55,
                    57,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T10:45:40Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    10,
                    45,
                    40,
                    1,
                    121,
                    0
                ],
                "title": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs"
                },
                "summary": "Nigeria is a multilingual country with 500+ languages. Naija is a\nNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed\nlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has\nmainly been a spoken language until recently, there are now various platforms\npublishing exclusively in Naija such as Naija Wikipedia. However, it is hard to\ndistinguish by non-native from a larger pidgin languages spoken across West\nAfrica known as West African Pidgin English (WAPE) -- which is more simplied\nand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news\nplatform publishes exclusively in WAPE to cater for several countries in West\nAfrica. In our paper, we show through statistical analyses and Machine\nTranslation experiments that these two creole varieties do not represent each\nother (i.e., there are linguistic differences in word order and vocabulary) and\nGenerative AI operates only based on WAPE. In other words, Naija is\nunder-represented in Generative AI, and it is hard to teach LLMs with few\nexamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nigeria is a multilingual country with 500+ languages. Naija is a\nNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed\nlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has\nmainly been a spoken language until recently, there are now various platforms\npublishing exclusively in Naija such as Naija Wikipedia. However, it is hard to\ndistinguish by non-native from a larger pidgin languages spoken across West\nAfrica known as West African Pidgin English (WAPE) -- which is more simplied\nand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news\nplatform publishes exclusively in WAPE to cater for several countries in West\nAfrica. In our paper, we show through statistical analyses and Machine\nTranslation experiments that these two creole varieties do not represent each\nother (i.e., there are linguistic differences in word order and vocabulary) and\nGenerative AI operates only based on WAPE. In other words, Naija is\nunder-represented in Generative AI, and it is hard to teach LLMs with few\nexamples."
                },
                "authors": [
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "A. Seza Doğruöz"
                    },
                    {
                        "name": "Iyanuoluwa Shode"
                    },
                    {
                        "name": "Anuoluwapo Aremu"
                    }
                ],
                "author_detail": {
                    "name": "Anuoluwapo Aremu"
                },
                "author": "Anuoluwapo Aremu",
                "arxiv_comment": "Accepted to NAACL 2025 (findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16191v1",
                "updated": "2025-01-27T16:45:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    45,
                    34,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T16:45:34Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    45,
                    34,
                    0,
                    27,
                    0
                ],
                "title": "Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python\n  using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python\n  using LLMs"
                },
                "summary": "Fixing Python dependency issues is a tedious and error-prone task for\ndevelopers, who must manually identify and resolve environment dependencies and\nversion constraints of third-party modules and Python interpreters. Researchers\nhave attempted to automate this process by relying on large knowledge graphs\nand database lookup tables. However, these traditional approaches face\nlimitations due to the variety of dependency error types, large sets of\npossible module versions, and conflicts among transitive dependencies. This\nstudy explores the potential of using large language models (LLMs) to\nautomatically fix dependency issues in Python programs. We introduce PLLM\n(pronounced \"plum\"), a novel technique that employs retrieval-augmented\ngeneration (RAG) to help an LLM infer Python versions and required modules for\na given Python file. PLLM builds a testing environment that iteratively (1)\nprompts the LLM for module combinations, (2) tests the suggested changes, and\n(3) provides feedback (error messages) to the LLM to refine the fix. This\nfeedback cycle leverages natural language processing (NLP) to intelligently\nparse and interpret build error messages. We benchmark PLLM on the Gistable\nHG2.9K dataset, a collection of challenging single-file Python gists. We\ncompare PLLM against two state-of-the-art automatic dependency inference\napproaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency\nissues. Our results indicate that PLLM can fix more dependency issues than the\ntwo baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%)\nover PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial\nfor projects with many dependencies and for specific third-party numerical and\nmachine-learning modules. Our findings demonstrate the potential of LLM-based\napproaches to iteratively resolve Python dependency issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixing Python dependency issues is a tedious and error-prone task for\ndevelopers, who must manually identify and resolve environment dependencies and\nversion constraints of third-party modules and Python interpreters. Researchers\nhave attempted to automate this process by relying on large knowledge graphs\nand database lookup tables. However, these traditional approaches face\nlimitations due to the variety of dependency error types, large sets of\npossible module versions, and conflicts among transitive dependencies. This\nstudy explores the potential of using large language models (LLMs) to\nautomatically fix dependency issues in Python programs. We introduce PLLM\n(pronounced \"plum\"), a novel technique that employs retrieval-augmented\ngeneration (RAG) to help an LLM infer Python versions and required modules for\na given Python file. PLLM builds a testing environment that iteratively (1)\nprompts the LLM for module combinations, (2) tests the suggested changes, and\n(3) provides feedback (error messages) to the LLM to refine the fix. This\nfeedback cycle leverages natural language processing (NLP) to intelligently\nparse and interpret build error messages. We benchmark PLLM on the Gistable\nHG2.9K dataset, a collection of challenging single-file Python gists. We\ncompare PLLM against two state-of-the-art automatic dependency inference\napproaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency\nissues. Our results indicate that PLLM can fix more dependency issues than the\ntwo baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%)\nover PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial\nfor projects with many dependencies and for specific third-party numerical and\nmachine-learning modules. Our findings demonstrate the potential of LLM-based\napproaches to iteratively resolve Python dependency issues."
                },
                "authors": [
                    {
                        "name": "Antony Bartlett"
                    },
                    {
                        "name": "Cynthia Liem"
                    },
                    {
                        "name": "Annibale Panichella"
                    }
                ],
                "author_detail": {
                    "name": "Annibale Panichella"
                },
                "author": "Annibale Panichella",
                "arxiv_comment": "Under submission to TOSEM, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04637v3",
                "updated": "2025-01-27T16:38:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    38,
                    30,
                    0,
                    27,
                    0
                ],
                "published": "2024-11-07T11:51:14Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    11,
                    51,
                    14,
                    3,
                    312,
                    0
                ],
                "title": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop"
                },
                "summary": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and deploying machine learning models relies on a large amount of\nhuman-annotated data. As human labeling becomes increasingly expensive and\ntime-consuming, recent research has developed multiple strategies to speed up\nannotation and reduce costs and human workload: generating synthetic training\ndata, active learning, and hybrid labeling. This tutorial is oriented toward\npractical applications: we will present the basics of each strategy, highlight\ntheir benefits and limitations, and discuss in detail real-life case studies.\nAdditionally, we will walk through best practices for managing human annotators\nand controlling the quality of the final dataset. The tutorial includes a\nhands-on workshop, where attendees will be guided in implementing a hybrid\nannotation setup. This tutorial is designed for NLP practitioners from both\nresearch and industry backgrounds who are involved in or interested in\noptimizing data labeling projects."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Akim Tsvigun"
                    },
                    {
                        "name": "Dominik Schlechtweg"
                    },
                    {
                        "name": "Natalia Fedorova"
                    },
                    {
                        "name": "Konstantin Chernyshev"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Boris Obmoroshev"
                    }
                ],
                "author_detail": {
                    "name": "Boris Obmoroshev"
                },
                "author": "Boris Obmoroshev",
                "arxiv_comment": "To be presented at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01834v3",
                "updated": "2025-01-27T16:34:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    34,
                    59,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-03T14:38:01Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    38,
                    1,
                    4,
                    3,
                    0
                ],
                "title": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning"
                },
                "summary": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we call MoColl, designed to effectively\nintegrate domain-specific and general knowledge. Specifically, our approach is\nto decompose complex image captioning tasks into a series of interconnected\nquestion-answer subtasks. A trainable visual question answering (VQA) model is\nemployed as a specialized tool to focus on domain-specific visual analysis,\nanswering task-specific questions based on image content. Concurrently, an\nLLM-based agent with general knowledge formulates these questions and\nsynthesizes the resulting question-answer pairs into coherent captions. Beyond\nits role in leveraging the VQA model, the agent further guides its training to\nenhance its domain-specific capabilities. Experimental results on radiology\nreport generation validate the effectiveness of the proposed framework,\ndemonstrating significant improvements in the quality of generated reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we call MoColl, designed to effectively\nintegrate domain-specific and general knowledge. Specifically, our approach is\nto decompose complex image captioning tasks into a series of interconnected\nquestion-answer subtasks. A trainable visual question answering (VQA) model is\nemployed as a specialized tool to focus on domain-specific visual analysis,\nanswering task-specific questions based on image content. Concurrently, an\nLLM-based agent with general knowledge formulates these questions and\nsynthesizes the resulting question-answer pairs into coherent captions. Beyond\nits role in leveraging the VQA model, the agent further guides its training to\nenhance its domain-specific capabilities. Experimental results on radiology\nreport generation validate the effectiveness of the proposed framework,\ndemonstrating significant improvements in the quality of generated reports."
                },
                "authors": [
                    {
                        "name": "Pu Yang"
                    },
                    {
                        "name": "Bin Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dong"
                },
                "author": "Bin Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16178v1",
                "updated": "2025-01-27T16:26:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    26,
                    7,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T16:26:07Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    26,
                    7,
                    0,
                    27,
                    0
                ],
                "title": "SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time\n  Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time\n  Series Forecasting"
                },
                "summary": "In recent work on time-series prediction, Transformers and even large\nlanguage models have garnered significant attention due to their strong\ncapabilities in sequence modeling. However, in practical deployments,\ntime-series prediction often requires operation in resource-constrained\nenvironments, such as edge devices, which are unable to handle the\ncomputational overhead of large models. To address such scenarios, some\nlightweight models have been proposed, but they exhibit poor performance on\nnon-stationary sequences. In this paper, we propose $\\textit{SWIFT}$, a\nlightweight model that is not only powerful, but also efficient in deployment\nand inference for Long-term Time Series Forecasting (LTSF). Our model is based\non three key points: (i) Utilizing wavelet transform to perform lossless\ndownsampling of time series. (ii) Achieving cross-band information fusion with\na learnable filter. (iii) Using only one shared linear layer or one shallow MLP\nfor sub-series' mapping. We conduct comprehensive experiments, and the results\nshow that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on\nmultiple datasets, offering a promising method for edge computing and\ndeployment in this task. Moreover, it is noteworthy that the number of\nparameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a\nsingle-layer linear model for time-domain prediction. Our code is available at\nhttps://github.com/LancelotXWX/SWIFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent work on time-series prediction, Transformers and even large\nlanguage models have garnered significant attention due to their strong\ncapabilities in sequence modeling. However, in practical deployments,\ntime-series prediction often requires operation in resource-constrained\nenvironments, such as edge devices, which are unable to handle the\ncomputational overhead of large models. To address such scenarios, some\nlightweight models have been proposed, but they exhibit poor performance on\nnon-stationary sequences. In this paper, we propose $\\textit{SWIFT}$, a\nlightweight model that is not only powerful, but also efficient in deployment\nand inference for Long-term Time Series Forecasting (LTSF). Our model is based\non three key points: (i) Utilizing wavelet transform to perform lossless\ndownsampling of time series. (ii) Achieving cross-band information fusion with\na learnable filter. (iii) Using only one shared linear layer or one shallow MLP\nfor sub-series' mapping. We conduct comprehensive experiments, and the results\nshow that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on\nmultiple datasets, offering a promising method for edge computing and\ndeployment in this task. Moreover, it is noteworthy that the number of\nparameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a\nsingle-layer linear model for time-domain prediction. Our code is available at\nhttps://github.com/LancelotXWX/SWIFT."
                },
                "authors": [
                    {
                        "name": "Wenxuan Xie"
                    },
                    {
                        "name": "Fanpu Cao"
                    }
                ],
                "author_detail": {
                    "name": "Fanpu Cao"
                },
                "author": "Fanpu Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07615v2",
                "updated": "2025-01-27T16:24:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    24,
                    47,
                    0,
                    27,
                    0
                ],
                "published": "2024-09-11T20:55:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    55,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "MOSAIC: Multiple Observers Spotting AI Content, a Robust Approach to\n  Machine-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSAIC: Multiple Observers Spotting AI Content, a Robust Approach to\n  Machine-Generated Text Detection"
                },
                "summary": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities has vastly increased the\nthreats posed by generative AI technologies by reducing the cost of producing\nharmful, toxic, faked or forged content. In response, various proposals have\nbeen made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a classification problem.\nMost approaches evaluate an input document by a well-chosen detector LLM,\nassuming that low-perplexity scores reliably signal machine-made content. As\nusing one single detector can induce brittleness of performance, we instead\nconsider several and derive a new, theoretically grounded approach to combine\ntheir respective strengths. Our experiments, using a variety of generator LLMs,\nsuggest that our method effectively leads to robust detection performances. An\nearly version of the code is available at\nhttps://github.com/BaggerOfWords/MOSAIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities has vastly increased the\nthreats posed by generative AI technologies by reducing the cost of producing\nharmful, toxic, faked or forged content. In response, various proposals have\nbeen made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a classification problem.\nMost approaches evaluate an input document by a well-chosen detector LLM,\nassuming that low-perplexity scores reliably signal machine-made content. As\nusing one single detector can induce brittleness of performance, we instead\nconsider several and derive a new, theoretically grounded approach to combine\ntheir respective strengths. Our experiments, using a variety of generator LLMs,\nsuggest that our method effectively leads to robust detection performances. An\nearly version of the code is available at\nhttps://github.com/BaggerOfWords/MOSAIC."
                },
                "authors": [
                    {
                        "name": "Matthieu Dubois"
                    },
                    {
                        "name": "François Yvon"
                    },
                    {
                        "name": "Pablo Piantanida"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Piantanida"
                },
                "author": "Pablo Piantanida",
                "arxiv_comment": "Still a work in progress, early version of the code can be found here\n  :https://github.com/BaggerOfWords/MOSAIC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10259v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10259v4",
                "updated": "2025-01-27T16:15:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    15,
                    26,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-16T03:26:43Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    3,
                    26,
                    43,
                    1,
                    107,
                    0
                ],
                "title": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy"
                },
                "summary": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events."
                },
                "authors": [
                    {
                        "name": "Tunazzina Islam"
                    },
                    {
                        "name": "Dan Goldwasser"
                    }
                ],
                "author_detail": {
                    "name": "Dan Goldwasser"
                },
                "author": "Dan Goldwasser",
                "arxiv_comment": "Accepted at the Findings of 2025 Annual Conference of the Nations of\n  the Americas Chapter of the ACL (NAACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10259v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10259v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16173v1",
                "updated": "2025-01-27T16:14:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    14,
                    33,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T16:14:33Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    14,
                    33,
                    0,
                    27,
                    0
                ],
                "title": "Will Systems of LLM Agents Cooperate: An Investigation into a Social\n  Dilemma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will Systems of LLM Agents Cooperate: An Investigation into a Social\n  Dilemma"
                },
                "summary": "As autonomous agents become more prevalent, understanding their collective\nbehaviour in strategic interactions is crucial. This study investigates the\nemergent cooperative tendencies of systems of Large Language Model (LLM) agents\nin a social dilemma. Unlike previous research where LLMs output individual\nactions, we prompt state-of-the-art LLMs to generate complete strategies for\niterated Prisoner's Dilemma. Using evolutionary game theory, we simulate\npopulations of agents with different strategic dispositions (aggressive,\ncooperative, or neutral) and observe their evolutionary dynamics. Our findings\nreveal that different LLMs exhibit distinct biases affecting the relative\nsuccess of aggressive versus cooperative strategies. This research provides\ninsights into the potential long-term behaviour of systems of deployed\nLLM-based autonomous agents and highlights the importance of carefully\nconsidering the strategic environments in which they operate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous agents become more prevalent, understanding their collective\nbehaviour in strategic interactions is crucial. This study investigates the\nemergent cooperative tendencies of systems of Large Language Model (LLM) agents\nin a social dilemma. Unlike previous research where LLMs output individual\nactions, we prompt state-of-the-art LLMs to generate complete strategies for\niterated Prisoner's Dilemma. Using evolutionary game theory, we simulate\npopulations of agents with different strategic dispositions (aggressive,\ncooperative, or neutral) and observe their evolutionary dynamics. Our findings\nreveal that different LLMs exhibit distinct biases affecting the relative\nsuccess of aggressive versus cooperative strategies. This research provides\ninsights into the potential long-term behaviour of systems of deployed\nLLM-based autonomous agents and highlights the importance of carefully\nconsidering the strategic environments in which they operate."
                },
                "authors": [
                    {
                        "name": "Richard Willis"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Joel Z Leibo"
                    },
                    {
                        "name": "Michael Luck"
                    }
                ],
                "author_detail": {
                    "name": "Michael Luck"
                },
                "author": "Michael Luck",
                "arxiv_comment": "7 pages (10 including references), 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11675v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11675v5",
                "updated": "2025-01-27T16:00:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    0,
                    59,
                    0,
                    27,
                    0
                ],
                "published": "2024-06-17T15:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    55,
                    38,
                    0,
                    169,
                    0
                ],
                "title": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) often suffer from overconfidence during\ninference, particularly when adapted to downstream domain-specific tasks with\nlimited data. Previous work addresses this issue by employing approximate\nBayesian estimation after the LLMs are trained, enabling them to quantify\nuncertainty. However, such post-training approaches' performance is severely\nlimited by the parameters learned during training. In this paper, we go beyond\npost-training Bayesianization and propose Bayesian Low-Rank Adaptation by\nBackpropagation (BLoB), an algorithm that continuously and jointly adjusts both\nthe mean and covariance of LLM parameters throughout the whole fine-tuning\nprocess. Our empirical results verify the effectiveness of BLoB in terms of\ngeneralization and uncertainty estimation, when evaluated on both\nin-distribution and out-of-distribution data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often suffer from overconfidence during\ninference, particularly when adapted to downstream domain-specific tasks with\nlimited data. Previous work addresses this issue by employing approximate\nBayesian estimation after the LLMs are trained, enabling them to quantify\nuncertainty. However, such post-training approaches' performance is severely\nlimited by the parameters learned during training. In this paper, we go beyond\npost-training Bayesianization and propose Bayesian Low-Rank Adaptation by\nBackpropagation (BLoB), an algorithm that continuously and jointly adjusts both\nthe mean and covariance of LLM parameters throughout the whole fine-tuning\nprocess. Our empirical results verify the effectiveness of BLoB in terms of\ngeneralization and uncertainty estimation, when evaluated on both\nin-distribution and out-of-distribution data."
                },
                "authors": [
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Dimitris Metaxas"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024. Additional experiments have been included\n  in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11675v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11675v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16164v1",
                "updated": "2025-01-27T15:59:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    59,
                    58,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:59:58Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    59,
                    58,
                    0,
                    27,
                    0
                ],
                "title": "MetaDecorator: Generating Immersive Virtual Tours through Multimodality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaDecorator: Generating Immersive Virtual Tours through Multimodality"
                },
                "summary": "MetaDecorator, is a framework that empowers users to personalize virtual\nspaces. By leveraging text-driven prompts and image synthesis techniques,\nMetaDecorator adorns static panoramas captured by 360{\\deg} imaging devices,\ntransforming them into uniquely styled and visually appealing environments.\nThis significantly enhances the realism and engagement of virtual tours\ncompared to traditional offerings. Beyond the core framework, we also discuss\nthe integration of Large Language Models (LLMs) and haptics in the VR\napplication to provide a more immersive experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaDecorator, is a framework that empowers users to personalize virtual\nspaces. By leveraging text-driven prompts and image synthesis techniques,\nMetaDecorator adorns static panoramas captured by 360{\\deg} imaging devices,\ntransforming them into uniquely styled and visually appealing environments.\nThis significantly enhances the realism and engagement of virtual tours\ncompared to traditional offerings. Beyond the core framework, we also discuss\nthe integration of Large Language Models (LLMs) and haptics in the VR\napplication to provide a more immersive experience."
                },
                "authors": [
                    {
                        "name": "Shuang Xie"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jeannie S. A. Lee"
                    },
                    {
                        "name": "Haiwei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Haiwei Dong"
                },
                "author": "Haiwei Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16155v1",
                "updated": "2025-01-27T15:49:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    49,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:49:24Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    49,
                    24,
                    0,
                    27,
                    0
                ],
                "title": "CITYWALK: Enhancing LLM-Based C++ Unit Test Generation via\n  Project-Dependency Awareness and Language-Specific Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITYWALK: Enhancing LLM-Based C++ Unit Test Generation via\n  Project-Dependency Awareness and Language-Specific Knowledge"
                },
                "summary": "Unit testing plays a pivotal role in the software development lifecycle, as\nit ensures code quality. However, writing high-quality unit tests remains a\ntime-consuming task for developers in practice. More recently, the application\nof large language models (LLMs) in automated unit test generation has\ndemonstrated promising results. Existing approaches primarily focus on\ninterpreted programming languages (e.g., Java), while mature solutions tailored\nto compiled programming languages like C++ are yet to be explored. The\nintricate language features of C++, such as pointers, templates, and virtual\nfunctions, pose particular challenges for LLMs in generating both executable\nand high-coverage unit tests. To tackle the aforementioned problems, this paper\nintroduces CITYWALK, a novel LLM-based framework for C++ unit test generation.\nCITYWALK enhances LLMs by providing a comprehensive understanding of the\ndependency relationships within the project under test via program analysis.\nFurthermore, CITYWALK incorporates language-specific knowledge about C++\nderived from project documentation and empirical observations, significantly\nimproving the correctness of the LLM-generated unit tests. We implement\nCITYWALK by employing the widely popular LLM GPT-4o. The experimental results\nshow that CITYWALK outperforms current state-of-the-art approaches on a\ncollection of eight popular C++ projects. Our findings demonstrate the\neffectiveness of CITYWALK in generating high-quality C++ unit tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing plays a pivotal role in the software development lifecycle, as\nit ensures code quality. However, writing high-quality unit tests remains a\ntime-consuming task for developers in practice. More recently, the application\nof large language models (LLMs) in automated unit test generation has\ndemonstrated promising results. Existing approaches primarily focus on\ninterpreted programming languages (e.g., Java), while mature solutions tailored\nto compiled programming languages like C++ are yet to be explored. The\nintricate language features of C++, such as pointers, templates, and virtual\nfunctions, pose particular challenges for LLMs in generating both executable\nand high-coverage unit tests. To tackle the aforementioned problems, this paper\nintroduces CITYWALK, a novel LLM-based framework for C++ unit test generation.\nCITYWALK enhances LLMs by providing a comprehensive understanding of the\ndependency relationships within the project under test via program analysis.\nFurthermore, CITYWALK incorporates language-specific knowledge about C++\nderived from project documentation and empirical observations, significantly\nimproving the correctness of the LLM-generated unit tests. We implement\nCITYWALK by employing the widely popular LLM GPT-4o. The experimental results\nshow that CITYWALK outperforms current state-of-the-art approaches on a\ncollection of eight popular C++ projects. Our findings demonstrate the\neffectiveness of CITYWALK in generating high-quality C++ unit tests."
                },
                "authors": [
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Qingyuan Lu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Wensheng Dou"
                    },
                    {
                        "name": "Jiaxin Zhu"
                    },
                    {
                        "name": "Li Qian"
                    },
                    {
                        "name": "Chunxi Zhang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Jun Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wei"
                },
                "author": "Jun Wei",
                "arxiv_comment": "13 tables, 12 figures. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16154v1",
                "updated": "2025-01-27T15:48:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    48,
                    57,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:48:57Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    48,
                    57,
                    0,
                    27,
                    0
                ],
                "title": "AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive\n  Chain-of-Thought"
                },
                "summary": "Large language models (LLMs) have shown impressive multilingual capabilities\nthrough pretraining on diverse corpora. While these models show strong\nreasoning abilities, their performance varies significantly across languages\ndue to uneven training data distribution. Existing approaches using machine\ntranslation, and extensive multilingual pretraining and cross-lingual tuning\nface scalability challenges and often fail to capture nuanced reasoning\nprocesses across languages. In this paper, we introduce AdaCoT (Adaptive\nChain-of-Thought), a framework that enhances multilingual reasoning by\ndynamically routing thought processes through intermediary \"thinking languages\"\nbefore generating target-language responses. AdaCoT leverages a\nlanguage-agnostic core and incorporates an adaptive, reward-based mechanism for\nselecting optimal reasoning pathways without requiring additional pretraining.\nOur comprehensive evaluation across multiple benchmarks demonstrates\nsubstantial improvements in both factual reasoning quality and cross-lingual\nconsistency, with particularly strong performance gains in low-resource\nlanguage settings. The results suggest that adaptive reasoning paths can\neffectively bridge the performance gap between high and low-resource languages\nwhile maintaining cultural and linguistic nuances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive multilingual capabilities\nthrough pretraining on diverse corpora. While these models show strong\nreasoning abilities, their performance varies significantly across languages\ndue to uneven training data distribution. Existing approaches using machine\ntranslation, and extensive multilingual pretraining and cross-lingual tuning\nface scalability challenges and often fail to capture nuanced reasoning\nprocesses across languages. In this paper, we introduce AdaCoT (Adaptive\nChain-of-Thought), a framework that enhances multilingual reasoning by\ndynamically routing thought processes through intermediary \"thinking languages\"\nbefore generating target-language responses. AdaCoT leverages a\nlanguage-agnostic core and incorporates an adaptive, reward-based mechanism for\nselecting optimal reasoning pathways without requiring additional pretraining.\nOur comprehensive evaluation across multiple benchmarks demonstrates\nsubstantial improvements in both factual reasoning quality and cross-lingual\nconsistency, with particularly strong performance gains in low-resource\nlanguage settings. The results suggest that adaptive reasoning paths can\neffectively bridge the performance gap between high and low-resource languages\nwhile maintaining cultural and linguistic nuances."
                },
                "authors": [
                    {
                        "name": "Xin Huang"
                    },
                    {
                        "name": "Tarun Kumar Vangani"
                    },
                    {
                        "name": "Zhengyuan Liu"
                    },
                    {
                        "name": "Bowei Zou"
                    },
                    {
                        "name": "Ai Ti Aw"
                    }
                ],
                "author_detail": {
                    "name": "Ai Ti Aw"
                },
                "author": "Ai Ti Aw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16150v1",
                "updated": "2025-01-27T15:44:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    44,
                    2,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:44:02Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    44,
                    2,
                    0,
                    27,
                    0
                ],
                "title": "AI Agents for Computer Use: A Review of Instruction-based Computer\n  Control, GUI Automation, and Operator Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents for Computer Use: A Review of Instruction-based Computer\n  Control, GUI Automation, and Operator Assistants"
                },
                "summary": "Instruction-based computer control agents (CCAs) execute complex action\nsequences on personal computers or mobile devices to fulfill tasks using the\nsame graphical user interfaces as a human user would, provided instructions in\nnatural language. This review offers a comprehensive overview of the emerging\nfield of instruction-based computer control, examining available agents --\ntheir taxonomy, development, and respective resources -- and emphasizing the\nshift from manually designed, specialized agents to leveraging foundation\nmodels such as large language models (LLMs) and vision-language models (VLMs).\nWe formalize the problem and establish a taxonomy of the field to analyze\nagents from three perspectives: (a) the environment perspective, analyzing\ncomputer environments; (b) the interaction perspective, describing observations\nspaces (e.g., screenshots, HTML) and action spaces (e.g., mouse and keyboard\nactions, executable code); and (c) the agent perspective, focusing on the core\nprinciple of how an agent acts and learns to act. Our framework encompasses\nboth specialized and foundation agents, facilitating their comparative analysis\nand revealing how prior solutions in specialized agents, such as an environment\nlearning step, can guide the development of more capable foundation agents.\nAdditionally, we review current CCA datasets and CCA evaluation methods and\noutline the challenges to deploying such agents in a productive setting. In\ntotal, we review and classify 86 CCAs and 33 related datasets. By highlighting\ntrends, limitations, and future research directions, this work presents a\ncomprehensive foundation to obtain a broad understanding of the field and push\nits future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-based computer control agents (CCAs) execute complex action\nsequences on personal computers or mobile devices to fulfill tasks using the\nsame graphical user interfaces as a human user would, provided instructions in\nnatural language. This review offers a comprehensive overview of the emerging\nfield of instruction-based computer control, examining available agents --\ntheir taxonomy, development, and respective resources -- and emphasizing the\nshift from manually designed, specialized agents to leveraging foundation\nmodels such as large language models (LLMs) and vision-language models (VLMs).\nWe formalize the problem and establish a taxonomy of the field to analyze\nagents from three perspectives: (a) the environment perspective, analyzing\ncomputer environments; (b) the interaction perspective, describing observations\nspaces (e.g., screenshots, HTML) and action spaces (e.g., mouse and keyboard\nactions, executable code); and (c) the agent perspective, focusing on the core\nprinciple of how an agent acts and learns to act. Our framework encompasses\nboth specialized and foundation agents, facilitating their comparative analysis\nand revealing how prior solutions in specialized agents, such as an environment\nlearning step, can guide the development of more capable foundation agents.\nAdditionally, we review current CCA datasets and CCA evaluation methods and\noutline the challenges to deploying such agents in a productive setting. In\ntotal, we review and classify 86 CCAs and 33 related datasets. By highlighting\ntrends, limitations, and future research directions, this work presents a\ncomprehensive foundation to obtain a broad understanding of the field and push\nits future development."
                },
                "authors": [
                    {
                        "name": "Pascal J. Sager"
                    },
                    {
                        "name": "Benjamin Meyer"
                    },
                    {
                        "name": "Peng Yan"
                    },
                    {
                        "name": "Rebekka von Wartburg-Kottler"
                    },
                    {
                        "name": "Layan Etaiwi"
                    },
                    {
                        "name": "Aref Enayati"
                    },
                    {
                        "name": "Gabriel Nobel"
                    },
                    {
                        "name": "Ahmed Abdulkadir"
                    },
                    {
                        "name": "Benjamin F. Grewe"
                    },
                    {
                        "name": "Thilo Stadelmann"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Stadelmann"
                },
                "author": "Thilo Stadelmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16149v1",
                "updated": "2025-01-27T15:43:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    43,
                    4,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:43:04Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    43,
                    4,
                    0,
                    27,
                    0
                ],
                "title": "PATCH: Empowering Large Language Model with Programmer-Intent Guidance\n  and Collaborative-Behavior Simulation for Automatic Bug Fixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCH: Empowering Large Language Model with Programmer-Intent Guidance\n  and Collaborative-Behavior Simulation for Automatic Bug Fixing"
                },
                "summary": "Bug fixing holds significant importance in software development and\nmaintenance. Recent research has made substantial strides in exploring the\npotential of large language models (LLMs) for automatically resolving software\nbugs. However, a noticeable gap in existing approaches lies in the oversight of\ncollaborative facets intrinsic to bug resolution, treating the process as a\nsingle-stage endeavor. Moreover, most approaches solely take the buggy code\nsnippet as input for LLMs during the patch generation stage. To mitigate the\naforementioned limitations, we introduce a novel stage-wise framework named\nPATCH. Specifically, we first augment the buggy code snippet with corresponding\ndependence context and intent information to better guide LLMs in generating\nthe correct candidate patches. Additionally, by taking inspiration from bug\nmanagement practices, we decompose the bug-fixing task into four distinct\nstages: bug reporting, bug diagnosis, patch generation, and patch verification.\nThese stages are performed interactively by LLMs, aiming to simulate the\ncollaborative behavior of programmers during the resolution of software bugs.\nBy harnessing these collective contributions, PATCH effectively enhances the\nbug-fixing capability of LLMs. We implement PATCH by employing the powerful\ndialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing\nbenchmark BFP demonstrates that PATCH has achieved better performance than\nstate-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bug fixing holds significant importance in software development and\nmaintenance. Recent research has made substantial strides in exploring the\npotential of large language models (LLMs) for automatically resolving software\nbugs. However, a noticeable gap in existing approaches lies in the oversight of\ncollaborative facets intrinsic to bug resolution, treating the process as a\nsingle-stage endeavor. Moreover, most approaches solely take the buggy code\nsnippet as input for LLMs during the patch generation stage. To mitigate the\naforementioned limitations, we introduce a novel stage-wise framework named\nPATCH. Specifically, we first augment the buggy code snippet with corresponding\ndependence context and intent information to better guide LLMs in generating\nthe correct candidate patches. Additionally, by taking inspiration from bug\nmanagement practices, we decompose the bug-fixing task into four distinct\nstages: bug reporting, bug diagnosis, patch generation, and patch verification.\nThese stages are performed interactively by LLMs, aiming to simulate the\ncollaborative behavior of programmers during the resolution of software bugs.\nBy harnessing these collective contributions, PATCH effectively enhances the\nbug-fixing capability of LLMs. We implement PATCH by employing the powerful\ndialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing\nbenchmark BFP demonstrates that PATCH has achieved better performance than\nstate-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Yuwei Zhang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Ying Xing"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Jiaxin Zhu"
                    },
                    {
                        "name": "Wensheng Dou"
                    },
                    {
                        "name": "Jun Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wei"
                },
                "author": "Jun Wei",
                "arxiv_comment": "8 tables, 18 figures. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07921v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07921v3",
                "updated": "2025-01-27T15:39:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    39,
                    26,
                    0,
                    27,
                    0
                ],
                "published": "2024-02-28T03:20:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    3,
                    20,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Merino: Entropy-driven Design for Generative Language Models on IoT\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merino: Entropy-driven Design for Generative Language Models on IoT\n  Devices"
                },
                "summary": "Generative Large Language Models (LLMs) stand as a revolutionary advancement\nin the modern era of artificial intelligence (AI). However, scaling down LLMs\nfor resource-constrained hardware, such as Internet-of-Things (IoT) devices\nrequires non-trivial efforts and domain knowledge. In this paper, we propose a\nnovel information-entropy framework for designing mobile-friendly generative\nlanguage models. The whole design procedure involves solving a mathematical\nprogramming (MP) problem, which can be done on the CPU within minutes, making\nit nearly zero-cost. We evaluate our designed models, termed MeRino, across\nfourteen NLP downstream tasks, showing their competitive performance against\nthe state-of-the-art autoregressive transformer models under the mobile\nsetting. Notably, MeRino achieves similar or better performance on both\nlanguage modeling and zero-shot learning tasks, compared to the 350M parameter\nOPT while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Language Models (LLMs) stand as a revolutionary advancement\nin the modern era of artificial intelligence (AI). However, scaling down LLMs\nfor resource-constrained hardware, such as Internet-of-Things (IoT) devices\nrequires non-trivial efforts and domain knowledge. In this paper, we propose a\nnovel information-entropy framework for designing mobile-friendly generative\nlanguage models. The whole design procedure involves solving a mathematical\nprogramming (MP) problem, which can be done on the CPU within minutes, making\nit nearly zero-cost. We evaluate our designed models, termed MeRino, across\nfourteen NLP downstream tasks, showing their competitive performance against\nthe state-of-the-art autoregressive transformer models under the mobile\nsetting. Notably, MeRino achieves similar or better performance on both\nlanguage modeling and zero-shot learning tasks, compared to the 350M parameter\nOPT while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model\nsize."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Ming Lin"
                    },
                    {
                        "name": "Huadong Tang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07921v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07921v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16143v1",
                "updated": "2025-01-27T15:36:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    36,
                    51,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:36:51Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    36,
                    51,
                    0,
                    27,
                    0
                ],
                "title": "Disruption-aware Microservice Re-orchestration for Cost-efficient\n  Multi-cloud Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disruption-aware Microservice Re-orchestration for Cost-efficient\n  Multi-cloud Deployments"
                },
                "summary": "Multi-cloud environments enable a cost-efficient scaling of cloud-native\napplications across geographically distributed virtual nodes with different\npricing models. In this context, the resource fragmentation caused by frequent\nchanges in the resource demands of deployed microservices, along with the\nallocation or termination of new and existing microservices, increases the\ndeployment cost. Therefore, re-orchestrating deployed microservices on a\ncheaper configuration of multi-cloud nodes offers a practical solution to\nrestore the cost efficiency of deployment. However, the rescheduling procedure\ncauses frequent service interruptions due to the continuous termination and\nrebooting of the containerized microservices. Moreover, it may potentially\ninterfere with and delay other deployment operations, compromising the\nstability of the running applications. To address this issue, we formulate a\nmulti-objective integer linear programming problem that computes a microservice\nrescheduling solution capable of providing minimum deployment cost without\nsignificantly affecting the service continuity. At the same time, the proposed\nformulation also preserves the quality of service (QoS) requirements, including\nlatency, expressed through microservice colocation constraints. Additionally,\nwe present a heuristic algorithm to approximate the optimal solution, striking\na balance between cost reduction and service disruption mitigation. We\nintegrate the proposed approach as a custom plugin of the Kubernetes scheduler.\nResults reveal that our approach significantly reduces multi-cloud deployment\ncosts and service disruptions compared to the default Kubernetes scheduler\nimplementation, while ensuring QoS requirements are consistently met.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-cloud environments enable a cost-efficient scaling of cloud-native\napplications across geographically distributed virtual nodes with different\npricing models. In this context, the resource fragmentation caused by frequent\nchanges in the resource demands of deployed microservices, along with the\nallocation or termination of new and existing microservices, increases the\ndeployment cost. Therefore, re-orchestrating deployed microservices on a\ncheaper configuration of multi-cloud nodes offers a practical solution to\nrestore the cost efficiency of deployment. However, the rescheduling procedure\ncauses frequent service interruptions due to the continuous termination and\nrebooting of the containerized microservices. Moreover, it may potentially\ninterfere with and delay other deployment operations, compromising the\nstability of the running applications. To address this issue, we formulate a\nmulti-objective integer linear programming problem that computes a microservice\nrescheduling solution capable of providing minimum deployment cost without\nsignificantly affecting the service continuity. At the same time, the proposed\nformulation also preserves the quality of service (QoS) requirements, including\nlatency, expressed through microservice colocation constraints. Additionally,\nwe present a heuristic algorithm to approximate the optimal solution, striking\na balance between cost reduction and service disruption mitigation. We\nintegrate the proposed approach as a custom plugin of the Kubernetes scheduler.\nResults reveal that our approach significantly reduces multi-cloud deployment\ncosts and service disruptions compared to the default Kubernetes scheduler\nimplementation, while ensuring QoS requirements are consistently met."
                },
                "authors": [
                    {
                        "name": "Marco Zambianco"
                    },
                    {
                        "name": "Silvio Cretti"
                    },
                    {
                        "name": "Domenico Siracusa"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Siracusa"
                },
                "author": "Domenico Siracusa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13587v2",
                "updated": "2025-01-27T15:30:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    30,
                    2,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-23T11:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    55,
                    13,
                    3,
                    23,
                    0
                ],
                "title": "Contrastive Representation Learning Helps Cross-institutional Knowledge\n  Transfer: A Study in Pediatric Ventilation Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Representation Learning Helps Cross-institutional Knowledge\n  Transfer: A Study in Pediatric Ventilation Management"
                },
                "summary": "Clinical machine learning deployment across institutions faces significant\nchallenges when patient populations and clinical practices differ\nsubstantially. We present a systematic framework for cross-institutional\nknowledge transfer in clinical time series, demonstrated through pediatric\nventilation management between a general pediatric intensive care unit (PICU)\nand a cardiac-focused unit. Using contrastive predictive coding (CPC) for\nrepresentation learning, we investigate how different data regimes and\nfine-tuning strategies affect knowledge transfer across institutional\nboundaries. Our results show that while direct model transfer performs poorly,\nCPC with appropriate fine-tuning enables effective knowledge sharing between\ninstitutions, with benefits particularly evident in limited data scenarios.\nAnalysis of transfer patterns reveals an important asymmetry: temporal\nprogression patterns transfer more readily than point-of-care decisions,\nsuggesting practical pathways for cross-institutional deployment. Through a\nsystematic evaluation of fine-tuning approaches and transfer patterns, our work\nprovides insights for developing more generalizable clinical decision support\nsystems while enabling smaller specialized units to leverage knowledge from\nlarger centers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical machine learning deployment across institutions faces significant\nchallenges when patient populations and clinical practices differ\nsubstantially. We present a systematic framework for cross-institutional\nknowledge transfer in clinical time series, demonstrated through pediatric\nventilation management between a general pediatric intensive care unit (PICU)\nand a cardiac-focused unit. Using contrastive predictive coding (CPC) for\nrepresentation learning, we investigate how different data regimes and\nfine-tuning strategies affect knowledge transfer across institutional\nboundaries. Our results show that while direct model transfer performs poorly,\nCPC with appropriate fine-tuning enables effective knowledge sharing between\ninstitutions, with benefits particularly evident in limited data scenarios.\nAnalysis of transfer patterns reveals an important asymmetry: temporal\nprogression patterns transfer more readily than point-of-care decisions,\nsuggesting practical pathways for cross-institutional deployment. Through a\nsystematic evaluation of fine-tuning approaches and transfer patterns, our work\nprovides insights for developing more generalizable clinical decision support\nsystems while enabling smaller specialized units to leverage knowledge from\nlarger centers."
                },
                "authors": [
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Jinpei Han"
                    },
                    {
                        "name": "Padmanabhan Ramnarayan"
                    },
                    {
                        "name": "A. Aldo Faisal"
                    }
                ],
                "author_detail": {
                    "name": "A. Aldo Faisal"
                },
                "author": "A. Aldo Faisal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16125v1",
                "updated": "2025-01-27T15:12:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    12,
                    27,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T15:12:27Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    12,
                    27,
                    0,
                    27,
                    0
                ],
                "title": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SampleLLM: Optimizing Tabular Data Synthesis in Recommendations"
                },
                "summary": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data synthesis is crucial in machine learning, yet existing general\nmethods-primarily based on statistical or deep learning models-are highly\ndata-dependent and often fall short in recommender systems. This limitation\narises from their difficulty in capturing complex distributions and\nunderstanding feature relationships from sparse and limited data, along with\ntheir inability to grasp semantic feature relations. Recently, Large Language\nModels (LLMs) have shown potential in generating synthetic data samples through\nfew-shot learning and semantic understanding. However, they often suffer from\ninconsistent distribution and lack of diversity due to their inherent\ndistribution disparity with the target dataset. To address these challenges and\nenhance tabular data synthesis for recommendation tasks, we propose a novel\ntwo-stage framework named SampleLLM to improve the quality of LLM-based tabular\ndata synthesis for recommendations by ensuring better distribution alignment.\nIn the first stage, SampleLLM employs LLMs with Chain-of-Thought prompts and\ndiverse exemplars to generate data that closely aligns with the target dataset\ndistribution, even when input samples are limited. The second stage uses an\nadvanced feature attribution-based importance sampling method to refine feature\nrelationships within the synthesized data, reducing any distribution biases\nintroduced by the LLM. Experimental results on three recommendation datasets,\ntwo general datasets, and online deployment illustrate that SampleLLM\nsignificantly surpasses existing methods for recommendation tasks and holds\npromise for a broader range of tabular data scenarios."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17890v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17890v3",
                "updated": "2025-01-27T15:12:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    12,
                    2,
                    0,
                    27,
                    0
                ],
                "published": "2024-05-28T07:12:06Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    7,
                    12,
                    6,
                    1,
                    149,
                    0
                ],
                "title": "SLMRec: Distilling Large Language Models into Small for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLMRec: Distilling Large Language Models into Small for Sequential\n  Recommendation"
                },
                "summary": "Sequential Recommendation (SR) task involves predicting the next item a user\nis likely to interact with, given their past interactions. The SR models\nexamine the sequence of a user's actions to discern more complex behavioral\npatterns and temporal dynamics. Recent research demonstrates the great impact\nof LLMs on sequential recommendation systems, either viewing sequential\nrecommendation as language modeling or serving as the backbone for user\nrepresentation. Although these methods deliver outstanding performance, there\nis scant evidence of the necessity of a large language model and how large the\nlanguage model is needed, especially in the sequential recommendation scene.\nMeanwhile, due to the huge size of LLMs, it is inefficient and impractical to\napply a LLM-based model in real-world platforms that often need to process\nbillions of traffic logs daily. In this paper, we explore the influence of\nLLMs' depth by conducting extensive experiments on large-scale industry\ndatasets. Surprisingly, our motivational experiments reveal that most\nintermediate layers of LLMs are redundant, indicating that pruning the\nremaining layers can still maintain strong performance. Motivated by this\ninsight, we empower small language models for SR, namely SLMRec, which adopt a\nsimple yet effective knowledge distillation method. Moreover, SLMRec is\northogonal to other post-training efficiency techniques, such as quantization\nand pruning, so that they can be leveraged in combination. Comprehensive\nexperimental results illustrate that the proposed SLMRec model attains the best\nperformance using only 13% of the parameters found in LLM-based recommendation\nmodels while simultaneously achieving up to 6.6x and 8.0x speedups in training\nand inference time costs, respectively. Besides, we provide a theoretical\njustification for why small language models can perform comparably to large\nlanguage models in SR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Recommendation (SR) task involves predicting the next item a user\nis likely to interact with, given their past interactions. The SR models\nexamine the sequence of a user's actions to discern more complex behavioral\npatterns and temporal dynamics. Recent research demonstrates the great impact\nof LLMs on sequential recommendation systems, either viewing sequential\nrecommendation as language modeling or serving as the backbone for user\nrepresentation. Although these methods deliver outstanding performance, there\nis scant evidence of the necessity of a large language model and how large the\nlanguage model is needed, especially in the sequential recommendation scene.\nMeanwhile, due to the huge size of LLMs, it is inefficient and impractical to\napply a LLM-based model in real-world platforms that often need to process\nbillions of traffic logs daily. In this paper, we explore the influence of\nLLMs' depth by conducting extensive experiments on large-scale industry\ndatasets. Surprisingly, our motivational experiments reveal that most\nintermediate layers of LLMs are redundant, indicating that pruning the\nremaining layers can still maintain strong performance. Motivated by this\ninsight, we empower small language models for SR, namely SLMRec, which adopt a\nsimple yet effective knowledge distillation method. Moreover, SLMRec is\northogonal to other post-training efficiency techniques, such as quantization\nand pruning, so that they can be leveraged in combination. Comprehensive\nexperimental results illustrate that the proposed SLMRec model attains the best\nperformance using only 13% of the parameters found in LLM-based recommendation\nmodels while simultaneously achieving up to 6.6x and 8.0x speedups in training\nand inference time costs, respectively. Besides, we provide a theoretical\njustification for why small language models can perform comparably to large\nlanguage models in SR."
                },
                "authors": [
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Qitian Wu"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Jiaojiao Han"
                    },
                    {
                        "name": "Xuying Ning"
                    },
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wenfang Lin"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "International Conference on Learning Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17890v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17890v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10994v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10994v3",
                "updated": "2025-01-27T15:00:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    15,
                    0,
                    31,
                    0,
                    27,
                    0
                ],
                "published": "2024-06-24T12:09:34Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    12,
                    9,
                    34,
                    0,
                    176,
                    0
                ],
                "title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant"
                },
                "summary": "The availability of powerful open-source large language models (LLMs) opens\nexciting use cases, such as automated personal assistants that adapt to the\nuser's unique data and demands. Two key requirements for such assistants are\npersonalization - in the sense that the assistant should reflect the user's own\nwriting style - and privacy - users may prefer to always store their personal\ndata locally, on their own computing device. In this application paper, we\npresent a new design and evaluation for such an automated assistant, for the\nspecific use case of email generation, which we call Panza. Specifically, Panza\ncan be trained and deployed locally on commodity hardware, and is personalized\nto the user's writing style. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to better reflect a user's\nwriting style using limited data, while executing on extremely limited\nresources, e.g. on a free Google Colab instance. Our key methodological\ncontribution is what we believe to be the first detailed study of evaluation\nmetrics for this personalized writing task, and of how different choices of\nsystem components - e.g. the use of RAG and of different fine-tuning approaches\n- impact the system's performance. We also perform an ablation study showing\nthat less than 100 emails are generally sufficient to produce a credible Panza\nmodel. We are releasing the full Panza code as well as a new \"David\"\npersonalized email dataset licensed for research use, both available on\nhttps://github.com/IST-DASLab/PanzaMail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of powerful open-source large language models (LLMs) opens\nexciting use cases, such as automated personal assistants that adapt to the\nuser's unique data and demands. Two key requirements for such assistants are\npersonalization - in the sense that the assistant should reflect the user's own\nwriting style - and privacy - users may prefer to always store their personal\ndata locally, on their own computing device. In this application paper, we\npresent a new design and evaluation for such an automated assistant, for the\nspecific use case of email generation, which we call Panza. Specifically, Panza\ncan be trained and deployed locally on commodity hardware, and is personalized\nto the user's writing style. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to better reflect a user's\nwriting style using limited data, while executing on extremely limited\nresources, e.g. on a free Google Colab instance. Our key methodological\ncontribution is what we believe to be the first detailed study of evaluation\nmetrics for this personalized writing task, and of how different choices of\nsystem components - e.g. the use of RAG and of different fine-tuning approaches\n- impact the system's performance. We also perform an ablation study showing\nthat less than 100 emails are generally sufficient to produce a credible Panza\nmodel. We are releasing the full Panza code as well as a new \"David\"\npersonalized email dataset licensed for research use, both available on\nhttps://github.com/IST-DASLab/PanzaMail."
                },
                "authors": [
                    {
                        "name": "Armand Nicolicioiu"
                    },
                    {
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "name": "Andrej Jovanovic"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Ilia Markov"
                    },
                    {
                        "name": "Nir Shavit"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Panza is available at https://github.com/IST-DASLab/PanzaMail",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10994v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10994v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14334v2",
                "updated": "2025-01-27T14:50:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    50,
                    32,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-24T08:58:49Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    58,
                    49,
                    4,
                    24,
                    0
                ],
                "title": "Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the sustainable scaling of AI dilemma: A projective study of\n  corporations' AI environmental impacts"
                },
                "summary": "The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.\n  In this paper, we propose a methodology to estimate the environmental impact\nof a company's AI portfolio, providing actionable insights without\nnecessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results\nconfirm that large generative AI models consume up to 4600x more energy than\ntraditional models. Our modelling approach, which accounts for increased AI\nusage, hardware computing efficiency, and changes in electricity mix in line\nwith IPCC scenarios, forecasts AI electricity use up to 2030. Under a high\nadoption scenario, driven by widespread Generative AI and agents adoption\nassociated to increasingly complex models and frameworks, AI electricity use is\nprojected to rise by a factor of 24.4.\n  Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n\"Return on Environment\" metric to align AI development with net-zero goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of artificial intelligence (AI), particularly Large Language\nModels (LLMs), has raised concerns regarding its global environmental impact\nthat extends beyond greenhouse gas emissions to include consideration of\nhardware fabrication and end-of-life processes. The opacity from major\nproviders hinders companies' abilities to evaluate their AI-related\nenvironmental impacts and achieve net-zero targets.\n  In this paper, we propose a methodology to estimate the environmental impact\nof a company's AI portfolio, providing actionable insights without\nnecessitating extensive AI and Life-Cycle Assessment (LCA) expertise. Results\nconfirm that large generative AI models consume up to 4600x more energy than\ntraditional models. Our modelling approach, which accounts for increased AI\nusage, hardware computing efficiency, and changes in electricity mix in line\nwith IPCC scenarios, forecasts AI electricity use up to 2030. Under a high\nadoption scenario, driven by widespread Generative AI and agents adoption\nassociated to increasingly complex models and frameworks, AI electricity use is\nprojected to rise by a factor of 24.4.\n  Mitigating the environmental impact of Generative AI by 2030 requires\ncoordinated efforts across the AI value chain. Isolated measures in hardware\nefficiency, model efficiency, or grid improvements alone are insufficient. We\nadvocate for standardized environmental assessment frameworks, greater\ntransparency from the all actors of the value chain and the introduction of a\n\"Return on Environment\" metric to align AI development with net-zero goals."
                },
                "authors": [
                    {
                        "name": "Clément Desroches"
                    },
                    {
                        "name": "Martin Chauvin"
                    },
                    {
                        "name": "Louis Ladan"
                    },
                    {
                        "name": "Caroline Vateau"
                    },
                    {
                        "name": "Simon Gosset"
                    },
                    {
                        "name": "Philippe Cordier"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Cordier"
                },
                "author": "Philippe Cordier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16100v1",
                "updated": "2025-01-27T14:50:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    50,
                    13,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:50:13Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    50,
                    13,
                    0,
                    27,
                    0
                ],
                "title": "Automated Detection of Sport Highlights from Audio and Video Sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Detection of Sport Highlights from Audio and Video Sources"
                },
                "summary": "This study presents a novel Deep Learning-based and lightweight approach for\nthe automated detection of sports highlights (HLs) from audio and video\nsources. HL detection is a key task in sports video analysis, traditionally\nrequiring significant human effort. Our solution leverages Deep Learning (DL)\nmodels trained on relatively small datasets of audio Mel-spectrograms and\ngrayscale video frames, achieving promising accuracy rates of 89% and 83% for\naudio and video detection, respectively. The use of small datasets, combined\nwith simple architectures, demonstrates the practicality of our method for fast\nand cost-effective deployment. Furthermore, an ensemble model combining both\nmodalities shows improved robustness against false positives and false\nnegatives. The proposed methodology offers a scalable solution for automated HL\ndetection across various types of sports video content, reducing the need for\nmanual intervention. Future work will focus on enhancing model architectures\nand extending this approach to broader scene-detection tasks in media analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a novel Deep Learning-based and lightweight approach for\nthe automated detection of sports highlights (HLs) from audio and video\nsources. HL detection is a key task in sports video analysis, traditionally\nrequiring significant human effort. Our solution leverages Deep Learning (DL)\nmodels trained on relatively small datasets of audio Mel-spectrograms and\ngrayscale video frames, achieving promising accuracy rates of 89% and 83% for\naudio and video detection, respectively. The use of small datasets, combined\nwith simple architectures, demonstrates the practicality of our method for fast\nand cost-effective deployment. Furthermore, an ensemble model combining both\nmodalities shows improved robustness against false positives and false\nnegatives. The proposed methodology offers a scalable solution for automated HL\ndetection across various types of sports video content, reducing the need for\nmanual intervention. Future work will focus on enhancing model architectures\nand extending this approach to broader scene-detection tasks in media analysis."
                },
                "authors": [
                    {
                        "name": "Francesco Della Santa"
                    },
                    {
                        "name": "Morgana Lalli"
                    }
                ],
                "author_detail": {
                    "name": "Morgana Lalli"
                },
                "author": "Morgana Lalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03800v3",
                "updated": "2025-01-27T14:44:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    44,
                    33,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-07T14:06:57Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    6,
                    57,
                    1,
                    7,
                    0
                ],
                "title": "MADation: Face Morphing Attack Detection with Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MADation: Face Morphing Attack Detection with Foundation Models"
                },
                "summary": "Despite the considerable performance improvements of face recognition\nalgorithms in recent years, the same scientific advances responsible for this\nprogress can also be used to create efficient ways to attack them, posing a\nthreat to their secure deployment. Morphing attack detection (MAD) systems aim\nto detect a specific type of threat, morphing attacks, at an early stage,\npreventing them from being considered for verification in critical processes.\nFoundation models (FM) learn from extensive amounts of unlabelled data,\nachieving remarkable zero-shot generalization to unseen domains. Although this\ngeneralization capacity might be weak when dealing with domain-specific\ndownstream tasks such as MAD, FMs can easily adapt to these settings while\nretaining the built-in knowledge acquired during pre-training. In this work, we\nrecognize the potential of FMs to perform well in the MAD task when properly\nadapted to its specificities. To this end, we adapt FM CLIP architectures with\nLoRA weights while simultaneously training a classification header. The\nproposed framework, MADation surpasses our alternative FM and transformer-based\nframeworks and constitutes the first adaption of FMs to the MAD task. MADation\npresents competitive results with current MAD solutions in the literature and\neven surpasses them in several evaluation scenarios. To encourage\nreproducibility and facilitate further research in MAD, we publicly release the\nimplementation of MADation at https://github.com/gurayozgur/MADation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the considerable performance improvements of face recognition\nalgorithms in recent years, the same scientific advances responsible for this\nprogress can also be used to create efficient ways to attack them, posing a\nthreat to their secure deployment. Morphing attack detection (MAD) systems aim\nto detect a specific type of threat, morphing attacks, at an early stage,\npreventing them from being considered for verification in critical processes.\nFoundation models (FM) learn from extensive amounts of unlabelled data,\nachieving remarkable zero-shot generalization to unseen domains. Although this\ngeneralization capacity might be weak when dealing with domain-specific\ndownstream tasks such as MAD, FMs can easily adapt to these settings while\nretaining the built-in knowledge acquired during pre-training. In this work, we\nrecognize the potential of FMs to perform well in the MAD task when properly\nadapted to its specificities. To this end, we adapt FM CLIP architectures with\nLoRA weights while simultaneously training a classification header. The\nproposed framework, MADation surpasses our alternative FM and transformer-based\nframeworks and constitutes the first adaption of FMs to the MAD task. MADation\npresents competitive results with current MAD solutions in the literature and\neven surpasses them in several evaluation scenarios. To encourage\nreproducibility and facilitate further research in MAD, we publicly release the\nimplementation of MADation at https://github.com/gurayozgur/MADation"
                },
                "authors": [
                    {
                        "name": "Eduarda Caldeira"
                    },
                    {
                        "name": "Guray Ozgur"
                    },
                    {
                        "name": "Tahar Chettaoui"
                    },
                    {
                        "name": "Marija Ivanovska"
                    },
                    {
                        "name": "Peter Peer"
                    },
                    {
                        "name": "Fadi Boutros"
                    },
                    {
                        "name": "Vitomir Struc"
                    },
                    {
                        "name": "Naser Damer"
                    }
                ],
                "author_detail": {
                    "name": "Naser Damer"
                },
                "author": "Naser Damer",
                "arxiv_comment": "Accepted at WACV 2025 workshops",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16084v1",
                "updated": "2025-01-27T14:33:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    33,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:33:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    33,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "The Shiny Scary Future of Automated Research Synthesis in HCI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Shiny Scary Future of Automated Research Synthesis in HCI"
                },
                "summary": "Automation and semi-automation through computational tools like LLMs are also\nmaking their way to deployment in research synthesis and secondary research,\nsuch as systematic reviews. In some steps of research synthesis, this has the\nopportunity to provide substantial benefits by saving time that previously was\nspent on repetitive tasks. The screening stages in particular may benefit from\ncarefully vetted computational support. However, this position paper argues for\nadditional caution when bringing in such tools to the analysis and synthesis\nphases, where human judgement and expertise should be paramount throughout the\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation and semi-automation through computational tools like LLMs are also\nmaking their way to deployment in research synthesis and secondary research,\nsuch as systematic reviews. In some steps of research synthesis, this has the\nopportunity to provide substantial benefits by saving time that previously was\nspent on repetitive tasks. The screening stages in particular may benefit from\ncarefully vetted computational support. However, this position paper argues for\nadditional caution when bringing in such tools to the analysis and synthesis\nphases, where human judgement and expertise should be paramount throughout the\nprocess."
                },
                "authors": [
                    {
                        "name": "Katja Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Katja Rogers"
                },
                "author": "Katja Rogers",
                "arxiv_comment": "Accepted at CHI 2024 workshop: \"LLMs as Research Tools: Applications\n  and Evaluations in HCI Data Work\" (https://doi.org/10.1145/3613905.3636301)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16078v1",
                "updated": "2025-01-27T14:28:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    28,
                    1,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:28:01Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    28,
                    1,
                    0,
                    27,
                    0
                ],
                "title": "Integration of LLM Quality Assurance into an NLG System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of LLM Quality Assurance into an NLG System"
                },
                "summary": "In this paper, we present a system that uses a Large Language Model (LLM) to\nperform grammar and spelling correction as a component of Quality Assurance\n(QA) for texts generated by NLG systems, which is important for text production\nin real-world scenarios. Evaluating the results of the system on\nwork-in-progress sports news texts in three languages, we show that it is able\nto deliver acceptable corrections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a system that uses a Large Language Model (LLM) to\nperform grammar and spelling correction as a component of Quality Assurance\n(QA) for texts generated by NLG systems, which is important for text production\nin real-world scenarios. Evaluating the results of the system on\nwork-in-progress sports news texts in three languages, we show that it is able\nto deliver acceptable corrections."
                },
                "authors": [
                    {
                        "name": "Ching-Yi Chen"
                    },
                    {
                        "name": "Johanna Heininger"
                    },
                    {
                        "name": "Adela Schneider"
                    },
                    {
                        "name": "Christian Eckard"
                    },
                    {
                        "name": "Andreas Madsack"
                    },
                    {
                        "name": "Robert Weißgraeber"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weißgraeber"
                },
                "author": "Robert Weißgraeber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16075v1",
                "updated": "2025-01-27T14:26:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    26,
                    27,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T14:26:27Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    26,
                    27,
                    0,
                    27,
                    0
                ],
                "title": "PISCO: Pretty Simple Compression for Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PISCO: Pretty Simple Compression for Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models\n(LLMs) by retrieving relevant documents, but they face scalability issues due\nto high inference costs and limited context size. Document compression is a\npractical solution, but current soft compression methods suffer from accuracy\nlosses and require extensive pretraining. In this paper, we introduce PISCO, a\nnovel method that achieves a 16x compression rate with minimal accuracy loss\n(0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing\napproaches, PISCO requires no pretraining or annotated data, relying solely on\nsequence-level knowledge distillation from document-based questions. With the\nability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers\na highly efficient and scalable solution. We present comprehensive experiments\nshowing that PISCO outperforms existing compression models by 8% in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models\n(LLMs) by retrieving relevant documents, but they face scalability issues due\nto high inference costs and limited context size. Document compression is a\npractical solution, but current soft compression methods suffer from accuracy\nlosses and require extensive pretraining. In this paper, we introduce PISCO, a\nnovel method that achieves a 16x compression rate with minimal accuracy loss\n(0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing\napproaches, PISCO requires no pretraining or annotated data, relying solely on\nsequence-level knowledge distillation from document-based questions. With the\nability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers\na highly efficient and scalable solution. We present comprehensive experiments\nshowing that PISCO outperforms existing compression models by 8% in accuracy."
                },
                "authors": [
                    {
                        "name": "Maxime Louis"
                    },
                    {
                        "name": "Hervé Déjean"
                    },
                    {
                        "name": "Stéphane Clinchant"
                    }
                ],
                "author_detail": {
                    "name": "Stéphane Clinchant"
                },
                "author": "Stéphane Clinchant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16033v1",
                "updated": "2025-01-27T13:27:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    27,
                    4,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:27:04Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    27,
                    4,
                    0,
                    27,
                    0
                ],
                "title": "PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy\n  Assessment"
                },
                "summary": "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present PRISMe (Privacy Risk Information Scanner for Me), a novel Large\nLanguage Model (LLM)-driven privacy policy assessment tool, which helps users\nto understand the essence of a lengthy, complex privacy policy while browsing.\nThe tool, a browser extension, integrates a dashboard and an LLM chat. One\nmajor contribution is the first rigorous evaluation of such a tool. In a\nmixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,\nunderstandability of the provided information, and impacts on awareness. While\nour tool improves privacy awareness by providing a comprehensible quick\noverview and a quality chat for in-depth discussion, users note issues with\nconsistency and building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present PRISMe (Privacy Risk Information Scanner for Me), a novel Large\nLanguage Model (LLM)-driven privacy policy assessment tool, which helps users\nto understand the essence of a lengthy, complex privacy policy while browsing.\nThe tool, a browser extension, integrates a dashboard and an LLM chat. One\nmajor contribution is the first rigorous evaluation of such a tool. In a\nmixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,\nunderstandability of the provided information, and impacts on awareness. While\nour tool improves privacy awareness by providing a comprehensible quick\noverview and a quality chat for in-depth discussion, users note issues with\nconsistency and building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools."
                },
                "authors": [
                    {
                        "name": "Vincent Freiberger"
                    },
                    {
                        "name": "Arthur Fleig"
                    },
                    {
                        "name": "Erik Buchmann"
                    }
                ],
                "author_detail": {
                    "name": "Erik Buchmann"
                },
                "author": "Erik Buchmann",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.m; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16029v1",
                "updated": "2025-01-27T13:18:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    18,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:18:40Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    18,
                    40,
                    0,
                    27,
                    0
                ],
                "title": "FDLLM: A Text Fingerprint Detection Method for LLMs in Multi-Language,\n  Multi-Domain Black-Box Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FDLLM: A Text Fingerprint Detection Method for LLMs in Multi-Language,\n  Multi-Domain Black-Box Environments"
                },
                "summary": "Using large language models (LLMs) integration platforms without transparency\nabout which LLM is being invoked can lead to potential security risks.\nSpecifically, attackers may exploit this black-box scenario to deploy malicious\nmodels and embed viruses in the code provided to users. In this context, it is\nincreasingly urgent for users to clearly identify the LLM they are interacting\nwith, in order to avoid unknowingly becoming victims of malicious models.\nHowever, existing studies primarily focus on mixed classification of human and\nmachine-generated text, with limited attention to classifying texts generated\nsolely by different models. Current research also faces dual bottlenecks: poor\nquality of LLM-generated text (LLMGT) datasets and limited coverage of\ndetectable LLMs, resulting in poor detection performance for various LLMGT in\nblack-box scenarios. We propose the first LLMGT fingerprint detection model,\n\\textbf{FDLLM}, based on Qwen2.5-7B and fine-tuned using LoRA to address these\nchallenges. FDLLM can more efficiently handle detection tasks across\nmultilingual and multi-domain scenarios. Furthermore, we constructed a dataset\nnamed \\textbf{FD-Datasets}, consisting of 90,000 samples that span multiple\nlanguages and domains, covering 20 different LLMs. Experimental results\ndemonstrate that FDLLM achieves a macro F1 score 16.7\\% higher than the best\nbaseline method, LM-D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using large language models (LLMs) integration platforms without transparency\nabout which LLM is being invoked can lead to potential security risks.\nSpecifically, attackers may exploit this black-box scenario to deploy malicious\nmodels and embed viruses in the code provided to users. In this context, it is\nincreasingly urgent for users to clearly identify the LLM they are interacting\nwith, in order to avoid unknowingly becoming victims of malicious models.\nHowever, existing studies primarily focus on mixed classification of human and\nmachine-generated text, with limited attention to classifying texts generated\nsolely by different models. Current research also faces dual bottlenecks: poor\nquality of LLM-generated text (LLMGT) datasets and limited coverage of\ndetectable LLMs, resulting in poor detection performance for various LLMGT in\nblack-box scenarios. We propose the first LLMGT fingerprint detection model,\n\\textbf{FDLLM}, based on Qwen2.5-7B and fine-tuned using LoRA to address these\nchallenges. FDLLM can more efficiently handle detection tasks across\nmultilingual and multi-domain scenarios. Furthermore, we constructed a dataset\nnamed \\textbf{FD-Datasets}, consisting of 90,000 samples that span multiple\nlanguages and domains, covering 20 different LLMs. Experimental results\ndemonstrate that FDLLM achieves a macro F1 score 16.7\\% higher than the best\nbaseline method, LM-D."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fu"
                    },
                    {
                        "name": "Junfan Chen"
                    },
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Ting Yang"
                    },
                    {
                        "name": "Ruidong Li"
                    },
                    {
                        "name": "Yuqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Zhang"
                },
                "author": "Yuqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19058v2",
                "updated": "2025-01-27T13:09:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    9,
                    41,
                    0,
                    27,
                    0
                ],
                "published": "2024-11-28T11:17:30Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    11,
                    17,
                    30,
                    3,
                    333,
                    0
                ],
                "title": "Quality Time: Carbon-Aware Quality Adaptation for Energy-Intensive\n  Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Time: Carbon-Aware Quality Adaptation for Energy-Intensive\n  Services"
                },
                "summary": "The energy demand of modern cloud services, particularly those related to\ngenerative AI, is increasing at an unprecedented pace. While hyperscalers\ncollectively fail to meet their self-imposed emission reduction targets, they\nface increasing pressure from environmental sustainability reporting across\nmany jurisdictions. To date, carbon-aware computing strategies have primarily\nfocused on batch process scheduling or geo-distributed load balancing. However,\nsuch approaches are not applicable to services that require constant\navailability at specific locations due to latency, privacy, data, or\ninfrastructure constraints.\n  In this paper, we explore how the carbon footprint of energy-intensive\nservices can be reduced by adjusting the fraction of requests served by\ndifferent service quality tiers. We show that adapting this quality of\nresponses with respect to grid carbon intensity can lead to additional carbon\nsavings beyond resource and energy efficiency. Building on this, we introduce a\nforecast-based multi-horizon optimization that reaches close-to-optimal carbon\nsavings and is able to automatically adapt service quality for best-effort\nusers to stay within an annual carbon budget. Our approach can reduce the\nemissions of large-scale LLM services, which we estimate at multiple 10,000\ntons of CO2 annually, by up to 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The energy demand of modern cloud services, particularly those related to\ngenerative AI, is increasing at an unprecedented pace. While hyperscalers\ncollectively fail to meet their self-imposed emission reduction targets, they\nface increasing pressure from environmental sustainability reporting across\nmany jurisdictions. To date, carbon-aware computing strategies have primarily\nfocused on batch process scheduling or geo-distributed load balancing. However,\nsuch approaches are not applicable to services that require constant\navailability at specific locations due to latency, privacy, data, or\ninfrastructure constraints.\n  In this paper, we explore how the carbon footprint of energy-intensive\nservices can be reduced by adjusting the fraction of requests served by\ndifferent service quality tiers. We show that adapting this quality of\nresponses with respect to grid carbon intensity can lead to additional carbon\nsavings beyond resource and energy efficiency. Building on this, we introduce a\nforecast-based multi-horizon optimization that reaches close-to-optimal carbon\nsavings and is able to automatically adapt service quality for best-effort\nusers to stay within an annual carbon budget. Our approach can reduce the\nemissions of large-scale LLM services, which we estimate at multiple 10,000\ntons of CO2 annually, by up to 10%."
                },
                "authors": [
                    {
                        "name": "Philipp Wiesner"
                    },
                    {
                        "name": "Dennis Grinwald"
                    },
                    {
                        "name": "Philipp Weiß"
                    },
                    {
                        "name": "Patrick Wilhelm"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Odej Kao"
                    }
                ],
                "author_detail": {
                    "name": "Odej Kao"
                },
                "author": "Odej Kao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15044v2",
                "updated": "2025-01-27T12:47:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    47,
                    28,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-19T09:04:01Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    9,
                    4,
                    1,
                    5,
                    293,
                    0
                ],
                "title": "Adanonymizer: Interactively Navigating and Balancing the Duality of\n  Privacy and Output Performance in Human-LLM Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adanonymizer: Interactively Navigating and Balancing the Duality of\n  Privacy and Output Performance in Human-LLM Interaction"
                },
                "summary": "Current Large Language Models (LLMs) cannot support users to precisely\nbalance privacy protection and output performance during individual\nconsultations. We introduce Adanonymizer, an anonymization plug-in that allows\nusers to control this balance by navigating a trade-off curve. A survey (N=221)\nrevealed a privacy paradox, where users frequently disclosed sensitive\ninformation despite acknowledging privacy risks. The study further demonstrated\nthat privacy risks were not significantly correlated with model output\nperformance, highlighting the potential to navigate this trade-off.\nAdanonymizer normalizes privacy and utility ratings by type and automates the\npseudonymization of sensitive terms based on user preferences, significantly\nreducing user effort. Its 2D color palette interface visualizes the\nprivacy-utility trade-off, allowing users to adjust the balance by manipulating\na point. An evaluation (N=36) compared Adanonymizer with ablation methods and\ndifferential privacy techniques, where Adanonymizer significantly reduced\nmodification time, achieved better perceived model performance and overall user\npreference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) cannot support users to precisely\nbalance privacy protection and output performance during individual\nconsultations. We introduce Adanonymizer, an anonymization plug-in that allows\nusers to control this balance by navigating a trade-off curve. A survey (N=221)\nrevealed a privacy paradox, where users frequently disclosed sensitive\ninformation despite acknowledging privacy risks. The study further demonstrated\nthat privacy risks were not significantly correlated with model output\nperformance, highlighting the potential to navigate this trade-off.\nAdanonymizer normalizes privacy and utility ratings by type and automates the\npseudonymization of sensitive terms based on user preferences, significantly\nreducing user effort. Its 2D color palette interface visualizes the\nprivacy-utility trade-off, allowing users to adjust the balance by manipulating\na point. An evaluation (N=36) compared Adanonymizer with ablation methods and\ndifferential privacy techniques, where Adanonymizer significantly reduced\nmodification time, achieved better perceived model performance and overall user\npreference."
                },
                "authors": [
                    {
                        "name": "Shuning Zhang"
                    },
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Haobin Xing"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Yongquan Hu"
                    },
                    {
                        "name": "Hewu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hewu Li"
                },
                "author": "Hewu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16007v1",
                "updated": "2025-01-27T12:46:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    46,
                    45,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T12:46:45Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    46,
                    45,
                    0,
                    27,
                    0
                ],
                "title": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable\n  Inference"
                },
                "summary": "Large language models (LLMs) have proven to be very capable, but access to\nthe best models currently rely on inference providers which introduces trust\nchallenges -- how can we be sure that the provider is using the model\nconfiguration they claim? We propose TOPLOC, a novel method for verifiable\ninference that addresses this problem. TOPLOC leverages a compact locality\nsensitive hashing mechanism for intermediate activations which can detect\nunauthorized modifications to models, prompts, or precision with 100% accuracy,\nachieving no false positives or negatives in our empirical evaluations. Our\napproach is robust across diverse hardware configurations, GPU types, and\nalgebraic reorderings, which allows for validation speeds significantly faster\nthan the original inference. By introducing a polynomial encoding scheme,\nTOPLOC minimizes memory overhead of the generated commits by $1000\\times$,\nrequiring only 258 bytes of storage per 32 new tokens compared to the 262KB\nrequirement of storing the token embeddings directly for Llama-3.1-8B-Instruct.\nOur method empowers users to verify LLM inference computations efficiently,\nfostering greater trust and transparency in open ecosystems and lays a\nfoundation for decentralized and verifiable AI services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have proven to be very capable, but access to\nthe best models currently rely on inference providers which introduces trust\nchallenges -- how can we be sure that the provider is using the model\nconfiguration they claim? We propose TOPLOC, a novel method for verifiable\ninference that addresses this problem. TOPLOC leverages a compact locality\nsensitive hashing mechanism for intermediate activations which can detect\nunauthorized modifications to models, prompts, or precision with 100% accuracy,\nachieving no false positives or negatives in our empirical evaluations. Our\napproach is robust across diverse hardware configurations, GPU types, and\nalgebraic reorderings, which allows for validation speeds significantly faster\nthan the original inference. By introducing a polynomial encoding scheme,\nTOPLOC minimizes memory overhead of the generated commits by $1000\\times$,\nrequiring only 258 bytes of storage per 32 new tokens compared to the 262KB\nrequirement of storing the token embeddings directly for Llama-3.1-8B-Instruct.\nOur method empowers users to verify LLM inference computations efficiently,\nfostering greater trust and transparency in open ecosystems and lays a\nfoundation for decentralized and verifiable AI services."
                },
                "authors": [
                    {
                        "name": "Jack Min Ong"
                    },
                    {
                        "name": "Matthew Di Ferrante"
                    },
                    {
                        "name": "Aaron Pazdera"
                    },
                    {
                        "name": "Ryan Garner"
                    },
                    {
                        "name": "Sami Jaghouar"
                    },
                    {
                        "name": "Manveer Basra"
                    },
                    {
                        "name": "Johannes Hagemann"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Hagemann"
                },
                "author": "Johannes Hagemann",
                "arxiv_comment": "18 pages, 13 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07623v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07623v4",
                "updated": "2025-01-27T12:32:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    32,
                    8,
                    0,
                    27,
                    0
                ],
                "published": "2024-05-13T10:30:33Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    10,
                    30,
                    33,
                    0,
                    134,
                    0
                ],
                "title": "COBias and Debias: Balancing Class Accuracies for Language Models in\n  Inference Time via Nonlinear Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COBias and Debias: Balancing Class Accuracies for Language Models in\n  Inference Time via Nonlinear Integer Programming"
                },
                "summary": "Large language models (LLMs) are good knowledge bases but struggle to perform\nequally well for all classes in text classification tasks. This paper\ninvestigates a fundamental inference-time problem in language models:\nimbalanced class accuracies. We find what's underneath the issue is a tendency\nto over-predict some classes while under-predicting some others. This class\naccuracy imbalance is difficult to solve from the root via better pre-training\nor fine-tuning strategies, but we show it can be effectively mitigated via\ninference-time combinatorial optimization. To this end, we conceptualize and\nquantify the over- and under-prediction issue as the Contextual Oddity Bias\n(COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP)\nmodel to correct in-context learned class probabilities based on minimizing\nCOBias and maximizing overall accuracy, without LLM parameter update.\nConsidering that the DNIP model implicitly contains non-differentiable\nelements, we therefore use the simulated annealing algorithm to solve it.\nExtensive evaluations on three LLMs across seven NLP classification tasks in\ndifferent prompting settings show that DNIP simultaneously achieves significant\nCOBias reduction (-27%) and accuracy improvement (+12%) over the conventional\nICL approach, suggesting that inference-time mitigation of class accuracy\nimbalance is a promising direction to push forward LLM performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are good knowledge bases but struggle to perform\nequally well for all classes in text classification tasks. This paper\ninvestigates a fundamental inference-time problem in language models:\nimbalanced class accuracies. We find what's underneath the issue is a tendency\nto over-predict some classes while under-predicting some others. This class\naccuracy imbalance is difficult to solve from the root via better pre-training\nor fine-tuning strategies, but we show it can be effectively mitigated via\ninference-time combinatorial optimization. To this end, we conceptualize and\nquantify the over- and under-prediction issue as the Contextual Oddity Bias\n(COBias), and propose the Debiasing as Nonlinear Integer Programming (DNIP)\nmodel to correct in-context learned class probabilities based on minimizing\nCOBias and maximizing overall accuracy, without LLM parameter update.\nConsidering that the DNIP model implicitly contains non-differentiable\nelements, we therefore use the simulated annealing algorithm to solve it.\nExtensive evaluations on three LLMs across seven NLP classification tasks in\ndifferent prompting settings show that DNIP simultaneously achieves significant\nCOBias reduction (-27%) and accuracy improvement (+12%) over the conventional\nICL approach, suggesting that inference-time mitigation of class accuracy\nimbalance is a promising direction to push forward LLM performances."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07623v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07623v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15994v1",
                "updated": "2025-01-27T12:29:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    29,
                    19,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T12:29:19Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    29,
                    19,
                    0,
                    27,
                    0
                ],
                "title": "Real-Time Brain Tumor Detection in Intraoperative Ultrasound Using\n  YOLO11: From Model Training to Deployment in the Operating Room",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Brain Tumor Detection in Intraoperative Ultrasound Using\n  YOLO11: From Model Training to Deployment in the Operating Room"
                },
                "summary": "Intraoperative ultrasound (ioUS) is a valuable tool in brain tumor surgery\ndue to its versatility, affordability, and seamless integration into the\nsurgical workflow. However, its adoption remains limited, primarily because of\nthe challenges associated with image interpretation and the steep learning\ncurve required for effective use. This study aimed to enhance the\ninterpretability of ioUS images by developing a real-time brain tumor detection\nsystem deployable in the operating room. We collected 2D ioUS images from the\nBrain Tumor Intraoperative Database (BraTioUS) and the public ReMIND dataset,\nannotated with expert-refined tumor labels. Using the YOLO11 architecture and\nits variants, we trained object detection models to identify brain tumors. The\ndataset included 1,732 images from 192 patients, divided into training,\nvalidation, and test sets. Data augmentation expanded the training set to\n11,570 images. In the test dataset, YOLO11s achieved the best balance of\nprecision and computational efficiency, with a mAP@50 of 0.95, mAP@50-95 of\n0.65, and a processing speed of 34.16 frames per second. The proposed solution\nwas prospectively validated in a cohort of 15 consecutively operated patients\ndiagnosed with brain tumors. Neurosurgeons confirmed its seamless integration\ninto the surgical workflow, with real-time predictions accurately delineating\ntumor regions. These findings highlight the potential of real-time object\ndetection algorithms to enhance ioUS-guided brain tumor surgery, addressing key\nchallenges in interpretation and providing a foundation for future development\nof computer vision-based tools for neuro-oncological surgery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intraoperative ultrasound (ioUS) is a valuable tool in brain tumor surgery\ndue to its versatility, affordability, and seamless integration into the\nsurgical workflow. However, its adoption remains limited, primarily because of\nthe challenges associated with image interpretation and the steep learning\ncurve required for effective use. This study aimed to enhance the\ninterpretability of ioUS images by developing a real-time brain tumor detection\nsystem deployable in the operating room. We collected 2D ioUS images from the\nBrain Tumor Intraoperative Database (BraTioUS) and the public ReMIND dataset,\nannotated with expert-refined tumor labels. Using the YOLO11 architecture and\nits variants, we trained object detection models to identify brain tumors. The\ndataset included 1,732 images from 192 patients, divided into training,\nvalidation, and test sets. Data augmentation expanded the training set to\n11,570 images. In the test dataset, YOLO11s achieved the best balance of\nprecision and computational efficiency, with a mAP@50 of 0.95, mAP@50-95 of\n0.65, and a processing speed of 34.16 frames per second. The proposed solution\nwas prospectively validated in a cohort of 15 consecutively operated patients\ndiagnosed with brain tumors. Neurosurgeons confirmed its seamless integration\ninto the surgical workflow, with real-time predictions accurately delineating\ntumor regions. These findings highlight the potential of real-time object\ndetection algorithms to enhance ioUS-guided brain tumor surgery, addressing key\nchallenges in interpretation and providing a foundation for future development\nof computer vision-based tools for neuro-oncological surgery."
                },
                "authors": [
                    {
                        "name": "Santiago Cepeda"
                    },
                    {
                        "name": "Olga Esteban-Sinovas"
                    },
                    {
                        "name": "Roberto Romero"
                    },
                    {
                        "name": "Vikas Singh"
                    },
                    {
                        "name": "Prakash Shetty"
                    },
                    {
                        "name": "Aliasgar Moiyadi"
                    },
                    {
                        "name": "Ilyess Zemmoura"
                    },
                    {
                        "name": "Giuseppe Roberto Giammalva"
                    },
                    {
                        "name": "Massimiliano Del Bene"
                    },
                    {
                        "name": "Arianna Barbotti"
                    },
                    {
                        "name": "Francesco DiMeco"
                    },
                    {
                        "name": "Timothy R. West"
                    },
                    {
                        "name": "Brian V. Nahed"
                    },
                    {
                        "name": "Ignacio Arrese"
                    },
                    {
                        "name": "Roberto Hornero"
                    },
                    {
                        "name": "Rosario Sarabia"
                    }
                ],
                "author_detail": {
                    "name": "Rosario Sarabia"
                },
                "author": "Rosario Sarabia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23000v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23000v3",
                "updated": "2025-01-27T11:58:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    58,
                    0,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-30T13:29:36Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    29,
                    36,
                    2,
                    304,
                    0
                ],
                "title": "Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented\n  Generation with Key Point Recall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented\n  Generation with Key Point Recall"
                },
                "summary": "Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a promising approach to address the\nlimitations of fixed knowledge in large language models (LLMs). However,\ncurrent benchmarks for evaluating RAG systems suffer from two key deficiencies:\n(1) they fail to adequately measure LLMs' capability in handling long-context\nretrieval due to a lack of datasets that reflect the characteristics of\nretrieved documents, and (2) they lack a comprehensive evaluation method for\nassessing LLMs' ability to generate long-form responses that effectively\nexploits retrieved information. To address these shortcomings, we introduce the\nLong$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG\ncomprises 280 questions spanning 10 domains and across 8 question categories,\neach associated with 5 retrieved documents with an average length of 2,444\nwords. KPR evaluates the extent to which LLMs incorporate key points extracted\nfrom the retrieved documents into their generated responses, providing a more\nnuanced assessment of their ability to exploit retrieved information."
                },
                "authors": [
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "Accepted to EMNLP'24 (Findings). Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23000v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23000v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02337v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02337v3",
                "updated": "2025-01-27T11:56:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    56,
                    15,
                    0,
                    27,
                    0
                ],
                "published": "2024-11-04T17:59:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    59,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum\n  Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential as autonomous\nagents, particularly in web-based tasks. However, existing LLM web agents\nheavily rely on expensive proprietary LLM APIs, while open LLMs lack the\nnecessary decision-making capabilities. This paper introduces WebRL, a\nself-evolving online curriculum reinforcement learning framework designed to\ntrain high-performance web agents using open LLMs. WebRL addresses three key\nchallenges in building LLM web agents, including the scarcity of training\ntasks, sparse feedback signals, and policy distribution drift in online\nlearning. Specifically, WebRL incorporates 1) a self-evolving curriculum that\ngenerates new tasks from unsuccessful attempts, 2) a robust outcome-supervised\nreward model (ORM), and 3) adaptive reinforcement learning strategies to ensure\nconsistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4\nmodels into proficient web agents. On WebArena-Lite, WebRL improves the success\nrate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.\nThese open models significantly surpass the performance of GPT-4-Turbo (17.6%)\nand GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained\non open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's\neffectiveness in bridging the gap between open and proprietary LLM-based web\nagents, paving the way for more accessible and powerful autonomous web\ninteraction systems."
                },
                "authors": [
                    {
                        "name": "Zehan Qi"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Iat Long Iong"
                    },
                    {
                        "name": "Hanyu Lai"
                    },
                    {
                        "name": "Xueqiao Sun"
                    },
                    {
                        "name": "Wenyi Zhao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Xinyue Yang"
                    },
                    {
                        "name": "Jiadai Sun"
                    },
                    {
                        "name": "Shuntian Yao"
                    },
                    {
                        "name": "Tianjie Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02337v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02337v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15268v3",
                "updated": "2025-01-27T11:35:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    35,
                    4,
                    0,
                    27,
                    0
                ],
                "published": "2024-09-23T17:58:07Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    17,
                    58,
                    7,
                    0,
                    267,
                    0
                ],
                "title": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style Outweighs Substance: Failure Modes of LLM Judges in Alignment\n  Benchmarking"
                },
                "summary": "The release of ChatGPT in November 2022 sparked an explosion of interest in\npost-training and an avalanche of new preference optimization (PO) methods.\nThese methods claim superior alignment by virtue of better correspondence with\nhuman pairwise preferences, often measured by LLM-judges. In this work, we\nattempt to answer the following question -- do LLM-judge preferences translate\nto progress on other, more concrete metrics for alignment, and if not, why not?\nWe define a concrete metric for alignment, and introduce SOS-Bench (Substance\nOutweighs Style Benchmark), which is to the best of our knowledge the largest\nstandardized, reproducible LLM meta-benchmark to date. We find that (1)\nLLM-judge preferences do not correlate with concrete measures of safety, world\nknowledge, and instruction following; (2) LLM-judges have powerful implicit\nbiases, prioritizing style over factuality and safety; and (3) the supervised\nfine-tuning (SFT) stage of post-training, and not the PO stage, has the\ngreatest impact on alignment, with data scaling and prompt diversity as the\ndriving factors. Our codebase and complete results can be found at\nhttps://github.com/penfever/sos-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The release of ChatGPT in November 2022 sparked an explosion of interest in\npost-training and an avalanche of new preference optimization (PO) methods.\nThese methods claim superior alignment by virtue of better correspondence with\nhuman pairwise preferences, often measured by LLM-judges. In this work, we\nattempt to answer the following question -- do LLM-judge preferences translate\nto progress on other, more concrete metrics for alignment, and if not, why not?\nWe define a concrete metric for alignment, and introduce SOS-Bench (Substance\nOutweighs Style Benchmark), which is to the best of our knowledge the largest\nstandardized, reproducible LLM meta-benchmark to date. We find that (1)\nLLM-judge preferences do not correlate with concrete measures of safety, world\nknowledge, and instruction following; (2) LLM-judges have powerful implicit\nbiases, prioritizing style over factuality and safety; and (3) the supervised\nfine-tuning (SFT) stage of post-training, and not the PO stage, has the\ngreatest impact on alignment, with data scaling and prompt diversity as the\ndriving factors. Our codebase and complete results can be found at\nhttps://github.com/penfever/sos-bench."
                },
                "authors": [
                    {
                        "name": "Benjamin Feuer"
                    },
                    {
                        "name": "Micah Goldblum"
                    },
                    {
                        "name": "Teresa Datta"
                    },
                    {
                        "name": "Sanjana Nambiar"
                    },
                    {
                        "name": "Raz Besaleli"
                    },
                    {
                        "name": "Samuel Dooley"
                    },
                    {
                        "name": "Max Cembalest"
                    },
                    {
                        "name": "John P. Dickerson"
                    }
                ],
                "author_detail": {
                    "name": "John P. Dickerson"
                },
                "author": "John P. Dickerson",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18193v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18193v3",
                "updated": "2025-01-27T11:32:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    32,
                    51,
                    0,
                    27,
                    0
                ],
                "published": "2024-09-26T18:10:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    10,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource\n  Languages Injected with Multilingual Graph Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource\n  Languages Injected with Multilingual Graph Knowledge"
                },
                "summary": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Using LLMs for such languages is often difficult due to a high\ncomputational cost; not only during training, but also during inference. Static\nword embeddings are much more resource-efficient (\"green\"), and thus still\nprovide value, particularly for very low-resource languages. There is, however,\na notable lack of comprehensive repositories with such embeddings for diverse\nlanguages. To address this gap, we present GrEmLIn, a centralized repository of\ngreen, static baseline embeddings for 87 mid- and low-resource languages. We\ncompute GrEmLIn embeddings with a novel method that enhances GloVe embeddings\nby integrating multilingual graph knowledge, which makes our static embeddings\ncompetitive with LLM representations, while being parameter-free at inference\ntime. Our experiments demonstrate that GrEmLIn embeddings outperform\nstate-of-the-art contextualized embeddings from E5 on the task of lexical\nsimilarity. They remain competitive in extrinsic evaluation tasks like\nsentiment analysis and natural language inference, with average performance\ngaps of just 5-10\\% or less compared to state-of-the-art models, given a\nsufficient vocabulary overlap with the target task, and underperform only on\ntopic classification. Our code and embeddings are publicly available at\nhttps://huggingface.co/DFKI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Using LLMs for such languages is often difficult due to a high\ncomputational cost; not only during training, but also during inference. Static\nword embeddings are much more resource-efficient (\"green\"), and thus still\nprovide value, particularly for very low-resource languages. There is, however,\na notable lack of comprehensive repositories with such embeddings for diverse\nlanguages. To address this gap, we present GrEmLIn, a centralized repository of\ngreen, static baseline embeddings for 87 mid- and low-resource languages. We\ncompute GrEmLIn embeddings with a novel method that enhances GloVe embeddings\nby integrating multilingual graph knowledge, which makes our static embeddings\ncompetitive with LLM representations, while being parameter-free at inference\ntime. Our experiments demonstrate that GrEmLIn embeddings outperform\nstate-of-the-art contextualized embeddings from E5 on the task of lexical\nsimilarity. They remain competitive in extrinsic evaluation tasks like\nsentiment analysis and natural language inference, with average performance\ngaps of just 5-10\\% or less compared to state-of-the-art models, given a\nsufficient vocabulary overlap with the target task, and underperform only on\ntopic classification. Our code and embeddings are publicly available at\nhttps://huggingface.co/DFKI."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Rishu Kumar"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Long paper, accepted to NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18193v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14908v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14908v4",
                "updated": "2025-01-27T11:25:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    11,
                    25,
                    33,
                    0,
                    27,
                    0
                ],
                "published": "2024-05-23T09:44:02Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    44,
                    2,
                    3,
                    144,
                    0
                ],
                "title": "BiMix: A Bivariate Data Mixing Law for Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiMix: A Bivariate Data Mixing Law for Language Model Pretraining"
                },
                "summary": "Large language models have demonstrated remarkable capabilities across\nvarious tasks, primarily attributed to the utilization of diversely sourced\ndata. However, the impact of pretraining data composition on model performance\nremains poorly understood. This paper introduces $\\textbf{BiMix}$, a novel\nbivariate data mixing law that models the joint scaling behavior of domain\nproportions and data volume in LLM pretraining. $\\textbf{BiMix}$ provides a\nsystematic framework for understanding and optimizing data mixtures across\ndiverse domains. Through extensive experiments on two large-scale datasets, we\ndemonstrate $\\textbf{BiMix}$'s high accuracy in loss extrapolation (mean\nrelative error < 0.2%) and its generalization to unseen mixtures (R${}^{2}$ >\n0.97). Optimization of domain proportions yields superior model performance\ncompared to existing methods. Furthermore, we establish entropy-based measures\nas efficient proxies for data mixing, offering a computationally lightweight\nstrategy. Our work contributes both theoretical insights into data mixing\ndynamics and practical tools for enhancing LLM training efficiency, paving the\nway for more effective scaling strategies in language model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable capabilities across\nvarious tasks, primarily attributed to the utilization of diversely sourced\ndata. However, the impact of pretraining data composition on model performance\nremains poorly understood. This paper introduces $\\textbf{BiMix}$, a novel\nbivariate data mixing law that models the joint scaling behavior of domain\nproportions and data volume in LLM pretraining. $\\textbf{BiMix}$ provides a\nsystematic framework for understanding and optimizing data mixtures across\ndiverse domains. Through extensive experiments on two large-scale datasets, we\ndemonstrate $\\textbf{BiMix}$'s high accuracy in loss extrapolation (mean\nrelative error < 0.2%) and its generalization to unseen mixtures (R${}^{2}$ >\n0.97). Optimization of domain proportions yields superior model performance\ncompared to existing methods. Furthermore, we establish entropy-based measures\nas efficient proxies for data mixing, offering a computationally lightweight\nstrategy. Our work contributes both theoretical insights into data mixing\ndynamics and practical tools for enhancing LLM training efficiency, paving the\nway for more effective scaling strategies in language model development."
                },
                "authors": [
                    {
                        "name": "Ce Ge"
                    },
                    {
                        "name": "Zhijian Ma"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "arxiv_comment": "Clarify details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14908v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14908v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15953v1",
                "updated": "2025-01-27T10:57:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    57,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T10:57:24Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    57,
                    24,
                    0,
                    27,
                    0
                ],
                "title": "Understanding Long Videos via LLM-Powered Entity Relation Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Long Videos via LLM-Powered Entity Relation Graphs"
                },
                "summary": "The analysis of extended video content poses unique challenges in artificial\nintelligence, particularly when dealing with the complexity of tracking and\nunderstanding visual elements across time. Current methodologies that process\nvideo frames sequentially struggle to maintain coherent tracking of objects,\nespecially when these objects temporarily vanish and later reappear in the\nfootage. A critical limitation of these approaches is their inability to\neffectively identify crucial moments in the video, largely due to their limited\ngrasp of temporal relationships. To overcome these obstacles, we present\nGraphVideoAgent, a cutting-edge system that leverages the power of graph-based\nobject tracking in conjunction with large language model capabilities. At its\ncore, our framework employs a dynamic graph structure that maps and monitors\nthe evolving relationships between visual entities throughout the video\nsequence. This innovative approach enables more nuanced understanding of how\nobjects interact and transform over time, facilitating improved frame selection\nthrough comprehensive contextual awareness. Our approach demonstrates\nremarkable effectiveness when tested against industry benchmarks. In\nevaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2\nimprovement over existing methods while requiring analysis of only 8.2 frames\non average. Similarly, testing on the NExT-QA benchmark yielded a 2.0\nperformance increase with an average frame requirement of 8.1. These results\nunderscore the efficiency of our graph-guided methodology in enhancing both\naccuracy and computational performance in long-form video understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of extended video content poses unique challenges in artificial\nintelligence, particularly when dealing with the complexity of tracking and\nunderstanding visual elements across time. Current methodologies that process\nvideo frames sequentially struggle to maintain coherent tracking of objects,\nespecially when these objects temporarily vanish and later reappear in the\nfootage. A critical limitation of these approaches is their inability to\neffectively identify crucial moments in the video, largely due to their limited\ngrasp of temporal relationships. To overcome these obstacles, we present\nGraphVideoAgent, a cutting-edge system that leverages the power of graph-based\nobject tracking in conjunction with large language model capabilities. At its\ncore, our framework employs a dynamic graph structure that maps and monitors\nthe evolving relationships between visual entities throughout the video\nsequence. This innovative approach enables more nuanced understanding of how\nobjects interact and transform over time, facilitating improved frame selection\nthrough comprehensive contextual awareness. Our approach demonstrates\nremarkable effectiveness when tested against industry benchmarks. In\nevaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2\nimprovement over existing methods while requiring analysis of only 8.2 frames\non average. Similarly, testing on the NExT-QA benchmark yielded a 2.0\nperformance increase with an average frame requirement of 8.1. These results\nunderscore the efficiency of our graph-guided methodology in enhancing both\naccuracy and computational performance in long-form video understanding tasks."
                },
                "authors": [
                    {
                        "name": "Meng Chu"
                    },
                    {
                        "name": "Yicong Li"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15867v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15867v3",
                "updated": "2025-01-27T10:34:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    34,
                    28,
                    0,
                    27,
                    0
                ],
                "published": "2024-08-28T15:35:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    35,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Practical Challenges for Reliable RIS Deployment in Heterogeneous\n  Multi-Operator Multi-Band Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Challenges for Reliable RIS Deployment in Heterogeneous\n  Multi-Operator Multi-Band Networks"
                },
                "summary": "Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of\nnearly passive elements with software-tunable electromagnetic properties to\ndynamically manipulate the reflection/transmission of radio signals. Research\nworks in this area are focused on two applications, namely {\\it user-assist}\nRIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target\nusers, and the {\\it malicious} RIS aiming for an attacker to degrade the QoS at\nvictim receivers through generating {\\it intended} destructive interference.\nWhile both user-assist and malicious RIS applications have been explored\nextensively, the impact of RIS deployments on imposing {\\it unintended}\ninterference on various wireless user-equipments (EUs) remains underexplored.\nThis paper investigates the challenges of integrating RISs into multi-carrier,\nmulti-user, and multi-operator networks. We discuss how RIS deployments\nintended to benefit specific users can negatively impact other users served at\nvarious carrier frequencies through different network operators. While not an\nideal solution, we discuss how ultra-narrowband metasurfaces can be\nincorporated into the manufacturing of RISs to mitigate some challenges of RIS\ndeployment in wireless networks. We also present a simulation scenario to\nilluminate some practical challenges associated with the deployment of RISs in\nshared public environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of\nnearly passive elements with software-tunable electromagnetic properties to\ndynamically manipulate the reflection/transmission of radio signals. Research\nworks in this area are focused on two applications, namely {\\it user-assist}\nRIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target\nusers, and the {\\it malicious} RIS aiming for an attacker to degrade the QoS at\nvictim receivers through generating {\\it intended} destructive interference.\nWhile both user-assist and malicious RIS applications have been explored\nextensively, the impact of RIS deployments on imposing {\\it unintended}\ninterference on various wireless user-equipments (EUs) remains underexplored.\nThis paper investigates the challenges of integrating RISs into multi-carrier,\nmulti-user, and multi-operator networks. We discuss how RIS deployments\nintended to benefit specific users can negatively impact other users served at\nvarious carrier frequencies through different network operators. While not an\nideal solution, we discuss how ultra-narrowband metasurfaces can be\nincorporated into the manufacturing of RISs to mitigate some challenges of RIS\ndeployment in wireless networks. We also present a simulation scenario to\nilluminate some practical challenges associated with the deployment of RISs in\nshared public environments."
                },
                "authors": [
                    {
                        "name": "Mehdi Monemi"
                    },
                    {
                        "name": "Mehdi Rasti"
                    },
                    {
                        "name": "Arthur S. de Sena"
                    },
                    {
                        "name": "Mohammad Amir Fallah"
                    },
                    {
                        "name": "Matti Latva-Aho"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15867v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15867v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15925v1",
                "updated": "2025-01-27T10:22:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    22,
                    38,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T10:22:38Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    22,
                    38,
                    0,
                    27,
                    0
                ],
                "title": "Efficient Distillation of Deep Spiking Neural Networks for Full-Range\n  Timestep Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Distillation of Deep Spiking Neural Networks for Full-Range\n  Timestep Deployment"
                },
                "summary": "Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative\nto traditional Artificial Neural Networks (ANNs), prized for their potential\nenergy efficiency on neuromorphic hardware. Despite this, SNNs often suffer\nfrom accuracy degradation compared to ANNs and face deployment challenges due\nto fixed inference timesteps, which require retraining for adjustments,\nlimiting operational flexibility. To address these issues, our work considers\nthe spatio-temporal property inherent in SNNs, and proposes a novel\ndistillation framework for deep SNNs that optimizes performance across\nfull-range timesteps without specific retraining, enhancing both efficacy and\ndeployment adaptability. We provide both theoretical analysis and empirical\nvalidations to illustrate that training guarantees the convergence of all\nimplicit models across full-range timesteps. Experimental results on CIFAR-10,\nCIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance\namong distillation-based SNNs training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative\nto traditional Artificial Neural Networks (ANNs), prized for their potential\nenergy efficiency on neuromorphic hardware. Despite this, SNNs often suffer\nfrom accuracy degradation compared to ANNs and face deployment challenges due\nto fixed inference timesteps, which require retraining for adjustments,\nlimiting operational flexibility. To address these issues, our work considers\nthe spatio-temporal property inherent in SNNs, and proposes a novel\ndistillation framework for deep SNNs that optimizes performance across\nfull-range timesteps without specific retraining, enhancing both efficacy and\ndeployment adaptability. We provide both theoretical analysis and empirical\nvalidations to illustrate that training guarantees the convergence of all\nimplicit models across full-range timesteps. Experimental results on CIFAR-10,\nCIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance\namong distillation-based SNNs training methods."
                },
                "authors": [
                    {
                        "name": "Chengting Yu"
                    },
                    {
                        "name": "Xiaochen Zhao"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Gaoang Wang"
                    },
                    {
                        "name": "Erping Li"
                    },
                    {
                        "name": "Aili Wang"
                    }
                ],
                "author_detail": {
                    "name": "Aili Wang"
                },
                "author": "Aili Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13187v2",
                "updated": "2025-01-27T10:19:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    19,
                    44,
                    0,
                    27,
                    0
                ],
                "published": "2024-03-19T22:56:53Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    22,
                    56,
                    53,
                    1,
                    79,
                    0
                ],
                "title": "Evolutionary Optimization of Model Merging Recipes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Optimization of Model Merging Recipes"
                },
                "summary": "Large language models (LLMs) have become increasingly capable, but their\ndevelopment often requires substantial computational resources. While model\nmerging has emerged as a cost-effective promising approach for creating new\nmodels by combining existing ones, it currently relies on human intuition and\ndomain knowledge, limiting its potential. Here, we propose an evolutionary\napproach that overcomes this limitation by automatically discovering effective\ncombinations of diverse open-source models, harnessing their collective\nintelligence without requiring extensive additional training data or compute.\nOur approach operates in both parameter space and data flow space, allowing for\noptimization beyond just the weights of the individual models. This approach\neven facilitates cross-domain merging, generating models like a Japanese LLM\nwith Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved\nstate-of-the-art performance on a variety of established Japanese LLM\nbenchmarks, even surpassing models with significantly more parameters, despite\nnot being explicitly trained for such tasks. Furthermore, a culturally-aware\nJapanese VLM generated through our approach demonstrates its effectiveness in\ndescribing Japanese culture-specific content, outperforming previous Japanese\nVLMs. This work not only contributes new state-of-the-art models back to the\nopen-source community, but also introduces a new paradigm for automated model\ncomposition, paving the way for exploring alternative, efficient approaches to\nfoundation model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become increasingly capable, but their\ndevelopment often requires substantial computational resources. While model\nmerging has emerged as a cost-effective promising approach for creating new\nmodels by combining existing ones, it currently relies on human intuition and\ndomain knowledge, limiting its potential. Here, we propose an evolutionary\napproach that overcomes this limitation by automatically discovering effective\ncombinations of diverse open-source models, harnessing their collective\nintelligence without requiring extensive additional training data or compute.\nOur approach operates in both parameter space and data flow space, allowing for\noptimization beyond just the weights of the individual models. This approach\neven facilitates cross-domain merging, generating models like a Japanese LLM\nwith Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved\nstate-of-the-art performance on a variety of established Japanese LLM\nbenchmarks, even surpassing models with significantly more parameters, despite\nnot being explicitly trained for such tasks. Furthermore, a culturally-aware\nJapanese VLM generated through our approach demonstrates its effectiveness in\ndescribing Japanese culture-specific content, outperforming previous Japanese\nVLMs. This work not only contributes new state-of-the-art models back to the\nopen-source community, but also introduces a new paradigm for automated model\ncomposition, paving the way for exploring alternative, efficient approaches to\nfoundation model development."
                },
                "authors": [
                    {
                        "name": "Takuya Akiba"
                    },
                    {
                        "name": "Makoto Shing"
                    },
                    {
                        "name": "Yujin Tang"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "David Ha"
                    }
                ],
                "author_detail": {
                    "name": "David Ha"
                },
                "author": "David Ha",
                "arxiv_doi": "10.1038/s42256-024-00975-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s42256-024-00975-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.13187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Authors' submitted version before final edits. Published in Nature\n  Machine Intelligence on January 27, 2025:\n  https://www.nature.com/articles/s42256-024-00975-8",
                "arxiv_journal_ref": "Nat Mach Intell (2025)",
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15922v1",
                "updated": "2025-01-27T10:17:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    17,
                    38,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T10:17:38Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    17,
                    38,
                    0,
                    27,
                    0
                ],
                "title": "SkillScope: A Tool to Predict Fine-Grained Skills Needed to Solve Issues\n  on GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkillScope: A Tool to Predict Fine-Grained Skills Needed to Solve Issues\n  on GitHub"
                },
                "summary": "New contributors often struggle to find tasks that they can tackle when\nonboarding onto a new Open Source Software (OSS) project. One reason for this\ndifficulty is that issue trackers lack explanations about the knowledge or\nskills needed to complete a given task successfully. These explanations can be\ncomplex and time-consuming to produce. Past research has partially addressed\nthis problem by labeling issues with issue types, issue difficulty level, and\nissue skills. However, current approaches are limited to a small set of labels\nand lack in-depth details about their semantics, which may not sufficiently\nhelp contributors identify suitable issues. To surmount this limitation, this\npaper explores large language models (LLMs) and Random Forest (RF) to predict\nthe multilevel skills required to solve the open issues. We introduce a novel\ntool, SkillScope, which retrieves current issues from Java projects hosted on\nGitHub and predicts the multilevel programming skills required to resolve these\nissues. In a case study, we demonstrate that SkillScope could predict 217\nmultilevel skills for tasks with 91% precision, 88% recall, and 89% F-measure\non average. Practitioners can use this tool to better delegate or choose tasks\nto solve in OSS projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New contributors often struggle to find tasks that they can tackle when\nonboarding onto a new Open Source Software (OSS) project. One reason for this\ndifficulty is that issue trackers lack explanations about the knowledge or\nskills needed to complete a given task successfully. These explanations can be\ncomplex and time-consuming to produce. Past research has partially addressed\nthis problem by labeling issues with issue types, issue difficulty level, and\nissue skills. However, current approaches are limited to a small set of labels\nand lack in-depth details about their semantics, which may not sufficiently\nhelp contributors identify suitable issues. To surmount this limitation, this\npaper explores large language models (LLMs) and Random Forest (RF) to predict\nthe multilevel skills required to solve the open issues. We introduce a novel\ntool, SkillScope, which retrieves current issues from Java projects hosted on\nGitHub and predicts the multilevel programming skills required to resolve these\nissues. In a case study, we demonstrate that SkillScope could predict 217\nmultilevel skills for tasks with 91% precision, 88% recall, and 89% F-measure\non average. Practitioners can use this tool to better delegate or choose tasks\nto solve in OSS projects."
                },
                "authors": [
                    {
                        "name": "Benjamin C. Carter"
                    },
                    {
                        "name": "Jonathan Rivas Contreras"
                    },
                    {
                        "name": "Carlos A. Llanes Villegas"
                    },
                    {
                        "name": "Pawan Acharya"
                    },
                    {
                        "name": "Jack Utzerath"
                    },
                    {
                        "name": "Adonijah O. Farner"
                    },
                    {
                        "name": "Hunter Jenkins"
                    },
                    {
                        "name": "Dylan Johnson"
                    },
                    {
                        "name": "Jacob Penney"
                    },
                    {
                        "name": "Igor Steinmacher"
                    },
                    {
                        "name": "Marco A. Gerosa"
                    },
                    {
                        "name": "Fabio Santos"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Santos"
                },
                "author": "Fabio Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15915v1",
                "updated": "2025-01-27T10:04:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    4,
                    49,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T10:04:49Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    4,
                    49,
                    0,
                    27,
                    0
                ],
                "title": "Parametric Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parametric Retrieval Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) techniques have emerged as a promising\nsolution to enhance the reliability of large language models (LLMs) by\naddressing issues like hallucinations, outdated knowledge, and domain\nadaptation. In particular, existing RAG methods append relevant documents\nretrieved from external corpus or databases to the input of LLMs to guide their\ngeneration process, which we refer to as the in-context knowledge injection\nmethod. While this approach is simple and often effective, it has inherent\nlimitations. Firstly, increasing the context length and number of relevant\ndocuments can lead to higher computational overhead and degraded performance,\nespecially in complex reasoning tasks. More importantly, in-context knowledge\ninjection operates primarily at the input level, but LLMs store their internal\nknowledge in their parameters. This gap fundamentally limits the capacity of\nin-context methods. To this end, we introduce Parametric retrieval-augmented\ngeneration (Parametric RAG), a new RAG paradigm that integrates external\nknowledge directly into the parameters of feed-forward networks (FFN) of an LLM\nthrough document parameterization. This approach not only saves online\ncomputational costs by eliminating the need to inject multiple documents into\nthe LLMs' input context, but also deepens the integration of external knowledge\ninto the parametric knowledge space of the LLM. Experimental results\ndemonstrate that Parametric RAG substantially enhances both the effectiveness\nand efficiency of knowledge augmentation in LLMs. Also, it can be combined with\nin-context RAG methods to achieve even better performance.\n  We have open-sourced all the code, data, and models in the following\nanonymized GitHub link: https://github.com/oneal2000/PRAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) techniques have emerged as a promising\nsolution to enhance the reliability of large language models (LLMs) by\naddressing issues like hallucinations, outdated knowledge, and domain\nadaptation. In particular, existing RAG methods append relevant documents\nretrieved from external corpus or databases to the input of LLMs to guide their\ngeneration process, which we refer to as the in-context knowledge injection\nmethod. While this approach is simple and often effective, it has inherent\nlimitations. Firstly, increasing the context length and number of relevant\ndocuments can lead to higher computational overhead and degraded performance,\nespecially in complex reasoning tasks. More importantly, in-context knowledge\ninjection operates primarily at the input level, but LLMs store their internal\nknowledge in their parameters. This gap fundamentally limits the capacity of\nin-context methods. To this end, we introduce Parametric retrieval-augmented\ngeneration (Parametric RAG), a new RAG paradigm that integrates external\nknowledge directly into the parameters of feed-forward networks (FFN) of an LLM\nthrough document parameterization. This approach not only saves online\ncomputational costs by eliminating the need to inject multiple documents into\nthe LLMs' input context, but also deepens the integration of external knowledge\ninto the parametric knowledge space of the LLM. Experimental results\ndemonstrate that Parametric RAG substantially enhances both the effectiveness\nand efficiency of knowledge augmentation in LLMs. Also, it can be combined with\nin-context RAG methods to achieve even better performance.\n  We have open-sourced all the code, data, and models in the following\nanonymized GitHub link: https://github.com/oneal2000/PRAG"
                },
                "authors": [
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Yichen Tang"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Junxi Yan"
                    },
                    {
                        "name": "Changyue Wang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Ziyi Ye"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15914v1",
                "updated": "2025-01-27T10:04:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    4,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T10:04:48Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    4,
                    48,
                    0,
                    27,
                    0
                ],
                "title": "Flexible delivery of broadband, 100-fs mid-infrared pulses in the\n  water-absorption band using hollow-core photonic crystal fibre",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible delivery of broadband, 100-fs mid-infrared pulses in the\n  water-absorption band using hollow-core photonic crystal fibre"
                },
                "summary": "High quality free-space and over-fibre transmission of mid-IR light is\nlimited by factors such as material-related absorption, diffraction, light\nleakage and nonlinearity. Conventional vacuum apparatus can be utilized for\nhigh-quality laser-beam delivery to address these issues, the deployment of\nsuch apparatus would, however, increase the system complexity, being\ndetrimental to their practical applications. Here we report the successful use\nof evacuated hollow-core photonic crystal fibre (PCF) to flexibly transmit\nultrafast mid-IR pulses over several meters, while preserving exceptional\nspatial, spectral and temporal fidelity. The PCF was engineered to feature a\nlow-loss transmission band within the water absorption range, and an evacuated\n5-m length was used to transmit Watt-level, 100 fs pulses centred at around 2.8\nmicrons. A comparison between free-space transmission and air-filled PCF\nhighlights the superior performance of the evacuated hollow-core PCF,\nindicating its strong suitability for the flexible delivery of sub-ps laser\npulses in the mid-IR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High quality free-space and over-fibre transmission of mid-IR light is\nlimited by factors such as material-related absorption, diffraction, light\nleakage and nonlinearity. Conventional vacuum apparatus can be utilized for\nhigh-quality laser-beam delivery to address these issues, the deployment of\nsuch apparatus would, however, increase the system complexity, being\ndetrimental to their practical applications. Here we report the successful use\nof evacuated hollow-core photonic crystal fibre (PCF) to flexibly transmit\nultrafast mid-IR pulses over several meters, while preserving exceptional\nspatial, spectral and temporal fidelity. The PCF was engineered to feature a\nlow-loss transmission band within the water absorption range, and an evacuated\n5-m length was used to transmit Watt-level, 100 fs pulses centred at around 2.8\nmicrons. A comparison between free-space transmission and air-filled PCF\nhighlights the superior performance of the evacuated hollow-core PCF,\nindicating its strong suitability for the flexible delivery of sub-ps laser\npulses in the mid-IR."
                },
                "authors": [
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Zeqing Li"
                    },
                    {
                        "name": "Yuewen Teng"
                    },
                    {
                        "name": "Jiapeng Huang"
                    },
                    {
                        "name": "Yun Zhao"
                    },
                    {
                        "name": "Zhuozhao Luo"
                    },
                    {
                        "name": "Weiyi Sun"
                    },
                    {
                        "name": "Cong Jiang"
                    },
                    {
                        "name": "Ruochen Yin"
                    },
                    {
                        "name": "Yu Zheng"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Meng Pang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Pang"
                },
                "author": "Meng Pang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12030v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12030v3",
                "updated": "2025-01-27T10:02:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    10,
                    2,
                    44,
                    0,
                    27,
                    0
                ],
                "published": "2024-02-19T10:37:29Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    10,
                    37,
                    29,
                    0,
                    50,
                    0
                ],
                "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation\n  Loss for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation\n  Loss for LLMs"
                },
                "summary": "Deploying large language models (LLMs) of several billion parameters can be\nimpractical in most industrial use cases due to constraints such as cost,\nlatency limitations, and hardware accessibility. Knowledge distillation (KD)\noffers a solution by compressing knowledge from resource-intensive large models\nto smaller ones. Various strategies exist, some relying on the text generated\nby the teacher model and optionally utilizing his logits to enhance learning.\nHowever, these methods based on logits often require both teacher and student\nmodels to share the same tokenizer, limiting their applicability across\ndifferent LLM families. In this paper, we introduce Universal Logit\nDistillation (ULD) loss, grounded in optimal transport, to address this\nlimitation. Our experimental results demonstrate the effectiveness of ULD loss\nin enabling distillation across models with different architectures and\ntokenizers, paving the way to a more widespread use of distillation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) of several billion parameters can be\nimpractical in most industrial use cases due to constraints such as cost,\nlatency limitations, and hardware accessibility. Knowledge distillation (KD)\noffers a solution by compressing knowledge from resource-intensive large models\nto smaller ones. Various strategies exist, some relying on the text generated\nby the teacher model and optionally utilizing his logits to enhance learning.\nHowever, these methods based on logits often require both teacher and student\nmodels to share the same tokenizer, limiting their applicability across\ndifferent LLM families. In this paper, we introduce Universal Logit\nDistillation (ULD) loss, grounded in optimal transport, to address this\nlimitation. Our experimental results demonstrate the effectiveness of ULD loss\nin enabling distillation across models with different architectures and\ntokenizers, paving the way to a more widespread use of distillation techniques."
                },
                "authors": [
                    {
                        "name": "Nicolas Boizard"
                    },
                    {
                        "name": "Kevin El Haddad"
                    },
                    {
                        "name": "Céline Hudelot"
                    },
                    {
                        "name": "Pierre Colombo"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Colombo"
                },
                "author": "Pierre Colombo",
                "arxiv_comment": "10 pages, 5 figures",
                "arxiv_journal_ref": "Transactions on Machine Learning Research, January 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12030v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12030v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15901v1",
                "updated": "2025-01-27T09:51:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    51,
                    48,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T09:51:48Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    51,
                    48,
                    0,
                    27,
                    0
                ],
                "title": "Robust Mobile Robot Path Planning via LLM-Based Dynamic Waypoint\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Mobile Robot Path Planning via LLM-Based Dynamic Waypoint\n  Generation"
                },
                "summary": "Mobile robot path planning in complex environments remains a significant\nchallenge, especially in achieving efficient, safe and robust paths. The\ntraditional path planning techniques like DRL models typically trained for a\ngiven configuration of the starting point and target positions, these models\nonly perform well when these conditions are satisfied. In this paper, we\nproposed a novel path planning framework that embeds Large Language Models to\nempower mobile robots with the capability of dynamically interpreting natural\nlanguage commands and autonomously generating efficient, collision-free\nnavigation paths. The proposed framework uses LLMs to translate high-level user\ninputs into actionable waypoints while dynamically adjusting paths in response\nto obstacles. We experimentally evaluated our proposed LLM-based approach\nacross three different environments of progressive complexity, showing the\nrobustness of our approach with llama3.1 model that outperformed other LLM\nmodels in path planning time, waypoint generation success rate, and collision\navoidance. This underlines the promising contribution of LLMs for enhancing the\ncapability of mobile robots, especially when their operation involves complex\ndecisions in large and complex environments. Our framework has provided safer,\nmore reliable navigation systems and opened a new direction for the future\nresearch. The source code of this work is publicly available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile robot path planning in complex environments remains a significant\nchallenge, especially in achieving efficient, safe and robust paths. The\ntraditional path planning techniques like DRL models typically trained for a\ngiven configuration of the starting point and target positions, these models\nonly perform well when these conditions are satisfied. In this paper, we\nproposed a novel path planning framework that embeds Large Language Models to\nempower mobile robots with the capability of dynamically interpreting natural\nlanguage commands and autonomously generating efficient, collision-free\nnavigation paths. The proposed framework uses LLMs to translate high-level user\ninputs into actionable waypoints while dynamically adjusting paths in response\nto obstacles. We experimentally evaluated our proposed LLM-based approach\nacross three different environments of progressive complexity, showing the\nrobustness of our approach with llama3.1 model that outperformed other LLM\nmodels in path planning time, waypoint generation success rate, and collision\navoidance. This underlines the promising contribution of LLMs for enhancing the\ncapability of mobile robots, especially when their operation involves complex\ndecisions in large and complex environments. Our framework has provided safer,\nmore reliable navigation systems and opened a new direction for the future\nresearch. The source code of this work is publicly available on GitHub."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Tariq"
                    },
                    {
                        "name": "Congqing Wang"
                    },
                    {
                        "name": "Yasir Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Yasir Hussain"
                },
                "author": "Yasir Hussain",
                "arxiv_comment": "18 pages, 6 figures, submitted in Journal Expert Systems with\n  Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02842v2",
                "updated": "2025-01-27T09:48:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    48,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2024-09-04T16:14:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    14,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "SNNAX -- Spiking Neural Networks in JAX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SNNAX -- Spiking Neural Networks in JAX"
                },
                "summary": "Spiking Neural Networks (SNNs) simulators are essential tools to prototype\nbiologically inspired models and neuromorphic hardware architectures and\npredict their performance. For such a tool, ease of use and flexibility are\ncritical, but so is simulation speed especially given the complexity inherent\nto simulating SNN. Here, we present SNNAX, a JAX-based framework for simulating\nand training such models with PyTorch-like intuitiveness and JAX-like execution\nspeed. SNNAX models are easily extended and customized to fit the desired model\nspecifications and target neuromorphic hardware. Additionally, SNNAX offers key\nfeatures for optimizing the training and deployment of SNNs such as flexible\nautomatic differentiation and just-in-time compilation. We evaluate and compare\nSNNAX to other commonly used machine learning (ML) frameworks used for\nprogramming SNNs. We provide key performance metrics, best practices,\ndocumented examples for simulating SNNs in SNNAX, and implement several\nbenchmarks used in the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) simulators are essential tools to prototype\nbiologically inspired models and neuromorphic hardware architectures and\npredict their performance. For such a tool, ease of use and flexibility are\ncritical, but so is simulation speed especially given the complexity inherent\nto simulating SNN. Here, we present SNNAX, a JAX-based framework for simulating\nand training such models with PyTorch-like intuitiveness and JAX-like execution\nspeed. SNNAX models are easily extended and customized to fit the desired model\nspecifications and target neuromorphic hardware. Additionally, SNNAX offers key\nfeatures for optimizing the training and deployment of SNNs such as flexible\nautomatic differentiation and just-in-time compilation. We evaluate and compare\nSNNAX to other commonly used machine learning (ML) frameworks used for\nprogramming SNNs. We provide key performance metrics, best practices,\ndocumented examples for simulating SNNs in SNNAX, and implement several\nbenchmarks used in the literature."
                },
                "authors": [
                    {
                        "name": "Jamie Lohoff"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_journal_ref": "Proceedings of the International Conference on Neuromorphic\n  Systems. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13444v3",
                "updated": "2025-01-27T09:39:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    39,
                    49,
                    0,
                    27,
                    0
                ],
                "published": "2024-01-24T13:36:50Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    13,
                    36,
                    50,
                    2,
                    24,
                    0
                ],
                "title": "Fine-Grained Stateful Knowledge Exploration: A Novel Paradigm for\n  Integrating Knowledge Graphs with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Stateful Knowledge Exploration: A Novel Paradigm for\n  Integrating Knowledge Graphs with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown impressive capabilities, yet updating\ntheir knowledge remains a significant challenge, often leading to outdated or\ninaccurate responses. A proposed solution is the integration of external\nknowledge bases, such as knowledge graphs, with LLMs. Most existing methods use\na paradigm that treats the question as the objective, with relevant knowledge\nbeing incrementally retrieved from the knowledge graph. However, this strategy\nfrequently experiences an mismatch in the granularity of knowledge between the\ntarget question and the entities and relations being retrieved. As a result,\nthe information in the question cannot precisely correspond to the retrieved\nknowledge. This may cause redundant exploration or omission of vital knowledge,\nthereby leading to enhanced computational consumption and reduced retrieval\naccuracy. In this paper, we propose a novel paradigm of fine-grained stateful\nknowledge exploration, which addresses the `information granularity mismatch'\nissue. We extract fine-grained information from questions and explore the\nsemantic mapping between this information and the knowledge in graph. By\ndynamically updating the mapping records, we avoid redundant exploration and\nensure no pertinent information is overlooked, thereby reducing computational\noverhead and improving the accuracy of knowledge exploration. The use of\nfine-grained information also eliminates the need for a priori knowledge, a\ncommon requirement in existing methods. Experiments on multiple datasets\nrevealed that our paradigm surpasses current advanced methods in knowledge\nretrieval while significantly reducing the average number of LLM invocations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive capabilities, yet updating\ntheir knowledge remains a significant challenge, often leading to outdated or\ninaccurate responses. A proposed solution is the integration of external\nknowledge bases, such as knowledge graphs, with LLMs. Most existing methods use\na paradigm that treats the question as the objective, with relevant knowledge\nbeing incrementally retrieved from the knowledge graph. However, this strategy\nfrequently experiences an mismatch in the granularity of knowledge between the\ntarget question and the entities and relations being retrieved. As a result,\nthe information in the question cannot precisely correspond to the retrieved\nknowledge. This may cause redundant exploration or omission of vital knowledge,\nthereby leading to enhanced computational consumption and reduced retrieval\naccuracy. In this paper, we propose a novel paradigm of fine-grained stateful\nknowledge exploration, which addresses the `information granularity mismatch'\nissue. We extract fine-grained information from questions and explore the\nsemantic mapping between this information and the knowledge in graph. By\ndynamically updating the mapping records, we avoid redundant exploration and\nensure no pertinent information is overlooked, thereby reducing computational\noverhead and improving the accuracy of knowledge exploration. The use of\nfine-grained information also eliminates the need for a priori knowledge, a\ncommon requirement in existing methods. Experiments on multiple datasets\nrevealed that our paradigm surpasses current advanced methods in knowledge\nretrieval while significantly reducing the average number of LLM invocations."
                },
                "authors": [
                    {
                        "name": "Dehao Tao"
                    },
                    {
                        "name": "Congqi Wang"
                    },
                    {
                        "name": "Feng Huang"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Minghu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Minghu Jiang"
                },
                "author": "Minghu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12333v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12333v3",
                "updated": "2025-01-27T09:18:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    18,
                    7,
                    0,
                    27,
                    0
                ],
                "published": "2024-08-22T12:21:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    21,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "GRATR: Zero-Shot Evidence Graph Retrieval-Augmented Trustworthiness\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRATR: Zero-Shot Evidence Graph Retrieval-Augmented Trustworthiness\n  Reasoning"
                },
                "summary": "Trustworthiness reasoning aims to enable agents in multiplayer games with\nincomplete information to identify potential allies and adversaries, thereby\nenhancing decision-making. In this paper, we introduce the graph\nretrieval-augmented trustworthiness reasoning (GRATR) framework, which\nretrieves observable evidence from the game environment to inform\ndecision-making by large language models (LLMs) without requiring additional\ntraining, making it a zero-shot approach. Within the GRATR framework, agents\nfirst observe the actions of other players and evaluate the resulting shifts in\ninter-player trust, constructing a corresponding trustworthiness graph. During\ndecision-making, the agent performs multi-hop retrieval to evaluate\ntrustworthiness toward a specific target, where evidence chains are retrieved\nfrom multiple trusted sources to form a comprehensive assessment. Experiments\nin the multiplayer game \\emph{Werewolf} demonstrate that GRATR outperforms the\nalternatives, improving reasoning accuracy by 50.5\\% and reducing hallucination\nby 30.6\\% compared to the baseline method. Additionally, when tested on a\ndataset of Twitter tweets during the U.S. election period, GRATR surpasses the\nbaseline method by 10.4\\% in accuracy, highlighting its potential in real-world\napplications such as intent analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness reasoning aims to enable agents in multiplayer games with\nincomplete information to identify potential allies and adversaries, thereby\nenhancing decision-making. In this paper, we introduce the graph\nretrieval-augmented trustworthiness reasoning (GRATR) framework, which\nretrieves observable evidence from the game environment to inform\ndecision-making by large language models (LLMs) without requiring additional\ntraining, making it a zero-shot approach. Within the GRATR framework, agents\nfirst observe the actions of other players and evaluate the resulting shifts in\ninter-player trust, constructing a corresponding trustworthiness graph. During\ndecision-making, the agent performs multi-hop retrieval to evaluate\ntrustworthiness toward a specific target, where evidence chains are retrieved\nfrom multiple trusted sources to form a comprehensive assessment. Experiments\nin the multiplayer game \\emph{Werewolf} demonstrate that GRATR outperforms the\nalternatives, improving reasoning accuracy by 50.5\\% and reducing hallucination\nby 30.6\\% compared to the baseline method. Additionally, when tested on a\ndataset of Twitter tweets during the U.S. election period, GRATR surpasses the\nbaseline method by 10.4\\% in accuracy, highlighting its potential in real-world\napplications such as intent analysis."
                },
                "authors": [
                    {
                        "name": "Ying Zhu"
                    },
                    {
                        "name": "Shengchang Li"
                    },
                    {
                        "name": "Ziqian Kong"
                    },
                    {
                        "name": "Qiang Yang"
                    },
                    {
                        "name": "Peilan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Peilan Xu"
                },
                "author": "Peilan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12333v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12333v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03667v2",
                "updated": "2025-01-27T09:02:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    9,
                    2,
                    46,
                    0,
                    27,
                    0
                ],
                "published": "2024-02-06T03:41:12Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    3,
                    41,
                    12,
                    1,
                    37,
                    0
                ],
                "title": "Large Language Models as an Indirect Reasoner: Contrapositive and\n  Contradiction for Automated Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as an Indirect Reasoner: Contrapositive and\n  Contradiction for Automated Reasoning"
                },
                "summary": "Recently, increasing attention has been focused on improving the ability of\nLarge Language Models (LLMs) to perform complex reasoning. Advanced methods,\nsuch as Chain-of-Thought (CoT) and its variants, are found to enhance their\nreasoning skills by designing suitable prompts or breaking down complex\nproblems into more manageable sub-problems. However, little concentration has\nbeen put on exploring the reasoning process, \\textit{i.e.}, we discovered that\nmost methods resort to Direct Reasoning (DR) and disregard Indirect Reasoning\n(IR). This can make LLMs difficult to solve IR tasks, which are often\nencountered in the real world. To address this issue, we propose a\nDirect-Indirect Reasoning (DIR) method, which considers DR and IR as multiple\nparallel reasoning paths that are merged to derive the final answer. We\nstimulate LLMs to implement IR by crafting prompt templates incorporating the\nprinciples of contrapositive and contradiction. These templates trigger LLMs to\nassume the negation of the conclusion as true, combine it with the premises to\ndeduce a conclusion, and utilize the logical equivalence of the contrapositive\nto enhance their comprehension of the rules used in the reasoning process. Our\nDIR method is simple yet effective and can be straightforwardly integrated with\nexisting variants of CoT methods. Experimental results on four datasets related\nto logical reasoning and mathematic proof demonstrate that our DIR method, when\ncombined with various baseline methods, significantly outperforms all the\noriginal methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, increasing attention has been focused on improving the ability of\nLarge Language Models (LLMs) to perform complex reasoning. Advanced methods,\nsuch as Chain-of-Thought (CoT) and its variants, are found to enhance their\nreasoning skills by designing suitable prompts or breaking down complex\nproblems into more manageable sub-problems. However, little concentration has\nbeen put on exploring the reasoning process, \\textit{i.e.}, we discovered that\nmost methods resort to Direct Reasoning (DR) and disregard Indirect Reasoning\n(IR). This can make LLMs difficult to solve IR tasks, which are often\nencountered in the real world. To address this issue, we propose a\nDirect-Indirect Reasoning (DIR) method, which considers DR and IR as multiple\nparallel reasoning paths that are merged to derive the final answer. We\nstimulate LLMs to implement IR by crafting prompt templates incorporating the\nprinciples of contrapositive and contradiction. These templates trigger LLMs to\nassume the negation of the conclusion as true, combine it with the premises to\ndeduce a conclusion, and utilize the logical equivalence of the contrapositive\nto enhance their comprehension of the rules used in the reasoning process. Our\nDIR method is simple yet effective and can be straightforwardly integrated with\nexisting variants of CoT methods. Experimental results on four datasets related\nto logical reasoning and mathematic proof demonstrate that our DIR method, when\ncombined with various baseline methods, significantly outperforms all the\noriginal methods."
                },
                "authors": [
                    {
                        "name": "Yanfang Zhang"
                    },
                    {
                        "name": "Yiliu Sun"
                    },
                    {
                        "name": "Yibing Zhan"
                    },
                    {
                        "name": "Dapeng Tao"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Chen Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chen Gong"
                },
                "author": "Chen Gong",
                "arxiv_comment": "Accepted by COLING 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15875v1",
                "updated": "2025-01-27T08:59:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    59,
                    10,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T08:59:10Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    59,
                    10,
                    0,
                    27,
                    0
                ],
                "title": "LCTG Bench: LLM Controlled Text Generation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCTG Bench: LLM Controlled Text Generation Benchmark"
                },
                "summary": "The rise of large language models (LLMs) has led to more diverse and\nhigher-quality machine-generated text. However, their high expressive power\nmakes it difficult to control outputs based on specific business instructions.\nIn response, benchmarks focusing on the controllability of LLMs have been\ndeveloped, but several issues remain: (1) They primarily cover major languages\nlike English and Chinese, neglecting low-resource languages like Japanese; (2)\nCurrent benchmarks employ task-specific evaluation metrics, lacking a unified\nframework for selecting models based on controllability across different use\ncases. To address these challenges, this research introduces LCTG Bench, the\nfirst Japanese benchmark for evaluating the controllability of LLMs. LCTG Bench\nprovides a unified framework for assessing control performance, enabling users\nto select the most suitable model for their use cases based on controllability.\nBy evaluating nine diverse Japanese-specific and multilingual LLMs like GPT-4,\nwe highlight the current state and challenges of controllability in Japanese\nLLMs and reveal the significant gap between multilingual models and\nJapanese-specific models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has led to more diverse and\nhigher-quality machine-generated text. However, their high expressive power\nmakes it difficult to control outputs based on specific business instructions.\nIn response, benchmarks focusing on the controllability of LLMs have been\ndeveloped, but several issues remain: (1) They primarily cover major languages\nlike English and Chinese, neglecting low-resource languages like Japanese; (2)\nCurrent benchmarks employ task-specific evaluation metrics, lacking a unified\nframework for selecting models based on controllability across different use\ncases. To address these challenges, this research introduces LCTG Bench, the\nfirst Japanese benchmark for evaluating the controllability of LLMs. LCTG Bench\nprovides a unified framework for assessing control performance, enabling users\nto select the most suitable model for their use cases based on controllability.\nBy evaluating nine diverse Japanese-specific and multilingual LLMs like GPT-4,\nwe highlight the current state and challenges of controllability in Japanese\nLLMs and reveal the significant gap between multilingual models and\nJapanese-specific models."
                },
                "authors": [
                    {
                        "name": "Kentaro Kurihara"
                    },
                    {
                        "name": "Masato Mita"
                    },
                    {
                        "name": "Peinan Zhang"
                    },
                    {
                        "name": "Shota Sasaki"
                    },
                    {
                        "name": "Ryosuke Ishigami"
                    },
                    {
                        "name": "Naoaki Okazaki"
                    }
                ],
                "author_detail": {
                    "name": "Naoaki Okazaki"
                },
                "author": "Naoaki Okazaki",
                "arxiv_comment": "15 pages, 11 figures. Project page: this\n  [URL](https://github.com/CyberAgentAILab/LCTG-Bench)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10827v2",
                "updated": "2025-01-27T08:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    51,
                    37,
                    0,
                    27,
                    0
                ],
                "published": "2024-12-14T13:12:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    13,
                    12,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "Rethinking Chain-of-Thought from the Perspective of Self-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Chain-of-Thought from the Perspective of Self-Training"
                },
                "summary": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Zongqian Wu"
                    },
                    {
                        "name": "Baoduo Xu"
                    },
                    {
                        "name": "Ruochen Cui"
                    },
                    {
                        "name": "Mengmeng Zhan"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Lei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Lei Feng"
                },
                "author": "Lei Feng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14197v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14197v4",
                "updated": "2025-01-27T08:51:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    51,
                    16,
                    0,
                    27,
                    0
                ],
                "published": "2023-12-21T01:08:39Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    1,
                    8,
                    39,
                    3,
                    355,
                    0
                ],
                "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on\n  Large Language Models"
                },
                "summary": "The integration of large language models with external content has enabled\napplications such as Microsoft Copilot but also introduced vulnerabilities to\nindirect prompt injection attacks. In these attacks, malicious instructions\nembedded within external content can manipulate LLM outputs, causing deviations\nfrom user expectations. To address this critical yet under-explored issue, we\nintroduce the first benchmark for indirect prompt injection attacks, named\nBIPIA, to assess the risk of such vulnerabilities. Using BIPIA, we evaluate\nexisting LLMs and find them universally vulnerable. Our analysis identifies two\nkey factors contributing to their success: LLMs' inability to distinguish\nbetween informational context and actionable instructions, and their lack of\nawareness in avoiding the execution of instructions within external content.\nBased on these findings, we propose two novel defense mechanisms-boundary\nawareness and explicit reminder-to address these vulnerabilities in both\nblack-box and white-box settings. Extensive experiments demonstrate that our\nblack-box defense provides substantial mitigation, while our white-box defense\nreduces the attack success rate to near-zero levels, all while preserving the\noutput quality of LLMs. We hope this work inspires further research into\nsecuring LLM applications and fostering their safe and reliable use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models with external content has enabled\napplications such as Microsoft Copilot but also introduced vulnerabilities to\nindirect prompt injection attacks. In these attacks, malicious instructions\nembedded within external content can manipulate LLM outputs, causing deviations\nfrom user expectations. To address this critical yet under-explored issue, we\nintroduce the first benchmark for indirect prompt injection attacks, named\nBIPIA, to assess the risk of such vulnerabilities. Using BIPIA, we evaluate\nexisting LLMs and find them universally vulnerable. Our analysis identifies two\nkey factors contributing to their success: LLMs' inability to distinguish\nbetween informational context and actionable instructions, and their lack of\nawareness in avoiding the execution of instructions within external content.\nBased on these findings, we propose two novel defense mechanisms-boundary\nawareness and explicit reminder-to address these vulnerabilities in both\nblack-box and white-box settings. Extensive experiments demonstrate that our\nblack-box defense provides substantial mitigation, while our white-box defense\nreduces the attack success rate to near-zero levels, all while preserving the\noutput quality of LLMs. We hope this work inspires further research into\nsecuring LLM applications and fostering their safe and reliable use."
                },
                "authors": [
                    {
                        "name": "Jingwei Yi"
                    },
                    {
                        "name": "Yueqi Xie"
                    },
                    {
                        "name": "Bin Zhu"
                    },
                    {
                        "name": "Emre Kiciman"
                    },
                    {
                        "name": "Guangzhong Sun"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Fangzhao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhao Wu"
                },
                "author": "Fangzhao Wu",
                "arxiv_doi": "10.1145/3690624.3709179",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3690624.3709179",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.14197v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14197v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by KDD 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20007v2",
                "updated": "2025-01-27T08:46:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    46,
                    9,
                    0,
                    27,
                    0
                ],
                "published": "2024-09-30T07:01:21Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    1,
                    21,
                    0,
                    274,
                    0
                ],
                "title": "DeSTA2: Developing Instruction-Following Speech Language Model Without\n  Speech Instruction-Tuning Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeSTA2: Developing Instruction-Following Speech Language Model Without\n  Speech Instruction-Tuning Data"
                },
                "summary": "Recent end-to-end speech language models (SLMs) have expanded upon the\ncapabilities of large language models (LLMs) by incorporating pre-trained\nspeech models. However, these SLMs often undergo extensive speech\ninstruction-tuning to bridge the gap between speech and text modalities. This\nrequires significant annotation efforts and risks catastrophic forgetting of\nthe original language capabilities. In this work, we present a simple yet\neffective automatic process for creating speech-text pair data that carefully\ninjects speech paralinguistic understanding abilities into SLMs while\npreserving the inherent language capabilities of the text-based LLM. Our model\ndemonstrates general capabilities for speech-related tasks without the need for\nspeech instruction-tuning data, achieving impressive performance on\nDynamic-SUPERB and AIR-Bench-Chat benchmarks. Furthermore, our model exhibits\nthe ability to follow complex instructions derived from LLMs, such as specific\noutput formatting and chain-of-thought reasoning. Our approach not only\nenhances the versatility and effectiveness of SLMs but also reduces reliance on\nextensive annotated datasets, paving the way for more efficient and capable\nspeech understanding systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent end-to-end speech language models (SLMs) have expanded upon the\ncapabilities of large language models (LLMs) by incorporating pre-trained\nspeech models. However, these SLMs often undergo extensive speech\ninstruction-tuning to bridge the gap between speech and text modalities. This\nrequires significant annotation efforts and risks catastrophic forgetting of\nthe original language capabilities. In this work, we present a simple yet\neffective automatic process for creating speech-text pair data that carefully\ninjects speech paralinguistic understanding abilities into SLMs while\npreserving the inherent language capabilities of the text-based LLM. Our model\ndemonstrates general capabilities for speech-related tasks without the need for\nspeech instruction-tuning data, achieving impressive performance on\nDynamic-SUPERB and AIR-Bench-Chat benchmarks. Furthermore, our model exhibits\nthe ability to follow complex instructions derived from LLMs, such as specific\noutput formatting and chain-of-thought reasoning. Our approach not only\nenhances the versatility and effectiveness of SLMs but also reduces reliance on\nextensive annotated datasets, paving the way for more efficient and capable\nspeech understanding systems."
                },
                "authors": [
                    {
                        "name": "Ke-Han Lu"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Szu-Wei Fu"
                    },
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15860v1",
                "updated": "2025-01-27T08:36:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    36,
                    14,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T08:36:14Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    36,
                    14,
                    0,
                    27,
                    0
                ],
                "title": "The Components of Collaborative Joint Perception and Prediction -- A\n  Conceptual Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Components of Collaborative Joint Perception and Prediction -- A\n  Conceptual Framework"
                },
                "summary": "Connected Autonomous Vehicles (CAVs) benefit from Vehicle-to-Everything (V2X)\ncommunication, which enables the exchange of sensor data to achieve\nCollaborative Perception (CP). To reduce cumulative errors in perception\nmodules and mitigate the visual occlusion, this paper introduces a new task,\nCollaborative Joint Perception and Prediction (Co-P&P), and provides a\nconceptual framework for its implementation to improve motion prediction of\nsurrounding objects, thereby enhancing vehicle awareness in complex traffic\nscenarios. The framework consists of two decoupled core modules, Collaborative\nScene Completion (CSC) and Joint Perception and Prediction (P&P) module, which\nsimplify practical deployment and enhance scalability. Additionally, we outline\nthe challenges in Co-P&P and discuss future directions for this research area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connected Autonomous Vehicles (CAVs) benefit from Vehicle-to-Everything (V2X)\ncommunication, which enables the exchange of sensor data to achieve\nCollaborative Perception (CP). To reduce cumulative errors in perception\nmodules and mitigate the visual occlusion, this paper introduces a new task,\nCollaborative Joint Perception and Prediction (Co-P&P), and provides a\nconceptual framework for its implementation to improve motion prediction of\nsurrounding objects, thereby enhancing vehicle awareness in complex traffic\nscenarios. The framework consists of two decoupled core modules, Collaborative\nScene Completion (CSC) and Joint Perception and Prediction (P&P) module, which\nsimplify practical deployment and enhance scalability. Additionally, we outline\nthe challenges in Co-P&P and discuss future directions for this research area."
                },
                "authors": [
                    {
                        "name": "Lei Wan"
                    },
                    {
                        "name": "Hannan Ejaz Keen"
                    },
                    {
                        "name": "Alexey Vinel"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Vinel"
                },
                "author": "Alexey Vinel",
                "arxiv_comment": "8 pages, 4 figures, accepted by conference VEHITS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15850v1",
                "updated": "2025-01-27T08:18:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    18,
                    52,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T08:18:52Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    18,
                    52,
                    0,
                    27,
                    0
                ],
                "title": "LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for\n  Autonomous Driving with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for\n  Autonomous Driving with Large Language Models"
                },
                "summary": "Ensuring and improving the safety of autonomous driving systems (ADS) is\ncrucial for the deployment of highly automated vehicles, especially in\nsafety-critical events. To address the rarity issue, adversarial scenario\ngeneration methods are developed, in which behaviors of traffic participants\nare manipulated to induce safety-critical events. However, existing methods\nstill face two limitations. First, identification of the adversarial\nparticipant directly impacts the effectiveness of the generation. However, the\ncomplexity of real-world scenarios, with numerous participants and diverse\nbehaviors, makes identification challenging. Second, the potential of generated\nsafety-critical scenarios to continuously improve ADS performance remains\nunderexplored. To address these issues, we propose LLM-attacker: a closed-loop\nadversarial scenario generation framework leveraging large language models\n(LLMs). Specifically, multiple LLM agents are designed and coordinated to\nidentify optimal attackers. Then, the trajectories of the attackers are\noptimized to generate adversarial scenarios. These scenarios are iteratively\nrefined based on the performance of ADS, forming a feedback loop to improve\nADS. Experimental results show that LLM-attacker can create more dangerous\nscenarios than other methods, and the ADS trained with it achieves a collision\nrate half that of training with normal scenarios. This indicates the ability of\nLLM-attacker to test and enhance the safety and robustness of ADS. Video\ndemonstrations are provided at:\nhttps://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring and improving the safety of autonomous driving systems (ADS) is\ncrucial for the deployment of highly automated vehicles, especially in\nsafety-critical events. To address the rarity issue, adversarial scenario\ngeneration methods are developed, in which behaviors of traffic participants\nare manipulated to induce safety-critical events. However, existing methods\nstill face two limitations. First, identification of the adversarial\nparticipant directly impacts the effectiveness of the generation. However, the\ncomplexity of real-world scenarios, with numerous participants and diverse\nbehaviors, makes identification challenging. Second, the potential of generated\nsafety-critical scenarios to continuously improve ADS performance remains\nunderexplored. To address these issues, we propose LLM-attacker: a closed-loop\nadversarial scenario generation framework leveraging large language models\n(LLMs). Specifically, multiple LLM agents are designed and coordinated to\nidentify optimal attackers. Then, the trajectories of the attackers are\noptimized to generate adversarial scenarios. These scenarios are iteratively\nrefined based on the performance of ADS, forming a feedback loop to improve\nADS. Experimental results show that LLM-attacker can create more dangerous\nscenarios than other methods, and the ADS trained with it achieves a collision\nrate half that of training with normal scenarios. This indicates the ability of\nLLM-attacker to test and enhance the safety and robustness of ADS. Video\ndemonstrations are provided at:\nhttps://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view."
                },
                "authors": [
                    {
                        "name": "Yuewen Mei"
                    },
                    {
                        "name": "Tong Nie"
                    },
                    {
                        "name": "Jian Sun"
                    },
                    {
                        "name": "Ye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Ye Tian"
                },
                "author": "Ye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15836v1",
                "updated": "2025-01-27T07:51:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    7,
                    51,
                    51,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T07:51:51Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    7,
                    51,
                    51,
                    0,
                    27,
                    0
                ],
                "title": "Intelligent Code Embedding Framework for High-Precision Ransomware\n  Detection via Multimodal Execution Path Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Code Embedding Framework for High-Precision Ransomware\n  Detection via Multimodal Execution Path Analysis"
                },
                "summary": "Modern threat landscapes continue to evolve with increasing sophistication,\nchallenging traditional detection methodologies and necessitating innovative\nsolutions capable of addressing complex adversarial tactics. A novel framework\nwas developed to identify ransomware activity through multimodal execution path\nanalysis, integrating high-dimensional embeddings and dynamic heuristic\nderivation mechanisms to capture behavioral patterns across diverse attack\nvariants. The approach demonstrated high adaptability, effectively mitigating\nobfuscation strategies and polymorphic characteristics often employed by\nransomware families to evade detection. Comprehensive experimental evaluations\nrevealed significant advancements in precision, recall, and accuracy metrics\ncompared to baseline techniques, particularly under conditions of variable\nencryption speeds and obfuscated execution flows. The framework achieved\nscalable and computationally efficient performance, ensuring robust\napplicability across a range of system configurations, from\nresource-constrained environments to high-performance infrastructures. Notable\nfindings included reduced false positive rates and enhanced detection latency,\neven for ransomware families employing sophisticated encryption mechanisms. The\nmodular design allowed seamless integration of additional modalities, enabling\nextensibility and future-proofing against emerging threat vectors. Quantitative\nanalyses further highlighted the system's energy efficiency, emphasizing its\npracticality for deployment in environments with stringent operational\nconstraints. The results underline the importance of integrating advanced\ncomputational techniques and dynamic adaptability to safeguard digital\necosystems from increasingly complex threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern threat landscapes continue to evolve with increasing sophistication,\nchallenging traditional detection methodologies and necessitating innovative\nsolutions capable of addressing complex adversarial tactics. A novel framework\nwas developed to identify ransomware activity through multimodal execution path\nanalysis, integrating high-dimensional embeddings and dynamic heuristic\nderivation mechanisms to capture behavioral patterns across diverse attack\nvariants. The approach demonstrated high adaptability, effectively mitigating\nobfuscation strategies and polymorphic characteristics often employed by\nransomware families to evade detection. Comprehensive experimental evaluations\nrevealed significant advancements in precision, recall, and accuracy metrics\ncompared to baseline techniques, particularly under conditions of variable\nencryption speeds and obfuscated execution flows. The framework achieved\nscalable and computationally efficient performance, ensuring robust\napplicability across a range of system configurations, from\nresource-constrained environments to high-performance infrastructures. Notable\nfindings included reduced false positive rates and enhanced detection latency,\neven for ransomware families employing sophisticated encryption mechanisms. The\nmodular design allowed seamless integration of additional modalities, enabling\nextensibility and future-proofing against emerging threat vectors. Quantitative\nanalyses further highlighted the system's energy efficiency, emphasizing its\npracticality for deployment in environments with stringent operational\nconstraints. The results underline the importance of integrating advanced\ncomputational techniques and dynamic adaptability to safeguard digital\necosystems from increasingly complex threats."
                },
                "authors": [
                    {
                        "name": "Levi Gareth"
                    },
                    {
                        "name": "Maximilian Fairbrother"
                    },
                    {
                        "name": "Peregrine Blackwood"
                    },
                    {
                        "name": "Lucasta Underhill"
                    },
                    {
                        "name": "Benedict Ruthermore"
                    }
                ],
                "author_detail": {
                    "name": "Benedict Ruthermore"
                },
                "author": "Benedict Ruthermore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15829v1",
                "updated": "2025-01-27T07:29:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    7,
                    29,
                    8,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T07:29:08Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    7,
                    29,
                    8,
                    0,
                    27,
                    0
                ],
                "title": "Aging-aware CPU Core Management for Embodied Carbon Amortization in\n  Cloud LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aging-aware CPU Core Management for Embodied Carbon Amortization in\n  Cloud LLM Inference"
                },
                "summary": "Broad adoption of Large Language Models (LLM) demands rapid expansions of\ncloud LLM inference clusters, leading to accumulation of embodied carbon$-$the\nemissions from manufacturing and supplying IT assets$-$that mostly concentrate\non inference server CPU. This paper delves into the challenges of sustainable\ngrowth of cloud LLM inference, emphasizing extended amortization of CPU\nembodied over an increased lifespan. Given the reliability risks of silicon\naging, we propose an aging-aware CPU core management technique to delay CPU\naging effects, allowing the cluster operator to safely increase CPU life. Our\ntechnique exploits CPU underutilization patterns that we uncover in cloud LLM\ninference by halting aging in unused cores and even-outing aging in active\ncores via selective deep idling and aging-aware inference task allocation.\nThrough extensive simulations using real-world Azure inference traces and an\nextended LLM cluster simulator from Microsoft, we show superior performance of\nour technique over existing methods with an estimated 37.67\\% reduction in\nyearly embodied carbon emissions through p99 performance of managing CPU aging\neffects, a 77\\% reduction in CPU underutilization, and less than 10\\% impact to\nthe inference service quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broad adoption of Large Language Models (LLM) demands rapid expansions of\ncloud LLM inference clusters, leading to accumulation of embodied carbon$-$the\nemissions from manufacturing and supplying IT assets$-$that mostly concentrate\non inference server CPU. This paper delves into the challenges of sustainable\ngrowth of cloud LLM inference, emphasizing extended amortization of CPU\nembodied over an increased lifespan. Given the reliability risks of silicon\naging, we propose an aging-aware CPU core management technique to delay CPU\naging effects, allowing the cluster operator to safely increase CPU life. Our\ntechnique exploits CPU underutilization patterns that we uncover in cloud LLM\ninference by halting aging in unused cores and even-outing aging in active\ncores via selective deep idling and aging-aware inference task allocation.\nThrough extensive simulations using real-world Azure inference traces and an\nextended LLM cluster simulator from Microsoft, we show superior performance of\nour technique over existing methods with an estimated 37.67\\% reduction in\nyearly embodied carbon emissions through p99 performance of managing CPU aging\neffects, a 77\\% reduction in CPU underutilization, and less than 10\\% impact to\nthe inference service quality."
                },
                "authors": [
                    {
                        "name": "Tharindu B. Hewage"
                    },
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Maria Rodriguez Read"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15826v1",
                "updated": "2025-01-27T07:18:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    7,
                    18,
                    47,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T07:18:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    7,
                    18,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "MADP: Multi-Agent Deductive Planning for Enhanced Cognitive-Behavioral\n  Mental Health Question Answer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MADP: Multi-Agent Deductive Planning for Enhanced Cognitive-Behavioral\n  Mental Health Question Answer"
                },
                "summary": "The Mental Health Question Answer (MHQA) task requires the seeker and\nsupporter to complete the support process in one-turn dialogue. Given the\nrichness of help-seeker posts, supporters must thoroughly understand the\ncontent and provide logical, comprehensive, and well-structured responses.\nPrevious works in MHQA mostly focus on single-agent approaches based on the\ncognitive element of Cognitive Behavioral Therapy (CBT), but they overlook the\ninteractions among various CBT elements, such as emotion and cognition. This\nlimitation hinders the models' ability to thoroughly understand the distress of\nhelp-seekers. To address this, we propose a framework named Multi-Agent\nDeductive Planning (MADP), which is based on the interactions between the\nvarious psychological elements of CBT. This method guides Large Language Models\n(LLMs) to achieve a deeper understanding of the seeker's context and provide\nmore personalized assistance based on individual circumstances. Furthermore, we\nconstruct a new dataset based on the MADP framework and use it to fine-tune\nLLMs, resulting in a specialized model named MADP-LLM. We conduct extensive\nexperiments, including comparisons with multiple LLMs, human evaluations, and\nautomatic evaluations, to validate the effectiveness of the MADP framework and\nMADP-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mental Health Question Answer (MHQA) task requires the seeker and\nsupporter to complete the support process in one-turn dialogue. Given the\nrichness of help-seeker posts, supporters must thoroughly understand the\ncontent and provide logical, comprehensive, and well-structured responses.\nPrevious works in MHQA mostly focus on single-agent approaches based on the\ncognitive element of Cognitive Behavioral Therapy (CBT), but they overlook the\ninteractions among various CBT elements, such as emotion and cognition. This\nlimitation hinders the models' ability to thoroughly understand the distress of\nhelp-seekers. To address this, we propose a framework named Multi-Agent\nDeductive Planning (MADP), which is based on the interactions between the\nvarious psychological elements of CBT. This method guides Large Language Models\n(LLMs) to achieve a deeper understanding of the seeker's context and provide\nmore personalized assistance based on individual circumstances. Furthermore, we\nconstruct a new dataset based on the MADP framework and use it to fine-tune\nLLMs, resulting in a specialized model named MADP-LLM. We conduct extensive\nexperiments, including comparisons with multiple LLMs, human evaluations, and\nautomatic evaluations, to validate the effectiveness of the MADP framework and\nMADP-LLM."
                },
                "authors": [
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Dexi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dexi Liu"
                },
                "author": "Dexi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03775v3",
                "updated": "2025-01-27T07:02:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    7,
                    2,
                    4,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-03T03:08:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    3,
                    8,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "Beyond correlation: The Impact of Human Uncertainty in Measuring the\n  Effectiveness of Automatic Evaluation and LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond correlation: The Impact of Human Uncertainty in Measuring the\n  Effectiveness of Automatic Evaluation and LLM-as-a-Judge"
                },
                "summary": "The effectiveness of automatic evaluation of generative models is typically\nmeasured by comparing the labels generated via automation with labels by humans\nusing correlation metrics. However, metrics like Krippendorff's $\\alpha$ and\nRandolph's $\\kappa$ were originally designed to measure the reliability of\nhuman labeling, thus make assumptions about typical human labeling behavior,\nand these assumptions may not be applicable to machine generated labels. In\nthis paper, we show how *relying on a single aggregate correlation score* can\nobscure fundamental differences between human labels and those from automatic\nevaluation, including LLM-as-a-Judge. Specifically, we demonstrate that when\nthe proportion of samples with variation or uncertainty in human assigned\nlabels is relatively high, machine labels (generated by automatic evaluation\nmethods) may superficially appear to have similar or better correlation with\nthe human majority label compared to the human-to-human (HH) correlation. This\ncan create the illusion that labels from automatic evaluation approximates the\nhuman majority label. However, as the proportion of samples with consistent\nhuman labels increases, the correlation between machine and human labels fall\nwell below HH correlation. Based on these findings, we first propose\nstratifying data by human label uncertainty to provide a more robust analysis\nof automatic evaluation performance. Second, recognizing that uncertainty and\nvariation are inherent in perception-based human evaluations, such as those\ninvolving attitudes or preferences, we introduce a new metric - binned\nJensen-Shannon Divergence for perception for such scenarios to better measure\nthe effectiveness of automatic evaluations. We present visualization techniques\n-- perception charts, to contextualize correlation measures appropriately. We\nhave open-sourced at https://github.com/amazon-science/BeyondCorrelation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of automatic evaluation of generative models is typically\nmeasured by comparing the labels generated via automation with labels by humans\nusing correlation metrics. However, metrics like Krippendorff's $\\alpha$ and\nRandolph's $\\kappa$ were originally designed to measure the reliability of\nhuman labeling, thus make assumptions about typical human labeling behavior,\nand these assumptions may not be applicable to machine generated labels. In\nthis paper, we show how *relying on a single aggregate correlation score* can\nobscure fundamental differences between human labels and those from automatic\nevaluation, including LLM-as-a-Judge. Specifically, we demonstrate that when\nthe proportion of samples with variation or uncertainty in human assigned\nlabels is relatively high, machine labels (generated by automatic evaluation\nmethods) may superficially appear to have similar or better correlation with\nthe human majority label compared to the human-to-human (HH) correlation. This\ncan create the illusion that labels from automatic evaluation approximates the\nhuman majority label. However, as the proportion of samples with consistent\nhuman labels increases, the correlation between machine and human labels fall\nwell below HH correlation. Based on these findings, we first propose\nstratifying data by human label uncertainty to provide a more robust analysis\nof automatic evaluation performance. Second, recognizing that uncertainty and\nvariation are inherent in perception-based human evaluations, such as those\ninvolving attitudes or preferences, we introduce a new metric - binned\nJensen-Shannon Divergence for perception for such scenarios to better measure\nthe effectiveness of automatic evaluations. We present visualization techniques\n-- perception charts, to contextualize correlation measures appropriately. We\nhave open-sourced at https://github.com/amazon-science/BeyondCorrelation."
                },
                "authors": [
                    {
                        "name": "Aparna Elangovan"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Jongwoo Ko"
                    },
                    {
                        "name": "Mahsa Elyasi"
                    },
                    {
                        "name": "Ling Liu"
                    },
                    {
                        "name": "Sravan Bodapati"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15820v1",
                "updated": "2025-01-27T06:55:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    55,
                    47,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T06:55:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    55,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "FuzzyLight: A Robust Two-Stage Fuzzy Approach for Traffic Signal Control\n  Works in Real Cities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuzzyLight: A Robust Two-Stage Fuzzy Approach for Traffic Signal Control\n  Works in Real Cities"
                },
                "summary": "Effective traffic signal control (TSC) is crucial in mitigating urban\ncongestion and reducing emissions. Recently, reinforcement learning (RL) has\nbeen the research trend for TSC. However, existing RL algorithms face several\nreal-world challenges that hinder their practical deployment in TSC: (1) Sensor\naccuracy deteriorates with increased sensor detection range, and data\ntransmission is prone to noise, potentially resulting in unsafe TSC decisions.\n(2) During the training of online RL, interactions with the environment could\nbe unstable, potentially leading to inappropriate traffic signal phase (TSP)\nselection and traffic congestion. (3) Most current TSC algorithms focus only on\nTSP decisions, overlooking the critical aspect of phase duration, affecting\nsafety and efficiency. To overcome these challenges, we propose a robust\ntwo-stage fuzzy approach called FuzzyLight, which integrates compressed sensing\nand RL for TSC deployment. FuzzyLight offers several key contributions: (1) It\nemploys fuzzy logic and compressed sensing to address sensor noise and enhances\nthe efficiency of TSP decisions. (2) It maintains stable performance during\ntraining and combines fuzzy logic with RL to generate precise phases. (3) It\nworks in real cities across 22 intersections and demonstrates superior\nperformance in both real-world and simulated environments. Experimental results\nindicate that FuzzyLight enhances traffic efficiency by 48% compared to\nexpert-designed timings in the real world. Furthermore, it achieves\nstate-of-the-art (SOTA) performance in simulated environments using six\nreal-world datasets with transmission noise. The code and deployment video are\navailable at the URL1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective traffic signal control (TSC) is crucial in mitigating urban\ncongestion and reducing emissions. Recently, reinforcement learning (RL) has\nbeen the research trend for TSC. However, existing RL algorithms face several\nreal-world challenges that hinder their practical deployment in TSC: (1) Sensor\naccuracy deteriorates with increased sensor detection range, and data\ntransmission is prone to noise, potentially resulting in unsafe TSC decisions.\n(2) During the training of online RL, interactions with the environment could\nbe unstable, potentially leading to inappropriate traffic signal phase (TSP)\nselection and traffic congestion. (3) Most current TSC algorithms focus only on\nTSP decisions, overlooking the critical aspect of phase duration, affecting\nsafety and efficiency. To overcome these challenges, we propose a robust\ntwo-stage fuzzy approach called FuzzyLight, which integrates compressed sensing\nand RL for TSC deployment. FuzzyLight offers several key contributions: (1) It\nemploys fuzzy logic and compressed sensing to address sensor noise and enhances\nthe efficiency of TSP decisions. (2) It maintains stable performance during\ntraining and combines fuzzy logic with RL to generate precise phases. (3) It\nworks in real cities across 22 intersections and demonstrates superior\nperformance in both real-world and simulated environments. Experimental results\nindicate that FuzzyLight enhances traffic efficiency by 48% compared to\nexpert-designed timings in the real world. Furthermore, it achieves\nstate-of-the-art (SOTA) performance in simulated environments using six\nreal-world datasets with transmission noise. The code and deployment video are\navailable at the URL1"
                },
                "authors": [
                    {
                        "name": "Mingyuan Li"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Jun Shen"
                    },
                    {
                        "name": "Qiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Wu"
                },
                "author": "Qiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08705v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08705v4",
                "updated": "2025-01-27T06:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    25,
                    47,
                    0,
                    27,
                    0
                ],
                "published": "2024-06-13T00:04:15Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    0,
                    4,
                    15,
                    3,
                    165,
                    0
                ],
                "title": "When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided\n  Search"
                },
                "summary": "Recent studies developed jailbreaking attacks, which construct jailbreaking\nprompts to fool LLMs into responding to harmful questions. Early-stage\njailbreaking attacks require access to model internals or significant human\nefforts. More advanced attacks utilize genetic algorithms for automatic and\nblack-box attacks. However, the random nature of genetic algorithms\nsignificantly limits the effectiveness of these attacks. In this paper, we\npropose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement\nlearning (DRL). We model jailbreaking as a search problem and design an RL\nagent to guide the search, which is more effective and has less randomness than\nstochastic search, such as genetic algorithms. Specifically, we design a\ncustomized DRL system for the jailbreaking problem, including a novel reward\nfunction and a customized proximal policy optimization (PPO) algorithm. Through\nextensive experiments, we demonstrate that RLbreaker is much more effective\nthan existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. We\nalso show that RLbreaker is robust against three SOTA defenses and its trained\nagents can transfer across different LLMs. We further validate the key design\nchoices of RLbreaker via a comprehensive ablation study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies developed jailbreaking attacks, which construct jailbreaking\nprompts to fool LLMs into responding to harmful questions. Early-stage\njailbreaking attacks require access to model internals or significant human\nefforts. More advanced attacks utilize genetic algorithms for automatic and\nblack-box attacks. However, the random nature of genetic algorithms\nsignificantly limits the effectiveness of these attacks. In this paper, we\npropose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement\nlearning (DRL). We model jailbreaking as a search problem and design an RL\nagent to guide the search, which is more effective and has less randomness than\nstochastic search, such as genetic algorithms. Specifically, we design a\ncustomized DRL system for the jailbreaking problem, including a novel reward\nfunction and a customized proximal policy optimization (PPO) algorithm. Through\nextensive experiments, we demonstrate that RLbreaker is much more effective\nthan existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. We\nalso show that RLbreaker is robust against three SOTA defenses and its trained\nagents can transfer across different LLMs. We further validate the key design\nchoices of RLbreaker via a comprehensive ablation study."
                },
                "authors": [
                    {
                        "name": "Xuan Chen"
                    },
                    {
                        "name": "Yuzhou Nie"
                    },
                    {
                        "name": "Wenbo Guo"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "arxiv_journal_ref": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08705v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08705v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15804v1",
                "updated": "2025-01-27T06:23:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    23,
                    37,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T06:23:37Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    23,
                    37,
                    0,
                    27,
                    0
                ],
                "title": "CodeImprove: Program Adaptation for Deep Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeImprove: Program Adaptation for Deep Code"
                },
                "summary": "Leveraging deep learning (DL)-based code analysis tools to solve software\nengineering tasks is becoming increasingly popular. Code models often suffer\nperformance degradation due to various reasons (e.g., code data shifts).\nRetraining is often required to address these issues, but frequent model\nupdates are costly in labeling and deployment. In this paper, we explore an\nalternative solution: Adapting the program inputs to the code models. This can\nbe achieved by two steps: 1) input validation that focuses on identifying\nwhether an input is an out-of-scope input program that are beyond a model's\nhandling capability, and 2) input adaptation that adapts out-of-scope inputs to\nbecome in-scope inputs. Validating program input is challenging, as current\ntechniques focus on continuous inputs such as image data and fail with discrete\ninputs like code data, which have unique characteristics and are processed\ndifferently by deep learning models. Adapting out-of-scope programs is also\nchallenging due to their vast search spaces. Therefore, in this paper, we\npropose CodeImprove, which distinguishes out-of-scope from normal inputs and\nconverts such out-of-scope inputs back to in-scope inputs through program\ntransformation. In particular, we propose a validity score metric to identify\nout-of-scope inputs and leverage genetic algorithms to apply semantic\npreserving program transformation to convert out-of-scope inputs to in-scope\ninputs. Our experimental results show CodeImprove can enhance up to 8.78% of\naccuracy, and 51.28% of relative improvements in three code models on two SE\ntasks. Additionally, our input validation is promising in detecting\nout-of-scope inputs (AUC score of 0.924).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging deep learning (DL)-based code analysis tools to solve software\nengineering tasks is becoming increasingly popular. Code models often suffer\nperformance degradation due to various reasons (e.g., code data shifts).\nRetraining is often required to address these issues, but frequent model\nupdates are costly in labeling and deployment. In this paper, we explore an\nalternative solution: Adapting the program inputs to the code models. This can\nbe achieved by two steps: 1) input validation that focuses on identifying\nwhether an input is an out-of-scope input program that are beyond a model's\nhandling capability, and 2) input adaptation that adapts out-of-scope inputs to\nbecome in-scope inputs. Validating program input is challenging, as current\ntechniques focus on continuous inputs such as image data and fail with discrete\ninputs like code data, which have unique characteristics and are processed\ndifferently by deep learning models. Adapting out-of-scope programs is also\nchallenging due to their vast search spaces. Therefore, in this paper, we\npropose CodeImprove, which distinguishes out-of-scope from normal inputs and\nconverts such out-of-scope inputs back to in-scope inputs through program\ntransformation. In particular, we propose a validity score metric to identify\nout-of-scope inputs and leverage genetic algorithms to apply semantic\npreserving program transformation to convert out-of-scope inputs to in-scope\ninputs. Our experimental results show CodeImprove can enhance up to 8.78% of\naccuracy, and 51.28% of relative improvements in three code models on two SE\ntasks. Additionally, our input validation is promising in detecting\nout-of-scope inputs (AUC score of 0.924)."
                },
                "authors": [
                    {
                        "name": "Ravishka Rathnasuriya"
                    },
                    {
                        "name": "Zijie Zhao"
                    },
                    {
                        "name": "Wei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang"
                },
                "author": "Wei Yang",
                "arxiv_comment": "In Proceedings of the 47th IEEE/ACM International Conference on\n  Software Engineering (ICSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15797v1",
                "updated": "2025-01-27T05:46:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    46,
                    6,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:46:06Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    46,
                    6,
                    0,
                    27,
                    0
                ],
                "title": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models"
                },
                "summary": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language."
                },
                "authors": [
                    {
                        "name": "Tianbo Yang"
                    },
                    {
                        "name": "Mingqi Yang"
                    },
                    {
                        "name": "Hongyi Zhao"
                    },
                    {
                        "name": "Tianshuo Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tianshuo Yang"
                },
                "author": "Tianshuo Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15791v1",
                "updated": "2025-01-27T05:35:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    35,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:35:25Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    35,
                    25,
                    0,
                    27,
                    0
                ],
                "title": "Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced\n  Error Detection in Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced\n  Error Detection in Knowledge Graphs"
                },
                "summary": "Knowledge graphs are widely used in industrial applications, making error\ndetection crucial for ensuring the reliability of downstream applications.\nExisting error detection methods often fail to effectively leverage\nfine-grained subgraph information and rely solely on fixed graph structures,\nwhile also lacking transparency in their decision-making processes, which\nresults in suboptimal detection performance. In this paper, we propose a novel\nMulti-Agent framework for Knowledge Graph Error Detection (MAKGED) that\nutilizes multiple large language models (LLMs) in a collaborative setting. By\nconcatenating fine-grained, bidirectional subgraph embeddings with LLM-based\nquery embeddings during training, our framework integrates these\nrepresentations to produce four specialized agents. These agents utilize\nsubgraph information from different dimensions to engage in multi-round\ndiscussions, thereby improving error detection accuracy and ensuring a\ntransparent decision-making process. Extensive experiments on FB15K and WN18RR\ndemonstrate that MAKGED outperforms state-of-the-art methods, enhancing the\naccuracy and robustness of KG evaluation. For specific industrial scenarios,\nour framework can facilitate the training of specialized agents using\ndomain-specific knowledge graphs for error detection, which highlights the\npotential industrial application value of our framework. Our code and datasets\nare available at https://github.com/kse-ElEvEn/MAKGED.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs are widely used in industrial applications, making error\ndetection crucial for ensuring the reliability of downstream applications.\nExisting error detection methods often fail to effectively leverage\nfine-grained subgraph information and rely solely on fixed graph structures,\nwhile also lacking transparency in their decision-making processes, which\nresults in suboptimal detection performance. In this paper, we propose a novel\nMulti-Agent framework for Knowledge Graph Error Detection (MAKGED) that\nutilizes multiple large language models (LLMs) in a collaborative setting. By\nconcatenating fine-grained, bidirectional subgraph embeddings with LLM-based\nquery embeddings during training, our framework integrates these\nrepresentations to produce four specialized agents. These agents utilize\nsubgraph information from different dimensions to engage in multi-round\ndiscussions, thereby improving error detection accuracy and ensuring a\ntransparent decision-making process. Extensive experiments on FB15K and WN18RR\ndemonstrate that MAKGED outperforms state-of-the-art methods, enhancing the\naccuracy and robustness of KG evaluation. For specific industrial scenarios,\nour framework can facilitate the training of specialized agents using\ndomain-specific knowledge graphs for error detection, which highlights the\npotential industrial application value of our framework. Our code and datasets\nare available at https://github.com/kse-ElEvEn/MAKGED."
                },
                "authors": [
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Junlan Feng"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Songlin Zhai"
                    },
                    {
                        "name": "Haohan Xue"
                    },
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Ruoyan Shen"
                    },
                    {
                        "name": "Tongtong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongtong Wu"
                },
                "author": "Tongtong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15773v1",
                "updated": "2025-01-27T04:43:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    4,
                    43,
                    18,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T04:43:18Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    4,
                    43,
                    18,
                    0,
                    27,
                    0
                ],
                "title": "Is It Navajo? Accurate Language Detection in Endangered Athabaskan\n  Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is It Navajo? Accurate Language Detection in Endangered Athabaskan\n  Languages"
                },
                "summary": "Endangered languages, such as Navajo - the most widely spoken Native American\nlanguage - are significantly underrepresented in contemporary language\ntechnologies, exacerbating the challenges of their preservation and\nrevitalization. This study evaluates Google's large language model (LLM)-based\nlanguage identification system, which consistently misidentifies Navajo,\nexposing inherent limitations when applied to low-resource Native American\nlanguages. To address this, we introduce a random forest classifier trained on\nNavajo and eight frequently confused languages. Despite its simplicity, the\nclassifier achieves near-perfect accuracy (97-100%), significantly\noutperforming Google's LLM-based system. Additionally, the model demonstrates\nrobustness across other Athabaskan languages - a family of Native American\nlanguages spoken primarily in Alaska, the Pacific Northwest, and parts of the\nSouthwestern United States - suggesting its potential for broader application.\nOur findings underscore the pressing need for NLP systems that prioritize\nlinguistic diversity and adaptability over centralized, one-size-fits-all\nsolutions, especially in supporting underrepresented languages in a\nmulticultural world. This work directly contributes to ongoing efforts to\naddress cultural biases in language models and advocates for the development of\nculturally localized NLP tools that serve diverse linguistic communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endangered languages, such as Navajo - the most widely spoken Native American\nlanguage - are significantly underrepresented in contemporary language\ntechnologies, exacerbating the challenges of their preservation and\nrevitalization. This study evaluates Google's large language model (LLM)-based\nlanguage identification system, which consistently misidentifies Navajo,\nexposing inherent limitations when applied to low-resource Native American\nlanguages. To address this, we introduce a random forest classifier trained on\nNavajo and eight frequently confused languages. Despite its simplicity, the\nclassifier achieves near-perfect accuracy (97-100%), significantly\noutperforming Google's LLM-based system. Additionally, the model demonstrates\nrobustness across other Athabaskan languages - a family of Native American\nlanguages spoken primarily in Alaska, the Pacific Northwest, and parts of the\nSouthwestern United States - suggesting its potential for broader application.\nOur findings underscore the pressing need for NLP systems that prioritize\nlinguistic diversity and adaptability over centralized, one-size-fits-all\nsolutions, especially in supporting underrepresented languages in a\nmulticultural world. This work directly contributes to ongoing efforts to\naddress cultural biases in language models and advocates for the development of\nculturally localized NLP tools that serve diverse linguistic communities."
                },
                "authors": [
                    {
                        "name": "Ivory Yang"
                    },
                    {
                        "name": "Weicheng Ma"
                    },
                    {
                        "name": "Chunhui Zhang"
                    },
                    {
                        "name": "Soroush Vosoughi"
                    }
                ],
                "author_detail": {
                    "name": "Soroush Vosoughi"
                },
                "author": "Soroush Vosoughi",
                "arxiv_comment": "Accepted to NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13879v2",
                "updated": "2025-01-27T04:33:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    4,
                    33,
                    39,
                    0,
                    27,
                    0
                ],
                "published": "2024-12-18T14:19:23Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    19,
                    23,
                    2,
                    353,
                    0
                ],
                "title": "Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under\n  Black-box Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under\n  Black-box Settings"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks. LLMs continue to be vulnerable to external threats, particularly\nDenial-of-Service (DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust\ncomputational resources and block services. However, prior works tend to focus\non performing white-box attacks, overlooking black-box settings. In this work,\nwe propose an automated algorithm designed for black-box LLMs, called\nAuto-Generation for LLM-DoS Attack (AutoDoS). AutoDoS introduces DoS Attack\nTree and optimizes the prompt node coverage to enhance effectiveness under\nblack-box conditions. Our method can bypass existing defense with enhanced\nstealthiness via semantic improvement of prompt nodes. Furthermore, we reveal\nthat implanting Length Trojan in Basic DoS Prompt aids in achieving higher\nattack efficacy. Experimental results show that AutoDoS amplifies service\nresponse latency by over 250 $\\times \\uparrow$, leading to severe resource\nconsumption in terms of GPU utilization and memory usage. Our code is available\nat https://github.com/shuita2333/AutoDoS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks. LLMs continue to be vulnerable to external threats, particularly\nDenial-of-Service (DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust\ncomputational resources and block services. However, prior works tend to focus\non performing white-box attacks, overlooking black-box settings. In this work,\nwe propose an automated algorithm designed for black-box LLMs, called\nAuto-Generation for LLM-DoS Attack (AutoDoS). AutoDoS introduces DoS Attack\nTree and optimizes the prompt node coverage to enhance effectiveness under\nblack-box conditions. Our method can bypass existing defense with enhanced\nstealthiness via semantic improvement of prompt nodes. Furthermore, we reveal\nthat implanting Length Trojan in Basic DoS Prompt aids in achieving higher\nattack efficacy. Experimental results show that AutoDoS amplifies service\nresponse latency by over 250 $\\times \\uparrow$, leading to severe resource\nconsumption in terms of GPU utilization and memory usage. Our code is available\nat https://github.com/shuita2333/AutoDoS."
                },
                "authors": [
                    {
                        "name": "Yuanhe Zhang"
                    },
                    {
                        "name": "Zhenhong Zhou"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Xinyue Wang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Sen Su"
                    }
                ],
                "author_detail": {
                    "name": "Sen Su"
                },
                "author": "Sen Su",
                "arxiv_comment": "20 pages, 7 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02803v2",
                "updated": "2025-01-27T04:30:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    4,
                    30,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2024-02-05T08:25:22Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    8,
                    25,
                    22,
                    0,
                    36,
                    0
                ],
                "title": "Large Language Model Distilling Medication Recommendation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Distilling Medication Recommendation Model"
                },
                "summary": "The recommendation of medication is a vital aspect of intelligent healthcare\nsystems, as it involves prescribing the most suitable drugs based on a\npatient's specific health needs. Unfortunately, many sophisticated models\ncurrently in use tend to overlook the nuanced semantics of medical data, while\nonly relying heavily on identities. Furthermore, these models face significant\nchallenges in handling cases involving patients who are visiting the hospital\nfor the first time, as they lack prior prescription histories to draw upon. To\ntackle these issues, we harness the powerful semantic comprehension and\ninput-agnostic characteristics of Large Language Models (LLMs). Our research\naims to transform existing medication recommendation methodologies using LLMs.\nIn this paper, we introduce a novel approach called Large Language Model\nDistilling Medication Recommendation (LEADER). We begin by creating appropriate\nprompt templates that enable LLMs to suggest medications effectively. However,\nthe straightforward integration of LLMs into recommender systems leads to an\nout-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a\nnovel output layer and a refined tuning loss function. Although LLM-based\nmodels exhibit remarkable capabilities, they are plagued by high computational\ncosts during inference, which is impractical for the healthcare sector. To\nmitigate this, we have developed a feature-level knowledge distillation\ntechnique, which transfers the LLM's proficiency to a more compact model.\nExtensive experiments conducted on two real-world datasets, MIMIC-III and\nMIMIC-IV, demonstrate that our proposed model not only delivers effective\nresults but also is efficient. To ease the reproducibility of our experiments,\nwe release the implementation code online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recommendation of medication is a vital aspect of intelligent healthcare\nsystems, as it involves prescribing the most suitable drugs based on a\npatient's specific health needs. Unfortunately, many sophisticated models\ncurrently in use tend to overlook the nuanced semantics of medical data, while\nonly relying heavily on identities. Furthermore, these models face significant\nchallenges in handling cases involving patients who are visiting the hospital\nfor the first time, as they lack prior prescription histories to draw upon. To\ntackle these issues, we harness the powerful semantic comprehension and\ninput-agnostic characteristics of Large Language Models (LLMs). Our research\naims to transform existing medication recommendation methodologies using LLMs.\nIn this paper, we introduce a novel approach called Large Language Model\nDistilling Medication Recommendation (LEADER). We begin by creating appropriate\nprompt templates that enable LLMs to suggest medications effectively. However,\nthe straightforward integration of LLMs into recommender systems leads to an\nout-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a\nnovel output layer and a refined tuning loss function. Although LLM-based\nmodels exhibit remarkable capabilities, they are plagued by high computational\ncosts during inference, which is impractical for the healthcare sector. To\nmitigate this, we have developed a feature-level knowledge distillation\ntechnique, which transfers the LLM's proficiency to a more compact model.\nExtensive experiments conducted on two real-world datasets, MIMIC-III and\nMIMIC-IV, demonstrate that our proposed model not only delivers effective\nresults but also is efficient. To ease the reproducibility of our experiments,\nwe release the implementation code online."
                },
                "authors": [
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yuanshao Zhu"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Yefeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yefeng Zheng"
                },
                "author": "Yefeng Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15763v1",
                "updated": "2025-01-27T04:16:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    4,
                    16,
                    42,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T04:16:42Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    4,
                    16,
                    42,
                    0,
                    27,
                    0
                ],
                "title": "NanoHTNet: Nano Human Topology Network for Efficient 3D Human Pose\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoHTNet: Nano Human Topology Network for Efficient 3D Human Pose\n  Estimation"
                },
                "summary": "The widespread application of 3D human pose estimation (HPE) is limited by\nresource-constrained edge devices, requiring more efficient models. A key\napproach to enhancing efficiency involves designing networks based on the\nstructural characteristics of input data. However, effectively utilizing the\nstructural priors in human skeletal inputs remains challenging. To address\nthis, we leverage both explicit and implicit spatio-temporal priors of the\nhuman body through innovative model design and a pre-training proxy task.\nFirst, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE\nnetwork with stacked Hierarchical Mixers to capture explicit features.\nSpecifically, the spatial Hierarchical Mixer efficiently learns the human\nphysical topology across multiple semantic levels, while the temporal\nHierarchical Mixer with discrete cosine transform and low-pass filtering\ncaptures local instantaneous movements and global action coherence. Moreover,\nEfficient Temporal-Spatial Tokenization (ETST) is introduced to enhance\nspatio-temporal interaction and reduce computational complexity significantly.\nSecond, PoseCLR is proposed as a general pre-training method based on\ncontrastive learning for 3D HPE, aimed at extracting implicit representations\nof human topology. By aligning 2D poses from diverse viewpoints in the proxy\ntask, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing\nthe high-dimensional features of the human body, leading to further performance\nimprovements. Extensive experiments verify that NanoHTNet with PoseCLR\noutperforms other state-of-the-art methods in efficiency, making it ideal for\ndeployment on edge devices like the Jetson Nano. Code and models are available\nat https://github.com/vefalun/NanoHTNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread application of 3D human pose estimation (HPE) is limited by\nresource-constrained edge devices, requiring more efficient models. A key\napproach to enhancing efficiency involves designing networks based on the\nstructural characteristics of input data. However, effectively utilizing the\nstructural priors in human skeletal inputs remains challenging. To address\nthis, we leverage both explicit and implicit spatio-temporal priors of the\nhuman body through innovative model design and a pre-training proxy task.\nFirst, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE\nnetwork with stacked Hierarchical Mixers to capture explicit features.\nSpecifically, the spatial Hierarchical Mixer efficiently learns the human\nphysical topology across multiple semantic levels, while the temporal\nHierarchical Mixer with discrete cosine transform and low-pass filtering\ncaptures local instantaneous movements and global action coherence. Moreover,\nEfficient Temporal-Spatial Tokenization (ETST) is introduced to enhance\nspatio-temporal interaction and reduce computational complexity significantly.\nSecond, PoseCLR is proposed as a general pre-training method based on\ncontrastive learning for 3D HPE, aimed at extracting implicit representations\nof human topology. By aligning 2D poses from diverse viewpoints in the proxy\ntask, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing\nthe high-dimensional features of the human body, leading to further performance\nimprovements. Extensive experiments verify that NanoHTNet with PoseCLR\noutperforms other state-of-the-art methods in efficiency, making it ideal for\ndeployment on edge devices like the Jetson Nano. Code and models are available\nat https://github.com/vefalun/NanoHTNet."
                },
                "authors": [
                    {
                        "name": "Jialun Cai"
                    },
                    {
                        "name": "Mengyuan Liu"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Shuheng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuheng Zhou"
                },
                "author": "Shuheng Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05694v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05694v3",
                "updated": "2025-01-27T03:50:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    3,
                    50,
                    31,
                    0,
                    27,
                    0
                ],
                "published": "2023-10-09T13:15:23Z",
                "published_parsed": [
                    2023,
                    10,
                    9,
                    13,
                    15,
                    23,
                    0,
                    282,
                    0
                ],
                "title": "A Survey of Large Language Models for Healthcare: from Data, Technology,\n  and Applications to Accountability and Ethics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models for Healthcare: from Data, Technology,\n  and Applications to Accountability and Ethics"
                },
                "summary": "The utilization of large language models (LLMs) in the Healthcare domain has\ngenerated both excitement and concern due to their ability to effectively\nrespond to freetext queries with certain professional knowledge. This survey\noutlines the capabilities of the currently developed LLMs for Healthcare and\nexplicates their development process, with the aim of providing an overview of\nthe development roadmap from traditional Pretrained Language Models (PLMs) to\nLLMs. Specifically, we first explore the potential of LLMs to enhance the\nefficiency and effectiveness of various Healthcare applications highlighting\nboth the strengths and limitations. Secondly, we conduct a comparison between\nthe previous PLMs and the latest LLMs, as well as comparing various LLMs with\neach other. Then we summarize related Healthcare training data, training\nmethods, optimization strategies, and usage. Finally, the unique concerns\nassociated with deploying LLMs in Healthcare settings are investigated,\nparticularly regarding fairness, accountability, transparency and ethics. Our\nsurvey provide a comprehensive investigation from perspectives of both computer\nscience and Healthcare specialty. Besides the discussion about Healthcare\nconcerns, we supports the computer science community by compiling a collection\nof open source resources, such as accessible datasets, the latest\nmethodologies, code implementations, and evaluation benchmarks in the Github.\nSummarily, we contend that a significant paradigm shift is underway,\ntransitioning from PLMs to LLMs. This shift encompasses a move from\ndiscriminative AI approaches to generative AI approaches, as well as a shift\nfrom model-centered methodologies to data-centered methodologies. Also, we\ndetermine that the biggest obstacle of using LLMs in Healthcare are fairness,\naccountability, transparency and ethics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The utilization of large language models (LLMs) in the Healthcare domain has\ngenerated both excitement and concern due to their ability to effectively\nrespond to freetext queries with certain professional knowledge. This survey\noutlines the capabilities of the currently developed LLMs for Healthcare and\nexplicates their development process, with the aim of providing an overview of\nthe development roadmap from traditional Pretrained Language Models (PLMs) to\nLLMs. Specifically, we first explore the potential of LLMs to enhance the\nefficiency and effectiveness of various Healthcare applications highlighting\nboth the strengths and limitations. Secondly, we conduct a comparison between\nthe previous PLMs and the latest LLMs, as well as comparing various LLMs with\neach other. Then we summarize related Healthcare training data, training\nmethods, optimization strategies, and usage. Finally, the unique concerns\nassociated with deploying LLMs in Healthcare settings are investigated,\nparticularly regarding fairness, accountability, transparency and ethics. Our\nsurvey provide a comprehensive investigation from perspectives of both computer\nscience and Healthcare specialty. Besides the discussion about Healthcare\nconcerns, we supports the computer science community by compiling a collection\nof open source resources, such as accessible datasets, the latest\nmethodologies, code implementations, and evaluation benchmarks in the Github.\nSummarily, we contend that a significant paradigm shift is underway,\ntransitioning from PLMs to LLMs. This shift encompasses a move from\ndiscriminative AI approaches to generative AI approaches, as well as a shift\nfrom model-centered methodologies to data-centered methodologies. Also, we\ndetermine that the biggest obstacle of using LLMs in Healthcare are fairness,\naccountability, transparency and ethics."
                },
                "authors": [
                    {
                        "name": "Kai He"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Yucheng Ruan"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Mengling Feng"
                    },
                    {
                        "name": "Erik Cambria"
                    }
                ],
                "author_detail": {
                    "name": "Erik Cambria"
                },
                "author": "Erik Cambria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05694v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05694v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15755v1",
                "updated": "2025-01-27T03:50:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    3,
                    50,
                    30,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T03:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    3,
                    50,
                    30,
                    0,
                    27,
                    0
                ],
                "title": "GraphICL: Unlocking Graph Learning Potential in LLMs through Structured\n  Prompt Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphICL: Unlocking Graph Learning Potential in LLMs through Structured\n  Prompt Design"
                },
                "summary": "The growing importance of textual and relational systems has driven interest\nin enhancing large language models (LLMs) for graph-structured data,\nparticularly Text-Attributed Graphs (TAGs), where samples are represented by\ntextual descriptions interconnected by edges. While research has largely\nfocused on developing specialized graph LLMs through task-specific instruction\ntuning, a comprehensive benchmark for evaluating LLMs solely through prompt\ndesign remains surprisingly absent. Without such a carefully crafted evaluation\nbenchmark, most if not all, tailored graph LLMs are compared against general\nLLMs using simplistic queries (e.g., zero-shot reasoning with LLaMA), which can\npotentially camouflage many advantages as well as unexpected predicaments of\nthem. To achieve more general evaluations and unveil the true potential of LLMs\nfor graph tasks, we introduce Graph In-context Learning (GraphICL) Benchmark, a\ncomprehensive benchmark comprising novel prompt templates designed to capture\ngraph structure and handle limited label knowledge. Our systematic evaluation\nshows that general-purpose LLMs equipped with our GraphICL outperform\nstate-of-the-art specialized graph LLMs and graph neural network models in\nresource-constrained settings and out-of-domain tasks. These findings highlight\nthe significant potential of prompt engineering to enhance LLM performance on\ngraph learning tasks without training and offer a strong baseline for advancing\nresearch in graph LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing importance of textual and relational systems has driven interest\nin enhancing large language models (LLMs) for graph-structured data,\nparticularly Text-Attributed Graphs (TAGs), where samples are represented by\ntextual descriptions interconnected by edges. While research has largely\nfocused on developing specialized graph LLMs through task-specific instruction\ntuning, a comprehensive benchmark for evaluating LLMs solely through prompt\ndesign remains surprisingly absent. Without such a carefully crafted evaluation\nbenchmark, most if not all, tailored graph LLMs are compared against general\nLLMs using simplistic queries (e.g., zero-shot reasoning with LLaMA), which can\npotentially camouflage many advantages as well as unexpected predicaments of\nthem. To achieve more general evaluations and unveil the true potential of LLMs\nfor graph tasks, we introduce Graph In-context Learning (GraphICL) Benchmark, a\ncomprehensive benchmark comprising novel prompt templates designed to capture\ngraph structure and handle limited label knowledge. Our systematic evaluation\nshows that general-purpose LLMs equipped with our GraphICL outperform\nstate-of-the-art specialized graph LLMs and graph neural network models in\nresource-constrained settings and out-of-domain tasks. These findings highlight\nthe significant potential of prompt engineering to enhance LLM performance on\ngraph learning tasks without training and offer a strong baseline for advancing\nresearch in graph LLMs."
                },
                "authors": [
                    {
                        "name": "Yuanfu Sun"
                    },
                    {
                        "name": "Zhengnan Ma"
                    },
                    {
                        "name": "Yi Fang"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Qiaoyu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoyu Tan"
                },
                "author": "Qiaoyu Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15749v1",
                "updated": "2025-01-27T03:29:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    3,
                    29,
                    44,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T03:29:44Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    3,
                    29,
                    44,
                    0,
                    27,
                    0
                ],
                "title": "LLM-powered Multi-agent Framework for Goal-oriented Learning in\n  Intelligent Tutoring System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered Multi-agent Framework for Goal-oriented Learning in\n  Intelligent Tutoring System"
                },
                "summary": "Intelligent Tutoring Systems (ITSs) have revolutionized education by offering\npersonalized learning experiences. However, as goal-oriented learning, which\nemphasizes efficiently achieving specific objectives, becomes increasingly\nimportant in professional contexts, existing ITSs often struggle to deliver\nthis type of targeted learning experience. In this paper, we propose GenMentor,\nan LLM-powered multi-agent framework designed to deliver goal-oriented,\npersonalized learning within ITS. GenMentor begins by accurately mapping\nlearners' goals to required skills using a fine-tuned LLM trained on a custom\ngoal-to-skill dataset. After identifying the skill gap, it schedules an\nefficient learning path using an evolving optimization approach, driven by a\ncomprehensive and dynamic profile of learners' multifaceted status.\nAdditionally, GenMentor tailors learning content with an\nexploration-drafting-integration mechanism to align with individual learner\nneeds. Extensive automated and human evaluations demonstrate GenMentor's\neffectiveness in learning guidance and content quality. Furthermore, we have\ndeployed it in practice and also implemented it as an application. Practical\nhuman study with professional learners further highlights its effectiveness in\ngoal alignment and resource targeting, leading to enhanced personalization.\nSupplementary resources are available at\nhttps://github.com/GeminiLight/gen-mentor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Tutoring Systems (ITSs) have revolutionized education by offering\npersonalized learning experiences. However, as goal-oriented learning, which\nemphasizes efficiently achieving specific objectives, becomes increasingly\nimportant in professional contexts, existing ITSs often struggle to deliver\nthis type of targeted learning experience. In this paper, we propose GenMentor,\nan LLM-powered multi-agent framework designed to deliver goal-oriented,\npersonalized learning within ITS. GenMentor begins by accurately mapping\nlearners' goals to required skills using a fine-tuned LLM trained on a custom\ngoal-to-skill dataset. After identifying the skill gap, it schedules an\nefficient learning path using an evolving optimization approach, driven by a\ncomprehensive and dynamic profile of learners' multifaceted status.\nAdditionally, GenMentor tailors learning content with an\nexploration-drafting-integration mechanism to align with individual learner\nneeds. Extensive automated and human evaluations demonstrate GenMentor's\neffectiveness in learning guidance and content quality. Furthermore, we have\ndeployed it in practice and also implemented it as an application. Practical\nhuman study with professional learners further highlights its effectiveness in\ngoal alignment and resource targeting, leading to enhanced personalization.\nSupplementary resources are available at\nhttps://github.com/GeminiLight/gen-mentor."
                },
                "authors": [
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Yi Zhan"
                    },
                    {
                        "name": "Jianxun Lian"
                    },
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Nicholas Jing Yuan"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by WWW 2025 (Industry Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14417v2",
                "updated": "2025-01-27T03:28:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    3,
                    28,
                    11,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-24T11:34:13Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    34,
                    13,
                    4,
                    24,
                    0
                ],
                "title": "DeepFlow: Serverless Large Language Model Serving at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepFlow: Serverless Large Language Model Serving at Scale"
                },
                "summary": "This paper introduces DeepFlow, a scalable and serverless AI platform\ndesigned to efficiently serve large language models (LLMs) at scale in cloud\nenvironments. DeepFlow addresses key challenges such as resource allocation,\nserving efficiency, and cold start latencies through four main design\ncomponents. First, it uses a simple serverless abstraction called the\nrequest-job-task model, which helps manage AI workloads across post-training\nand model serving tasks. Second, it builds an in-house serving engine FlowServe\nusing a microkernel-inspired design, NPU-centric execution, and SPMD-based\nparallelism to optimize LLM serving. The system also includes novel scheduling\npolicies tailored for both PD-disaggregated and PD-colocated configurations.\nWith optimizations like pre-warmed pods, DRAM pre-loading, and NPU-fork,\nDeepFlow can scale up to 64 instances in seconds. DeepFlow has been in\nproduction for over a year, operating on a large Ascend NPU cluster and\nproviding industrystandard APIs for fine-tuning, agent serving, and model\nserving to our customers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces DeepFlow, a scalable and serverless AI platform\ndesigned to efficiently serve large language models (LLMs) at scale in cloud\nenvironments. DeepFlow addresses key challenges such as resource allocation,\nserving efficiency, and cold start latencies through four main design\ncomponents. First, it uses a simple serverless abstraction called the\nrequest-job-task model, which helps manage AI workloads across post-training\nand model serving tasks. Second, it builds an in-house serving engine FlowServe\nusing a microkernel-inspired design, NPU-centric execution, and SPMD-based\nparallelism to optimize LLM serving. The system also includes novel scheduling\npolicies tailored for both PD-disaggregated and PD-colocated configurations.\nWith optimizations like pre-warmed pods, DRAM pre-loading, and NPU-fork,\nDeepFlow can scale up to 64 instances in seconds. DeepFlow has been in\nproduction for over a year, operating on a large Ascend NPU cluster and\nproviding industrystandard APIs for fine-tuning, agent serving, and model\nserving to our customers."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Yulong He"
                    },
                    {
                        "name": "Yuetao Chen"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Baoquan Zhang"
                    },
                    {
                        "name": "Shining Wan"
                    },
                    {
                        "name": "Gengyuan Dan"
                    },
                    {
                        "name": "Zhiyu Dong"
                    },
                    {
                        "name": "Zhihao Ren"
                    },
                    {
                        "name": "Jie Meng"
                    },
                    {
                        "name": "Chao He"
                    },
                    {
                        "name": "Changhong Liu"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Dayun Lin"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]