[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.03215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03215v1",
                "updated": "2025-10-03T17:52:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models"
                },
                "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Zihan Min"
                    },
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Jichao Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03198v1",
                "updated": "2025-10-03T17:35:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:35:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft"
                },
                "summary": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences."
                },
                "authors": [
                    {
                        "name": "Junchao Huang"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Boyao Han"
                    },
                    {
                        "name": "Shaoshuai Shi"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Li Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Li Jiang"
                },
                "author": "Li Jiang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v2",
                "updated": "2025-10-03T15:37:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    37,
                    19,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures; Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02866v1",
                "updated": "2025-10-03T10:06:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T10:06:44Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "title": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling"
                },
                "summary": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject."
                },
                "authors": [
                    {
                        "name": "Bassel Diban"
                    },
                    {
                        "name": "Giovanni Mazzanti"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Mazzanti"
                },
                "author": "Giovanni Mazzanti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02758v1",
                "updated": "2025-10-03T06:43:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:43:24Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "title": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling"
                },
                "summary": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Chuheng Du"
                    },
                    {
                        "name": "Renyuan Liu"
                    },
                    {
                        "name": "Shuochao Yao"
                    },
                    {
                        "name": "Dingtian Yan"
                    },
                    {
                        "name": "Jiang Liao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Accepted by EuroSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02750v1",
                "updated": "2025-10-03T06:27:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:27:33Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "title": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models"
                },
                "summary": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks."
                },
                "authors": [
                    {
                        "name": "Lihua Zhou"
                    },
                    {
                        "name": "Mao Ye"
                    },
                    {
                        "name": "Shuaifeng Li"
                    },
                    {
                        "name": "Nianxin Li"
                    },
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Lei Deng"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02084v2",
                "updated": "2025-10-03T05:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    5,
                    10,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T14:50:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    50,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting"
                },
                "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries."
                },
                "authors": [
                    {
                        "name": "Kuiye Ding"
                    },
                    {
                        "name": "Fanda Fan"
                    },
                    {
                        "name": "Zheya Wang"
                    },
                    {
                        "name": "Hongxiao Li"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Chunjie Luo"
                    },
                    {
                        "name": "Jianfeng Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Zhan"
                },
                "author": "Jianfeng Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25188v2",
                "updated": "2025-10-03T00:40:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    0,
                    40,
                    49,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-29T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    54,
                    0,
                    272,
                    0
                ],
                "title": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache."
                },
                "authors": [
                    {
                        "name": "Wenrui Bao"
                    },
                    {
                        "name": "Zhiben Chen"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02613v1",
                "updated": "2025-10-02T23:16:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    23,
                    16,
                    35,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T23:16:35Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    23,
                    16,
                    35,
                    3,
                    275,
                    0
                ],
                "title": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts\n  Models"
                },
                "summary": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Haley Li"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qintao Zhang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "arxiv_affiliation": "Huawei Technologies Canada",
                "author": "Zhenan Fan",
                "arxiv_comment": "19 pages, 15 figures, Under Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v5",
                "updated": "2025-10-02T19:25:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    19,
                    25,
                    29,
                    3,
                    275,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence after the aLoRA is invoked. This change crucially allows\naLoRA to accept the base model's KV cache of the input string, meaning that\naLoRA can be instantly activated whenever needed in a chain without recomputing\nthe prior keys and values. This enables building what we call intrinsics, i.e.\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\ntrain a set of aLoRA-based intrinsics models, demonstrating competitive\naccuracy with standard LoRA while significantly improving inference efficiency.\nWe contributed our Activated LoRA implementation to the Huggingface PEFT\nlibrary https://github.com/huggingface/peft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence after the aLoRA is invoked. This change crucially allows\naLoRA to accept the base model's KV cache of the input string, meaning that\naLoRA can be instantly activated whenever needed in a chain without recomputing\nthe prior keys and values. This enables building what we call intrinsics, i.e.\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\ntrain a set of aLoRA-based intrinsics models, demonstrating competitive\naccuracy with standard LoRA while significantly improving inference efficiency.\nWe contributed our Activated LoRA implementation to the Huggingface PEFT\nlibrary https://github.com/huggingface/peft."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v4",
                "updated": "2025-10-02T19:09:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    19,
                    9,
                    19,
                    3,
                    275,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v2",
                "updated": "2025-10-02T18:38:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    18,
                    38,
                    0,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "arxiv_comment": "project page: https://soroush-mim.github.io/projects/evict3r/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v3",
                "updated": "2025-10-02T18:20:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    18,
                    20,
                    18,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02312v1",
                "updated": "2025-10-02T17:59:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation"
                },
                "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference."
                },
                "authors": [
                    {
                        "name": "Anna Kuzina"
                    },
                    {
                        "name": "Maciej Pioro"
                    },
                    {
                        "name": "Paul N. Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v2",
                "updated": "2025-10-02T14:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    42,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "Project Page: https://bujiazi.github.io/dicache.github.io/ Code:\n  https://github.com/Bujiazi/DiCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v3",
                "updated": "2025-10-02T14:09:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    9,
                    3,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01884v1",
                "updated": "2025-10-02T10:49:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T10:49:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "Study of the $^{20}$Ne($p,$)$^{21}$Na reaction at LUNA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the $^{20}$Ne($p,$)$^{21}$Na reaction at LUNA"
                },
                "summary": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate."
                },
                "authors": [
                    {
                        "name": "A. Caciolli"
                    }
                ],
                "author_detail": {
                    "name": "A. Caciolli"
                },
                "arxiv_affiliation": "on behalf of the LUNA collaboration",
                "author": "A. Caciolli",
                "arxiv_doi": "10.1051/epjconf/202429207005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/epjconf/202429207005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "EPJ Web Conf., 292 (2024) 07005",
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20211v2",
                "updated": "2025-10-02T04:11:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    4,
                    11,
                    7,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-26T16:52:40Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    52,
                    40,
                    0,
                    146,
                    0
                ],
                "title": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection"
                },
                "summary": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness."
                },
                "authors": [
                    {
                        "name": "Junseo Hwang"
                    },
                    {
                        "name": "Wonguk Cho"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07447v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07447v4",
                "updated": "2025-10-01T20:30:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    20,
                    30,
                    18,
                    2,
                    274,
                    0
                ],
                "published": "2024-11-12T00:10:34Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    0,
                    10,
                    34,
                    1,
                    317,
                    0
                ],
                "title": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies"
                },
                "summary": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts."
                },
                "authors": [
                    {
                        "name": "Kyoungmin Kim"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Kijae Hong"
                    },
                    {
                        "name": "Anastasia Ailamaki"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Ailamaki"
                },
                "author": "Anastasia Ailamaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07447v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07447v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v2",
                "updated": "2025-10-01T19:06:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    19,
                    6,
                    10,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v2",
                "updated": "2025-10-01T18:55:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    55,
                    20,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01336v1",
                "updated": "2025-10-01T18:04:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T18:04:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiSpec: Hierarchical Speculative Decoding for LLMs"
                },
                "summary": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Sujay Sanghavi"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00948v1",
                "updated": "2025-10-01T14:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T14:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution"
                },
                "summary": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR."
                },
                "authors": [
                    {
                        "name": "Ziqing Zhang"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Xi Li"
                    },
                    {
                        "name": "Yucong Chen"
                    },
                    {
                        "name": "Bingnan Duan"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "arxiv_comment": "Code will be available at https://github.com/Kai-Liu001/InfVSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v2",
                "updated": "2025-10-01T11:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    26,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00636v1",
                "updated": "2025-10-01T08:12:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T08:12:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution"
                },
                "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Maximilian Jeblick"
                    },
                    {
                        "name": "Simon Jgou"
                    }
                ],
                "author_detail": {
                    "name": "Simon Jgou"
                },
                "author": "Simon Jgou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v1",
                "updated": "2025-10-01T06:38:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Panagiotis Karras"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Jignesh M. Patel"
                },
                "author": "Jignesh M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00536v1",
                "updated": "2025-10-01T05:37:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T05:37:54Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness"
                },
                "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance."
                },
                "authors": [
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Haoyi Qiu"
                    },
                    {
                        "name": "Yutong Dai"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25454v2",
                "updated": "2025-10-01T05:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    9,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T20:00:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    0,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search"
                },
                "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation."
                },
                "authors": [
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Aaron Tu"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01290v1",
                "updated": "2025-10-01T04:09:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T04:09:02Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models"
                },
                "summary": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Akshat Ramachandran"
                    },
                    {
                        "name": "Marina Neseem"
                    },
                    {
                        "name": "Charbel Sakr"
                    },
                    {
                        "name": "Rangharajan Venkatesan"
                    },
                    {
                        "name": "Brucek Khailany"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01289v1",
                "updated": "2025-10-01T02:56:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T02:56:59Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "title": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field"
                },
                "summary": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%."
                },
                "authors": [
                    {
                        "name": "Osama A. Marzouk"
                    }
                ],
                "author_detail": {
                    "name": "Osama A. Marzouk"
                },
                "author": "Osama A. Marzouk",
                "arxiv_doi": "10.37256/cm.6420256918",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.37256/cm.6420256918",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "41 pages, 8 figures, 4 tables, published journal article,\n  peer-reviewed, open access",
                "arxiv_journal_ref": "Contemporary Mathematics. volume 6, issue 4, pages 4060-4100,\n  https://ojs.wiserpub.com/index.php/CM/article/view/6918 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00A79, 03H10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02388v1",
                "updated": "2025-09-30T22:19:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T22:19:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "title": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance on general\nQuestion Answering (QA), yet they often struggle in domain-specific scenarios\nwhere accurate and up-to-date information is required. Retrieval-Augmented\nGeneration (RAG) addresses this limitation by enriching LLMs with external\nknowledge, but existing systems primarily rely on unstructured documents, while\nlargely overlooking relational databases, which provide precise, timely, and\nefficiently queryable factual information, serving as indispensable\ninfrastructure in domains such as finance, healthcare, and scientific research.\nMotivated by this gap, we conduct a systematic analysis that reveals three\ncentral observations: (i) databases and documents offer complementary strengths\nacross queries, (ii) naively combining both sources introduces noise and cost\nwithout consistent accuracy gains, and (iii) selecting the most suitable source\nfor each query is crucial to balance effectiveness and efficiency. We further\nobserve that query types show consistent regularities in their alignment with\nretrieval paths, suggesting that routing decisions can be effectively guided by\nsystematic rules that capture these patterns. Building on these insights, we\npropose a rule-driven routing framework. A routing agent scores candidate\naugmentation paths based on explicit rules and selects the most suitable one; a\nrule-making expert agent refines the rules over time using QA feedback to\nmaintain adaptability; and a path-level meta-cache reuses past routing\ndecisions for semantically similar queries to reduce latency and cost.\nExperiments on three QA benchmarks demonstrate that our framework consistently\noutperforms static strategies and learned routing baselines, achieving higher\naccuracy while maintaining moderate computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance on general\nQuestion Answering (QA), yet they often struggle in domain-specific scenarios\nwhere accurate and up-to-date information is required. Retrieval-Augmented\nGeneration (RAG) addresses this limitation by enriching LLMs with external\nknowledge, but existing systems primarily rely on unstructured documents, while\nlargely overlooking relational databases, which provide precise, timely, and\nefficiently queryable factual information, serving as indispensable\ninfrastructure in domains such as finance, healthcare, and scientific research.\nMotivated by this gap, we conduct a systematic analysis that reveals three\ncentral observations: (i) databases and documents offer complementary strengths\nacross queries, (ii) naively combining both sources introduces noise and cost\nwithout consistent accuracy gains, and (iii) selecting the most suitable source\nfor each query is crucial to balance effectiveness and efficiency. We further\nobserve that query types show consistent regularities in their alignment with\nretrieval paths, suggesting that routing decisions can be effectively guided by\nsystematic rules that capture these patterns. Building on these insights, we\npropose a rule-driven routing framework. A routing agent scores candidate\naugmentation paths based on explicit rules and selects the most suitable one; a\nrule-making expert agent refines the rules over time using QA feedback to\nmaintain adaptability; and a path-level meta-cache reuses past routing\ndecisions for semantically similar queries to reduce latency and cost.\nExperiments on three QA benchmarks demonstrate that our framework consistently\noutperforms static strategies and learned routing baselines, achieving higher\naccuracy while maintaining moderate computational cost."
                },
                "authors": [
                    {
                        "name": "Haoyue Bai"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Shengyu Chen"
                    },
                    {
                        "name": "Zhengzhang Chen"
                    },
                    {
                        "name": "Lu-An Tang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00294v1",
                "updated": "2025-09-30T21:28:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T21:28:04Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models"
                },
                "summary": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Shutong Wu"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00231v1",
                "updated": "2025-09-30T19:55:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:55:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "The Pitfalls of KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pitfalls of KV Cache Compression"
                },
                "summary": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks."
                },
                "authors": [
                    {
                        "name": "Alex Chen"
                    },
                    {
                        "name": "Renato Geh"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Daniel Israel"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Israel"
                },
                "author": "Daniel Israel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00184v1",
                "updated": "2025-09-30T19:03:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:03:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls"
                },
                "summary": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Bai"
                    },
                    {
                        "name": "Itamar Pres"
                    },
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Chenhao Tan"
                    },
                    {
                        "name": "Stuart Shieber"
                    },
                    {
                        "name": "Fernanda Vigas"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Andrew Lee"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lee"
                },
                "author": "Andrew Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v1",
                "updated": "2025-09-30T17:15:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v2",
                "updated": "2025-09-30T16:42:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    42,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v3",
                "updated": "2025-09-30T15:44:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    44,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando Garca-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_doi": "10.1109/TED.2025.3617043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TED.2025.3617043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IEEE Trans. Elec. Dev. Work enabled in part by NanoIC\n  pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26328v1",
                "updated": "2025-09-30T14:40:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:40:18Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM v2: Efficient Block-Diffusion LLM"
                },
                "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v4",
                "updated": "2025-09-30T14:13:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    13,
                    20,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code at https://github.com/NVlabs/Long-RL\n  and model at https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v2",
                "updated": "2025-09-30T09:10:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    9,
                    10,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi",
                "arxiv_comment": "Accepted by ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v2",
                "updated": "2025-09-30T02:51:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    51,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "NeurIPS 2025 Oral. Code: https://github.com/snu-mllab/KVzip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25681v1",
                "updated": "2025-09-30T02:36:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T02:36:11Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought"
                },
                "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Yi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Xu"
                },
                "author": "Yi Xu",
                "arxiv_comment": "technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25401v1",
                "updated": "2025-09-29T18:57:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T18:57:14Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers"
                },
                "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality."
                },
                "authors": [
                    {
                        "name": "Liang Qiao"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Hongyu Kan"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25155v1",
                "updated": "2025-09-29T17:55:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:55:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "title": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units"
                },
                "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts."
                },
                "authors": [
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IEEE HiPC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v2",
                "updated": "2025-09-29T15:20:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    20,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "EMNLP 2025; 15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24791v1",
                "updated": "2025-09-29T13:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:45:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "Vision Function Layer in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Function Layer in Multimodal LLMs"
                },
                "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models."
                },
                "authors": [
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang",
                "arxiv_comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v3",
                "updated": "2025-09-29T12:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    34,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v1",
                "updated": "2025-09-29T12:28:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24626v1",
                "updated": "2025-09-29T11:35:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T11:35:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving"
                },
                "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24407v1",
                "updated": "2025-09-29T07:54:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T07:54:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network"
                },
                "summary": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter."
                },
                "authors": [
                    {
                        "name": "Karl C. Linne"
                    },
                    {
                        "name": "Yuanyuan Li"
                    },
                    {
                        "name": "Debashri Roy"
                    },
                    {
                        "name": "Kaushik Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Chowdhury"
                },
                "arxiv_affiliation": "Kai Li",
                "author": "Kaushik Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v2",
                "updated": "2025-09-29T05:12:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    5,
                    12,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v2",
                "updated": "2025-09-29T02:46:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    2,
                    46,
                    45,
                    0,
                    272,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24178v1",
                "updated": "2025-09-29T01:52:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T01:52:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring"
                },
                "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhou"
                    },
                    {
                        "name": "Steve Majerus"
                    },
                    {
                        "name": "Gourav Datta"
                    }
                ],
                "author_detail": {
                    "name": "Gourav Datta"
                },
                "author": "Gourav Datta",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24088v1",
                "updated": "2025-09-28T21:47:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T21:47:20Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Moyan Li"
                    },
                    {
                        "name": "Shaoyuan Xu"
                    },
                    {
                        "name": "Jinmiao Fu"
                    },
                    {
                        "name": "Xinhai Hou"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24007v1",
                "updated": "2025-09-28T17:59:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T17:59:15Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "title": "Sequential Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Diffusion Language Models"
                },
                "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM"
                },
                "authors": [
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Yanting Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "14 pages, 5 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23928v1",
                "updated": "2025-09-28T15:05:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T15:05:21Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models"
                },
                "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference."
                },
                "authors": [
                    {
                        "name": "Zhinan Xie"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02361v1",
                "updated": "2025-09-28T11:04:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    11,
                    4,
                    0,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T11:04:00Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    11,
                    4,
                    0,
                    6,
                    271,
                    0
                ],
                "title": "ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs\n  Inference"
                },
                "summary": "Transformer-based large models excel in natural language processing and\ncomputer vision, but face severe computational inefficiencies due to the\nself-attention's quadratic complexity with input tokens. Recently, researchers\nhave proposed a series of methods based on block selection and compression to\nalleviate this problem, but they either have issues with semantic\nincompleteness or poor training-inference efficiency. To comprehensively\naddress these challenges, we propose ChunkLLM, a lightweight and pluggable\ntraining framework. Specifically, we introduce two components: QK Adapter\n(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each\nTransformer layer, serving dual purposes of feature compression and chunk\nattention acquisition. The latter operates at the bottommost layer of the\nmodel, functioning to detect chunk boundaries by leveraging contextual semantic\ninformation. During the training phase, the parameters of the backbone remain\nfrozen, with only the QK Adapter and Chunk Adapter undergoing training.\nNotably, we design an attention distillation method for training the QK\nAdapter, which enhances the recall rate of key chunks. During the inference\nphase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference.\nExperimental evaluations are conducted on a diverse set of long-text and\nshort-text benchmark datasets spanning multiple tasks. ChunkLLM not only\nattains comparable performance on short-text benchmarks but also maintains\n98.64% of the performance on long-context benchmarks while preserving a 48.58%\nkey-value cache retention rate. Particularly, ChunkLLM attains a maximum\nspeedup of 4.48x in comparison to the vanilla Transformer in the processing of\n120K long texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large models excel in natural language processing and\ncomputer vision, but face severe computational inefficiencies due to the\nself-attention's quadratic complexity with input tokens. Recently, researchers\nhave proposed a series of methods based on block selection and compression to\nalleviate this problem, but they either have issues with semantic\nincompleteness or poor training-inference efficiency. To comprehensively\naddress these challenges, we propose ChunkLLM, a lightweight and pluggable\ntraining framework. Specifically, we introduce two components: QK Adapter\n(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each\nTransformer layer, serving dual purposes of feature compression and chunk\nattention acquisition. The latter operates at the bottommost layer of the\nmodel, functioning to detect chunk boundaries by leveraging contextual semantic\ninformation. During the training phase, the parameters of the backbone remain\nfrozen, with only the QK Adapter and Chunk Adapter undergoing training.\nNotably, we design an attention distillation method for training the QK\nAdapter, which enhances the recall rate of key chunks. During the inference\nphase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference.\nExperimental evaluations are conducted on a diverse set of long-text and\nshort-text benchmark datasets spanning multiple tasks. ChunkLLM not only\nattains comparable performance on short-text benchmarks but also maintains\n98.64% of the performance on long-context benchmarks while preserving a 48.58%\nkey-value cache retention rate. Particularly, ChunkLLM attains a maximum\nspeedup of 4.48x in comparison to the vanilla Transformer in the processing of\n120K long texts."
                },
                "authors": [
                    {
                        "name": "Haojie Ouyang"
                    },
                    {
                        "name": "Jianwei Lv"
                    },
                    {
                        "name": "Lei Ren"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Xiaojie Wang"
                    },
                    {
                        "name": "Fangxiang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fangxiang Feng"
                },
                "author": "Fangxiang Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v2",
                "updated": "2025-09-28T08:32:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    8,
                    32,
                    26,
                    6,
                    271,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23601v1",
                "updated": "2025-09-28T03:12:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T03:12:43Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration"
                },
                "summary": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba."
                },
                "authors": [
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v2",
                "updated": "2025-09-27T20:13:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    13,
                    25,
                    5,
                    270,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Fei Yi"
                    },
                    {
                        "name": "Weidi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Zeng"
                },
                "author": "Weidi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23179v1",
                "updated": "2025-09-27T08:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T08:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "title": "A Near-Cache Architectural Framework for Cryptographic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Near-Cache Architectural Framework for Cryptographic Computing"
                },
                "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath."
                },
                "authors": [
                    {
                        "name": "Jingyao Zhang"
                    },
                    {
                        "name": "Elaheh Sadredini"
                    }
                ],
                "author_detail": {
                    "name": "Elaheh Sadredini"
                },
                "author": "Elaheh Sadredini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v4",
                "updated": "2025-09-27T07:41:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    7,
                    41,
                    38,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "Runtime Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23094v1",
                "updated": "2025-09-27T04:07:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T04:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"
                },
                "summary": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache."
                },
                "authors": [
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Yue Cai"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v3",
                "updated": "2025-09-27T03:37:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    3,
                    37,
                    40,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v4",
                "updated": "2025-09-26T21:40:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    21,
                    40,
                    58,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22875v1",
                "updated": "2025-09-26T19:40:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T19:40:33Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "title": "On KV-Poisson Structure and related invariants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On KV-Poisson Structure and related invariants"
                },
                "summary": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures."
                },
                "authors": [
                    {
                        "name": "Prosper Rosaire Mama Assandje"
                    },
                    {
                        "name": "Herguey Mopeng"
                    },
                    {
                        "name": "Joseph Dongho"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Dongho"
                },
                "author": "Joseph Dongho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v2",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Controlling Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Controlling Frozen LLMs"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "James R. Glass"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v1",
                "updated": "2025-09-26T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22548v1",
                "updated": "2025-09-26T16:29:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:29:37Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation"
                },
                "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/."
                },
                "authors": [
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Dekang Qi"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Feng Xiong"
                    },
                    {
                        "name": "Shichao Xie"
                    },
                    {
                        "name": "Xiaolong Wu"
                    },
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "arxiv_comment": "Project page: https://miv-xjtu.github.io/JanusVLN.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22516v1",
                "updated": "2025-09-26T16:00:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:00:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments"
                },
                "summary": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment."
                },
                "authors": [
                    {
                        "name": "Rakesh Thakur"
                    },
                    {
                        "name": "Shivaansh Kaushik"
                    },
                    {
                        "name": "Gauri Chopra"
                    },
                    {
                        "name": "Harsh Rohilla"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Rohilla"
                },
                "author": "Harsh Rohilla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22512v1",
                "updated": "2025-09-26T15:54:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:54:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability"
                },
                "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Soroush Ahadi"
                    },
                    {
                        "name": "Mehdi Modarressi"
                    },
                    {
                        "name": "Masoud Daneshtalab"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Daneshtalab"
                },
                "author": "Masoud Daneshtalab",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "n/a",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22488v1",
                "updated": "2025-09-26T15:35:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:35:05Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "title": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT"
                },
                "summary": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches."
                },
                "authors": [
                    {
                        "name": "Pierre-Antoine Rodesch"
                    },
                    {
                        "name": "Anas Viry"
                    },
                    {
                        "name": "Mouad Khorsi"
                    },
                    {
                        "name": "Fabio Becce"
                    },
                    {
                        "name": "Jrme Damet"
                    },
                    {
                        "name": "Luca Gallego Manzano"
                    }
                ],
                "author_detail": {
                    "name": "Luca Gallego Manzano"
                },
                "author": "Luca Gallego Manzano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v3",
                "updated": "2025-09-26T14:35:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    35,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning"
                },
                "summary": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22323v1",
                "updated": "2025-09-26T13:20:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:20:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Zhiwei Tang"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v8",
                "updated": "2025-09-26T10:00:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    10,
                    0,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22756v1",
                "updated": "2025-09-26T09:33:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T09:33:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving"
                },
                "summary": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences."
                },
                "authors": [
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Changjie Wu"
                    },
                    {
                        "name": "Huiyuan Yan"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Xinran Liu"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Yujian Yuan"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v2",
                "updated": "2025-09-26T07:14:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    7,
                    14,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21917v1",
                "updated": "2025-09-26T05:57:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T05:57:04Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "title": "Taming Flow-based I2V Models for Creative Video Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Flow-based I2V Models for Creative Video Editing"
                },
                "summary": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity."
                },
                "authors": [
                    {
                        "name": "Xianghao Kong"
                    },
                    {
                        "name": "Hansheng Chen"
                    },
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Anyi Rao"
                    }
                ],
                "author_detail": {
                    "name": "Anyi Rao"
                },
                "author": "Anyi Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21857v1",
                "updated": "2025-09-26T04:32:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:32:56Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "title": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width"
                },
                "summary": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices."
                },
                "authors": [
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21842v1",
                "updated": "2025-09-26T04:03:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:03:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents"
                },
                "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks."
                },
                "authors": [
                    {
                        "name": "Yansong Ning"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Jun Fang"
                    },
                    {
                        "name": "Kan Zheng"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v3",
                "updated": "2025-09-26T03:24:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    24,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v3",
                "updated": "2025-09-26T03:17:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    17,
                    15,
                    4,
                    269,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21623v1",
                "updated": "2025-09-25T21:42:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T21:42:27Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "title": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule"
                },
                "summary": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21463v1",
                "updated": "2025-09-25T19:29:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T19:29:25Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "title": "Enhanced Generative Machine Listener",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Generative Machine Listener"
                },
                "summary": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies."
                },
                "authors": [
                    {
                        "name": "Vishnu Raj"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Shiv Gehlot"
                    },
                    {
                        "name": "Lars Villemoes"
                    },
                    {
                        "name": "Arijit Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Biswas"
                },
                "author": "Arijit Biswas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v2",
                "updated": "2025-09-25T13:55:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    55,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21153v1",
                "updated": "2025-09-25T13:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:39:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP"
                },
                "summary": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings."
                },
                "authors": [
                    {
                        "name": "Moshe Kimhi"
                    },
                    {
                        "name": "Erez Koifman"
                    },
                    {
                        "name": "Ehud Rivlin"
                    },
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v2",
                "updated": "2025-09-25T13:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    45,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v2",
                "updated": "2025-09-25T10:24:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    24,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20979v1",
                "updated": "2025-09-25T10:23:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T10:23:50Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference"
                },
                "summary": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Yirong Zhang"
                    },
                    {
                        "name": "Jiahong Yu"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jianping Zou"
                    },
                    {
                        "name": "Gang Xiong"
                    },
                    {
                        "name": "Kingsum Chow"
                    },
                    {
                        "name": "Shuibing He"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v5",
                "updated": "2025-09-25T09:49:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    9,
                    49,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02323v1",
                "updated": "2025-09-25T07:01:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    7,
                    1,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T07:01:44Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    7,
                    1,
                    44,
                    3,
                    268,
                    0
                ],
                "title": "NetCAS: Dynamic Cache and Backend Device Management in Networked\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetCAS: Dynamic Cache and Backend Device Management in Networked\n  Environments"
                },
                "summary": "Modern storage systems often combine fast cache with slower backend devices\nto accelerate I/O. As performance gaps narrow, concurrently accessing both\ndevices, rather than relying solely on cache hits, can improve throughput.\nHowever, in data centers, remote backend storage accessed over networks suffers\nfrom unpredictable contention, complicating this split. We present NetCAS, a\nframework that dynamically splits I/O between cache and backend devices based\non real-time network feedback and a precomputed Perf Profile. Unlike\ntraditional hit-rate-based policies, NetCAS adapts split ratios to workload\nconfiguration and networking performance. NetCAS employs a low-overhead batched\nround-robin scheduler to enforce splits, avoiding per-request costs. It\nachieves up to 174% higher performance than traditional caching in remote\nstorage environments and outperforms converging schemes like Orthus by up to\n3.5X under fluctuating network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern storage systems often combine fast cache with slower backend devices\nto accelerate I/O. As performance gaps narrow, concurrently accessing both\ndevices, rather than relying solely on cache hits, can improve throughput.\nHowever, in data centers, remote backend storage accessed over networks suffers\nfrom unpredictable contention, complicating this split. We present NetCAS, a\nframework that dynamically splits I/O between cache and backend devices based\non real-time network feedback and a precomputed Perf Profile. Unlike\ntraditional hit-rate-based policies, NetCAS adapts split ratios to workload\nconfiguration and networking performance. NetCAS employs a low-overhead batched\nround-robin scheduler to enforce splits, avoiding per-request costs. It\nachieves up to 174% higher performance than traditional caching in remote\nstorage environments and outperforms converging schemes like Orthus by up to\n3.5X under fluctuating network conditions."
                },
                "authors": [
                    {
                        "name": "Joon Yong Hwang"
                    },
                    {
                        "name": "Chanseo Park"
                    },
                    {
                        "name": "Ikjun Yeom"
                    },
                    {
                        "name": "Younghoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Younghoon Kim"
                },
                "author": "Younghoon Kim",
                "arxiv_comment": "10 pages, 8 figures, submitted to USENIX FAST 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.2; D.4.8; C.2.1; C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v2",
                "updated": "2025-09-25T03:30:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    30,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v2",
                "updated": "2025-09-25T03:00:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    0,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20617v1",
                "updated": "2025-09-24T23:47:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T23:47:55Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "title": "DELM: a Python toolkit for Data Extraction with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELM: a Python toolkit for Data Extraction with Language Models"
                },
                "summary": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}."
                },
                "authors": [
                    {
                        "name": "Eric Fithian"
                    },
                    {
                        "name": "Kirill Skobelev"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Skobelev"
                },
                "author": "Kirill Skobelev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v2",
                "updated": "2025-09-24T16:56:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    56,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective"
                },
                "summary": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19729v1",
                "updated": "2025-09-24T03:15:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T03:15:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference"
                },
                "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Haoyu Chen"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Kun Qian"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Jin Zhao"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "12 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v2",
                "updated": "2025-09-24T01:32:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    32,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_doi": "10.1145/3731599.3767498",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767498",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01239v1",
                "updated": "2025-09-24T01:20:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    20,
                    47,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T01:20:47Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    20,
                    47,
                    2,
                    267,
                    0
                ],
                "title": "CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn\n  Interactions with a Single On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn\n  Interactions with a Single On-Device LLM"
                },
                "summary": "We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which\nis a novel execution system for efficient sub-task handling in multi-turn\ninteractions with a single on-device large language model (LLM). As LLMs become\nincreasingly capable, a single model is expected to handle diverse sub-tasks\nthat more effectively and comprehensively support answering user requests.\nNaive approach reprocesses the entire conversation context when switching\nbetween main and sub-tasks (e.g., query rewriting, summarization), incurring\nsignificant computational overhead. CIFLEX mitigates this overhead by reusing\nthe key-value (KV) cache from the main task and injecting only task-specific\ninstructions into isolated side paths. After sub-task execution, the model\nrolls back to the main path via cached context, thereby avoiding redundant\nprefill computation. To support sub-task selection, we also develop a\nhierarchical classification strategy tailored for small-scale models,\ndecomposing multi-choice decisions into binary ones. Experiments show that\nCIFLEX significantly reduces computational costs without degrading task\nperformance, enabling scalable and efficient multi-task dialogue on-device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which\nis a novel execution system for efficient sub-task handling in multi-turn\ninteractions with a single on-device large language model (LLM). As LLMs become\nincreasingly capable, a single model is expected to handle diverse sub-tasks\nthat more effectively and comprehensively support answering user requests.\nNaive approach reprocesses the entire conversation context when switching\nbetween main and sub-tasks (e.g., query rewriting, summarization), incurring\nsignificant computational overhead. CIFLEX mitigates this overhead by reusing\nthe key-value (KV) cache from the main task and injecting only task-specific\ninstructions into isolated side paths. After sub-task execution, the model\nrolls back to the main path via cached context, thereby avoiding redundant\nprefill computation. To support sub-task selection, we also develop a\nhierarchical classification strategy tailored for small-scale models,\ndecomposing multi-choice decisions into binary ones. Experiments show that\nCIFLEX significantly reduces computational costs without degrading task\nperformance, enabling scalable and efficient multi-task dialogue on-device."
                },
                "authors": [
                    {
                        "name": "Juntae Lee"
                    },
                    {
                        "name": "Jihwan Bang"
                    },
                    {
                        "name": "Seunghan Yang"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "accepted at EMNLP 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.03230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03230v1",
                "updated": "2025-10-03T17:59:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    59,
                    34,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:59:34Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    59,
                    34,
                    4,
                    276,
                    0
                ],
                "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping"
                },
                "summary": "GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms."
                },
                "authors": [
                    {
                        "name": "Suyuchen Wang"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Ahmed Masry"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Spandana Gella"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03229v1",
                "updated": "2025-10-03T17:59:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    59,
                    3,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:59:03Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    59,
                    3,
                    4,
                    276,
                    0
                ],
                "title": "Robust magnetic field estimates in star-forming galaxies with the\n  equipartition formula in the absence of equipartition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust magnetic field estimates in star-forming galaxies with the\n  equipartition formula in the absence of equipartition"
                },
                "summary": "The equipartition model is widely used to estimate magnetic field strength\nfrom synchrotron intensity in radio galaxies, yet the validity of its\nunderlying assumptions remains uncertain. Using an Arepo simulation which\nincorporates a two-moment cosmic ray (CR) transport scheme and a multiphase\ninterstellar medium, we compare magnetic fields inferred from synthetic\nsynchrotron emission maps with the true fields in the simulation. Starting from\nthe derivation of the equipartition formula, we find that the deviation between\nthe equipartition magnetic field and the true magnetic field depends only\nweakly on the ratio of the magnetic to the CR energy density. In practice, for\nboth face-on and edge-on projections, the equipartition model slightly\noverestimates the total synchrotron-weighted magnetic field with mean offsets\nof 32% (0.17 dex) and 36% (0.2 dex), even though the energy equipartition does\nnot hold locally. Beyond these average offsets, a clear trend emerges in\nedge-on projections that the model underestimates the field in the disk and\noverestimates it in the halo. Our results demonstrate that the validity of the\nequipartition model depends only weakly on the strict fulfillment of energy\nequipartition, and that the equipartition model remains a practical method for\nestimating magnetic field strengths in face-on projection maps based on our\nCR-magnetohydrodynamics simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The equipartition model is widely used to estimate magnetic field strength\nfrom synchrotron intensity in radio galaxies, yet the validity of its\nunderlying assumptions remains uncertain. Using an Arepo simulation which\nincorporates a two-moment cosmic ray (CR) transport scheme and a multiphase\ninterstellar medium, we compare magnetic fields inferred from synthetic\nsynchrotron emission maps with the true fields in the simulation. Starting from\nthe derivation of the equipartition formula, we find that the deviation between\nthe equipartition magnetic field and the true magnetic field depends only\nweakly on the ratio of the magnetic to the CR energy density. In practice, for\nboth face-on and edge-on projections, the equipartition model slightly\noverestimates the total synchrotron-weighted magnetic field with mean offsets\nof 32% (0.17 dex) and 36% (0.2 dex), even though the energy equipartition does\nnot hold locally. Beyond these average offsets, a clear trend emerges in\nedge-on projections that the model underestimates the field in the disk and\noverestimates it in the halo. Our results demonstrate that the validity of the\nequipartition model depends only weakly on the strict fulfillment of energy\nequipartition, and that the equipartition model remains a practical method for\nestimating magnetic field strengths in face-on projection maps based on our\nCR-magnetohydrodynamics simulation."
                },
                "authors": [
                    {
                        "name": "H. -H. Sandy Chiu"
                    },
                    {
                        "name": "Mateusz Ruszkowski"
                    },
                    {
                        "name": "Maria Werhahn"
                    },
                    {
                        "name": "Christoph Pfrommer"
                    },
                    {
                        "name": "Timon Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Timon Thomas"
                },
                "author": "Timon Thomas",
                "arxiv_comment": "14 pages; submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07998v2",
                "updated": "2025-10-03T17:58:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    58,
                    30,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-09T17:58:36Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    58,
                    36,
                    0,
                    160,
                    0
                ],
                "title": "Generative Modeling of Weights: Generalization or Memorization?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Modeling of Weights: Generalization or Memorization?"
                },
                "summary": "Generative models have recently been explored for synthesizing neural network\nweights. These approaches take neural network checkpoints as training data and\naim to generate high-performing weights during inference. In this work, we\nexamine four representative, well-known methods on their ability to generate\nnovel model weights, i.e., weights that are different from the checkpoints seen\nduring training. Contrary to claims in prior work, we find that these methods\nsynthesize weights largely by memorization: they produce either replicas, or,\nat best, simple interpolations of the training checkpoints. Moreover, they fail\nto outperform simple baselines, such as adding noise to the weights or taking a\nsimple weight ensemble, in obtaining different and simultaneously\nhigh-performing models. Our further analysis suggests that this memorization\nmight result from limited data, overparameterized models, and the underuse of\nstructural priors specific to weight data. These findings highlight the need\nfor more careful design and rigorous evaluation of generative models when\napplied to new domains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have recently been explored for synthesizing neural network\nweights. These approaches take neural network checkpoints as training data and\naim to generate high-performing weights during inference. In this work, we\nexamine four representative, well-known methods on their ability to generate\nnovel model weights, i.e., weights that are different from the checkpoints seen\nduring training. Contrary to claims in prior work, we find that these methods\nsynthesize weights largely by memorization: they produce either replicas, or,\nat best, simple interpolations of the training checkpoints. Moreover, they fail\nto outperform simple baselines, such as adding noise to the weights or taking a\nsimple weight ensemble, in obtaining different and simultaneously\nhigh-performing models. Our further analysis suggests that this memorization\nmight result from limited data, overparameterized models, and the underuse of\nstructural priors specific to weight data. These findings highlight the need\nfor more careful design and rigorous evaluation of generative models when\napplied to new domains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization."
                },
                "authors": [
                    {
                        "name": "Boya Zeng"
                    },
                    {
                        "name": "Yida Yin"
                    },
                    {
                        "name": "Zhiqiu Xu"
                    },
                    {
                        "name": "Zhuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Liu"
                },
                "author": "Zhuang Liu",
                "arxiv_comment": "Project page at https://boyazeng.github.io/weight_memorization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03223v1",
                "updated": "2025-10-03T17:56:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    56,
                    33,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:56:33Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    56,
                    33,
                    4,
                    276,
                    0
                ],
                "title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention\n  Alignment"
                },
                "summary": "To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining."
                },
                "authors": [
                    {
                        "name": "Hongxiang Zhang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tianyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhang"
                },
                "author": "Tianyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03221v1",
                "updated": "2025-10-03T17:54:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    54,
                    56,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:54:56Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    54,
                    56,
                    4,
                    276,
                    0
                ],
                "title": "Inferring Stellar Densities with Flexible Models I: The Distribution of\n  RR Lyrae in the Milky Way with $\\textit{Gaia}$ DR3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Stellar Densities with Flexible Models I: The Distribution of\n  RR Lyrae in the Milky Way with $\\textit{Gaia}$ DR3"
                },
                "summary": "Understanding the formation and evolutionary history of the Milky Way\nrequires detailed mapping of its stellar components, which preserve fossil\nrecords of the Galaxy's assembly through cosmic time. RR Lyrae stars are\nparticularly well-suited for this endeavor, as they are old, standard candle\nvariables that probe the Galaxy's earliest formation epochs. In this work, we\nemploy a hierarchical Bayesian Gaussian Mixture Model (GMM) to characterize the\nthree-dimensional density distribution of RR Lyrae stars in the Milky Way. This\napproach provides a flexible framework for modeling complex stellar\ndistributions, particularly in the inner Galaxy where the bulge, disk, and halo\ncomponents overlap. Our analysis reveals that the inner Galaxy is dominated by\na distinct prolate stellar population with axis ratio $q$=1.30. Consistent with\nprevious work, we find the halo follows a $r^{-4}$ power-law profile that\nflattens within 12 kpc of the Galactic center. We also confirm the halo is\noblate ($q$=0.62) with a tilt angle of $12.22^{\\circ}$. We report for the first\ntime that this tilt aligns the halo major axis in the direction of the\nSagittarius dwarf galaxy. These results establish GMMs as an effective and\nflexible tool for modeling Galactic structure and provide new constraints on\nthe distribution of old stars in the inner Galaxy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the formation and evolutionary history of the Milky Way\nrequires detailed mapping of its stellar components, which preserve fossil\nrecords of the Galaxy's assembly through cosmic time. RR Lyrae stars are\nparticularly well-suited for this endeavor, as they are old, standard candle\nvariables that probe the Galaxy's earliest formation epochs. In this work, we\nemploy a hierarchical Bayesian Gaussian Mixture Model (GMM) to characterize the\nthree-dimensional density distribution of RR Lyrae stars in the Milky Way. This\napproach provides a flexible framework for modeling complex stellar\ndistributions, particularly in the inner Galaxy where the bulge, disk, and halo\ncomponents overlap. Our analysis reveals that the inner Galaxy is dominated by\na distinct prolate stellar population with axis ratio $q$=1.30. Consistent with\nprevious work, we find the halo follows a $r^{-4}$ power-law profile that\nflattens within 12 kpc of the Galactic center. We also confirm the halo is\noblate ($q$=0.62) with a tilt angle of $12.22^{\\circ}$. We report for the first\ntime that this tilt aligns the halo major axis in the direction of the\nSagittarius dwarf galaxy. These results establish GMMs as an effective and\nflexible tool for modeling Galactic structure and provide new constraints on\nthe distribution of old stars in the inner Galaxy."
                },
                "authors": [
                    {
                        "name": "Madeline Lucey"
                    },
                    {
                        "name": "Cecilia Mateu"
                    },
                    {
                        "name": "Adrian Price-Whelan"
                    },
                    {
                        "name": "David Hogg"
                    },
                    {
                        "name": "Hans-Walter Rix"
                    },
                    {
                        "name": "Robyn Sanderson"
                    }
                ],
                "author_detail": {
                    "name": "Robyn Sanderson"
                },
                "author": "Robyn Sanderson",
                "arxiv_comment": "13 pages, 6 figures, submitted to ApJ, comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03217v1",
                "updated": "2025-10-03T17:53:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    53,
                    28,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:53:28Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    53,
                    28,
                    4,
                    276,
                    0
                ],
                "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic\n  Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic\n  Program Repair"
                },
                "summary": "Agentic Automated Program Repair (APR) is increasingly tackling complex,\nrepository-level bugs in industry, but ultimately agent-generated patches still\nneed to be reviewed by a human before committing them to ensure they address\nthe bug. Showing unlikely patches to developers can lead to substantial noise,\nwasting valuable developer time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such noise: bug\nabstention and patch validation policies. Bug abstention excludes bugs that the\nagentic APR system is unlikely to fix. Patch validation rejects patches that\nare unlikely to be a good fix for the given bug. We evaluate both policies on\nthree sets of bugs from Google's codebase, and their candidate patches\ngenerated by an internal agentic APR system. On a set of 174 human-reported\nbugs, removing bugs and patch trajectories rejected by our policies can raise\nsuccess rates by up to 13 percentage points and 15 percentage points,\nrespectively, and by up to 39 percentage points in combination. On null pointer\nexceptions and sanitizer-reported bugs with machine-generated bug reports,\npatch validation also improves average single-sample success rates. This\ntwo-policy approach provides a practical path to the reliable, industrial-scale\ndeployment of agentic APR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Automated Program Repair (APR) is increasingly tackling complex,\nrepository-level bugs in industry, but ultimately agent-generated patches still\nneed to be reviewed by a human before committing them to ensure they address\nthe bug. Showing unlikely patches to developers can lead to substantial noise,\nwasting valuable developer time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such noise: bug\nabstention and patch validation policies. Bug abstention excludes bugs that the\nagentic APR system is unlikely to fix. Patch validation rejects patches that\nare unlikely to be a good fix for the given bug. We evaluate both policies on\nthree sets of bugs from Google's codebase, and their candidate patches\ngenerated by an internal agentic APR system. On a set of 174 human-reported\nbugs, removing bugs and patch trajectories rejected by our policies can raise\nsuccess rates by up to 13 percentage points and 15 percentage points,\nrespectively, and by up to 39 percentage points in combination. On null pointer\nexceptions and sanitizer-reported bugs with machine-generated bug reports,\npatch validation also improves average single-sample success rates. This\ntwo-policy approach provides a practical path to the reliable, industrial-scale\ndeployment of agentic APR systems."
                },
                "authors": [
                    {
                        "name": "Jos Cambronero"
                    },
                    {
                        "name": "Michele Tufano"
                    },
                    {
                        "name": "Sherry Shi"
                    },
                    {
                        "name": "Renyao Wei"
                    },
                    {
                        "name": "Grant Uy"
                    },
                    {
                        "name": "Runxiang Cheng"
                    },
                    {
                        "name": "Chin-Jung Liu"
                    },
                    {
                        "name": "Shiying Pan"
                    },
                    {
                        "name": "Satish Chandra"
                    },
                    {
                        "name": "Pat Rondon"
                    }
                ],
                "author_detail": {
                    "name": "Pat Rondon"
                },
                "author": "Pat Rondon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03215v1",
                "updated": "2025-10-03T17:52:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models"
                },
                "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Zihan Min"
                    },
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Jichao Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07714v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07714v3",
                "updated": "2025-10-03T17:50:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    50,
                    53,
                    4,
                    276,
                    0
                ],
                "published": "2024-06-11T20:48:28Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    20,
                    48,
                    28,
                    1,
                    163,
                    0
                ],
                "title": "LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing"
                },
                "summary": "Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in\nprograms. However, randomized mutation strategies have limited the fuzzer's\nperformance on structured data. Specialized fuzzers can handle complex\nstructured data, but require additional efforts in grammar and suffer from low\nthroughput.\n  In this paper, we explore the potential of utilizing the Large Language Model\nto enhance greybox fuzzing for structured data. We utilize the pre-trained\nknowledge of LLM about data conversion and format to generate new valid inputs.\nWe further fine-tuned it with paired mutation seeds to learn structured format\nand mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ,\nintegrates the power of LLM to understand and mutate structured data to\nfuzzing. We conduct experiments on the standard bug-based benchmark Magma and a\nwide variety of real-world programs. LLAMAFUZZ outperforms our top competitor\nby 41 bugs on average. We also identified 47 unique bugs across all trials.\nMoreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and\nbug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in\nreal-world program sets on average. We also demonstrate a case study to explain\nhow LLMs enhance the fuzzing process in terms of code coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in\nprograms. However, randomized mutation strategies have limited the fuzzer's\nperformance on structured data. Specialized fuzzers can handle complex\nstructured data, but require additional efforts in grammar and suffer from low\nthroughput.\n  In this paper, we explore the potential of utilizing the Large Language Model\nto enhance greybox fuzzing for structured data. We utilize the pre-trained\nknowledge of LLM about data conversion and format to generate new valid inputs.\nWe further fine-tuned it with paired mutation seeds to learn structured format\nand mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ,\nintegrates the power of LLM to understand and mutate structured data to\nfuzzing. We conduct experiments on the standard bug-based benchmark Magma and a\nwide variety of real-world programs. LLAMAFUZZ outperforms our top competitor\nby 41 bugs on average. We also identified 47 unique bugs across all trials.\nMoreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and\nbug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in\nreal-world program sets on average. We also demonstrate a case study to explain\nhow LLMs enhance the fuzzing process in terms of code coverage."
                },
                "authors": [
                    {
                        "name": "Hongxiang Zhang"
                    },
                    {
                        "name": "Yuyang Rong"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07714v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07714v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21634v2",
                "updated": "2025-10-03T17:43:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    43,
                    57,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-25T21:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    49,
                    43,
                    3,
                    268,
                    0
                ],
                "title": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G\n  Open RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G\n  Open RANs"
                },
                "summary": "The evolution toward 6G networks is being accelerated by the Open Radio\nAccess Network (O-RAN) paradigm -- an open, interoperable architecture that\nenables intelligent, modular applications across public telecom and private\nenterprise domains. While this openness creates unprecedented opportunities for\ninnovation, it also expands the attack surface, demanding resilient, low-cost,\nand autonomous security solutions. Legacy defenses remain largely reactive,\nlabor-intensive, and inadequate for the scale and complexity of next-generation\nsystems. Current O-RAN applications focus mainly on network optimization or\npassive threat detection, with limited capability for closed-loop, automated\nresponse.\n  To address this critical gap, we present an agentic AI framework for fully\nautomated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM\norchestrates security workflows through a modular multi-agent system powered by\nLarge Language Models (LLMs). The framework features a Threat Analysis Agent\nfor real-time data triage, a Threat Classification Agent that uses\nRetrieval-Augmented Generation (RAG) to map anomalies to specific\ncountermeasures, and a Threat Response Agent that safely operationalizes\nmitigation actions via O-RAN control interfaces. Grounded in trusted knowledge\nbases such as the MITRE FiGHT framework and 3GPP specifications, and equipped\nwith robust safety guardrails, MobiLLM provides a blueprint for trustworthy\nAI-driven network security. Initial evaluations demonstrate that MobiLLM can\neffectively identify and orchestrate complex mitigation strategies,\nsignificantly reducing response latency and showcasing the feasibility of\nautonomous security operations in 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution toward 6G networks is being accelerated by the Open Radio\nAccess Network (O-RAN) paradigm -- an open, interoperable architecture that\nenables intelligent, modular applications across public telecom and private\nenterprise domains. While this openness creates unprecedented opportunities for\ninnovation, it also expands the attack surface, demanding resilient, low-cost,\nand autonomous security solutions. Legacy defenses remain largely reactive,\nlabor-intensive, and inadequate for the scale and complexity of next-generation\nsystems. Current O-RAN applications focus mainly on network optimization or\npassive threat detection, with limited capability for closed-loop, automated\nresponse.\n  To address this critical gap, we present an agentic AI framework for fully\nautomated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM\norchestrates security workflows through a modular multi-agent system powered by\nLarge Language Models (LLMs). The framework features a Threat Analysis Agent\nfor real-time data triage, a Threat Classification Agent that uses\nRetrieval-Augmented Generation (RAG) to map anomalies to specific\ncountermeasures, and a Threat Response Agent that safely operationalizes\nmitigation actions via O-RAN control interfaces. Grounded in trusted knowledge\nbases such as the MITRE FiGHT framework and 3GPP specifications, and equipped\nwith robust safety guardrails, MobiLLM provides a blueprint for trustworthy\nAI-driven network security. Initial evaluations demonstrate that MobiLLM can\neffectively identify and orchestrate complex mitigation strategies,\nsignificantly reducing response latency and showcasing the feasibility of\nautonomous security operations in 6G."
                },
                "authors": [
                    {
                        "name": "Prakhar Sharma"
                    },
                    {
                        "name": "Haohuang Wen"
                    },
                    {
                        "name": "Vinod Yegneswaran"
                    },
                    {
                        "name": "Ashish Gehani"
                    },
                    {
                        "name": "Phillip Porras"
                    },
                    {
                        "name": "Zhiqiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lin"
                },
                "author": "Zhiqiang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18340v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18340v3",
                "updated": "2025-10-03T17:43:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    43,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-23T06:42:48Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    42,
                    48,
                    0,
                    174,
                    0
                ],
                "title": "Controlled Generation with Equivariant Variational Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlled Generation with Equivariant Variational Flow Matching"
                },
                "summary": "We derive a controlled generation objective within the framework of\nVariational Flow Matching (VFM), which casts flow matching as a variational\ninference problem. We demonstrate that controlled generation can be implemented\ntwo ways: (1) by way of end-to-end training of conditional generative models,\nor (2) as a Bayesian inference problem, enabling post hoc control of\nunconditional models without retraining. Furthermore, we establish the\nconditions required for equivariant generation and provide an equivariant\nformulation of VFM tailored for molecular generation, ensuring invariance to\nrotations, translations, and permutations. We evaluate our approach on both\nuncontrolled and controlled molecular generation, achieving state-of-the-art\nperformance on uncontrolled generation and outperforming state-of-the-art\nmodels in controlled generation, both with end-to-end training and in the\nBayesian inference setting. This work strengthens the connection between\nflow-based generative modeling and Bayesian inference, offering a scalable and\nprincipled framework for constraint-driven and symmetry-aware generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We derive a controlled generation objective within the framework of\nVariational Flow Matching (VFM), which casts flow matching as a variational\ninference problem. We demonstrate that controlled generation can be implemented\ntwo ways: (1) by way of end-to-end training of conditional generative models,\nor (2) as a Bayesian inference problem, enabling post hoc control of\nunconditional models without retraining. Furthermore, we establish the\nconditions required for equivariant generation and provide an equivariant\nformulation of VFM tailored for molecular generation, ensuring invariance to\nrotations, translations, and permutations. We evaluate our approach on both\nuncontrolled and controlled molecular generation, achieving state-of-the-art\nperformance on uncontrolled generation and outperforming state-of-the-art\nmodels in controlled generation, both with end-to-end training and in the\nBayesian inference setting. This work strengthens the connection between\nflow-based generative modeling and Bayesian inference, offering a scalable and\nprincipled framework for constraint-driven and symmetry-aware generation."
                },
                "authors": [
                    {
                        "name": "Floor Eijkelboom"
                    },
                    {
                        "name": "Heiko Zimmermann"
                    },
                    {
                        "name": "Sharvaree Vadgama"
                    },
                    {
                        "name": "Erik J Bekkers"
                    },
                    {
                        "name": "Max Welling"
                    },
                    {
                        "name": "Christian A. Naesseth"
                    },
                    {
                        "name": "Jan-Willem van de Meent"
                    }
                ],
                "author_detail": {
                    "name": "Jan-Willem van de Meent"
                },
                "author": "Jan-Willem van de Meent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18340v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18340v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00430v2",
                "updated": "2025-10-03T17:42:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    42,
                    59,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-31T07:17:48Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    7,
                    17,
                    48,
                    5,
                    151,
                    0
                ],
                "title": "MIRROR: Modular Internal Processing for Personalized Safety in LLM\n  Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRROR: Modular Internal Processing for Personalized Safety in LLM\n  Dialogue"
                },
                "summary": "Large language models frequently generate harmful recommendations in personal\nmulti-turn dialogue by ignoring user-specific safety context, exhibiting\nsycophantic agreement, and compromising user safety for larger group\npreferences. We introduce MIRROR, a modular production-focused architecture\nthat prevents these failures through a persistent, bounded internal state that\npreserves personal conversational information across conversational turns. Our\ndual-component design inspired by Dual Process Theory separates immediate\nresponse generation (Talker) from asynchronous deliberative processing\n(Thinker), which synthesizes parallel reasoning threads between turns with\nmarginal latency. On the CuRaTe personalized safety benchmark, MIRROR-augmented\nmodels achieve a 21% relative improvement (69% to 84%) across seven diverse\nfrontier models, with open-source Llama 4 and Mistral 3 variants surpassing\nboth GPT-4o and Claude 3.7 Sonnet at only \\$0.0028 to \\$0.0172 additional cost\nper turn, narrowing the gap between affordable open-source models to frontier\nsystems in the safety space. The modular architecture enables flexible\ndeployment: full internal processing for affordable models or single-component\nconfigurations for expensive systems, democratizing access to safer,\npersonalized AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models frequently generate harmful recommendations in personal\nmulti-turn dialogue by ignoring user-specific safety context, exhibiting\nsycophantic agreement, and compromising user safety for larger group\npreferences. We introduce MIRROR, a modular production-focused architecture\nthat prevents these failures through a persistent, bounded internal state that\npreserves personal conversational information across conversational turns. Our\ndual-component design inspired by Dual Process Theory separates immediate\nresponse generation (Talker) from asynchronous deliberative processing\n(Thinker), which synthesizes parallel reasoning threads between turns with\nmarginal latency. On the CuRaTe personalized safety benchmark, MIRROR-augmented\nmodels achieve a 21% relative improvement (69% to 84%) across seven diverse\nfrontier models, with open-source Llama 4 and Mistral 3 variants surpassing\nboth GPT-4o and Claude 3.7 Sonnet at only \\$0.0028 to \\$0.0172 additional cost\nper turn, narrowing the gap between affordable open-source models to frontier\nsystems in the safety space. The modular architecture enables flexible\ndeployment: full internal processing for affordable models or single-component\nconfigurations for expensive systems, democratizing access to safer,\npersonalized AI."
                },
                "authors": [
                    {
                        "name": "Nicole Hsing"
                    }
                ],
                "author_detail": {
                    "name": "Nicole Hsing"
                },
                "author": "Nicole Hsing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03204v1",
                "updated": "2025-10-03T17:41:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    41,
                    30,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:41:30Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    41,
                    30,
                    4,
                    276,
                    0
                ],
                "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents"
                },
                "summary": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure."
                },
                "authors": [
                    {
                        "name": "Imene Kerboua"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Xing Han L"
                    },
                    {
                        "name": "Lo Boisvert"
                    },
                    {
                        "name": "Massimo Caccia"
                    },
                    {
                        "name": "Jrmy Espinas"
                    },
                    {
                        "name": "Alexandre Aussem"
                    },
                    {
                        "name": "Vronique Eglin"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Lacoste"
                },
                "author": "Alexandre Lacoste",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10150v2",
                "updated": "2025-10-03T17:36:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    36,
                    51,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-11T20:10:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    20,
                    10,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "When Large Language Models are Reliable for Judging Empathic\n  Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Large Language Models are Reliable for Judging Empathic\n  Communication"
                },
                "summary": "Large language models (LLMs) excel at generating empathic responses in\ntext-based conversations. But, how reliably do they judge the nuances of\nempathic communication? We investigate this question by comparing how experts,\ncrowdworkers, and LLMs annotate empathic communication across four evaluative\nframeworks drawn from psychology, natural language processing, and\ncommunications applied to 200 real-world conversations where one speaker shares\na personal problem and the other offers support. Drawing on 3,150 expert\nannotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess\ninter-rater reliability between these three annotator groups. We find that\nexpert agreement is high but varies across the frameworks' sub-components\ndepending on their clarity, complexity, and subjectivity. We show that expert\nagreement offers a more informative benchmark for contextualizing LLM\nperformance than standard classification metrics. Across all four frameworks,\nLLMs consistently approach this expert level benchmark and exceed the\nreliability of crowdworkers. These results demonstrate how LLMs, when validated\non specific tasks with appropriate benchmarks, can support transparency and\noversight in emotionally sensitive applications including their use as\nconversational companions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at generating empathic responses in\ntext-based conversations. But, how reliably do they judge the nuances of\nempathic communication? We investigate this question by comparing how experts,\ncrowdworkers, and LLMs annotate empathic communication across four evaluative\nframeworks drawn from psychology, natural language processing, and\ncommunications applied to 200 real-world conversations where one speaker shares\na personal problem and the other offers support. Drawing on 3,150 expert\nannotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess\ninter-rater reliability between these three annotator groups. We find that\nexpert agreement is high but varies across the frameworks' sub-components\ndepending on their clarity, complexity, and subjectivity. We show that expert\nagreement offers a more informative benchmark for contextualizing LLM\nperformance than standard classification metrics. Across all four frameworks,\nLLMs consistently approach this expert level benchmark and exceed the\nreliability of crowdworkers. These results demonstrate how LLMs, when validated\non specific tasks with appropriate benchmarks, can support transparency and\noversight in emotionally sensitive applications including their use as\nconversational companions."
                },
                "authors": [
                    {
                        "name": "Aakriti Kumar"
                    },
                    {
                        "name": "Nalin Poungpeth"
                    },
                    {
                        "name": "Diyi Yang"
                    },
                    {
                        "name": "Erina Farrell"
                    },
                    {
                        "name": "Bruce Lambert"
                    },
                    {
                        "name": "Matthew Groh"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Groh"
                },
                "author": "Matthew Groh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14052v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14052v4",
                "updated": "2025-10-03T17:35:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    52,
                    4,
                    276,
                    0
                ],
                "published": "2025-08-07T22:15:22Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    22,
                    15,
                    22,
                    3,
                    219,
                    0
                ],
                "title": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial\n  Question Answering"
                },
                "summary": "Accurate information retrieval (IR) is critical in the financial domain,\nwhere investors must identify relevant information from large collections of\ndocuments. Traditional IR methods -- whether sparse or dense -- often fall\nshort in retrieval accuracy, as it requires not only capturing semantic\nsimilarity but also performing fine-grained reasoning over document structure\nand domain-specific knowledge. Recent advances in large language models (LLMs)\nhave opened up new opportunities for retrieval with multi-step reasoning, where\nthe model ranks passages through iterative reasoning about which information is\nmost relevant to a given query. However, there exists no benchmark to evaluate\nsuch capabilities in the financial domain. To address this gap, we introduce\nFinAgentBench, the first large-scale benchmark for evaluating retrieval with\nmulti-step reasoning in finance -- a setting we term agentic retrieval. The\nbenchmark consists of 26K expert-annotated examples on S&P-500 listed firms and\nassesses whether LLM agents can (1) identify the most relevant document type\namong candidates, and (2) pinpoint the key passage within the selected\ndocument. Our evaluation framework explicitly separates these two reasoning\nsteps to address context limitations. This design enables to provide a\nquantitative basis for understanding retrieval-centric LLM behavior in finance.\nWe evaluate a suite of state-of-the-art models and further demonstrated how\ntargeted fine-tuning can significantly improve agentic retrieval performance.\nOur benchmark provides a foundation for studying retrieval-centric LLM behavior\nin complex, domain-specific tasks for finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate information retrieval (IR) is critical in the financial domain,\nwhere investors must identify relevant information from large collections of\ndocuments. Traditional IR methods -- whether sparse or dense -- often fall\nshort in retrieval accuracy, as it requires not only capturing semantic\nsimilarity but also performing fine-grained reasoning over document structure\nand domain-specific knowledge. Recent advances in large language models (LLMs)\nhave opened up new opportunities for retrieval with multi-step reasoning, where\nthe model ranks passages through iterative reasoning about which information is\nmost relevant to a given query. However, there exists no benchmark to evaluate\nsuch capabilities in the financial domain. To address this gap, we introduce\nFinAgentBench, the first large-scale benchmark for evaluating retrieval with\nmulti-step reasoning in finance -- a setting we term agentic retrieval. The\nbenchmark consists of 26K expert-annotated examples on S&P-500 listed firms and\nassesses whether LLM agents can (1) identify the most relevant document type\namong candidates, and (2) pinpoint the key passage within the selected\ndocument. Our evaluation framework explicitly separates these two reasoning\nsteps to address context limitations. This design enables to provide a\nquantitative basis for understanding retrieval-centric LLM behavior in finance.\nWe evaluate a suite of state-of-the-art models and further demonstrated how\ntargeted fine-tuning can significantly improve agentic retrieval performance.\nOur benchmark provides a foundation for studying retrieval-centric LLM behavior\nin complex, domain-specific tasks for finance."
                },
                "authors": [
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Jihoon Kwon"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Chaewoon Kim"
                    },
                    {
                        "name": "Minjae Kim"
                    },
                    {
                        "name": "Juneha Hwang"
                    },
                    {
                        "name": "Jaeseon Ha"
                    },
                    {
                        "name": "Hojun Choi"
                    },
                    {
                        "name": "Suyeol Yun"
                    },
                    {
                        "name": "Yongjin Kim"
                    },
                    {
                        "name": "Yongjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yongjae Lee"
                },
                "author": "Yongjae Lee",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14052v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14052v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03199v1",
                "updated": "2025-10-03T17:35:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    45,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:35:45Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    45,
                    4,
                    276,
                    0
                ],
                "title": "Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference\n  Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference\n  Scaling"
                },
                "summary": "LLM inference often generates a batch of candidates for a prompt and selects\none via strategies like majority voting or Best-of- N (BoN). For difficult\ntasks, this single-shot selection often underperforms. Consequently,\nevaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,\nand only the best of them is used when computing regret. Motivated by this, we\nstudy inference scaling in the more general Pass@$k$ inference setting, and\nprove that neither majority voting nor BoN exhibits the desirable scaling with\n$k$ and the sampling budget $N$. Combining the advantages of majority voting\nand BoN, we propose a new inference strategy called Best-of-Majority (BoM),\nwith a pivotal step that restricts the candidates to the responses with high\nfrequency in the $N$ samples before selecting the top-$k$ rewards. We prove\nthat when the sampling budget is $N=\\tilde\\Omega(C^*)$, the regret of BoM is\n$O(\\epsilon_{\\mathrm{opt}}+\\sqrt{\\epsilon_{\\mathrm{RM}}^2C^*/k})$, where $C^*$\nis the coverage coefficient, $\\epsilon_{\\mathrm{RM}}$ is the estimation error\nof the reward model, and $\\epsilon_{\\mathrm{opt}}$ is the estimation error of\nreward at the optimal response. We further establish a matching lower bound,\ncertifying that our algorithm is minimax optimal. Beyond optimality, BoM has a\nkey advantage: unlike majority voting and BoN, its performance does not degrade\nwhen increasing $N$. Experimental results of inference on math problems show\nBoM outperforming both majority voting and BoN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference often generates a batch of candidates for a prompt and selects\none via strategies like majority voting or Best-of- N (BoN). For difficult\ntasks, this single-shot selection often underperforms. Consequently,\nevaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,\nand only the best of them is used when computing regret. Motivated by this, we\nstudy inference scaling in the more general Pass@$k$ inference setting, and\nprove that neither majority voting nor BoN exhibits the desirable scaling with\n$k$ and the sampling budget $N$. Combining the advantages of majority voting\nand BoN, we propose a new inference strategy called Best-of-Majority (BoM),\nwith a pivotal step that restricts the candidates to the responses with high\nfrequency in the $N$ samples before selecting the top-$k$ rewards. We prove\nthat when the sampling budget is $N=\\tilde\\Omega(C^*)$, the regret of BoM is\n$O(\\epsilon_{\\mathrm{opt}}+\\sqrt{\\epsilon_{\\mathrm{RM}}^2C^*/k})$, where $C^*$\nis the coverage coefficient, $\\epsilon_{\\mathrm{RM}}$ is the estimation error\nof the reward model, and $\\epsilon_{\\mathrm{opt}}$ is the estimation error of\nreward at the optimal response. We further establish a matching lower bound,\ncertifying that our algorithm is minimax optimal. Beyond optimality, BoM has a\nkey advantage: unlike majority voting and BoN, its performance does not degrade\nwhen increasing $N$. Experimental results of inference on math problems show\nBoM outperforming both majority voting and BoN."
                },
                "authors": [
                    {
                        "name": "Qiwei Di"
                    },
                    {
                        "name": "Kaixuan Ji"
                    },
                    {
                        "name": "Xuheng Li"
                    },
                    {
                        "name": "Heyang Zhao"
                    },
                    {
                        "name": "Quanquan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Quanquan Gu"
                },
                "author": "Quanquan Gu",
                "arxiv_comment": "29 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17157v2",
                "updated": "2025-10-03T17:34:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    34,
                    28,
                    4,
                    276,
                    0
                ],
                "published": "2025-01-28T18:56:47Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    56,
                    47,
                    1,
                    28,
                    0
                ],
                "title": "A novel inversion algorithm for weak gravitational lensing using\n  quasi-conformal geometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel inversion algorithm for weak gravitational lensing using\n  quasi-conformal geometry"
                },
                "summary": "The challenge in weak gravitational lensing caused by galaxies and clusters\nis to infer the projected mass density distribution from gravitational lensing\nmeasurements, known as the inversion problem. We introduce a novel theoretical\napproach to solving the inversion problem. The cornerstone of the proposed\nmethod lies in a complex formalism that describes the lens mapping as a\nquasi-conformal mapping with the Beltrami coefficient given by the negative of\nthe reduced shear, which can, in principle, be observed from the image\nellipticities. We propose an algorithm called QCLens that is based on this\ncomplex formalism. QCLens computes the underlying quasi-conformal mapping using\na finite element approach by reducing the problem to two elliptic partial\ndifferential equations that solely depend on the reduced shear field.\nExperimental results for both the Schwarzschild and the singular isothermal\nlens demonstrate the agreement of our proposed method with the analytically\ncomputable solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge in weak gravitational lensing caused by galaxies and clusters\nis to infer the projected mass density distribution from gravitational lensing\nmeasurements, known as the inversion problem. We introduce a novel theoretical\napproach to solving the inversion problem. The cornerstone of the proposed\nmethod lies in a complex formalism that describes the lens mapping as a\nquasi-conformal mapping with the Beltrami coefficient given by the negative of\nthe reduced shear, which can, in principle, be observed from the image\nellipticities. We propose an algorithm called QCLens that is based on this\ncomplex formalism. QCLens computes the underlying quasi-conformal mapping using\na finite element approach by reducing the problem to two elliptic partial\ndifferential equations that solely depend on the reduced shear field.\nExperimental results for both the Schwarzschild and the singular isothermal\nlens demonstrate the agreement of our proposed method with the analytically\ncomputable solutions."
                },
                "authors": [
                    {
                        "name": "Jan Jakob"
                    },
                    {
                        "name": "Bjrn Malte Schfer"
                    }
                ],
                "author_detail": {
                    "name": "Bjrn Malte Schfer"
                },
                "author": "Bjrn Malte Schfer",
                "arxiv_doi": "10.1051/0004-6361/202554048",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202554048",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint, 6 pages, 5 figures",
                "arxiv_journal_ref": "A&A 701, A233 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01412v2",
                "updated": "2025-10-03T17:33:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    33,
                    14,
                    4,
                    276,
                    0
                ],
                "published": "2023-03-02T17:03:02Z",
                "published_parsed": [
                    2023,
                    3,
                    2,
                    17,
                    3,
                    2,
                    3,
                    61,
                    0
                ],
                "title": "The Challenges of Hyperparameter Tuning for Accurate Causal Effect\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Challenges of Hyperparameter Tuning for Accurate Causal Effect\n  Estimation"
                },
                "summary": "ML is playing an increasingly crucial role in estimating causal effects of\ntreatments on outcomes from observational data. Many ML methods (`causal\nestimators') have been proposed for this task. All of these methods, as with\nany ML approach, require extensive hyperparameter tuning. For non-causal\npredictive tasks, there is a consensus on the choice of tuning metrics (e.g.\nmean squared error), making it simple to compare models. However, for causal\ninference tasks, such a consensus is yet to be reached, making any comparison\nof causal models difficult. On top of that, there is no ideal metric on which\nto tune causal estimators, so one must rely on proxies. Furthermore, the fact\nthat model selection in causal inference involves multiple components (causal\nestimator, ML regressor, hyperparameters, metric), complicates the issue even\nfurther. In order to evaluate the importance of each component, we perform an\nextensive empirical study on their combination. Our experimental setup involves\nmany commonly used causal estimators, regressors (`base learners' henceforth)\nand metrics applied to four well-known causal inference benchmark datasets. Our\nresults show that hyperparameter tuning increased the probability of reaching\nstate-of-the-art performance in average ($65\\% {\\rightarrow} 81\\%$) and\nindividualised ($50\\% {\\rightarrow} 57\\%$) effect estimation with only commonly\nused estimators. We also show that the performance of standard metrics can be\ninconsistent across different scenarios. Our findings highlight the need for\nfurther research to establish whether metrics uniformly capable of\nstate-of-the-art performance in causal model evaluation can be found.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML is playing an increasingly crucial role in estimating causal effects of\ntreatments on outcomes from observational data. Many ML methods (`causal\nestimators') have been proposed for this task. All of these methods, as with\nany ML approach, require extensive hyperparameter tuning. For non-causal\npredictive tasks, there is a consensus on the choice of tuning metrics (e.g.\nmean squared error), making it simple to compare models. However, for causal\ninference tasks, such a consensus is yet to be reached, making any comparison\nof causal models difficult. On top of that, there is no ideal metric on which\nto tune causal estimators, so one must rely on proxies. Furthermore, the fact\nthat model selection in causal inference involves multiple components (causal\nestimator, ML regressor, hyperparameters, metric), complicates the issue even\nfurther. In order to evaluate the importance of each component, we perform an\nextensive empirical study on their combination. Our experimental setup involves\nmany commonly used causal estimators, regressors (`base learners' henceforth)\nand metrics applied to four well-known causal inference benchmark datasets. Our\nresults show that hyperparameter tuning increased the probability of reaching\nstate-of-the-art performance in average ($65\\% {\\rightarrow} 81\\%$) and\nindividualised ($50\\% {\\rightarrow} 57\\%$) effect estimation with only commonly\nused estimators. We also show that the performance of standard metrics can be\ninconsistent across different scenarios. Our findings highlight the need for\nfurther research to establish whether metrics uniformly capable of\nstate-of-the-art performance in causal model evaluation can be found."
                },
                "authors": [
                    {
                        "name": "Damian Machlanski"
                    },
                    {
                        "name": "Spyridon Samothrakis"
                    },
                    {
                        "name": "Paul Clarke"
                    }
                ],
                "author_detail": {
                    "name": "Paul Clarke"
                },
                "author": "Paul Clarke",
                "arxiv_comment": "Substantially revised version. 18 pages of main content (33 pages in\n  total), 4 main figures (11 in total)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.01412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03195v1",
                "updated": "2025-10-03T17:30:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    30,
                    56,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:30:56Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    30,
                    56,
                    4,
                    276,
                    0
                ],
                "title": "Can LLMs Hit Moving Targets? Tracking Evolving Signals in Corporate\n  Disclosures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Hit Moving Targets? Tracking Evolving Signals in Corporate\n  Disclosures"
                },
                "summary": "Moving targets -- managers' strategic shifting of key performance metrics\nwhen the original targets become difficult to achieve -- have been shown to\npredict subsequent stock underperformance. However, our work reveals that the\nmethod employed in that study exhibits two key limitations that hinder the\naccuracy -- noise in the extracted targets and loss of contextual information\n-- both of which stem primarily from the use of a named entity recognition\n(NER). To address these two limitations, we propose an LLM-based target\nextraction} method with a newly defined metric that better captures semantic\ncontext. This approach preserves semantic context beyond simple entity\nrecognition and yields consistently higher predictive power than the original\napproach. Overall, our approach enhances the granularity and accuracy of\nfinancial text-based performance prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moving targets -- managers' strategic shifting of key performance metrics\nwhen the original targets become difficult to achieve -- have been shown to\npredict subsequent stock underperformance. However, our work reveals that the\nmethod employed in that study exhibits two key limitations that hinder the\naccuracy -- noise in the extracted targets and loss of contextual information\n-- both of which stem primarily from the use of a named entity recognition\n(NER). To address these two limitations, we propose an LLM-based target\nextraction} method with a newly defined metric that better captures semantic\ncontext. This approach preserves semantic context beyond simple entity\nrecognition and yields consistently higher predictive power than the original\napproach. Overall, our approach enhances the granularity and accuracy of\nfinancial text-based performance prediction."
                },
                "authors": [
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Jihoon Kwon"
                    },
                    {
                        "name": "Minjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Minjae Kim"
                },
                "author": "Minjae Kim",
                "arxiv_comment": "8 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03194v1",
                "updated": "2025-10-03T17:30:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    30,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:30:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    30,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoDA: Agentic Systems for Collaborative Data Visualization"
                },
                "summary": "Deep research has revolutionized data analysis, yet data scientists still\ndevote substantial time to manually crafting visualizations, highlighting the\nneed for robust automation from natural language queries. However, current\nsystems struggle with complex datasets containing multiple files and iterative\nrefinement. Existing approaches, including simple single- or multi-agent\nsystems, often oversimplify the task, focusing on initial query parsing while\nfailing to robustly manage data complexity, code errors, or final visualization\nquality. In this paper, we reframe this challenge as a collaborative\nmulti-agent problem. We introduce CoDA, a multi-agent system that employs\nspecialized LLM agents for metadata analysis, task planning, code generation,\nand self-reflection. We formalize this pipeline, demonstrating how\nmetadata-focused analysis bypasses token limits and quality-driven refinement\nensures robustness. Extensive evaluations show CoDA achieves substantial gains\nin the overall score, outperforming competitive baselines by up to 41.5%. This\nwork demonstrates that the future of visualization automation lies not in\nisolated code generation but in integrated, collaborative agentic workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep research has revolutionized data analysis, yet data scientists still\ndevote substantial time to manually crafting visualizations, highlighting the\nneed for robust automation from natural language queries. However, current\nsystems struggle with complex datasets containing multiple files and iterative\nrefinement. Existing approaches, including simple single- or multi-agent\nsystems, often oversimplify the task, focusing on initial query parsing while\nfailing to robustly manage data complexity, code errors, or final visualization\nquality. In this paper, we reframe this challenge as a collaborative\nmulti-agent problem. We introduce CoDA, a multi-agent system that employs\nspecialized LLM agents for metadata analysis, task planning, code generation,\nand self-reflection. We formalize this pipeline, demonstrating how\nmetadata-focused analysis bypasses token limits and quality-driven refinement\nensures robustness. Extensive evaluations show CoDA achieves substantial gains\nin the overall score, outperforming competitive baselines by up to 41.5%. This\nwork demonstrates that the future of visualization automation lies not in\nisolated code generation but in integrated, collaborative agentic workflows."
                },
                "authors": [
                    {
                        "name": "Zichen Chen"
                    },
                    {
                        "name": "Jiefeng Chen"
                    },
                    {
                        "name": "Sercan . Arik"
                    },
                    {
                        "name": "Misha Sra"
                    },
                    {
                        "name": "Tomas Pfister"
                    },
                    {
                        "name": "Jinsung Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Jinsung Yoon"
                },
                "author": "Jinsung Yoon",
                "arxiv_comment": "31 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24015v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24015v3",
                "updated": "2025-10-03T17:26:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    26,
                    56,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-30T16:19:38Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    19,
                    38,
                    0,
                    181,
                    0
                ],
                "title": "Hierarchical Knowledge Injection for Improving LLM-based Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Knowledge Injection for Improving LLM-based Program Repair"
                },
                "summary": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Esteban Parra"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "arxiv_comment": "Accepted at IEEE/ACM Automated Software Engineering (ASE) 2025\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24015v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24015v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01903v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01903v3",
                "updated": "2025-10-03T17:11:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    11,
                    21,
                    4,
                    276,
                    0
                ],
                "published": "2024-04-02T12:44:44Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    12,
                    44,
                    44,
                    1,
                    93,
                    0
                ],
                "title": "Understanding How CodeLLMs (Mis)Predict Types with Activation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding How CodeLLMs (Mis)Predict Types with Activation Steering"
                },
                "summary": "Large Language Models (LLMs) are widely used by software engineers for\nprogramming tasks. However, research shows that LLMs often lack a deep\nunderstanding of program semantics. Even minor changes to syntax, such as\nrenaming variables, can significantly degrade performance across various tasks.\nIn this work, we examine the task of type prediction: given a partially typed\nprogram, can a model predict a missing type annotations such that the resulting\nprogram is more typed? We construct a dataset of adversarial examples where\nmodels initially predict the correct types, but begin to fail after\nsemantically irrelevant edits. This is problematic, as models should ideally\ngeneralize across different syntactic forms of semantically equivalent code.\nThis lack of robustness suggests that models may have a shallow understanding\nof code semantics. Despite this, we provide evidence that LLMs do, in fact,\nlearn robust mechanisms for type prediction-though these mechanisms often fail\nto activate in adversarial scenarios. By using activation steering, a method\nthat manipulates a model's internal activations to guide it toward using latent\nknowledge, we restore accurate predictions on adversarial inputs. We show that\nsteering successfully activates a type prediction mechanism that is shared by\nboth Python and TypeScript, and is more effective than prompting with\nin-context examples. Across five different models, our comprehensive evaluation\ndemonstrates that LLMs can learn generalizable representations of code\nsemantics that transfer across programming languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used by software engineers for\nprogramming tasks. However, research shows that LLMs often lack a deep\nunderstanding of program semantics. Even minor changes to syntax, such as\nrenaming variables, can significantly degrade performance across various tasks.\nIn this work, we examine the task of type prediction: given a partially typed\nprogram, can a model predict a missing type annotations such that the resulting\nprogram is more typed? We construct a dataset of adversarial examples where\nmodels initially predict the correct types, but begin to fail after\nsemantically irrelevant edits. This is problematic, as models should ideally\ngeneralize across different syntactic forms of semantically equivalent code.\nThis lack of robustness suggests that models may have a shallow understanding\nof code semantics. Despite this, we provide evidence that LLMs do, in fact,\nlearn robust mechanisms for type prediction-though these mechanisms often fail\nto activate in adversarial scenarios. By using activation steering, a method\nthat manipulates a model's internal activations to guide it toward using latent\nknowledge, we restore accurate predictions on adversarial inputs. We show that\nsteering successfully activates a type prediction mechanism that is shared by\nboth Python and TypeScript, and is more effective than prompting with\nin-context examples. Across five different models, our comprehensive evaluation\ndemonstrates that LLMs can learn generalizable representations of code\nsemantics that transfer across programming languages."
                },
                "authors": [
                    {
                        "name": "Francesca Lucchetti"
                    },
                    {
                        "name": "Arjun Guha"
                    }
                ],
                "author_detail": {
                    "name": "Arjun Guha"
                },
                "author": "Arjun Guha",
                "arxiv_comment": "40 pages, 67 figures. To be published at BlackBoxNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01903v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01903v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03185v1",
                "updated": "2025-10-03T17:09:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    9,
                    3,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:09:03Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    9,
                    3,
                    4,
                    276,
                    0
                ],
                "title": "PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning"
                },
                "summary": "Benchmarks for competition-style reasoning have advanced evaluation in\nmathematics and programming, yet physics remains comparatively explored. Most\nexisting physics benchmarks evaluate only final answers, which fail to capture\nreasoning processes, while recent stepwise methods rely on heuristic\nLLM-as-judge scoring or restrictive linear assumptions, limiting reliability\nand diagnostic validity. We introduce PRISM-Physics, a process-level evaluation\nframework and benchmark for complex physics reasoning problems. Solutions are\nrepresented as directed acyclic graphs (DAGs) of formulas, explicitly encoding\ncausal dependencies among intermediate steps to enable fine-grained,\ninterpretable, and theoretically grounded scoring. We prove the optimality of\nthe DAG representation and the corresponding scoring policy. Combining with a\nfully rule-based method for symbolic formula equivalence matching that we\ndeveloped, we ensure consistent validation across diverse formulations without\nheuristic judgments. Results show that our evaluation framework is more aligned\nwith human experts' scoring. Experiments on state-of-the-art LLMs reveal\npersistent reasoning failures in physics, while step-level scoring offers both\ndiagnostic insight and rich signals for later training. By combining structural\nrigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides\na principled foundation for advancing process-level evaluation and guiding the\ndevelopment of models with deeper scientific reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks for competition-style reasoning have advanced evaluation in\nmathematics and programming, yet physics remains comparatively explored. Most\nexisting physics benchmarks evaluate only final answers, which fail to capture\nreasoning processes, while recent stepwise methods rely on heuristic\nLLM-as-judge scoring or restrictive linear assumptions, limiting reliability\nand diagnostic validity. We introduce PRISM-Physics, a process-level evaluation\nframework and benchmark for complex physics reasoning problems. Solutions are\nrepresented as directed acyclic graphs (DAGs) of formulas, explicitly encoding\ncausal dependencies among intermediate steps to enable fine-grained,\ninterpretable, and theoretically grounded scoring. We prove the optimality of\nthe DAG representation and the corresponding scoring policy. Combining with a\nfully rule-based method for symbolic formula equivalence matching that we\ndeveloped, we ensure consistent validation across diverse formulations without\nheuristic judgments. Results show that our evaluation framework is more aligned\nwith human experts' scoring. Experiments on state-of-the-art LLMs reveal\npersistent reasoning failures in physics, while step-level scoring offers both\ndiagnostic insight and rich signals for later training. By combining structural\nrigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides\na principled foundation for advancing process-level evaluation and guiding the\ndevelopment of models with deeper scientific reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Qinwei Ma"
                    },
                    {
                        "name": "Jingzhe Shi"
                    },
                    {
                        "name": "Shirley Wu"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Si-Yuan Chen"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Ludwig Schmidt"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01443v2",
                "updated": "2025-10-03T17:04:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    4,
                    35,
                    4,
                    276,
                    0
                ],
                "published": "2025-08-02T17:11:40Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    17,
                    11,
                    40,
                    5,
                    214,
                    0
                ],
                "title": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial\n  Perspective"
                },
                "summary": "There is a growing interest in leveraging multiple large language models\n(LLMs) for automated code optimization. However, industrial platforms deploying\nmultiple LLMs face a critical challenge: prompts optimized for one LLM often\nfail with others, requiring expensive model-specific prompt engineering. This\ncross-model prompt engineering bottleneck severely limits the practical\ndeployment of multi-LLM systems in production environments. We introduce\nMeta-Prompted Code Optimization (MPCO), a framework that automatically\ngenerates high-quality, task-specific prompts across diverse LLMs while\nmaintaining industrial efficiency requirements. MPCO leverages metaprompting to\ndynamically synthesize context-aware optimization prompts by integrating\nproject metadata, task requirements, and LLM-specific contexts. It is an\nessential part of the ARTEMIS code optimization platform for automated\nvalidation and scaling. Our comprehensive evaluation on five real-world\ncodebases with 366 hours of runtime benchmarking demonstrates MPCO's\neffectiveness: it achieves overall performance improvements up to 19.06% with\nthe best statistical rank across all systems compared to baseline methods.\nAnalysis shows that 96% of the top-performing optimizations stem from\nmeaningful edits. Through systematic ablation studies and meta-prompter\nsensitivity analysis, we identify that comprehensive context integration is\nessential for effective meta-prompting and that major LLMs can serve\neffectively as meta-prompters, providing actionable insights for industrial\npractitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in leveraging multiple large language models\n(LLMs) for automated code optimization. However, industrial platforms deploying\nmultiple LLMs face a critical challenge: prompts optimized for one LLM often\nfail with others, requiring expensive model-specific prompt engineering. This\ncross-model prompt engineering bottleneck severely limits the practical\ndeployment of multi-LLM systems in production environments. We introduce\nMeta-Prompted Code Optimization (MPCO), a framework that automatically\ngenerates high-quality, task-specific prompts across diverse LLMs while\nmaintaining industrial efficiency requirements. MPCO leverages metaprompting to\ndynamically synthesize context-aware optimization prompts by integrating\nproject metadata, task requirements, and LLM-specific contexts. It is an\nessential part of the ARTEMIS code optimization platform for automated\nvalidation and scaling. Our comprehensive evaluation on five real-world\ncodebases with 366 hours of runtime benchmarking demonstrates MPCO's\neffectiveness: it achieves overall performance improvements up to 19.06% with\nthe best statistical rank across all systems compared to baseline methods.\nAnalysis shows that 96% of the top-performing optimizations stem from\nmeaningful edits. Through systematic ablation studies and meta-prompter\nsensitivity analysis, we identify that comprehensive context integration is\nessential for effective meta-prompting and that major LLMs can serve\neffectively as meta-prompters, providing actionable insights for industrial\npractitioners."
                },
                "authors": [
                    {
                        "name": "Jingzhi Gong"
                    },
                    {
                        "name": "Rafail Giavrimis"
                    },
                    {
                        "name": "Paul Brookes"
                    },
                    {
                        "name": "Vardan Voskanyan"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Mari Ashiga"
                    },
                    {
                        "name": "Matthew Truscott"
                    },
                    {
                        "name": "Mike Basios"
                    },
                    {
                        "name": "Leslie Kanthan"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "arxiv_comment": "Accepted by ASE'25 Industry Showcase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03178v1",
                "updated": "2025-10-03T16:53:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    53,
                    13,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:53:13Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    53,
                    13,
                    4,
                    276,
                    0
                ],
                "title": "When Names Disappear: Revealing What LLMs Actually Understand About Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Names Disappear: Revealing What LLMs Actually Understand About Code"
                },
                "summary": "Large Language Models (LLMs) achieve strong results on code tasks, but how\nthey derive program meaning remains unclear. We argue that code communicates\nthrough two channels: structural semantics, which define formal behavior, and\nhuman-interpretable naming, which conveys intent. Removing the naming channel\nseverely degrades intent-level tasks such as summarization, where models\nregress to line-by-line descriptions. Surprisingly, we also observe consistent\nreductions on execution tasks that should depend only on structure, revealing\nthat current benchmarks reward memorization of naming patterns rather than\ngenuine semantic reasoning. To disentangle these effects, we introduce a suite\nof semantics-preserving obfuscations and show that they expose identifier\nleakage across both summarization and execution. Building on these insights, we\nrelease ClassEval-Obf, an obfuscation-enhanced benchmark that systematically\nsuppresses naming cues while preserving behavior. Our results demonstrate that\nClassEval-Obf reduces inflated performance gaps, weakens memorization\nshortcuts, and provides a more reliable basis for assessing LLMs' code\nunderstanding and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve strong results on code tasks, but how\nthey derive program meaning remains unclear. We argue that code communicates\nthrough two channels: structural semantics, which define formal behavior, and\nhuman-interpretable naming, which conveys intent. Removing the naming channel\nseverely degrades intent-level tasks such as summarization, where models\nregress to line-by-line descriptions. Surprisingly, we also observe consistent\nreductions on execution tasks that should depend only on structure, revealing\nthat current benchmarks reward memorization of naming patterns rather than\ngenuine semantic reasoning. To disentangle these effects, we introduce a suite\nof semantics-preserving obfuscations and show that they expose identifier\nleakage across both summarization and execution. Building on these insights, we\nrelease ClassEval-Obf, an obfuscation-enhanced benchmark that systematically\nsuppresses naming cues while preserving behavior. Our results demonstrate that\nClassEval-Obf reduces inflated performance gaps, weakens memorization\nshortcuts, and provides a more reliable basis for assessing LLMs' code\nunderstanding and generalization."
                },
                "authors": [
                    {
                        "name": "Cuong Chi Le"
                    },
                    {
                        "name": "Minh V. T. Pham"
                    },
                    {
                        "name": "Cuong Duc Van"
                    },
                    {
                        "name": "Hoang N. Phan"
                    },
                    {
                        "name": "Huy N. Phan"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Tien N. Nguyen"
                },
                "author": "Tien N. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14743v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14743v4",
                "updated": "2025-10-03T16:48:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    48,
                    34,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-20T07:25:00Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    25,
                    0,
                    1,
                    140,
                    0
                ],
                "title": "Exploring the Interior Structure and Mode of Tidal Heating in Enceladus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Interior Structure and Mode of Tidal Heating in Enceladus"
                },
                "summary": "Enceladus is among the most intriguing bodies in the solar system due to its\nastrobiological potential. Determining the extent and duration of habitability\n(i.e., sustained habitability) requires characterizing the interior properties\nand the level and distribution of tidal heating in Enceladus. Inferring the\nintensity of geophysical activity in the core has direct implications for the\npotential hydrothermal activity and supply of chemical species important for\nhabitability to the ocean. We build a statistical framework to constrain the\ninterior using estimates of libration, shape, heat flux, gravity, and total\nmass. We use this framework to examine the extent that geodetic measurements\ncan improve our understanding of the interior structure, with an emphasis on\npartitioning of dissipation between the shell and the core. We quantify\nplausible ranges of gravitational (k2) and displacement (h2, l2) tidal Love\nnumbers consistent with existing observations. We demonstrate that measuring k2\nalone can only constrain the total tidally dissipated energy, but not its\nradial distribution. However, measuring the amplitude and phase of h2 or l2\nfacilitates determining the extent of tidal dissipation in the shell and the\ncore. We provide the precisions required for measuring k2, h2, and l2 that\nenable distinguishing between the main tidal heating scenarios, i.e., in the\nshell versus the core. We also explore the effect of the structural\nheterogeneities of the shell on the tidal response. Lastly, we evaluate the\nefficacy of future geodetic measurements to constrain key interior properties\nessential to understand the present-day (instantaneous) and long-term\n(sustained) habitability at Enceladus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enceladus is among the most intriguing bodies in the solar system due to its\nastrobiological potential. Determining the extent and duration of habitability\n(i.e., sustained habitability) requires characterizing the interior properties\nand the level and distribution of tidal heating in Enceladus. Inferring the\nintensity of geophysical activity in the core has direct implications for the\npotential hydrothermal activity and supply of chemical species important for\nhabitability to the ocean. We build a statistical framework to constrain the\ninterior using estimates of libration, shape, heat flux, gravity, and total\nmass. We use this framework to examine the extent that geodetic measurements\ncan improve our understanding of the interior structure, with an emphasis on\npartitioning of dissipation between the shell and the core. We quantify\nplausible ranges of gravitational (k2) and displacement (h2, l2) tidal Love\nnumbers consistent with existing observations. We demonstrate that measuring k2\nalone can only constrain the total tidally dissipated energy, but not its\nradial distribution. However, measuring the amplitude and phase of h2 or l2\nfacilitates determining the extent of tidal dissipation in the shell and the\ncore. We provide the precisions required for measuring k2, h2, and l2 that\nenable distinguishing between the main tidal heating scenarios, i.e., in the\nshell versus the core. We also explore the effect of the structural\nheterogeneities of the shell on the tidal response. Lastly, we evaluate the\nefficacy of future geodetic measurements to constrain key interior properties\nessential to understand the present-day (instantaneous) and long-term\n(sustained) habitability at Enceladus."
                },
                "authors": [
                    {
                        "name": "Amirhossein Bagheri"
                    },
                    {
                        "name": "Mark Simons"
                    },
                    {
                        "name": "Ryan S. Park"
                    },
                    {
                        "name": "Alexander Berne"
                    },
                    {
                        "name": "Douglas Hemingway"
                    },
                    {
                        "name": "Mohit Melwani Daswani"
                    },
                    {
                        "name": "Steven D Vance"
                    }
                ],
                "author_detail": {
                    "name": "Steven D Vance"
                },
                "author": "Steven D Vance",
                "arxiv_doi": "10.3847/PSJ/ae0cab",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/PSJ/ae0cab",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.14743v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14743v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03174v1",
                "updated": "2025-10-03T16:48:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    48,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:48:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    48,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Topic Modeling as Long-Form Generation: Can Long-Context LLMs\n  revolutionize NTM via Zero-Shot Prompting?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic Modeling as Long-Form Generation: Can Long-Context LLMs\n  revolutionize NTM via Zero-Shot Prompting?"
                },
                "summary": "Traditional topic models such as neural topic models rely on inference and\ngeneration networks to learn latent topic distributions. This paper explores a\nnew paradigm for topic modeling in the era of large language models, framing TM\nas a long-form generation task whose definition is updated in this paradigm. We\npropose a simple but practical approach to implement LLM-based topic model\ntasks out of the box (sample a data subset, generate topics and representative\ntext with our prompt, text assignment with keyword match). We then investigate\nwhether the long-form generation paradigm can beat NTMs via zero-shot\nprompting. We conduct a systematic comparison between NTMs and LLMs in terms of\ntopic quality and empirically examine the claim that \"a majority of NTMs are\noutdated.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional topic models such as neural topic models rely on inference and\ngeneration networks to learn latent topic distributions. This paper explores a\nnew paradigm for topic modeling in the era of large language models, framing TM\nas a long-form generation task whose definition is updated in this paradigm. We\npropose a simple but practical approach to implement LLM-based topic model\ntasks out of the box (sample a data subset, generate topics and representative\ntext with our prompt, text assignment with keyword match). We then investigate\nwhether the long-form generation paradigm can beat NTMs via zero-shot\nprompting. We conduct a systematic comparison between NTMs and LLMs in terms of\ntopic quality and empirically examine the claim that \"a majority of NTMs are\noutdated.\""
                },
                "authors": [
                    {
                        "name": "Xuan Xu"
                    },
                    {
                        "name": "Haolun Li"
                    },
                    {
                        "name": "Zhongliang Yang"
                    },
                    {
                        "name": "Beilin Chu"
                    },
                    {
                        "name": "Jia Song"
                    },
                    {
                        "name": "Moxuan Xu"
                    },
                    {
                        "name": "Linna Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Linna Zhou"
                },
                "author": "Linna Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24827v2",
                "updated": "2025-10-03T16:46:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    46,
                    28,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-29T14:13:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    13,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "Putnam-like dataset summary: LLMs as mathematical competition\n  contestants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putnam-like dataset summary: LLMs as mathematical competition\n  contestants"
                },
                "summary": "In this paper we summarize the results of the Putnam-like benchmark published\nby Google DeepMind. This dataset consists of 96 original problems in the spirit\nof the Putnam Competition and 576 solutions of LLMs. We analyse the performance\nof models on this set of problems to verify their ability to solve problems\nfrom mathematical contests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we summarize the results of the Putnam-like benchmark published\nby Google DeepMind. This dataset consists of 96 original problems in the spirit\nof the Putnam Competition and 576 solutions of LLMs. We analyse the performance\nof models on this set of problems to verify their ability to solve problems\nfrom mathematical contests."
                },
                "authors": [
                    {
                        "name": "Bartosz Bieganowski"
                    },
                    {
                        "name": "Daniel Strzelecki"
                    },
                    {
                        "name": "Robert Skiba"
                    },
                    {
                        "name": "Mateusz Topolewski"
                    }
                ],
                "author_detail": {
                    "name": "Mateusz Topolewski"
                },
                "author": "Mateusz Topolewski",
                "arxiv_comment": "11 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03160v1",
                "updated": "2025-10-03T16:32:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    32,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:32:02Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    32,
                    2,
                    4,
                    276,
                    0
                ],
                "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus"
                },
                "summary": "Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs."
                },
                "authors": [
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Wenhui Dong"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Zhonghao Zhang"
                    },
                    {
                        "name": "Zian Zhou"
                    },
                    {
                        "name": "Yunzhi Guan"
                    },
                    {
                        "name": "Liukun Xu"
                    },
                    {
                        "name": "Wei Peng"
                    },
                    {
                        "name": "Zhaoyang Gong"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Dachuan Li"
                    },
                    {
                        "name": "Xiaosheng Ma"
                    },
                    {
                        "name": "Yuli Ma"
                    },
                    {
                        "name": "Jianing Ni"
                    },
                    {
                        "name": "Changjiang Jiang"
                    },
                    {
                        "name": "Lixia Tian"
                    },
                    {
                        "name": "Qixin Chen"
                    },
                    {
                        "name": "Kaishun Xia"
                    },
                    {
                        "name": "Pingping Liu"
                    },
                    {
                        "name": "Tongshun Zhang"
                    },
                    {
                        "name": "Zhiqiang Liu"
                    },
                    {
                        "name": "Zhongan Bi"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Tiansheng Sun"
                    },
                    {
                        "name": "Caifeng Shan"
                    }
                ],
                "author_detail": {
                    "name": "Caifeng Shan"
                },
                "author": "Caifeng Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03153v1",
                "updated": "2025-10-03T16:25:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    25,
                    48,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:25:48Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    25,
                    48,
                    4,
                    276,
                    0
                ],
                "title": "Improving Cooperation in Collaborative Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Cooperation in Collaborative Embodied AI"
                },
                "summary": "The integration of Large Language Models (LLMs) into multiagent systems has\nopened new possibilities for collaborative reasoning and cooperation with AI\nagents. This paper explores different prompting methods and evaluates their\neffectiveness in enhancing agent collaborative behaviour and decision-making.\nWe enhance CoELA, a framework designed for building Collaborative Embodied\nAgents that leverage LLMs for multi-agent communication, reasoning, and task\ncoordination in shared virtual spaces. Through systematic experimentation, we\nexamine different LLMs and prompt engineering strategies to identify optimised\ncombinations that maximise collaboration performance. Furthermore, we extend\nour research by integrating speech capabilities, enabling seamless\ncollaborative voice-based interactions. Our findings highlight the\neffectiveness of prompt optimisation in enhancing collaborative agent\nperformance; for example, our best combination improved the efficiency of the\nsystem running with Gemma3 by 22% compared to the original CoELA system. In\naddition, the speech integration provides a more engaging user interface for\niterative system development and demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into multiagent systems has\nopened new possibilities for collaborative reasoning and cooperation with AI\nagents. This paper explores different prompting methods and evaluates their\neffectiveness in enhancing agent collaborative behaviour and decision-making.\nWe enhance CoELA, a framework designed for building Collaborative Embodied\nAgents that leverage LLMs for multi-agent communication, reasoning, and task\ncoordination in shared virtual spaces. Through systematic experimentation, we\nexamine different LLMs and prompt engineering strategies to identify optimised\ncombinations that maximise collaboration performance. Furthermore, we extend\nour research by integrating speech capabilities, enabling seamless\ncollaborative voice-based interactions. Our findings highlight the\neffectiveness of prompt optimisation in enhancing collaborative agent\nperformance; for example, our best combination improved the efficiency of the\nsystem running with Gemma3 by 22% compared to the original CoELA system. In\naddition, the speech integration provides a more engaging user interface for\niterative system development and demonstrations."
                },
                "authors": [
                    {
                        "name": "Hima Jacob Leven Suprabha"
                    },
                    {
                        "name": "Laxmi Nag Laxminarayan Nagesh"
                    },
                    {
                        "name": "Ajith Nair"
                    },
                    {
                        "name": "Alvin Reuben Amal Selvaster"
                    },
                    {
                        "name": "Ayan Khan"
                    },
                    {
                        "name": "Raghuram Damarla"
                    },
                    {
                        "name": "Sanju Hannah Samuel"
                    },
                    {
                        "name": "Sreenithi Saravana Perumal"
                    },
                    {
                        "name": "Titouan Puech"
                    },
                    {
                        "name": "Venkataramireddy Marella"
                    },
                    {
                        "name": "Vishal Sonar"
                    },
                    {
                        "name": "Alessandro Suglia"
                    },
                    {
                        "name": "Oliver Lemon"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Lemon"
                },
                "author": "Oliver Lemon",
                "arxiv_comment": "In proceedings of UKCI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03151v1",
                "updated": "2025-10-03T16:24:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    24,
                    50,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:24:50Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    24,
                    50,
                    4,
                    276,
                    0
                ],
                "title": "Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory\n  Perspective"
                },
                "summary": "This paper uses classical high-rate quantization theory to provide new\ninsights into mixture-of-experts (MoE) models for regression tasks. Our MoE is\ndefined by a segmentation of the input space to regions, each with a\nsingle-parameter expert that acts as a constant predictor with zero-compute at\ninference. Motivated by high-rate quantization theory assumptions, we assume\nthat the number of experts is sufficiently large to make their input-space\nregions very small. This lets us to study the approximation error of our MoE\nmodel class: (i) for one-dimensional inputs, we formulate the test error and\nits minimizing segmentation and experts; (ii) for multidimensional inputs, we\nformulate an upper bound for the test error and study its minimization.\nMoreover, we consider the learning of the expert parameters from a training\ndataset, given an input-space segmentation, and formulate their statistical\nlearning properties. This leads us to theoretically and empirically show how\nthe tradeoff between approximation and estimation errors in MoE learning\ndepends on the number of experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper uses classical high-rate quantization theory to provide new\ninsights into mixture-of-experts (MoE) models for regression tasks. Our MoE is\ndefined by a segmentation of the input space to regions, each with a\nsingle-parameter expert that acts as a constant predictor with zero-compute at\ninference. Motivated by high-rate quantization theory assumptions, we assume\nthat the number of experts is sufficiently large to make their input-space\nregions very small. This lets us to study the approximation error of our MoE\nmodel class: (i) for one-dimensional inputs, we formulate the test error and\nits minimizing segmentation and experts; (ii) for multidimensional inputs, we\nformulate an upper bound for the test error and study its minimization.\nMoreover, we consider the learning of the expert parameters from a training\ndataset, given an input-space segmentation, and formulate their statistical\nlearning properties. This leads us to theoretically and empirically show how\nthe tradeoff between approximation and estimation errors in MoE learning\ndepends on the number of experts."
                },
                "authors": [
                    {
                        "name": "Yehuda Dar"
                    }
                ],
                "author_detail": {
                    "name": "Yehuda Dar"
                },
                "author": "Yehuda Dar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09901v2",
                "updated": "2025-10-03T16:12:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    12,
                    51,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-15T02:09:18Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    2,
                    9,
                    18,
                    3,
                    135,
                    0
                ],
                "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans:\n  Insights from Standard Multi-armed Bandit Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Exploration-Exploitation Strategies of LLMs and Humans:\n  Insights from Standard Multi-armed Bandit Experiments"
                },
                "summary": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making settings. A natural\nquestion is then whether LLMs exhibit similar decision-making behavior to\nhumans, and can achieve comparable (or superior) performance. In this work, we\nfocus on the exploration-exploitation (E&E) tradeoff, a fundamental aspect of\ndynamic decision-making under uncertainty. We employ canonical multi-armed\nbandit (MAB) experiments introduced in the cognitive science and psychiatry\nliterature to conduct a comparative study of the E&E strategies of LLMs,\nhumans, and MAB algorithms. We use interpretable choice models to capture the\nE&E strategies of the agents and investigate how enabling thinking traces,\nthrough both prompting strategies and thinking models, shapes LLM\ndecision-making. We find that enabling thinking in LLMs shifts their behavior\ntoward more human-like behavior, characterized by a mix of random and directed\nexploration. In a simple stationary setting, thinking-enabled LLMs exhibit\nsimilar levels of random and directed exploration compared to humans. However,\nin more complex, non-stationary environments, LLMs struggle to match human\nadaptability, particularly in effective directed exploration, despite achieving\nsimilar regret in certain scenarios. Our findings highlight both the promise\nand limits of LLMs as simulators of human behavior and tools for automated\ndecision-making and point to potential areas for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making settings. A natural\nquestion is then whether LLMs exhibit similar decision-making behavior to\nhumans, and can achieve comparable (or superior) performance. In this work, we\nfocus on the exploration-exploitation (E&E) tradeoff, a fundamental aspect of\ndynamic decision-making under uncertainty. We employ canonical multi-armed\nbandit (MAB) experiments introduced in the cognitive science and psychiatry\nliterature to conduct a comparative study of the E&E strategies of LLMs,\nhumans, and MAB algorithms. We use interpretable choice models to capture the\nE&E strategies of the agents and investigate how enabling thinking traces,\nthrough both prompting strategies and thinking models, shapes LLM\ndecision-making. We find that enabling thinking in LLMs shifts their behavior\ntoward more human-like behavior, characterized by a mix of random and directed\nexploration. In a simple stationary setting, thinking-enabled LLMs exhibit\nsimilar levels of random and directed exploration compared to humans. However,\nin more complex, non-stationary environments, LLMs struggle to match human\nadaptability, particularly in effective directed exploration, despite achieving\nsimilar regret in certain scenarios. Our findings highlight both the promise\nand limits of LLMs as simulators of human behavior and tools for automated\ndecision-making and point to potential areas for improvement."
                },
                "authors": [
                    {
                        "name": "Ziyuan Zhang"
                    },
                    {
                        "name": "Darcy Wang"
                    },
                    {
                        "name": "Ningyuan Chen"
                    },
                    {
                        "name": "Rodrigo Mansur"
                    },
                    {
                        "name": "Vahid Sarhangian"
                    }
                ],
                "author_detail": {
                    "name": "Vahid Sarhangian"
                },
                "author": "Vahid Sarhangian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03141v1",
                "updated": "2025-10-03T16:11:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    11,
                    36,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:11:36Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    11,
                    36,
                    4,
                    276,
                    0
                ],
                "title": "Nonmodal growth and optimal perturbations in magnetohydrodynamic shear\n  flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonmodal growth and optimal perturbations in magnetohydrodynamic shear\n  flows"
                },
                "summary": "In astrophysical shear flows, the Kelvin-Helmholtz (KH) instability is\ngenerally suppressed by magnetic tension provided a sufficiently strong\nstreamwise magnetic field. This is often used to infer upper (or lower) bounds\non field strengths in systems where shear-driven fluctuations are (or are not)\nobserved, on the basis that fluctuations cannot grow in the absence of linear\ninstability. On the contrary, by calculating the maximum growth that\nsmall-amplitude perturbations can achieve in finite time for such a system, we\nshow that perturbations can grow in energy by orders of magnitude even when the\nflow is sub-Alfv\\'enic, suggesting that shear-driven turbulence is possible\neven in the presence of strong magnetic fields, and challenging inferences from\nthe observed presence or absence of shear-driven fluctuations. We further show\nthat magnetic fields introduce additional nonmodal growth mechanisms relative\nto the hydrodynamic case, and that 2D simulations miss key aspects of these\ngrowth mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In astrophysical shear flows, the Kelvin-Helmholtz (KH) instability is\ngenerally suppressed by magnetic tension provided a sufficiently strong\nstreamwise magnetic field. This is often used to infer upper (or lower) bounds\non field strengths in systems where shear-driven fluctuations are (or are not)\nobserved, on the basis that fluctuations cannot grow in the absence of linear\ninstability. On the contrary, by calculating the maximum growth that\nsmall-amplitude perturbations can achieve in finite time for such a system, we\nshow that perturbations can grow in energy by orders of magnitude even when the\nflow is sub-Alfv\\'enic, suggesting that shear-driven turbulence is possible\neven in the presence of strong magnetic fields, and challenging inferences from\nthe observed presence or absence of shear-driven fluctuations. We further show\nthat magnetic fields introduce additional nonmodal growth mechanisms relative\nto the hydrodynamic case, and that 2D simulations miss key aspects of these\ngrowth mechanisms."
                },
                "authors": [
                    {
                        "name": "Adrian E. Fraser"
                    },
                    {
                        "name": "Alexis K. Kaminski"
                    },
                    {
                        "name": "Jeffrey S. Oishi"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey S. Oishi"
                },
                "author": "Jeffrey S. Oishi",
                "arxiv_comment": "8 pages, 2 figures, submitted to PRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03136v1",
                "updated": "2025-10-03T16:07:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    7,
                    15,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:07:15Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    7,
                    15,
                    4,
                    276,
                    0
                ],
                "title": "Beyond the Final Layer: Intermediate Representations for Better\n  Multilingual Calibration in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Final Layer: Intermediate Representations for Better\n  Multilingual Calibration in Large Language Models"
                },
                "summary": "Confidence calibration, the alignment of a model's predicted confidence with\nits actual accuracy, is crucial for the reliable deployment of Large Language\nModels (LLMs). However, this critical property remains largely under-explored\nin multilingual contexts. In this work, we conduct the first large-scale,\nsystematic studies of multilingual calibration across six model families and\nover 100 languages, revealing that non-English languages suffer from\nsystematically worse calibration. To diagnose this, we investigate the model's\ninternal representations and find that the final layer, biased by\nEnglish-centric training, provides a poor signal for multilingual confidence.\nIn contrast, our layer-wise analysis uncovers a key insight that\nlate-intermediate layers consistently offer a more reliable and\nbetter-calibrated signal. Building on this, we introduce a suite of\ntraining-free methods, including Language-Aware Confidence Ensemble (LACE),\nwhich adaptively selects an optimal ensemble of layers for each specific\nlanguage. Our study highlights the hidden costs of English-centric alignment\nand offer a new path toward building more globally equitable and trustworthy\nLLMs by looking beyond the final layer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence calibration, the alignment of a model's predicted confidence with\nits actual accuracy, is crucial for the reliable deployment of Large Language\nModels (LLMs). However, this critical property remains largely under-explored\nin multilingual contexts. In this work, we conduct the first large-scale,\nsystematic studies of multilingual calibration across six model families and\nover 100 languages, revealing that non-English languages suffer from\nsystematically worse calibration. To diagnose this, we investigate the model's\ninternal representations and find that the final layer, biased by\nEnglish-centric training, provides a poor signal for multilingual confidence.\nIn contrast, our layer-wise analysis uncovers a key insight that\nlate-intermediate layers consistently offer a more reliable and\nbetter-calibrated signal. Building on this, we introduce a suite of\ntraining-free methods, including Language-Aware Confidence Ensemble (LACE),\nwhich adaptively selects an optimal ensemble of layers for each specific\nlanguage. Our study highlights the hidden costs of English-centric alignment\nand offer a new path toward building more globally equitable and trustworthy\nLLMs by looking beyond the final layer."
                },
                "authors": [
                    {
                        "name": "Ej Zhou"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Chengzu Li"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Ivan Vuli"
                    },
                    {
                        "name": "Anna Korhonen"
                    }
                ],
                "author_detail": {
                    "name": "Anna Korhonen"
                },
                "author": "Anna Korhonen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01784v2",
                "updated": "2025-10-03T16:01:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    1,
                    28,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T08:22:46Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    8,
                    22,
                    46,
                    3,
                    275,
                    0
                ],
                "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pack and Force Your Memory: Long-form and Consistent Video Generation"
                },
                "summary": "Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video generation presents a dual challenge: models must capture\nlong-range dependencies while preventing the error accumulation inherent in\nautoregressive decoding. To address these challenges, we make two\ncontributions. First, for dynamic context modeling, we propose MemoryPack, a\nlearnable context-retrieval mechanism that leverages both textual and image\ninformation as global guidance to jointly model short- and long-term\ndependencies, achieving minute-level temporal consistency. This design scales\ngracefully with video length, preserves computational efficiency, and maintains\nlinear complexity. Second, to mitigate error accumulation, we introduce Direct\nForcing, an efficient single-step approximating strategy that improves\ntraining-inference alignment and thereby curtails error propagation during\ninference. Together, MemoryPack and Direct Forcing substantially enhance the\ncontext consistency and reliability of long-form video generation, advancing\nthe practical usability of autoregressive video models."
                },
                "authors": [
                    {
                        "name": "Xiaofei Wu"
                    },
                    {
                        "name": "Guozhen Zhang"
                    },
                    {
                        "name": "Zhiyong Xu"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Qinglin Lu"
                    },
                    {
                        "name": "Xuming He"
                    }
                ],
                "author_detail": {
                    "name": "Xuming He"
                },
                "author": "Xuming He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.08854v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.08854v5",
                "updated": "2025-10-03T16:00:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    0,
                    52,
                    4,
                    276,
                    0
                ],
                "published": "2023-02-17T12:53:15Z",
                "published_parsed": [
                    2023,
                    2,
                    17,
                    12,
                    53,
                    15,
                    4,
                    48,
                    0
                ],
                "title": "Post Reinforcement Learning Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post Reinforcement Learning Inference"
                },
                "summary": "We study estimation and inference using data collected by reinforcement\nlearning (RL) algorithms. These algorithms adaptively experiment by interacting\nwith individual units over multiple stages, updating their strategies based on\npast outcomes. Our goal is to evaluate a counterfactual policy after data\ncollection and estimate structural parameters, such as dynamic treatment\neffects, that support credit assignment and quantify the impact of early\nactions on final outcomes. These parameters can often be defined as solutions\nto moment equations, motivating moment-based estimation methods developed for\nstatic data. In RL settings, however, data are often collected adaptively under\nnonstationary behavior policies. As a result, standard estimators fail to\nachieve asymptotic normality due to time-varying variance. We propose a\nweighted generalized method of moments (GMM) approach that uses adaptive\nweights to stabilize this variance. We characterize weighting schemes that\nensure consistency and asymptotic normality of the weighted GMM estimators,\nenabling valid hypothesis testing and uniform confidence region construction.\nKey applications include dynamic treatment effect estimation and dynamic\noff-policy evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study estimation and inference using data collected by reinforcement\nlearning (RL) algorithms. These algorithms adaptively experiment by interacting\nwith individual units over multiple stages, updating their strategies based on\npast outcomes. Our goal is to evaluate a counterfactual policy after data\ncollection and estimate structural parameters, such as dynamic treatment\neffects, that support credit assignment and quantify the impact of early\nactions on final outcomes. These parameters can often be defined as solutions\nto moment equations, motivating moment-based estimation methods developed for\nstatic data. In RL settings, however, data are often collected adaptively under\nnonstationary behavior policies. As a result, standard estimators fail to\nachieve asymptotic normality due to time-varying variance. We propose a\nweighted generalized method of moments (GMM) approach that uses adaptive\nweights to stabilize this variance. We characterize weighting schemes that\nensure consistency and asymptotic normality of the weighted GMM estimators,\nenabling valid hypothesis testing and uniform confidence region construction.\nKey applications include dynamic treatment effect estimation and dynamic\noff-policy evaluation."
                },
                "authors": [
                    {
                        "name": "Vasilis Syrgkanis"
                    },
                    {
                        "name": "Ruohan Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Ruohan Zhan"
                },
                "author": "Ruohan Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.08854v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.08854v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07052v2",
                "updated": "2025-10-03T15:56:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    56,
                    43,
                    4,
                    276,
                    0
                ],
                "published": "2025-04-09T17:12:49Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    12,
                    49,
                    2,
                    99,
                    0
                ],
                "title": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model\n  Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved their reasoning abilities, particularly through techniques involving\nsearch and backtracking. Backtracking naturally scales test-time compute by\nenabling sequential, linearized exploration via long chain-of-thought (CoT)\ngeneration. However, this is not the only strategy for scaling test\ntime-compute: parallel sampling with best-of-N selection provides an\nalternative that generates diverse solutions simultaneously. Despite the\ngrowing adoption of sequential search, its advantages over parallel\nsampling-especially under a fixed compute budget-remain poorly understood. In\nthis paper, we systematically compare these two approaches on two challenging\nreasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential\nsearch underperforms parallel sampling on CountDown but outperforms it on\nSudoku, suggesting that backtracking is not universally beneficial. We identify\ntwo factors that can cause backtracking to degrade performance: (1) training on\nfixed search traces can lock models intro suboptimal strategies, and (2)\nexplicit CoT supervision can discourage implicit (non verbalized) reasoning.\nExtending our analysis to reinforcement learning (RL), we show that models with\nbacktracking capabilities benefit significantly from RL fine-tuning, while\nmodels without backtracking see limited, mixed gains. Together, these findings\nchallenge the assumption that backtracking universally enhances LLM reasoning,\ninstead revealing a complex interaction between task structure, training data,\nmodel scale, and learning paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nimproved their reasoning abilities, particularly through techniques involving\nsearch and backtracking. Backtracking naturally scales test-time compute by\nenabling sequential, linearized exploration via long chain-of-thought (CoT)\ngeneration. However, this is not the only strategy for scaling test\ntime-compute: parallel sampling with best-of-N selection provides an\nalternative that generates diverse solutions simultaneously. Despite the\ngrowing adoption of sequential search, its advantages over parallel\nsampling-especially under a fixed compute budget-remain poorly understood. In\nthis paper, we systematically compare these two approaches on two challenging\nreasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential\nsearch underperforms parallel sampling on CountDown but outperforms it on\nSudoku, suggesting that backtracking is not universally beneficial. We identify\ntwo factors that can cause backtracking to degrade performance: (1) training on\nfixed search traces can lock models intro suboptimal strategies, and (2)\nexplicit CoT supervision can discourage implicit (non verbalized) reasoning.\nExtending our analysis to reinforcement learning (RL), we show that models with\nbacktracking capabilities benefit significantly from RL fine-tuning, while\nmodels without backtracking see limited, mixed gains. Together, these findings\nchallenge the assumption that backtracking universally enhances LLM reasoning,\ninstead revealing a complex interaction between task structure, training data,\nmodel scale, and learning paradigm."
                },
                "authors": [
                    {
                        "name": "Tian Qin"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    },
                    {
                        "name": "Samy Jelassi"
                    },
                    {
                        "name": "Eran Malach"
                    }
                ],
                "author_detail": {
                    "name": "Eran Malach"
                },
                "author": "Eran Malach",
                "arxiv_comment": "COLM 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03127v1",
                "updated": "2025-10-03T15:53:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    53,
                    28,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:53:28Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    53,
                    28,
                    4,
                    276,
                    0
                ],
                "title": "A Study of Rule Omission in Raven's Progressive Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of Rule Omission in Raven's Progressive Matrices"
                },
                "summary": "Analogical reasoning lies at the core of human cognition and remains a\nfundamental challenge for artificial intelligence. Raven's Progressive Matrices\n(RPM) serve as a widely used benchmark to assess abstract reasoning by\nrequiring the inference of underlying structural rules. While many vision-based\nand language-based models have achieved success on RPM tasks, it remains\nunclear whether their performance reflects genuine reasoning ability or\nreliance on statistical shortcuts. This study investigates the generalization\ncapacity of modern AI systems under conditions of incomplete training by\ndeliberately omitting several structural rules during training. Both\nsequence-to-sequence transformer models and vision-based architectures such as\nCoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN\n(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate\nstrong performance on familiar rules, their accuracy declines sharply when\nfaced with novel or omitted rules. Moreover, the gap between token-level\naccuracy and complete answer accuracy highlights fundamental limitations in\ncurrent approaches. These findings provide new insights into the reasoning\nmechanisms underlying deep learning models and underscore the need for\narchitectures that move beyond pattern recognition toward robust abstract\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogical reasoning lies at the core of human cognition and remains a\nfundamental challenge for artificial intelligence. Raven's Progressive Matrices\n(RPM) serve as a widely used benchmark to assess abstract reasoning by\nrequiring the inference of underlying structural rules. While many vision-based\nand language-based models have achieved success on RPM tasks, it remains\nunclear whether their performance reflects genuine reasoning ability or\nreliance on statistical shortcuts. This study investigates the generalization\ncapacity of modern AI systems under conditions of incomplete training by\ndeliberately omitting several structural rules during training. Both\nsequence-to-sequence transformer models and vision-based architectures such as\nCoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN\n(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate\nstrong performance on familiar rules, their accuracy declines sharply when\nfaced with novel or omitted rules. Moreover, the gap between token-level\naccuracy and complete answer accuracy highlights fundamental limitations in\ncurrent approaches. These findings provide new insights into the reasoning\nmechanisms underlying deep learning models and underscore the need for\narchitectures that move beyond pattern recognition toward robust abstract\nreasoning."
                },
                "authors": [
                    {
                        "name": "Binze Li"
                    }
                ],
                "author_detail": {
                    "name": "Binze Li"
                },
                "author": "Binze Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22811v2",
                "updated": "2025-10-03T15:53:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    53,
                    5,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-28T19:40:34Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    19,
                    40,
                    34,
                    2,
                    148,
                    0
                ],
                "title": "Highly Efficient and Effective LLMs with Multi-Boolean Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Highly Efficient and Effective LLMs with Multi-Boolean Architectures"
                },
                "summary": "Weight binarization has emerged as a promising strategy to reduce the\ncomplexity of large language models (LLMs). Existing approaches fall into\npost-training binarization, which is simple but causes severe performance loss,\nand training-aware methods, which depend on full-precision latent weights,\nadding complexity and limiting efficiency. We propose a novel framework that\nrepresents LLMs with multi-kernel Boolean parameters and, for the first time,\nenables direct finetuning LMMs in the Boolean domain, eliminating the need for\nlatent weights. This enhances representational capacity and dramatically\nreduces complexity during both finetuning and inference. Extensive experiments\nacross diverse LLMs show our method outperforms recent ultra low-bit\nquantization and binarization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight binarization has emerged as a promising strategy to reduce the\ncomplexity of large language models (LLMs). Existing approaches fall into\npost-training binarization, which is simple but causes severe performance loss,\nand training-aware methods, which depend on full-precision latent weights,\nadding complexity and limiting efficiency. We propose a novel framework that\nrepresents LLMs with multi-kernel Boolean parameters and, for the first time,\nenables direct finetuning LMMs in the Boolean domain, eliminating the need for\nlatent weights. This enhances representational capacity and dramatically\nreduces complexity during both finetuning and inference. Extensive experiments\nacross diverse LLMs show our method outperforms recent ultra low-bit\nquantization and binarization techniques."
                },
                "authors": [
                    {
                        "name": "Ba-Hien Tran"
                    },
                    {
                        "name": "Van Minh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Van Minh Nguyen"
                },
                "author": "Van Minh Nguyen",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03120v1",
                "updated": "2025-10-03T15:49:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    49,
                    9,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:49:09Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    49,
                    9,
                    4,
                    276,
                    0
                ],
                "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?"
                },
                "summary": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation)."
                },
                "authors": [
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Xuzhou Zhu"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v2",
                "updated": "2025-10-03T15:37:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    37,
                    19,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures; Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03109v1",
                "updated": "2025-10-03T15:36:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    36,
                    11,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:36:11Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    36,
                    11,
                    4,
                    276,
                    0
                ],
                "title": "Rates of Convergence of Generalised Variational Inference Posteriors\n  under Prior Misspecification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rates of Convergence of Generalised Variational Inference Posteriors\n  under Prior Misspecification"
                },
                "summary": "We prove rates of convergence and robustness to prior misspecification within\na Generalised Variational Inference (GVI) framework with bounded divergences.\nThis addresses a significant open challenge for GVI and Federated GVI that\nemploy a different divergence to the Kullback--Leibler under prior\nmisspecification, operate within a subset of possible probability measures, and\nresult in intractable posteriors. Our theoretical contributions cover severe\nprior misspecification while relying on our ability to restrict the space of\npossible GVI posterior measures, and infer properties based on this space. In\nparticular, we are able to establish sufficient conditions for existence and\nuniqueness of GVI posteriors on arbitrary Polish spaces, prove that the GVI\nposterior measure concentrates on a neighbourhood of loss minimisers, and\nextend this to rates of convergence regardless of the prior measure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove rates of convergence and robustness to prior misspecification within\na Generalised Variational Inference (GVI) framework with bounded divergences.\nThis addresses a significant open challenge for GVI and Federated GVI that\nemploy a different divergence to the Kullback--Leibler under prior\nmisspecification, operate within a subset of possible probability measures, and\nresult in intractable posteriors. Our theoretical contributions cover severe\nprior misspecification while relying on our ability to restrict the space of\npossible GVI posterior measures, and infer properties based on this space. In\nparticular, we are able to establish sufficient conditions for existence and\nuniqueness of GVI posteriors on arbitrary Polish spaces, prove that the GVI\nposterior measure concentrates on a neighbourhood of loss minimisers, and\nextend this to rates of convergence regardless of the prior measure."
                },
                "authors": [
                    {
                        "name": "Terje Mildner"
                    },
                    {
                        "name": "Paris Giampouras"
                    },
                    {
                        "name": "Theodoros Damoulas"
                    }
                ],
                "author_detail": {
                    "name": "Theodoros Damoulas"
                },
                "author": "Theodoros Damoulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00175v2",
                "updated": "2025-10-03T15:34:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    34,
                    53,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-30T18:51:16Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    18,
                    51,
                    16,
                    1,
                    273,
                    0
                ],
                "title": "Icy or Rocky? Convective or Stable? New interior models of Uranus and\n  Neptune",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Icy or Rocky? Convective or Stable? New interior models of Uranus and\n  Neptune"
                },
                "summary": "We present a new framework for constructing agnostic and yet physical models\nfor planetary interiors and apply it to Uranus and Neptune. Unlike previous\nresearch that either impose rigid assumptions or rely on simplified empirical\nprofiles, our approach bridges both paradigms. Starting from randomly generated\ndensity profiles, we applied an iterative algorithm that converges towards\nmodels that simultaneously satisfy hydrostatic equilibrium, match the observed\ngravitational moments, and remain thermodynamically and compositionally\nconsistent. The inferred interior models for Uranus and Neptune span a wide\nrange of possible interior structures, in particular encompassing both\nwater-dominated and rock-dominated configurations (rock-to-water mass ratios\nbetween 0.04-3.92 for Uranus and 0.20-1.78 for Neptune). All models contain\nconvective regions with ionic water and have temperature-pressure profiles that\nremain above the demixing curves for hydrogen-helium-water mixtures. This\noffers both a plausible explanation for the observed non-dipolar magnetic\nfields and indicates that no hydrogen-helium-water demixing occurs. We find a\nhigher H-He mass fraction in the outermost convection zones for Uranus\n(0.62-0.73) compared to Neptune (0.25-0.49) and that Uranus' magnetic field is\nlikely generated deeper in the interior compared to Neptune. We infer upper\nlimits of 0.69-0.74 (Uranus) versus 0.78-0.92 (Neptune) for the outer edges of\nthe dynamo regions in units of normalised radii. Overall, our findings\nchallenge the conventional classification of Uranus and Neptune as 'ice giants'\nand underscore the need for improved observational data or formation\nconstraints to break compositional degeneracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new framework for constructing agnostic and yet physical models\nfor planetary interiors and apply it to Uranus and Neptune. Unlike previous\nresearch that either impose rigid assumptions or rely on simplified empirical\nprofiles, our approach bridges both paradigms. Starting from randomly generated\ndensity profiles, we applied an iterative algorithm that converges towards\nmodels that simultaneously satisfy hydrostatic equilibrium, match the observed\ngravitational moments, and remain thermodynamically and compositionally\nconsistent. The inferred interior models for Uranus and Neptune span a wide\nrange of possible interior structures, in particular encompassing both\nwater-dominated and rock-dominated configurations (rock-to-water mass ratios\nbetween 0.04-3.92 for Uranus and 0.20-1.78 for Neptune). All models contain\nconvective regions with ionic water and have temperature-pressure profiles that\nremain above the demixing curves for hydrogen-helium-water mixtures. This\noffers both a plausible explanation for the observed non-dipolar magnetic\nfields and indicates that no hydrogen-helium-water demixing occurs. We find a\nhigher H-He mass fraction in the outermost convection zones for Uranus\n(0.62-0.73) compared to Neptune (0.25-0.49) and that Uranus' magnetic field is\nlikely generated deeper in the interior compared to Neptune. We infer upper\nlimits of 0.69-0.74 (Uranus) versus 0.78-0.92 (Neptune) for the outer edges of\nthe dynamo regions in units of normalised radii. Overall, our findings\nchallenge the conventional classification of Uranus and Neptune as 'ice giants'\nand underscore the need for improved observational data or formation\nconstraints to break compositional degeneracy."
                },
                "authors": [
                    {
                        "name": "Luca Morf"
                    },
                    {
                        "name": "Ravit Helled"
                    }
                ],
                "author_detail": {
                    "name": "Ravit Helled"
                },
                "author": "Ravit Helled",
                "arxiv_comment": "Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03102v1",
                "updated": "2025-10-03T15:31:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    31,
                    11,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:31:11Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    31,
                    11,
                    4,
                    276,
                    0
                ],
                "title": "Semantic Similarity in Radiology Reports via LLMs and NER",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Similarity in Radiology Reports via LLMs and NER"
                },
                "summary": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at:\n\\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\\_reports}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at:\n\\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\\_reports}"
                },
                "authors": [
                    {
                        "name": "Beth Pearson"
                    },
                    {
                        "name": "Ahmed Adnan"
                    },
                    {
                        "name": "Zahraa Abdallah"
                    }
                ],
                "author_detail": {
                    "name": "Zahraa Abdallah"
                },
                "author": "Zahraa Abdallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03095v1",
                "updated": "2025-10-03T15:25:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    25,
                    8,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:25:08Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    25,
                    8,
                    4,
                    276,
                    0
                ],
                "title": "Distilled Protein Backbone Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilled Protein Backbone Generation"
                },
                "summary": "Diffusion- and flow-based generative models have recently demonstrated strong\nperformance in protein backbone generation tasks, offering unprecedented\ncapabilities for de novo protein design. However, while achieving notable\nperformance in generation quality, these models are limited by their generating\nspeed, often requiring hundreds of iterative steps in the reverse-diffusion\nprocess. This computational bottleneck limits their practical utility in\nlarge-scale protein discovery, where thousands to millions of candidate\nstructures are needed. To address this challenge, we explore the techniques of\nscore distillation, which has shown great success in reducing the number of\nsampling steps in the vision domain while maintaining high generation quality.\nHowever, a straightforward adaptation of these methods results in unacceptably\nlow designability. Through extensive study, we have identified how to\nappropriately adapt Score identity Distillation (SiD), a state-of-the-art score\ndistillation strategy, to train few-step protein backbone generators which\nsignificantly reduce sampling time, while maintaining comparable performance to\ntheir pretrained teacher model. In particular, multistep generation combined\nwith inference time noise modulation is key to the success. We demonstrate that\nour distilled few-step generators achieve more than a 20-fold improvement in\nsampling speed, while achieving similar levels of designability, diversity, and\nnovelty as the Proteina teacher model. This reduction in inference cost enables\nlarge-scale in silico protein design, thereby bringing diffusion-based models\ncloser to real-world protein engineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion- and flow-based generative models have recently demonstrated strong\nperformance in protein backbone generation tasks, offering unprecedented\ncapabilities for de novo protein design. However, while achieving notable\nperformance in generation quality, these models are limited by their generating\nspeed, often requiring hundreds of iterative steps in the reverse-diffusion\nprocess. This computational bottleneck limits their practical utility in\nlarge-scale protein discovery, where thousands to millions of candidate\nstructures are needed. To address this challenge, we explore the techniques of\nscore distillation, which has shown great success in reducing the number of\nsampling steps in the vision domain while maintaining high generation quality.\nHowever, a straightforward adaptation of these methods results in unacceptably\nlow designability. Through extensive study, we have identified how to\nappropriately adapt Score identity Distillation (SiD), a state-of-the-art score\ndistillation strategy, to train few-step protein backbone generators which\nsignificantly reduce sampling time, while maintaining comparable performance to\ntheir pretrained teacher model. In particular, multistep generation combined\nwith inference time noise modulation is key to the success. We demonstrate that\nour distilled few-step generators achieve more than a 20-fold improvement in\nsampling speed, while achieving similar levels of designability, diversity, and\nnovelty as the Proteina teacher model. This reduction in inference cost enables\nlarge-scale in silico protein design, thereby bringing diffusion-based models\ncloser to real-world protein engineering applications."
                },
                "authors": [
                    {
                        "name": "Liyang Xie"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zhendong Wang"
                    },
                    {
                        "name": "Wesley Tansey"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Mingyuan Zhou"
                },
                "author": "Mingyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03093v1",
                "updated": "2025-10-03T15:23:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    23,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:23:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    23,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better\n  Scaling than CoT Prompting?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better\n  Scaling than CoT Prompting?"
                },
                "summary": "Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based\nmodels, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,\nwhere the model is guided to first transcribe the speech and then translate it.\nCoT typically outperforms direct prompting primarily because it can exploit\nabundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)\ndatasets to explicitly model its steps. In this paper, we systematically\ncompare CoT and Direct prompting under increasing amounts of S2TT data. To this\nend, we pseudo-label an ASR corpus by translating its transcriptions into six\nEuropean languages, and train LLM-based S2TT systems with both prompting\nstrategies at different data scales. Our results show that Direct improves more\nconsistently as the amount of data increases, suggesting that it may become a\nmore effective approach as larger S2TT resources are created.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based\nmodels, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,\nwhere the model is guided to first transcribe the speech and then translate it.\nCoT typically outperforms direct prompting primarily because it can exploit\nabundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)\ndatasets to explicitly model its steps. In this paper, we systematically\ncompare CoT and Direct prompting under increasing amounts of S2TT data. To this\nend, we pseudo-label an ASR corpus by translating its transcriptions into six\nEuropean languages, and train LLM-based S2TT systems with both prompting\nstrategies at different data scales. Our results show that Direct improves more\nconsistently as the amount of data increases, suggesting that it may become a\nmore effective approach as larger S2TT resources are created."
                },
                "authors": [
                    {
                        "name": "Oriol Pareras"
                    },
                    {
                        "name": "Gerard I. Gllego"
                    },
                    {
                        "name": "Federico Costa"
                    },
                    {
                        "name": "Cristina Espaa-Bonet"
                    },
                    {
                        "name": "Javier Hernando"
                    }
                ],
                "author_detail": {
                    "name": "Javier Hernando"
                },
                "author": "Javier Hernando",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06472v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06472v2",
                "updated": "2025-10-03T15:23:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    23,
                    19,
                    4,
                    276,
                    0
                ],
                "published": "2025-08-08T17:21:26Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    17,
                    21,
                    26,
                    4,
                    220,
                    0
                ],
                "title": "Revisiting the Gas Dynamics of Henize 2-10: Possible Drivers of the\n  Starburst",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Gas Dynamics of Henize 2-10: Possible Drivers of the\n  Starburst"
                },
                "summary": "The triggers of starburst episodes are a key component to our understanding\nof the baryon cycle in galaxies. Galaxy mergers are a commonly suggested\ncatalyst for starbursts, but once the galaxies coalesce into a single\nkinematically disturbed system, their merger history can be difficult to\nassess. This is particularly true for dwarf galaxies, which are expected to\ndominate the merger rate at all redshifts due to their large numbers. One such\ndwarf galaxy undergoing an enigmatic starburst episode is Henize 2-10, which\nappears to be isolated. Possible scenarios that might have caused the starburst\nepisode include a previous merger or stochastic processes within the galaxy\nitself, such as self-regulation via feedback processes. We present new VLA\n21-cm observations and unpublished archival CARMA CO data to investigate the\ndynamical state and star formation activity in the galaxy. We do not detect an\nHI tail consistent with the structure reported by Kobulnicky et al. (1995),\nwhich was suggested as evidence for a merger or interaction, but rather these\nnew observations indicate an extended HI distribution. We also find that the HI\nappears dynamically decoupled from an extended CO feature (inferred to be a\ntidal tail in previous work), suggesting large-scale dynamical processes of\nsome type are affecting the gas in this system. We provide a meta-analysis of\navailable results to enhance our understanding of what might be triggering the\nstarburst episode in Henize 2-10, and speculate that the large CO feature could\nbe falling into the galaxy and potentially trigger starburst activity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The triggers of starburst episodes are a key component to our understanding\nof the baryon cycle in galaxies. Galaxy mergers are a commonly suggested\ncatalyst for starbursts, but once the galaxies coalesce into a single\nkinematically disturbed system, their merger history can be difficult to\nassess. This is particularly true for dwarf galaxies, which are expected to\ndominate the merger rate at all redshifts due to their large numbers. One such\ndwarf galaxy undergoing an enigmatic starburst episode is Henize 2-10, which\nappears to be isolated. Possible scenarios that might have caused the starburst\nepisode include a previous merger or stochastic processes within the galaxy\nitself, such as self-regulation via feedback processes. We present new VLA\n21-cm observations and unpublished archival CARMA CO data to investigate the\ndynamical state and star formation activity in the galaxy. We do not detect an\nHI tail consistent with the structure reported by Kobulnicky et al. (1995),\nwhich was suggested as evidence for a merger or interaction, but rather these\nnew observations indicate an extended HI distribution. We also find that the HI\nappears dynamically decoupled from an extended CO feature (inferred to be a\ntidal tail in previous work), suggesting large-scale dynamical processes of\nsome type are affecting the gas in this system. We provide a meta-analysis of\navailable results to enhance our understanding of what might be triggering the\nstarburst episode in Henize 2-10, and speculate that the large CO feature could\nbe falling into the galaxy and potentially trigger starburst activity."
                },
                "authors": [
                    {
                        "name": "Josephine M. Dalsin"
                    },
                    {
                        "name": "Allison H. Costa"
                    },
                    {
                        "name": "Remy Indebetouw"
                    },
                    {
                        "name": "Kelsey E. Johnson"
                    },
                    {
                        "name": "Natalie O. Butterfield"
                    },
                    {
                        "name": "Sabrina Stierwalt"
                    }
                ],
                "author_detail": {
                    "name": "Sabrina Stierwalt"
                },
                "author": "Sabrina Stierwalt",
                "arxiv_doi": "10.3847/1538-4357/adf857",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adf857",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.06472v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06472v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2025 ApJ 992 44",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03086v1",
                "updated": "2025-10-03T15:17:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    17,
                    0,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:17:00Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    17,
                    0,
                    4,
                    276,
                    0
                ],
                "title": "Bootstrap Learning for Combinatorial Graph Alignment with Sequential\n  GNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrap Learning for Combinatorial Graph Alignment with Sequential\n  GNNs"
                },
                "summary": "Graph neural networks (GNNs) have struggled to outperform traditional\noptimization methods on combinatorial problems, limiting their practical\nimpact. We address this gap by introducing a novel chaining procedure for the\ngraph alignment problem, a fundamental NP-hard task of finding optimal node\ncorrespondences between unlabeled graphs using only structural information. Our\nmethod trains a sequence of GNNs where each network learns to iteratively\nrefine similarity matrices produced by previous networks. During inference,\nthis creates a bootstrap effect: each GNN improves upon partial solutions by\nincorporating discrete ranking information about node alignment quality from\nprior iterations. We combine this with a powerful architecture that operates on\nnode pairs rather than individual nodes, capturing global structural patterns\nessential for alignment that standard message-passing networks cannot\nrepresent. Extensive experiments on synthetic benchmarks demonstrate\nsubstantial improvements: our chained GNNs achieve over 3x better accuracy than\nexisting methods on challenging instances, and uniquely solve regular graphs\nwhere all competing approaches fail. When combined with traditional\noptimization as post-processing, our method substantially outperforms\nstate-of-the-art solvers on the graph alignment benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have struggled to outperform traditional\noptimization methods on combinatorial problems, limiting their practical\nimpact. We address this gap by introducing a novel chaining procedure for the\ngraph alignment problem, a fundamental NP-hard task of finding optimal node\ncorrespondences between unlabeled graphs using only structural information. Our\nmethod trains a sequence of GNNs where each network learns to iteratively\nrefine similarity matrices produced by previous networks. During inference,\nthis creates a bootstrap effect: each GNN improves upon partial solutions by\nincorporating discrete ranking information about node alignment quality from\nprior iterations. We combine this with a powerful architecture that operates on\nnode pairs rather than individual nodes, capturing global structural patterns\nessential for alignment that standard message-passing networks cannot\nrepresent. Extensive experiments on synthetic benchmarks demonstrate\nsubstantial improvements: our chained GNNs achieve over 3x better accuracy than\nexisting methods on challenging instances, and uniquely solve regular graphs\nwhere all competing approaches fail. When combined with traditional\noptimization as post-processing, our method substantially outperforms\nstate-of-the-art solvers on the graph alignment benchmark."
                },
                "authors": [
                    {
                        "name": "Marc Lelarge"
                    }
                ],
                "author_detail": {
                    "name": "Marc Lelarge"
                },
                "author": "Marc Lelarge",
                "arxiv_comment": "27 pages, 10 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11400v2",
                "updated": "2025-10-03T15:11:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    11,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-17T03:34:31Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    3,
                    34,
                    31,
                    0,
                    48,
                    0
                ],
                "title": "On the Diminishing Returns of Complex Robust RAG Training in the Era of\n  Powerful LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Diminishing Returns of Complex Robust RAG Training in the Era of\n  Powerful LLMs"
                },
                "summary": "Retrieval-augmented generation (RAG) systems traditionally employ\nsophisticated training strategies to enhance robustness against retrieval\nnoise. In this work, we investigate a critical question: does the benefit of\nthese complex robust training methods diminish as language models become more\npowerful? Through systematic evaluation across multiple model scales and\nquestion-answering datasets, our analysis reveals a consistent trend: \\emph{the\nmarginal robustness benefit of sophisticated training strategies decreases\nsubstantially as model capacity increases.} While smaller models show\nsignificant performance improvements from complex document selection and\nadversarial objectives, more capable models achieve comparable or even superior\nperformance with simpler training approaches. Further investigation\ndemonstrates that stronger models naturally exhibit better confidence\ncalibration, cross-dataset generalization capability, and more effective\nattention patterns, even under simple training regimes. These findings suggest\nthat as foundation models evolve, the engineering effort invested in complex\nrobust training may yield diminishing returns, indicating that simplified RAG\npipelines could suffice for powerful models while maintaining competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems traditionally employ\nsophisticated training strategies to enhance robustness against retrieval\nnoise. In this work, we investigate a critical question: does the benefit of\nthese complex robust training methods diminish as language models become more\npowerful? Through systematic evaluation across multiple model scales and\nquestion-answering datasets, our analysis reveals a consistent trend: \\emph{the\nmarginal robustness benefit of sophisticated training strategies decreases\nsubstantially as model capacity increases.} While smaller models show\nsignificant performance improvements from complex document selection and\nadversarial objectives, more capable models achieve comparable or even superior\nperformance with simpler training approaches. Further investigation\ndemonstrates that stronger models naturally exhibit better confidence\ncalibration, cross-dataset generalization capability, and more effective\nattention patterns, even under simple training regimes. These findings suggest\nthat as foundation models evolve, the engineering effort invested in complex\nrobust training may yield diminishing returns, indicating that simplified RAG\npipelines could suffice for powerful models while maintaining competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Hanxing Ding"
                    },
                    {
                        "name": "Shuchang Tao"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Zihao Wei"
                    },
                    {
                        "name": "Liwei Chen"
                    },
                    {
                        "name": "Kun Xu"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted at SIGIR-AP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03074v1",
                "updated": "2025-10-03T15:01:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    1,
                    34,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:01:34Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    1,
                    34,
                    4,
                    276,
                    0
                ],
                "title": "Neural Posterior Estimation with Autoregressive Tiling for Detecting\n  Objects in Astronomical Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Posterior Estimation with Autoregressive Tiling for Detecting\n  Objects in Astronomical Images"
                },
                "summary": "Upcoming astronomical surveys will produce petabytes of high-resolution\nimages of the night sky, providing information about billions of stars and\ngalaxies. Detecting and characterizing the astronomical objects in these images\nis a fundamental task in astronomy -- and a challenging one, as most of these\nobjects are faint and many visually overlap with other objects. We propose an\namortized variational inference procedure to solve this instance of\nsmall-object detection. Our key innovation is a family of spatially\nautoregressive variational distributions that partition and order the latent\nspace according to a $K$-color checkerboard pattern. By construction, the\nconditional independencies of this variational family mirror those of the\nposterior distribution. We fit the variational distribution, which is\nparameterized by a convolutional neural network, using neural posterior\nestimation (NPE) to minimize an expectation of the forward KL divergence. Using\nimages from the Sloan Digital Sky Survey, our method achieves state-of-the-art\nperformance. We further demonstrate that the proposed autoregressive structure\ngreatly improves posterior calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upcoming astronomical surveys will produce petabytes of high-resolution\nimages of the night sky, providing information about billions of stars and\ngalaxies. Detecting and characterizing the astronomical objects in these images\nis a fundamental task in astronomy -- and a challenging one, as most of these\nobjects are faint and many visually overlap with other objects. We propose an\namortized variational inference procedure to solve this instance of\nsmall-object detection. Our key innovation is a family of spatially\nautoregressive variational distributions that partition and order the latent\nspace according to a $K$-color checkerboard pattern. By construction, the\nconditional independencies of this variational family mirror those of the\nposterior distribution. We fit the variational distribution, which is\nparameterized by a convolutional neural network, using neural posterior\nestimation (NPE) to minimize an expectation of the forward KL divergence. Using\nimages from the Sloan Digital Sky Survey, our method achieves state-of-the-art\nperformance. We further demonstrate that the proposed autoregressive structure\ngreatly improves posterior calibration."
                },
                "authors": [
                    {
                        "name": "Jeffrey Regier"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Regier"
                },
                "author": "Jeffrey Regier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18660v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18660v4",
                "updated": "2025-10-03T14:57:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    57,
                    10,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-24T11:53:35Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    11,
                    53,
                    35,
                    5,
                    144,
                    0
                ],
                "title": "So-Fake: Benchmarking and Explaining Social Media Image Forgery\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "So-Fake: Benchmarking and Explaining Social Media Image Forgery\n  Detection"
                },
                "summary": "Recent advances in AI-powered generative models have enabled the creation of\nincreasingly realistic synthetic images, posing significant risks to\ninformation integrity and public trust on social media platforms. While robust\ndetection frameworks and diverse, large-scale datasets are essential to\nmitigate these risks, existing academic efforts remain limited in scope:\ncurrent datasets lack the diversity, scale, and realism required for social\nmedia contexts, while detection methods struggle with generalization to unseen\ngenerative technologies. To bridge this gap, we introduce So-Fake-Set, a\ncomprehensive social media-oriented dataset with over 2 million high-quality\nimages, diverse generative sources, and photorealistic imagery synthesized\nusing 35 state-of-the-art generative models. To rigorously evaluate\ncross-domain robustness, we establish a novel and large-scale (100K)\nout-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from\ncommercial models explicitly excluded from the training distribution, creating\na realistic testbed for evaluating real-world performance. Leveraging these\nresources, we present So-Fake-R1, an advanced vision-language framework that\nemploys reinforcement learning for highly accurate forgery detection, precise\nlocalization, and explainable inference through interpretable visual\nrationales. Extensive experiments show that So-Fake-R1 outperforms the\nsecond-best method, with a 1.3% gain in detection accuracy and a 4.5% increase\nin localization IoU. By integrating a scalable dataset, a challenging OOD\nbenchmark, and an advanced detection framework, this work establishes a new\nfoundation for social media-centric forgery detection research. The code,\nmodels, and datasets will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in AI-powered generative models have enabled the creation of\nincreasingly realistic synthetic images, posing significant risks to\ninformation integrity and public trust on social media platforms. While robust\ndetection frameworks and diverse, large-scale datasets are essential to\nmitigate these risks, existing academic efforts remain limited in scope:\ncurrent datasets lack the diversity, scale, and realism required for social\nmedia contexts, while detection methods struggle with generalization to unseen\ngenerative technologies. To bridge this gap, we introduce So-Fake-Set, a\ncomprehensive social media-oriented dataset with over 2 million high-quality\nimages, diverse generative sources, and photorealistic imagery synthesized\nusing 35 state-of-the-art generative models. To rigorously evaluate\ncross-domain robustness, we establish a novel and large-scale (100K)\nout-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from\ncommercial models explicitly excluded from the training distribution, creating\na realistic testbed for evaluating real-world performance. Leveraging these\nresources, we present So-Fake-R1, an advanced vision-language framework that\nemploys reinforcement learning for highly accurate forgery detection, precise\nlocalization, and explainable inference through interpretable visual\nrationales. Extensive experiments show that So-Fake-R1 outperforms the\nsecond-best method, with a 1.3% gain in detection accuracy and a 4.5% increase\nin localization IoU. By integrating a scalable dataset, a challenging OOD\nbenchmark, and an advanced detection framework, this work establishes a new\nfoundation for social media-centric forgery detection research. The code,\nmodels, and datasets will be released publicly."
                },
                "authors": [
                    {
                        "name": "Zhenglin Huang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Haiquan Wen"
                    },
                    {
                        "name": "Yiwei He"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Xiaowei Huang"
                    },
                    {
                        "name": "Bei Peng"
                    },
                    {
                        "name": "Guangliang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Guangliang Cheng"
                },
                "author": "Guangliang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18660v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18660v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22860v2",
                "updated": "2025-10-03T14:50:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    50,
                    37,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-28T20:47:02Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    20,
                    47,
                    2,
                    2,
                    148,
                    0
                ],
                "title": "Permissioned LLMs: Enforcing Access Control in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Permissioned LLMs: Enforcing Access Control in Large Language Models"
                },
                "summary": "In enterprise settings, organizational data is segregated, siloed and\ncarefully protected by elaborate access control frameworks. These access\ncontrol structures can completely break down if an LLM fine-tuned on the siloed\ndata serves requests, for downstream tasks, from individuals with disparate\naccess privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs\nthat superimpose the organizational data access control structures on query\nresponses they generate. We formalize abstractions underpinning the means to\ndetermine whether access control enforcement happens correctly over LLM query\nresponses. Our formalism introduces the notion of a relevant response that can\nbe used to prove whether a PermLLM mechanism has been implemented correctly. We\nalso introduce a novel metric, called access advantage, to empirically evaluate\nthe efficacy of a PermLLM mechanism. We introduce three novel PermLLM\nmechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired\naccess control. We furthermore present two instantiations of access\nadvantage--(i) Domain Distinguishability Index (DDI) based on Membership\nInference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility\nevaluation. We demonstrate the efficacy of our PermLLM mechanisms through\nextensive experiments on five public datasets (GPQA, RCV1, SimpleQA, WMDP, and\nPubMedQA), in addition to evaluating the validity of DDI and UGI metrics\nthemselves for quantifying access control in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In enterprise settings, organizational data is segregated, siloed and\ncarefully protected by elaborate access control frameworks. These access\ncontrol structures can completely break down if an LLM fine-tuned on the siloed\ndata serves requests, for downstream tasks, from individuals with disparate\naccess privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs\nthat superimpose the organizational data access control structures on query\nresponses they generate. We formalize abstractions underpinning the means to\ndetermine whether access control enforcement happens correctly over LLM query\nresponses. Our formalism introduces the notion of a relevant response that can\nbe used to prove whether a PermLLM mechanism has been implemented correctly. We\nalso introduce a novel metric, called access advantage, to empirically evaluate\nthe efficacy of a PermLLM mechanism. We introduce three novel PermLLM\nmechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired\naccess control. We furthermore present two instantiations of access\nadvantage--(i) Domain Distinguishability Index (DDI) based on Membership\nInference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility\nevaluation. We demonstrate the efficacy of our PermLLM mechanisms through\nextensive experiments on five public datasets (GPQA, RCV1, SimpleQA, WMDP, and\nPubMedQA), in addition to evaluating the validity of DDI and UGI metrics\nthemselves for quantifying access control in LLMs."
                },
                "authors": [
                    {
                        "name": "Bargav Jayaraman"
                    },
                    {
                        "name": "Virendra J. Marathe"
                    },
                    {
                        "name": "Hamid Mozaffari"
                    },
                    {
                        "name": "William F. Shen"
                    },
                    {
                        "name": "Krishnaram Kenthapadi"
                    }
                ],
                "author_detail": {
                    "name": "Krishnaram Kenthapadi"
                },
                "author": "Krishnaram Kenthapadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16106v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16106v4",
                "updated": "2025-10-03T14:41:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    41,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2024-10-21T15:34:44Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    34,
                    44,
                    0,
                    295,
                    0
                ],
                "title": "Statistical Inference for Temporal Difference Learning with Linear\n  Function Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Temporal Difference Learning with Linear\n  Function Approximation"
                },
                "summary": "We investigate the statistical properties of Temporal Difference (TD)\nlearning with Polyak-Ruppert averaging, arguably one of the most widely used\nalgorithms in reinforcement learning, for the task of estimating the parameters\nof the optimal linear approximation to the value function. Assuming independent\nsamples, we make three theoretical contributions that improve upon the current\nstate-of-the-art results: (i) we derive sharper high probability convergence\nguarantees that depend explicitly on the asymptotic variance and hold under\nweaker conditions than those adopted in the literature; (ii) we establish\nrefined high-dimensional Berry-Esseen bounds over the class of convex sets,\nachieving faster rates than the best known results, and (iii) we propose and\nanalyze a novel, computationally efficient online plug-in estimator of the\nasymptotic covariance matrix. These results enable the construction of\nconfidence regions and simultaneous confidence intervals for the linear\nparameters of the value function approximation, with guaranteed finite-sample\ncoverage. We demonstrate the applicability of our theoretical findings through\nnumerical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the statistical properties of Temporal Difference (TD)\nlearning with Polyak-Ruppert averaging, arguably one of the most widely used\nalgorithms in reinforcement learning, for the task of estimating the parameters\nof the optimal linear approximation to the value function. Assuming independent\nsamples, we make three theoretical contributions that improve upon the current\nstate-of-the-art results: (i) we derive sharper high probability convergence\nguarantees that depend explicitly on the asymptotic variance and hold under\nweaker conditions than those adopted in the literature; (ii) we establish\nrefined high-dimensional Berry-Esseen bounds over the class of convex sets,\nachieving faster rates than the best known results, and (iii) we propose and\nanalyze a novel, computationally efficient online plug-in estimator of the\nasymptotic covariance matrix. These results enable the construction of\nconfidence regions and simultaneous confidence intervals for the linear\nparameters of the value function approximation, with guaranteed finite-sample\ncoverage. We demonstrate the applicability of our theoretical findings through\nnumerical experiments."
                },
                "authors": [
                    {
                        "name": "Weichen Wu"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Yuting Wei"
                    },
                    {
                        "name": "Alessandro Rinaldo"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Rinaldo"
                },
                "author": "Alessandro Rinaldo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16106v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16106v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03047v1",
                "updated": "2025-10-03T14:31:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    31,
                    9,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T14:31:09Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    31,
                    9,
                    4,
                    276,
                    0
                ],
                "title": "Structural Chirality and Natural Optical Activity across the\n  $$-to-$$ Phase Transition in SiO$_2$ and AlPO$_4$ from\n  first-principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural Chirality and Natural Optical Activity across the\n  $$-to-$$ Phase Transition in SiO$_2$ and AlPO$_4$ from\n  first-principles"
                },
                "summary": "Natural optical activity (NOA), the ability of a material to rotate the plane\nof polarized light, has traditionally been associated with structural\nchirality. However, this relationship has often been oversimplified, leading to\nconceptual misunderstandings, particularly when attempts are made to directly\ncorrelate structural handedness with optical rotatory power. In reality, the\nrelationship between chirality and NOA is more nuanced: optical activity can\narise in both chiral and achiral crystal structures, and the sign of the\nrotation cannot necessarily be inferred from the handedness of the space group.\n% In this work, we conduct a first-principles investigation of natural optical\nactivity in SiO$_2$ and AlPO$_4$ crystals, focusing on their enantiomorphic\nstructural phase transition from high-symmetry hexagonal ($P6_422$ or $P6_222$)\nto low-symmetry trigonal ($P3_121$ or $P3_221$) space groups. This transition,\ndriven by the condensation of a zone-center $\\Gamma_3$ phonon mode, reverses\nthe screw axis type given by the space group symbol while leaving the sign of\nthe optical activity unchanged. By following the evolution of the structure and\nthe optical response along the transition pathway, we clarify the microscopic\norigin of this behavior. We demonstrate that the sense of optical rotation is\ndetermined not by the nominal helicity of the screw axis given in the space\ngroup symbol, but by the atomic-scale helicity of the most polarizable atoms of\nthe structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural optical activity (NOA), the ability of a material to rotate the plane\nof polarized light, has traditionally been associated with structural\nchirality. However, this relationship has often been oversimplified, leading to\nconceptual misunderstandings, particularly when attempts are made to directly\ncorrelate structural handedness with optical rotatory power. In reality, the\nrelationship between chirality and NOA is more nuanced: optical activity can\narise in both chiral and achiral crystal structures, and the sign of the\nrotation cannot necessarily be inferred from the handedness of the space group.\n% In this work, we conduct a first-principles investigation of natural optical\nactivity in SiO$_2$ and AlPO$_4$ crystals, focusing on their enantiomorphic\nstructural phase transition from high-symmetry hexagonal ($P6_422$ or $P6_222$)\nto low-symmetry trigonal ($P3_121$ or $P3_221$) space groups. This transition,\ndriven by the condensation of a zone-center $\\Gamma_3$ phonon mode, reverses\nthe screw axis type given by the space group symbol while leaving the sign of\nthe optical activity unchanged. By following the evolution of the structure and\nthe optical response along the transition pathway, we clarify the microscopic\norigin of this behavior. We demonstrate that the sense of optical rotation is\ndetermined not by the nominal helicity of the screw axis given in the space\ngroup symbol, but by the atomic-scale helicity of the most polarizable atoms of\nthe structure."
                },
                "authors": [
                    {
                        "name": "F. Gmez-Ortiz"
                    },
                    {
                        "name": "A. Zabalo"
                    },
                    {
                        "name": "A. M. Glazer"
                    },
                    {
                        "name": "E. E. McCabe"
                    },
                    {
                        "name": "A. H. Romero"
                    },
                    {
                        "name": "E. Bousquet"
                    }
                ],
                "author_detail": {
                    "name": "E. Bousquet"
                },
                "author": "E. Bousquet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03038v1",
                "updated": "2025-10-03T14:20:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    20,
                    45,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T14:20:45Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    20,
                    45,
                    4,
                    276,
                    0
                ],
                "title": "CHORD: Customizing Hybrid-precision On-device Model for Sequential\n  Recommendation with Device-cloud Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHORD: Customizing Hybrid-precision On-device Model for Sequential\n  Recommendation with Device-cloud Collaboration"
                },
                "summary": "With the advancement of mobile device capabilities, deploying reranking\nmodels directly on devices has become feasible, enabling real-time contextual\nrecommendations. When migrating models from cloud to devices, resource\nheterogeneity inevitably necessitates model compression. Recent quantization\nmethods show promise for efficient deployment, yet they overlook\ndevice-specific user interests, resulting in compromised recommendation\naccuracy. While on-device finetuning captures personalized user preference, it\nimposes additional computational burden through local retraining. To address\nthese challenges, we propose a framework for \\underline{\\textbf{C}}ustomizing\n\\underline{\\textbf{H}}ybrid-precision \\underline{\\textbf{O}}n-device model for\nsequential \\underline{\\textbf{R}}ecommendation with\n\\underline{\\textbf{D}}evice-cloud collaboration (\\textbf{CHORD}), leveraging\nchannel-wise mixed-precision quantization to simultaneously achieve\npersonalization and resource-adaptive deployment. CHORD distributes randomly\ninitialized models across heterogeneous devices and identifies user-specific\ncritical parameters through auxiliary hypernetwork modules on the cloud. Our\nparameter sensitivity analysis operates across multiple granularities (layer,\nfilter, and element levels), enabling precise mapping from user profiles to\nquantization strategy. Through on-device mixed-precision quantization, CHORD\ndelivers dynamic model adaptation and accelerated inference without\nbackpropagation, eliminating costly retraining cycles. We minimize\ncommunication overhead by encoding quantization strategies using only 2 bits\nper channel instead of 32-bit weights. Experiments on three real-world datasets\nwith two popular backbones (SASRec and Caser) demonstrate the accuracy,\nefficiency, and adaptivity of CHORD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of mobile device capabilities, deploying reranking\nmodels directly on devices has become feasible, enabling real-time contextual\nrecommendations. When migrating models from cloud to devices, resource\nheterogeneity inevitably necessitates model compression. Recent quantization\nmethods show promise for efficient deployment, yet they overlook\ndevice-specific user interests, resulting in compromised recommendation\naccuracy. While on-device finetuning captures personalized user preference, it\nimposes additional computational burden through local retraining. To address\nthese challenges, we propose a framework for \\underline{\\textbf{C}}ustomizing\n\\underline{\\textbf{H}}ybrid-precision \\underline{\\textbf{O}}n-device model for\nsequential \\underline{\\textbf{R}}ecommendation with\n\\underline{\\textbf{D}}evice-cloud collaboration (\\textbf{CHORD}), leveraging\nchannel-wise mixed-precision quantization to simultaneously achieve\npersonalization and resource-adaptive deployment. CHORD distributes randomly\ninitialized models across heterogeneous devices and identifies user-specific\ncritical parameters through auxiliary hypernetwork modules on the cloud. Our\nparameter sensitivity analysis operates across multiple granularities (layer,\nfilter, and element levels), enabling precise mapping from user profiles to\nquantization strategy. Through on-device mixed-precision quantization, CHORD\ndelivers dynamic model adaptation and accelerated inference without\nbackpropagation, eliminating costly retraining cycles. We minimize\ncommunication overhead by encoding quantization strategies using only 2 bits\nper channel instead of 32-bit weights. Experiments on three real-world datasets\nwith two popular backbones (SASRec and Caser) demonstrate the accuracy,\nefficiency, and adaptivity of CHORD."
                },
                "authors": [
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Kairui Fu"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Wenyan Fan"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_doi": "10.1145/3746027.3755632",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755632",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.03038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted by ACM MM'25",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03029v1",
                "updated": "2025-10-03T14:09:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    9,
                    55,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T14:09:55Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    9,
                    55,
                    4,
                    276,
                    0
                ],
                "title": "Investigating The Smells of LLM Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating The Smells of LLM Generated Code"
                },
                "summary": "Context: Large Language Models (LLMs) are increasingly being used to generate\nprogram code. Much research has been reported on the functional correctness of\ngenerated code, but there is far less on code quality.\n  Objectives: In this study, we propose a scenario-based method of evaluating\nthe quality of LLM-generated code to identify the weakest scenarios in which\nthe quality of LLM generated code should be improved.\n  Methods: The method measures code smells, an important indicator of code\nquality, and compares them with a baseline formed from reference solutions of\nprofessionally written code. The test dataset is divided into various subsets\naccording to the topics of the code and complexity of the coding tasks to\nrepresent different scenarios of using LLMs for code generation. We will also\npresent an automated test system for this purpose and report experiments with\nthe Java programs generated in response to prompts given to four\nstate-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.\n  Results: We find that LLM-generated code has a higher incidence of code\nsmells compared to reference solutions. Falcon performed the least badly, with\na smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)\nand finally Codex (84.97%). The average smell increase across all LLMs was\n63.34%, comprising 73.35% for implementation smells and 21.42% for design\nsmells. We also found that the increase in code smells is greater for more\ncomplex coding tasks and for more advanced topics, such as those involving\nobject-orientated concepts.\n  Conclusion: In terms of code smells, LLM's performances on various coding\ntask complexities and topics are highly correlated to the quality of human\nwritten code in the corresponding scenarios. However, the quality of LLM\ngenerated code is noticeably poorer than human written code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Large Language Models (LLMs) are increasingly being used to generate\nprogram code. Much research has been reported on the functional correctness of\ngenerated code, but there is far less on code quality.\n  Objectives: In this study, we propose a scenario-based method of evaluating\nthe quality of LLM-generated code to identify the weakest scenarios in which\nthe quality of LLM generated code should be improved.\n  Methods: The method measures code smells, an important indicator of code\nquality, and compares them with a baseline formed from reference solutions of\nprofessionally written code. The test dataset is divided into various subsets\naccording to the topics of the code and complexity of the coding tasks to\nrepresent different scenarios of using LLMs for code generation. We will also\npresent an automated test system for this purpose and report experiments with\nthe Java programs generated in response to prompts given to four\nstate-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.\n  Results: We find that LLM-generated code has a higher incidence of code\nsmells compared to reference solutions. Falcon performed the least badly, with\na smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)\nand finally Codex (84.97%). The average smell increase across all LLMs was\n63.34%, comprising 73.35% for implementation smells and 21.42% for design\nsmells. We also found that the increase in code smells is greater for more\ncomplex coding tasks and for more advanced topics, such as those involving\nobject-orientated concepts.\n  Conclusion: In terms of code smells, LLM's performances on various coding\ntask complexities and topics are highly correlated to the quality of human\nwritten code in the corresponding scenarios. However, the quality of LLM\ngenerated code is noticeably poorer than human written code."
                },
                "authors": [
                    {
                        "name": "Debalina Ghosh Paul"
                    },
                    {
                        "name": "Hong Zhu"
                    },
                    {
                        "name": "Ian Bayley"
                    }
                ],
                "author_detail": {
                    "name": "Ian Bayley"
                },
                "author": "Ian Bayley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03016v1",
                "updated": "2025-10-03T14:00:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    0,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T14:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    0,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Learning Robust Diffusion Models from Imprecise Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Robust Diffusion Models from Imprecise Supervision"
                },
                "summary": "Conditional diffusion models have achieved remarkable success in various\ngenerative tasks recently, but their training typically relies on large-scale\ndatasets that inevitably contain imprecise information in conditional inputs.\nSuch supervision, often stemming from noisy, ambiguous, or incomplete labels,\nwill cause condition mismatch and degrade generation quality. To address this\nchallenge, we propose DMIS, a unified framework for training robust Diffusion\nModels from Imprecise Supervision, which is the first systematic study within\ndiffusion models. Our framework is derived from likelihood maximization and\ndecomposes the objective into generative and classification components: the\ngenerative component models imprecise-label distributions, while the\nclassification component leverages a diffusion classifier to infer\nclass-posterior probabilities, with its efficiency further improved by an\noptimized timestep sampling strategy. Extensive experiments on diverse forms of\nimprecise supervision, covering tasks of image generation, weakly supervised\nlearning, and noisy dataset condensation demonstrate that DMIS consistently\nproduces high-quality and class-discriminative samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional diffusion models have achieved remarkable success in various\ngenerative tasks recently, but their training typically relies on large-scale\ndatasets that inevitably contain imprecise information in conditional inputs.\nSuch supervision, often stemming from noisy, ambiguous, or incomplete labels,\nwill cause condition mismatch and degrade generation quality. To address this\nchallenge, we propose DMIS, a unified framework for training robust Diffusion\nModels from Imprecise Supervision, which is the first systematic study within\ndiffusion models. Our framework is derived from likelihood maximization and\ndecomposes the objective into generative and classification components: the\ngenerative component models imprecise-label distributions, while the\nclassification component leverages a diffusion classifier to infer\nclass-posterior probabilities, with its efficiency further improved by an\noptimized timestep sampling strategy. Extensive experiments on diverse forms of\nimprecise supervision, covering tasks of image generation, weakly supervised\nlearning, and noisy dataset condensation demonstrate that DMIS consistently\nproduces high-quality and class-discriminative samples."
                },
                "authors": [
                    {
                        "name": "Dong-Dong Wu"
                    },
                    {
                        "name": "Jiacheng Cui"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Zhiqiang She"
                    },
                    {
                        "name": "Masashi Sugiyama"
                    }
                ],
                "author_detail": {
                    "name": "Masashi Sugiyama"
                },
                "author": "Masashi Sugiyama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09669v2",
                "updated": "2025-10-03T13:54:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    54,
                    9,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-11T12:39:48Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    39,
                    48,
                    2,
                    162,
                    0
                ],
                "title": "Query-Level Uncertainty in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Level Uncertainty in Large Language Models"
                },
                "summary": "It is important for Large Language Models (LLMs) to be aware of the boundary\nof their knowledge, distinguishing queries they can confidently answer from\nthose that lie beyond their capabilities. Such awareness enables models to\nperform adaptive inference, such as invoking retrieval-augmented generation\n(RAG), engaging in slow and deep thinking, or abstaining from answering when\nappropriate. These mechanisms are key to developing efficient and trustworthy\nAI. In this work, we propose a method to detect knowledge boundaries via\nQuery-Level Uncertainty, which estimates if a model is capable of answering a\ngiven query before generating any tokens, thus avoiding the generation cost. To\nthis end, we propose a novel, training-free method called Internal Confidence,\nwhich leverages self-evaluations across layers and tokens to provide a reliable\nsignal of uncertainty. Empirical studies on both factual question answering and\nmathematical reasoning tasks demonstrate that our Internal Confidence\noutperforms several baselines in quality of confidence while being\ncomputationally cheaper. Furthermore, we demonstrate its benefits in adaptive\ninference settings, showing that for RAG and model cascading it reduces\ninference costs while preserving overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is important for Large Language Models (LLMs) to be aware of the boundary\nof their knowledge, distinguishing queries they can confidently answer from\nthose that lie beyond their capabilities. Such awareness enables models to\nperform adaptive inference, such as invoking retrieval-augmented generation\n(RAG), engaging in slow and deep thinking, or abstaining from answering when\nappropriate. These mechanisms are key to developing efficient and trustworthy\nAI. In this work, we propose a method to detect knowledge boundaries via\nQuery-Level Uncertainty, which estimates if a model is capable of answering a\ngiven query before generating any tokens, thus avoiding the generation cost. To\nthis end, we propose a novel, training-free method called Internal Confidence,\nwhich leverages self-evaluations across layers and tokens to provide a reliable\nsignal of uncertainty. Empirical studies on both factual question answering and\nmathematical reasoning tasks demonstrate that our Internal Confidence\noutperforms several baselines in quality of confidence while being\ncomputationally cheaper. Furthermore, we demonstrate its benefits in adaptive\ninference settings, showing that for RAG and model cascading it reduces\ninference costs while preserving overall performance."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gerard de Melo"
                    },
                    {
                        "name": "Fabian M. Suchanek"
                    },
                    {
                        "name": "Gal Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gal Varoquaux"
                },
                "author": "Gal Varoquaux",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02093v2",
                "updated": "2025-10-03T13:39:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    39,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-02T08:45:29Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    8,
                    45,
                    29,
                    1,
                    245,
                    0
                ],
                "title": "Better by Comparison: Retrieval-Augmented Contrastive Reasoning for\n  Automatic Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better by Comparison: Retrieval-Augmented Contrastive Reasoning for\n  Automatic Prompt Optimization"
                },
                "summary": "Automatic prompt optimization has recently emerged as a strategy for\nimproving the quality of prompts used in Large Language Models (LLMs), with the\ngoal of generating more accurate and useful responses. However, most prior work\nfocuses on direct prompt refinement or model fine-tuning, overlooking the\npotential of leveraging LLMs' inherent reasoning capability to learn from\ncontrasting examples. In this paper, we present Contrastive Reasoning Prompt\nOptimization (CRPO), a novel framework that formulates prompt optimization as a\nretrieval-augmented reasoning process. Our approach retrieves top k reference\nprompt-response pairs from the HelpSteer2 dataset, an open source collection\nwhere each response is annotated for helpfulness, correctness, coherence,\ncomplexity, and verbosity, and constructs two complementary optimization\nparadigms: (1) tiered contrastive reasoning, where the LLM compares high-,\nmedium-, and low-quality exemplars (both prompts and responses) to refine its\nown generation through reflective reasoning, and (2) multi-metric contrastive\nreasoning, where the LLM analyzes the best exemplars along each evaluation\ndimension and integrates their strengths into an optimized prompt. By\nexplicitly contrasting high and low quality exemplars, CRPO enables the model\nto deduce why certain prompts succeed while others fail, thereby achieving more\nrobust and interpretable optimization. Experimental results on the HelpSteer2\nbenchmark demonstrate that CRPO significantly outperforms baselines. Our\nfindings highlight the promise of contrastive, retrieval-augmented reasoning\nfor advancing automatic prompt optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic prompt optimization has recently emerged as a strategy for\nimproving the quality of prompts used in Large Language Models (LLMs), with the\ngoal of generating more accurate and useful responses. However, most prior work\nfocuses on direct prompt refinement or model fine-tuning, overlooking the\npotential of leveraging LLMs' inherent reasoning capability to learn from\ncontrasting examples. In this paper, we present Contrastive Reasoning Prompt\nOptimization (CRPO), a novel framework that formulates prompt optimization as a\nretrieval-augmented reasoning process. Our approach retrieves top k reference\nprompt-response pairs from the HelpSteer2 dataset, an open source collection\nwhere each response is annotated for helpfulness, correctness, coherence,\ncomplexity, and verbosity, and constructs two complementary optimization\nparadigms: (1) tiered contrastive reasoning, where the LLM compares high-,\nmedium-, and low-quality exemplars (both prompts and responses) to refine its\nown generation through reflective reasoning, and (2) multi-metric contrastive\nreasoning, where the LLM analyzes the best exemplars along each evaluation\ndimension and integrates their strengths into an optimized prompt. By\nexplicitly contrasting high and low quality exemplars, CRPO enables the model\nto deduce why certain prompts succeed while others fail, thereby achieving more\nrobust and interpretable optimization. Experimental results on the HelpSteer2\nbenchmark demonstrate that CRPO significantly outperforms baselines. Our\nfindings highlight the promise of contrastive, retrieval-augmented reasoning\nfor advancing automatic prompt optimization."
                },
                "authors": [
                    {
                        "name": "Juhyeon Lee"
                    },
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Hyunjin An"
                    },
                    {
                        "name": "Seunghyun Lee"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02999v1",
                "updated": "2025-10-03T13:38:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    38,
                    56,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T13:38:56Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    38,
                    56,
                    4,
                    276,
                    0
                ],
                "title": "Untargeted Jailbreak Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Untargeted Jailbreak Attack"
                },
                "summary": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs),\nsuch as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize\nadversarial suffixes to align the LLM output with a predefined target response.\nHowever, by restricting the optimization objective as inducing a predefined\ntarget, these methods inherently constrain the adversarial search space, which\nlimit their overall attack efficacy. Furthermore, existing methods typically\nrequire a large number of optimization iterations to fulfill the large gap\nbetween the fixed target and the original model response, resulting in low\nattack efficiency.\n  To overcome the limitations of targeted jailbreak attacks, we propose the\nfirst gradient-based untargeted jailbreak attack (UJA), aiming to elicit an\nunsafe response without enforcing any predefined patterns. Specifically, we\nformulate an untargeted attack objective to maximize the unsafety probability\nof the LLM response, which can be quantified using a judge model. Since the\nobjective is non-differentiable, we further decompose it into two\ndifferentiable sub-objectives for optimizing an optimal harmful response and\nthe corresponding adversarial prompt, with a theoretical analysis to validate\nthe decomposition. In contrast to targeted jailbreak attacks, UJA's\nunrestricted objective significantly expands the search space, enabling a more\nflexible and efficient exploration of LLM vulnerabilities.Extensive evaluations\ndemonstrate that \\textsc{UJA} can achieve over 80\\% attack success rates\nagainst recent safety-aligned LLMs with only 100 optimization iterations,\noutperforming the state-of-the-art gradient-based attacks such as I-GCG and\nCOLD-Attack by over 20\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs),\nsuch as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize\nadversarial suffixes to align the LLM output with a predefined target response.\nHowever, by restricting the optimization objective as inducing a predefined\ntarget, these methods inherently constrain the adversarial search space, which\nlimit their overall attack efficacy. Furthermore, existing methods typically\nrequire a large number of optimization iterations to fulfill the large gap\nbetween the fixed target and the original model response, resulting in low\nattack efficiency.\n  To overcome the limitations of targeted jailbreak attacks, we propose the\nfirst gradient-based untargeted jailbreak attack (UJA), aiming to elicit an\nunsafe response without enforcing any predefined patterns. Specifically, we\nformulate an untargeted attack objective to maximize the unsafety probability\nof the LLM response, which can be quantified using a judge model. Since the\nobjective is non-differentiable, we further decompose it into two\ndifferentiable sub-objectives for optimizing an optimal harmful response and\nthe corresponding adversarial prompt, with a theoretical analysis to validate\nthe decomposition. In contrast to targeted jailbreak attacks, UJA's\nunrestricted objective significantly expands the search space, enabling a more\nflexible and efficient exploration of LLM vulnerabilities.Extensive evaluations\ndemonstrate that \\textsc{UJA} can achieve over 80\\% attack success rates\nagainst recent safety-aligned LLMs with only 100 optimization iterations,\noutperforming the state-of-the-art gradient-based attacks such as I-GCG and\nCOLD-Attack by over 20\\%."
                },
                "authors": [
                    {
                        "name": "Xinzhe Huang"
                    },
                    {
                        "name": "Wenjing Hu"
                    },
                    {
                        "name": "Tianhang Zheng"
                    },
                    {
                        "name": "Kedong Xiu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02996v1",
                "updated": "2025-10-03T13:36:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    36,
                    57,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T13:36:57Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    36,
                    57,
                    4,
                    276,
                    0
                ],
                "title": "Onto-Epistemological Analysis of AI Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Onto-Epistemological Analysis of AI Explanations"
                },
                "summary": "Artificial intelligence (AI) is being applied in almost every field. At the\nsame time, the currently dominant deep learning methods are fundamentally\nblack-box systems that lack explanations for their inferences, significantly\nlimiting their trustworthiness and adoption. Explainable AI (XAI) methods aim\nto overcome this challenge by providing explanations of the models' decision\nprocess. Such methods are often proposed and developed by engineers and\nscientists with a predominantly technical background and incorporate their\nassumptions about the existence, validity, and explanatory utility of different\nconceivable explanatory mechanisms. However, the basic concept of an\nexplanation -- what it is, whether we can know it, whether it is absolute or\nrelative -- is far from trivial and has been the subject of deep philosophical\ndebate for millennia. As we point out here, the assumptions incorporated into\ndifferent XAI methods are not harmless and have important consequences for the\nvalidity and interpretation of AI explanations in different domains. We\ninvestigate ontological and epistemological assumptions in explainability\nmethods when they are applied to AI systems, meaning the assumptions we make\nabout the existence of explanations and our ability to gain knowledge about\nthose explanations. Our analysis shows how seemingly small technical changes to\nan XAI method may correspond to important differences in the underlying\nassumptions about explanations. We furthermore highlight the risks of ignoring\nthe underlying onto-epistemological paradigm when choosing an XAI method for a\ngiven application, and we discuss how to select and adapt appropriate XAI\nmethods for different domains of application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is being applied in almost every field. At the\nsame time, the currently dominant deep learning methods are fundamentally\nblack-box systems that lack explanations for their inferences, significantly\nlimiting their trustworthiness and adoption. Explainable AI (XAI) methods aim\nto overcome this challenge by providing explanations of the models' decision\nprocess. Such methods are often proposed and developed by engineers and\nscientists with a predominantly technical background and incorporate their\nassumptions about the existence, validity, and explanatory utility of different\nconceivable explanatory mechanisms. However, the basic concept of an\nexplanation -- what it is, whether we can know it, whether it is absolute or\nrelative -- is far from trivial and has been the subject of deep philosophical\ndebate for millennia. As we point out here, the assumptions incorporated into\ndifferent XAI methods are not harmless and have important consequences for the\nvalidity and interpretation of AI explanations in different domains. We\ninvestigate ontological and epistemological assumptions in explainability\nmethods when they are applied to AI systems, meaning the assumptions we make\nabout the existence of explanations and our ability to gain knowledge about\nthose explanations. Our analysis shows how seemingly small technical changes to\nan XAI method may correspond to important differences in the underlying\nassumptions about explanations. We furthermore highlight the risks of ignoring\nthe underlying onto-epistemological paradigm when choosing an XAI method for a\ngiven application, and we discuss how to select and adapt appropriate XAI\nmethods for different domains of application."
                },
                "authors": [
                    {
                        "name": "Martina Mattioli"
                    },
                    {
                        "name": "Eike Petersen"
                    },
                    {
                        "name": "Aasa Feragen"
                    },
                    {
                        "name": "Marcello Pelillo"
                    },
                    {
                        "name": "Siavash A. Bigdeli"
                    }
                ],
                "author_detail": {
                    "name": "Siavash A. Bigdeli"
                },
                "author": "Siavash A. Bigdeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02995v1",
                "updated": "2025-10-03T13:35:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    35,
                    45,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T13:35:45Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    35,
                    45,
                    4,
                    276,
                    0
                ],
                "title": "AudioToolAgent: An Agentic Framework for Audio-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioToolAgent: An Agentic Framework for Audio-Language Models"
                },
                "summary": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks\nbut lack multi-step reasoning and tool-calling found in recent Large Language\nModels (LLMs). This paper presents AudioToolAgent, a framework that coordinates\naudio-language models as tools via a central LLM agent that accesses tool\nadapters for audio question answering and speech-to-text. The agent selects\ntools, asks follow-up questions, and compares outputs for verification.\nExperiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to\n74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling\nfor shapley values across 374 configurations identifies effective agent-tool\ncombinations. The modular design allows integration of new tools and eliminates\nthe use of data and training costs. Code and reproduction materials are\navailable at: github.com/GLJS/AudioToolAgent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks\nbut lack multi-step reasoning and tool-calling found in recent Large Language\nModels (LLMs). This paper presents AudioToolAgent, a framework that coordinates\naudio-language models as tools via a central LLM agent that accesses tool\nadapters for audio question answering and speech-to-text. The agent selects\ntools, asks follow-up questions, and compares outputs for verification.\nExperiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to\n74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling\nfor shapley values across 374 configurations identifies effective agent-tool\ncombinations. The modular design allows integration of new tools and eliminates\nthe use of data and training costs. Code and reproduction materials are\navailable at: github.com/GLJS/AudioToolAgent"
                },
                "authors": [
                    {
                        "name": "Gijs Wijngaard"
                    },
                    {
                        "name": "Elia Formisano"
                    },
                    {
                        "name": "Michel Dumontier"
                    }
                ],
                "author_detail": {
                    "name": "Michel Dumontier"
                },
                "author": "Michel Dumontier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07044v2",
                "updated": "2025-10-03T13:29:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    29,
                    21,
                    4,
                    276,
                    0
                ],
                "published": "2025-03-10T08:32:33Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    32,
                    33,
                    0,
                    69,
                    0
                ],
                "title": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and\n  Robust Data Science Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and\n  Robust Data Science Automation"
                },
                "summary": "Existing large language model (LLM) agents for automating data science show\npromise, but they remain constrained by narrow task scopes, limited\ngeneralization across tasks and models, and over-reliance on state-of-the-art\n(SOTA) LLMs. We introduce DatawiseAgent, a notebook-centric LLM agent framework\nfor adaptive and robust data science automation. Inspired by how human data\nscientists work in computational notebooks, DatawiseAgent introduces a unified\ninteraction representation and a multi-stage architecture based on finite-state\ntransducers (FSTs). This design enables flexible long-horizon planning,\nprogressive solution development, and robust recovery from execution failures.\nExtensive experiments across diverse data science scenarios and models show\nthat DatawiseAgent consistently achieves SOTA performance by surpassing strong\nbaselines such as AutoGen and TaskWeaver, demonstrating superior effectiveness\nand adaptability. Further evaluations reveal graceful performance degradation\nunder weaker or smaller models, underscoring the robustness and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) agents for automating data science show\npromise, but they remain constrained by narrow task scopes, limited\ngeneralization across tasks and models, and over-reliance on state-of-the-art\n(SOTA) LLMs. We introduce DatawiseAgent, a notebook-centric LLM agent framework\nfor adaptive and robust data science automation. Inspired by how human data\nscientists work in computational notebooks, DatawiseAgent introduces a unified\ninteraction representation and a multi-stage architecture based on finite-state\ntransducers (FSTs). This design enables flexible long-horizon planning,\nprogressive solution development, and robust recovery from execution failures.\nExtensive experiments across diverse data science scenarios and models show\nthat DatawiseAgent consistently achieves SOTA performance by surpassing strong\nbaselines such as AutoGen and TaskWeaver, demonstrating superior effectiveness\nand adaptability. Further evaluations reveal graceful performance degradation\nunder weaker or smaller models, underscoring the robustness and scalability."
                },
                "authors": [
                    {
                        "name": "Ziming You"
                    },
                    {
                        "name": "Yumiao Zhang"
                    },
                    {
                        "name": "Dexuan Xu"
                    },
                    {
                        "name": "Yiwei Lou"
                    },
                    {
                        "name": "Yandong Yan"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Huaming Zhang"
                    },
                    {
                        "name": "Yu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Huang"
                },
                "author": "Yu Huang",
                "arxiv_comment": "The camera-ready version for EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02987v1",
                "updated": "2025-10-03T13:25:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    25,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T13:25:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    25,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via\n  Text-to-Image-to-Text Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via\n  Text-to-Image-to-Text Consistency"
                },
                "summary": "With the rapid advancement of large multimodal models (LMMs), recent\ntext-to-image (T2I) models can generate high-quality images and demonstrate\ngreat alignment to short prompts. However, they still struggle to effectively\nunderstand and follow long and detailed prompts, displaying inconsistent\ngeneration. To address this challenge, we introduce LPG-Bench, a comprehensive\nbenchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench\nfeatures 200 meticulously crafted prompts with an average length of over 250\nwords, approaching the input capacity of several leading commercial models.\nUsing these prompts, we generate 2,600 images from 13 state-of-the-art models\nand further perform comprehensive human-ranked annotations. Based on LPG-Bench,\nwe observe that state-of-the-art T2I alignment evaluation metrics exhibit poor\nconsistency with human preferences on long-prompt-based image generation. To\naddress the gap, we introduce a novel zero-shot metric based on\ntext-to-image-to-text consistency, termed TIT, for evaluating\nlong-prompt-generated images. The core concept of TIT is to quantify T2I\nalignment by directly comparing the consistency between the raw prompt and the\nLMM-produced description on the generated image, which includes an efficient\nscore-based instantiation TIT-Score and a large-language-model (LLM) based\ninstantiation TIT-Score-LLM. Extensive experiments demonstrate that our\nframework achieves superior alignment with human judgment compared to\nCLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute\nimprovement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT\nmethods together offer a deeper perspective to benchmark and foster the\ndevelopment of T2I models. All resources will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large multimodal models (LMMs), recent\ntext-to-image (T2I) models can generate high-quality images and demonstrate\ngreat alignment to short prompts. However, they still struggle to effectively\nunderstand and follow long and detailed prompts, displaying inconsistent\ngeneration. To address this challenge, we introduce LPG-Bench, a comprehensive\nbenchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench\nfeatures 200 meticulously crafted prompts with an average length of over 250\nwords, approaching the input capacity of several leading commercial models.\nUsing these prompts, we generate 2,600 images from 13 state-of-the-art models\nand further perform comprehensive human-ranked annotations. Based on LPG-Bench,\nwe observe that state-of-the-art T2I alignment evaluation metrics exhibit poor\nconsistency with human preferences on long-prompt-based image generation. To\naddress the gap, we introduce a novel zero-shot metric based on\ntext-to-image-to-text consistency, termed TIT, for evaluating\nlong-prompt-generated images. The core concept of TIT is to quantify T2I\nalignment by directly comparing the consistency between the raw prompt and the\nLMM-produced description on the generated image, which includes an efficient\nscore-based instantiation TIT-Score and a large-language-model (LLM) based\ninstantiation TIT-Score-LLM. Extensive experiments demonstrate that our\nframework achieves superior alignment with human judgment compared to\nCLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute\nimprovement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT\nmethods together offer a deeper perspective to benchmark and foster the\ndevelopment of T2I models. All resources will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Juntong Wang"
                    },
                    {
                        "name": "Huiyu Duan"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Ziheng Jia"
                    },
                    {
                        "name": "Guangtao Zhai"
                    },
                    {
                        "name": "Xiongkuo Min"
                    }
                ],
                "author_detail": {
                    "name": "Xiongkuo Min"
                },
                "author": "Xiongkuo Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02986v1",
                "updated": "2025-10-03T13:22:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    22,
                    54,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T13:22:54Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    22,
                    54,
                    4,
                    276,
                    0
                ],
                "title": "FR-LUX: Friction-Aware, Regime-Conditioned Policy Optimization for\n  Implementable Portfolio Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FR-LUX: Friction-Aware, Regime-Conditioned Policy Optimization for\n  Implementable Portfolio Management"
                },
                "summary": "Transaction costs and regime shifts are major reasons why paper portfolios\nfail in live trading. We introduce FR-LUX (Friction-aware, Regime-conditioned\nLearning under eXecution costs), a reinforcement learning framework that learns\nafter-cost trading policies and remains robust across volatility-liquidity\nregimes. FR-LUX integrates three ingredients: (i) a microstructure-consistent\nexecution model combining proportional and impact costs, directly embedded in\nthe reward; (ii) a trade-space trust region that constrains changes in\ninventory flow rather than logits, yielding stable low-turnover updates; and\n(iii) explicit regime conditioning so the policy specializes to LL/LH/HL/HH\nstates without fragmenting the data. On a 4 x 5 grid of regimes and cost levels\nwith multiple random seeds, FR-LUX achieves the top average Sharpe ratio with\nnarrow bootstrap confidence intervals, maintains a flatter cost-performance\nslope than strong baselines, and attains superior risk-return efficiency for a\ngiven turnover budget. Pairwise scenario-level improvements are strictly\npositive and remain statistically significant after multiple-testing\ncorrections. We provide formal guarantees on optimality under convex frictions,\nmonotonic improvement under a KL trust region, long-run turnover bounds and\ninduced inaction bands due to proportional costs, positive value advantage for\nregime-conditioned policies, and robustness to cost misspecification. The\nmethodology is implementable: costs are calibrated from standard liquidity\nproxies, scenario-level inference avoids pseudo-replication, and all figures\nand tables are reproducible from released artifacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction costs and regime shifts are major reasons why paper portfolios\nfail in live trading. We introduce FR-LUX (Friction-aware, Regime-conditioned\nLearning under eXecution costs), a reinforcement learning framework that learns\nafter-cost trading policies and remains robust across volatility-liquidity\nregimes. FR-LUX integrates three ingredients: (i) a microstructure-consistent\nexecution model combining proportional and impact costs, directly embedded in\nthe reward; (ii) a trade-space trust region that constrains changes in\ninventory flow rather than logits, yielding stable low-turnover updates; and\n(iii) explicit regime conditioning so the policy specializes to LL/LH/HL/HH\nstates without fragmenting the data. On a 4 x 5 grid of regimes and cost levels\nwith multiple random seeds, FR-LUX achieves the top average Sharpe ratio with\nnarrow bootstrap confidence intervals, maintains a flatter cost-performance\nslope than strong baselines, and attains superior risk-return efficiency for a\ngiven turnover budget. Pairwise scenario-level improvements are strictly\npositive and remain statistically significant after multiple-testing\ncorrections. We provide formal guarantees on optimality under convex frictions,\nmonotonic improvement under a KL trust region, long-run turnover bounds and\ninduced inaction bands due to proportional costs, positive value advantage for\nregime-conditioned policies, and robustness to cost misspecification. The\nmethodology is implementable: costs are calibrated from standard liquidity\nproxies, scenario-level inference avoids pseudo-replication, and all figures\nand tables are reproducible from released artifacts."
                },
                "authors": [
                    {
                        "name": "Jian'an Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian'an Zhang"
                },
                "author": "Jian'an Zhang",
                "arxiv_comment": "19 pages, 7 figures, includes theoretical guarantees and empirical\n  evaluation, submitted to AI/ML in Finance track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91G10, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02980v1",
                "updated": "2025-10-03T13:13:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    13,
                    57,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T13:13:57Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    13,
                    57,
                    4,
                    276,
                    0
                ],
                "title": "Likelihood-based reconstruction of muon lateral distribution function\n  using combined integrator and binary detector modes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood-based reconstruction of muon lateral distribution function\n  using combined integrator and binary detector modes"
                },
                "summary": "The origin of ultra-high-energy cosmic rays, with energies $E \\geq 10^{18}$\neV, remains unknown. Among the key observables used to investigate their nature\nare the energy spectrum, the arrival direction distribution, and the\ncomposition as a function of energy. The composition of the primary cosmic ray\nis inferred from properties of the extensive air showers they initiate,\nparticularly from parameters sensitive to the primary mass. The most sensitive\nparameters to the primary mass are the atmospheric depth of the shower maximum,\ntypically measured with fluorescence telescopes, and the muon content of the\nshower, measured using dedicated muon detectors. A commonly used observable in\ncomposition studies is the muon density at a fixed distance from the shower\naxis, derived by evaluating the reconstructed muon lateral distribution\nfunction (MLDF) at a reference distance. A specific type of muon detector\nfeatures two acquisition modes: binary and integrator (commonly referred to as\nADC mode, for Analog-to-Digital Converter). The binary mode allows for direct\nmuon counting, while the ADC mode infers the muon number from the integrated\nsignal of the detector response. Existing methods reconstruct the MLDF using\ndata from either acquisition mode individually, or by combining both, but\nusually assigning a single mode per detector station in a given event. This\nwork presents a novel method to reconstruct the MLDF based on a likelihood\napproach that simultaneously incorporates data from both acquisition modes at\neach detector station. We apply our method to the underground muon detectors of\nthe Pierre Auger Observatory as a case study. However, this general approach\ncan be applied to future detectors with dual acquisition capabilities. Our\nresults demonstrate that the combined method outperforms traditional techniques\nthat rely solely on either binary or ADC mode data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The origin of ultra-high-energy cosmic rays, with energies $E \\geq 10^{18}$\neV, remains unknown. Among the key observables used to investigate their nature\nare the energy spectrum, the arrival direction distribution, and the\ncomposition as a function of energy. The composition of the primary cosmic ray\nis inferred from properties of the extensive air showers they initiate,\nparticularly from parameters sensitive to the primary mass. The most sensitive\nparameters to the primary mass are the atmospheric depth of the shower maximum,\ntypically measured with fluorescence telescopes, and the muon content of the\nshower, measured using dedicated muon detectors. A commonly used observable in\ncomposition studies is the muon density at a fixed distance from the shower\naxis, derived by evaluating the reconstructed muon lateral distribution\nfunction (MLDF) at a reference distance. A specific type of muon detector\nfeatures two acquisition modes: binary and integrator (commonly referred to as\nADC mode, for Analog-to-Digital Converter). The binary mode allows for direct\nmuon counting, while the ADC mode infers the muon number from the integrated\nsignal of the detector response. Existing methods reconstruct the MLDF using\ndata from either acquisition mode individually, or by combining both, but\nusually assigning a single mode per detector station in a given event. This\nwork presents a novel method to reconstruct the MLDF based on a likelihood\napproach that simultaneously incorporates data from both acquisition modes at\neach detector station. We apply our method to the underground muon detectors of\nthe Pierre Auger Observatory as a case study. However, this general approach\ncan be applied to future detectors with dual acquisition capabilities. Our\nresults demonstrate that the combined method outperforms traditional techniques\nthat rely solely on either binary or ADC mode data."
                },
                "authors": [
                    {
                        "name": "A. D. Supanitsky"
                    },
                    {
                        "name": "D. Ravignani"
                    },
                    {
                        "name": "V. V. Kizakke Covilakam"
                    }
                ],
                "author_detail": {
                    "name": "V. V. Kizakke Covilakam"
                },
                "author": "V. V. Kizakke Covilakam",
                "arxiv_comment": "Accepted in Astroparticle Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02970v1",
                "updated": "2025-10-03T12:59:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    59,
                    59,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:59:59Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    59,
                    59,
                    4,
                    276,
                    0
                ],
                "title": "Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis"
                },
                "summary": "Separating shared and independent features is crucial for multi-phase\ncontrast-enhanced (CE) MRI synthesis. However, existing methods use deep\nautoencoder generators with low parameter efficiency and lack interpretable\ntraining strategies. In this paper, we propose Flip Distribution Alignment\nVariational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model\nfor multi-phase CE MRI synthesis. Our method encodes input and target images\ninto two latent distributions that are symmetric concerning a standard normal\ndistribution, effectively separating shared and independent features. The\nY-shaped bidirectional training strategy further enhances the interpretability\nof feature separation. Experimental results show that compared to existing deep\nautoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces\nmodel parameters and inference time while effectively improving synthesis\nquality. The source code is publicly available at\nhttps://github.com/QianMuXiao/FDA-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separating shared and independent features is crucial for multi-phase\ncontrast-enhanced (CE) MRI synthesis. However, existing methods use deep\nautoencoder generators with low parameter efficiency and lack interpretable\ntraining strategies. In this paper, we propose Flip Distribution Alignment\nVariational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model\nfor multi-phase CE MRI synthesis. Our method encodes input and target images\ninto two latent distributions that are symmetric concerning a standard normal\ndistribution, effectively separating shared and independent features. The\nY-shaped bidirectional training strategy further enhances the interpretability\nof feature separation. Experimental results show that compared to existing deep\nautoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces\nmodel parameters and inference time while effectively improving synthesis\nquality. The source code is publicly available at\nhttps://github.com/QianMuXiao/FDA-VAE."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Kui"
                    },
                    {
                        "name": "Qianmu Xiao"
                    },
                    {
                        "name": "Qqinsong Li"
                    },
                    {
                        "name": "Zexin Ji"
                    },
                    {
                        "name": "JIelin Zhang"
                    },
                    {
                        "name": "Beiji Zou"
                    }
                ],
                "author_detail": {
                    "name": "Beiji Zou"
                },
                "author": "Beiji Zou",
                "arxiv_doi": "10.1007/978-3-032-05185-1_21",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-05185-1_21",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.02970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been early accept by MICCAI 2025",
                "arxiv_journal_ref": "Medical Image Computing and Computer Assisted Intervention\n  (MICCAI), 2025, 208-218",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02967v1",
                "updated": "2025-10-03T12:57:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    57,
                    13,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:57:13Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    57,
                    13,
                    4,
                    276,
                    0
                ],
                "title": "Grounding Large Language Models in Clinical Evidence: A\n  Retrieval-Augmented Generation System for Querying UK NICE Clinical\n  Guidelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Large Language Models in Clinical Evidence: A\n  Retrieval-Augmented Generation System for Querying UK NICE Clinical\n  Guidelines"
                },
                "summary": "This paper presents the development and evaluation of a Retrieval-Augmented\nGeneration (RAG) system for querying the United Kingdom's National Institute\nfor Health and Care Excellence (NICE) clinical guidelines using Large Language\nModels (LLMs). The extensive length and volume of these guidelines can impede\ntheir utilisation within a time-constrained healthcare system, a challenge this\nproject addresses through the creation of a system capable of providing users\nwith precisely matched information in response to natural language queries. The\nsystem's retrieval architecture, composed of a hybrid embedding mechanism, was\nevaluated against a database of 10,195 text chunks derived from three hundred\nguidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)\nof 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten\nretrieved chunks, when evaluated on 7901 queries.\n  The most significant impact of the RAG system was observed during the\ngeneration phase. When evaluated on a manually curated dataset of seventy\nquestion-answer pairs, RAG-enhanced models showed substantial gains in\nperformance. Faithfulness, the measure of whether an answer is supported by the\nsource text, was increased by 64.7 percentage points to 99.5% for the\nRAG-enhanced O4-Mini model and significantly outperformed the medical-focused\nMeditron3-8B LLM, which scored 43%. This, combined with a perfect Context\nPrecision score of 1 for all RAG-enhanced models, confirms the system's ability\nto prevent information fabrication by grounding its answers in relevant source\nmaterial. This study thus establishes RAG as an effective, reliable, and\nscalable approach for applying generative AI in healthcare, enabling\ncost-effective access to medical guidelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and evaluation of a Retrieval-Augmented\nGeneration (RAG) system for querying the United Kingdom's National Institute\nfor Health and Care Excellence (NICE) clinical guidelines using Large Language\nModels (LLMs). The extensive length and volume of these guidelines can impede\ntheir utilisation within a time-constrained healthcare system, a challenge this\nproject addresses through the creation of a system capable of providing users\nwith precisely matched information in response to natural language queries. The\nsystem's retrieval architecture, composed of a hybrid embedding mechanism, was\nevaluated against a database of 10,195 text chunks derived from three hundred\nguidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)\nof 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten\nretrieved chunks, when evaluated on 7901 queries.\n  The most significant impact of the RAG system was observed during the\ngeneration phase. When evaluated on a manually curated dataset of seventy\nquestion-answer pairs, RAG-enhanced models showed substantial gains in\nperformance. Faithfulness, the measure of whether an answer is supported by the\nsource text, was increased by 64.7 percentage points to 99.5% for the\nRAG-enhanced O4-Mini model and significantly outperformed the medical-focused\nMeditron3-8B LLM, which scored 43%. This, combined with a perfect Context\nPrecision score of 1 for all RAG-enhanced models, confirms the system's ability\nto prevent information fabrication by grounding its answers in relevant source\nmaterial. This study thus establishes RAG as an effective, reliable, and\nscalable approach for applying generative AI in healthcare, enabling\ncost-effective access to medical guidelines."
                },
                "authors": [
                    {
                        "name": "Matthew Lewis"
                    },
                    {
                        "name": "Samuel Thio"
                    },
                    {
                        "name": "Richard JB Dobson"
                    },
                    {
                        "name": "Spiros Denaxas"
                    }
                ],
                "author_detail": {
                    "name": "Spiros Denaxas"
                },
                "author": "Spiros Denaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02964v1",
                "updated": "2025-10-03T12:53:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    53,
                    45,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:53:45Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    53,
                    45,
                    4,
                    276,
                    0
                ],
                "title": "External Data Extraction Attacks against Retrieval-Augmented Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External Data Extraction Attacks against Retrieval-Augmented Large\n  Language Models"
                },
                "summary": "In recent years, RAG has emerged as a key paradigm for enhancing large\nlanguage models (LLMs). By integrating externally retrieved information, RAG\nalleviates issues like outdated knowledge and, crucially, insufficient domain\nexpertise. While effective, RAG introduces new risks of external data\nextraction attacks (EDEAs), where sensitive or copyrighted data in its\nknowledge base may be extracted verbatim. These risks are particularly acute\nwhen RAG is used to customize specialized LLM applications with private\nknowledge bases. Despite initial studies exploring these risks, they often lack\na formalized framework, robust attack performance, and comprehensive\nevaluation, leaving critical questions about real-world EDEA feasibility\nunanswered.\n  In this paper, we present the first comprehensive study to formalize EDEAs\nagainst retrieval-augmented LLMs. We first formally define EDEAs and propose a\nunified framework decomposing their design into three components: extraction\ninstruction, jailbreak operator, and retrieval trigger, under which prior\nattacks can be considered instances within our framework. Guided by this\nframework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction\naTtack. Specifically, SECRET incorporates (1) an adaptive optimization process\nusing LLMs as optimizers to generate specialized jailbreak prompts for EDEAs,\nand (2) cluster-focused triggering, an adaptive strategy that alternates\nbetween global exploration and local exploitation to efficiently generate\neffective retrieval triggers. Extensive evaluations across 4 models reveal that\nSECRET significantly outperforms previous attacks, and is highly effective\nagainst all 16 tested RAG instances. Notably, SECRET successfully extracts 35%\nof the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas\nother attacks yield 0% extraction. Our findings call for attention to this\nemerging threat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, RAG has emerged as a key paradigm for enhancing large\nlanguage models (LLMs). By integrating externally retrieved information, RAG\nalleviates issues like outdated knowledge and, crucially, insufficient domain\nexpertise. While effective, RAG introduces new risks of external data\nextraction attacks (EDEAs), where sensitive or copyrighted data in its\nknowledge base may be extracted verbatim. These risks are particularly acute\nwhen RAG is used to customize specialized LLM applications with private\nknowledge bases. Despite initial studies exploring these risks, they often lack\na formalized framework, robust attack performance, and comprehensive\nevaluation, leaving critical questions about real-world EDEA feasibility\nunanswered.\n  In this paper, we present the first comprehensive study to formalize EDEAs\nagainst retrieval-augmented LLMs. We first formally define EDEAs and propose a\nunified framework decomposing their design into three components: extraction\ninstruction, jailbreak operator, and retrieval trigger, under which prior\nattacks can be considered instances within our framework. Guided by this\nframework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction\naTtack. Specifically, SECRET incorporates (1) an adaptive optimization process\nusing LLMs as optimizers to generate specialized jailbreak prompts for EDEAs,\nand (2) cluster-focused triggering, an adaptive strategy that alternates\nbetween global exploration and local exploitation to efficiently generate\neffective retrieval triggers. Extensive evaluations across 4 models reveal that\nSECRET significantly outperforms previous attacks, and is highly effective\nagainst all 16 tested RAG instances. Notably, SECRET successfully extracts 35%\nof the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas\nother attacks yield 0% extraction. Our findings call for attention to this\nemerging threat."
                },
                "authors": [
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Yifei Chen"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Leyi Qi"
                    },
                    {
                        "name": "Boheng Li"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02962v1",
                "updated": "2025-10-03T12:53:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    53,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:53:02Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    53,
                    2,
                    4,
                    276,
                    0
                ],
                "title": "Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in\n  Large Language Models via Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in\n  Large Language Models via Watermarking"
                },
                "summary": "Large Language Models (LLMs) are increasingly fine-tuned on smaller,\ndomain-specific datasets to improve downstream performance. These datasets\noften contain proprietary or copyrighted material, raising the need for\nreliable safeguards against unauthorized use. Existing membership inference\nattacks (MIAs) and dataset-inference methods typically require access to\ninternal signals such as logits, while current black-box approaches often rely\non handcrafted prompts or a clean reference dataset for calibration, both of\nwhich limit practical applicability. Watermarking is a promising alternative,\nbut prior techniques can degrade text quality or reduce task performance. We\npropose TRACE, a practical framework for fully black-box detection of\ncopyrighted dataset usage in LLM fine-tuning. \\texttt{TRACE} rewrites datasets\nwith distortion-free watermarks guided by a private key, ensuring both text\nquality and downstream utility. At detection time, we exploit the radioactivity\neffect of fine-tuning on watermarked data and introduce an entropy-gated\nprocedure that selectively scores high-uncertainty tokens, substantially\namplifying detection power. Across diverse datasets and model families, TRACE\nconsistently achieves significant detections (p<0.05), often with extremely\nstrong statistical evidence. Furthermore, it supports multi-dataset attribution\nand remains robust even after continued pretraining on large non-watermarked\ncorpora. These results establish TRACE as a practical route to reliable\nblack-box verification of copyrighted dataset usage. We will make our code\navailable at: https://github.com/NusIoraPrivacy/TRACE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly fine-tuned on smaller,\ndomain-specific datasets to improve downstream performance. These datasets\noften contain proprietary or copyrighted material, raising the need for\nreliable safeguards against unauthorized use. Existing membership inference\nattacks (MIAs) and dataset-inference methods typically require access to\ninternal signals such as logits, while current black-box approaches often rely\non handcrafted prompts or a clean reference dataset for calibration, both of\nwhich limit practical applicability. Watermarking is a promising alternative,\nbut prior techniques can degrade text quality or reduce task performance. We\npropose TRACE, a practical framework for fully black-box detection of\ncopyrighted dataset usage in LLM fine-tuning. \\texttt{TRACE} rewrites datasets\nwith distortion-free watermarks guided by a private key, ensuring both text\nquality and downstream utility. At detection time, we exploit the radioactivity\neffect of fine-tuning on watermarked data and introduce an entropy-gated\nprocedure that selectively scores high-uncertainty tokens, substantially\namplifying detection power. Across diverse datasets and model families, TRACE\nconsistently achieves significant detections (p<0.05), often with extremely\nstrong statistical evidence. Furthermore, it supports multi-dataset attribution\nand remains robust even after continued pretraining on large non-watermarked\ncorpora. These results establish TRACE as a practical route to reliable\nblack-box verification of copyrighted dataset usage. We will make our code\navailable at: https://github.com/NusIoraPrivacy/TRACE."
                },
                "authors": [
                    {
                        "name": "Jingqi Zhang"
                    },
                    {
                        "name": "Ruibo Chen"
                    },
                    {
                        "name": "Yingqing Yang"
                    },
                    {
                        "name": "Peihua Mai"
                    },
                    {
                        "name": "Heng Huang"
                    },
                    {
                        "name": "Yan Pang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Pang"
                },
                "author": "Yan Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24183v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24183v3",
                "updated": "2025-10-03T12:52:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    52,
                    4,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-30T03:51:06Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    3,
                    51,
                    6,
                    4,
                    150,
                    0
                ],
                "title": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation"
                },
                "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while even exceeding the\nperformance of 671B DeepSeek-R1 on RTLLM. We have released our model, training\ncode, and dataset to facilitate research in EDA and LLM communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while even exceeding the\nperformance of 671B DeepSeek-R1 on RTLLM. We have released our model, training\ncode, and dataset to facilitate research in EDA and LLM communities."
                },
                "authors": [
                    {
                        "name": "Yaoyu Zhu"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Hanqi Lyu"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Chongxiao Li"
                    },
                    {
                        "name": "Wenxuan Shi"
                    },
                    {
                        "name": "Yutong Wu"
                    },
                    {
                        "name": "Jianan Mu"
                    },
                    {
                        "name": "Jinghua Wang"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yunji Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Chen"
                },
                "author": "Yunji Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24183v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24183v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02958v1",
                "updated": "2025-10-03T12:50:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    50,
                    3,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:50:03Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    50,
                    3,
                    4,
                    276,
                    0
                ],
                "title": "Sequence-Based Deep Learning for Handover Optimization in Dense Urban\n  Cellular Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-Based Deep Learning for Handover Optimization in Dense Urban\n  Cellular Network"
                },
                "summary": "Efficient handover management remains a critical challenge in dense urban\ncellular networks, where high cell density, user mobility, and diverse service\ndemands increase the likelihood of unnecessary handovers and ping-pong effects.\nThis paper leverages a real-world, multi-operator drive-test dataset of 30,925\nlabelled records collected within a 2 km area around Sunway City to investigate\nsequence-based deep learning approaches for handover detection and avoidance.\nWe formulate handover prediction as a sequence problem and evaluate Gated\nRecurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer\narchitectures under Reference Signal Received Power (RSRP)-only and all-feature\nsettings. The integration of multi-dimensional features significantly enhanced\nhandover performance in dense urban cellular networks. The proposed GRU-based\nmodel achieved a remarkable 98% reduction in ping-pong handovers, alongside a\n46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only\napproach which yielded a 22.19% reduction. Furthermore, the model demonstrated\na 46% improvement in Time of Stay (ToS), indicating more stable user\nconnections. With an inference time of just 0.91 seconds, the solution proves\nhighly efficient and well-suited for real-time edge deployment scenarios.\nCompared to the conventional 3GPP A3 algorithm, these improvements demonstrate\nsignificant gains in mobility robustness and user Quality of Experience (QoE)\nimprovement. The dataset is released to foster reproducibility and further\nresearch in intelligent mobility management for 5G and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient handover management remains a critical challenge in dense urban\ncellular networks, where high cell density, user mobility, and diverse service\ndemands increase the likelihood of unnecessary handovers and ping-pong effects.\nThis paper leverages a real-world, multi-operator drive-test dataset of 30,925\nlabelled records collected within a 2 km area around Sunway City to investigate\nsequence-based deep learning approaches for handover detection and avoidance.\nWe formulate handover prediction as a sequence problem and evaluate Gated\nRecurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer\narchitectures under Reference Signal Received Power (RSRP)-only and all-feature\nsettings. The integration of multi-dimensional features significantly enhanced\nhandover performance in dense urban cellular networks. The proposed GRU-based\nmodel achieved a remarkable 98% reduction in ping-pong handovers, alongside a\n46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only\napproach which yielded a 22.19% reduction. Furthermore, the model demonstrated\na 46% improvement in Time of Stay (ToS), indicating more stable user\nconnections. With an inference time of just 0.91 seconds, the solution proves\nhighly efficient and well-suited for real-time edge deployment scenarios.\nCompared to the conventional 3GPP A3 algorithm, these improvements demonstrate\nsignificant gains in mobility robustness and user Quality of Experience (QoE)\nimprovement. The dataset is released to foster reproducibility and further\nresearch in intelligent mobility management for 5G and beyond."
                },
                "authors": [
                    {
                        "name": "Muhammad Kabeer"
                    },
                    {
                        "name": "Rosdiadee Nordin"
                    },
                    {
                        "name": "Mehran Behjati"
                    },
                    {
                        "name": "Lau Sian Lun"
                    }
                ],
                "author_detail": {
                    "name": "Lau Sian Lun"
                },
                "author": "Lau Sian Lun",
                "arxiv_comment": "6 pages, 6 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13761v2",
                "updated": "2025-10-03T12:48:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    48,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-17T07:16:12Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    16,
                    12,
                    2,
                    260,
                    0
                ],
                "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both episode-level problem solving and step-level code generation. This is\nmotivated by our key insight that the success of an intermediate tool call is a\nstrong predictor of the final answer's correctness. Finally, THOR incorporates\na self-correction mechanism that leverages immediate tool feedback to\ndynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both episode-level problem solving and step-level code generation. This is\nmotivated by our key insight that the success of an intermediate tool call is a\nstrong predictor of the final answer's correctness. Finally, THOR incorporates\na self-correction mechanism that leverages immediate tool feedback to\ndynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR."
                },
                "authors": [
                    {
                        "name": "Qikai Chang"
                    },
                    {
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Jiefeng Ma"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Quan Liu"
                    },
                    {
                        "name": "Jianqing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianqing Gao"
                },
                "author": "Jianqing Gao",
                "arxiv_comment": "22 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02952v1",
                "updated": "2025-10-03T12:46:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    46,
                    24,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:46:24Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    46,
                    24,
                    4,
                    276,
                    0
                ],
                "title": "ContextFlow: Context-Aware Flow Matching For Trajectory Inference From\n  Spatial Omics Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextFlow: Context-Aware Flow Matching For Trajectory Inference From\n  Spatial Omics Data"
                },
                "summary": "Inferring trajectories from longitudinal spatially-resolved omics data is\nfundamental to understanding the dynamics of structural and functional tissue\nchanges in development, regeneration and repair, disease progression, and\nresponse to treatment. We propose ContextFlow, a novel context-aware flow\nmatching framework that incorporates prior knowledge to guide the inference of\nstructural tissue dynamics from spatially resolved omics data. Specifically,\nContextFlow integrates local tissue organization and ligand-receptor\ncommunication patterns into a transition plausibility matrix that regularizes\nthe optimal transport objective. By embedding these contextual constraints,\nContextFlow generates trajectories that are not only statistically consistent\nbut also biologically meaningful, making it a generalizable framework for\nmodeling spatiotemporal dynamics from longitudinal, spatially resolved omics\ndata. Evaluated on three datasets, ContextFlow consistently outperforms\nstate-of-the-art flow matching methods across multiple quantitative and\nqualitative metrics of inference accuracy and biological coherence. Our code is\navailable at: \\href{https://github.com/santanurathod/ContextFlow}{ContextFlow}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring trajectories from longitudinal spatially-resolved omics data is\nfundamental to understanding the dynamics of structural and functional tissue\nchanges in development, regeneration and repair, disease progression, and\nresponse to treatment. We propose ContextFlow, a novel context-aware flow\nmatching framework that incorporates prior knowledge to guide the inference of\nstructural tissue dynamics from spatially resolved omics data. Specifically,\nContextFlow integrates local tissue organization and ligand-receptor\ncommunication patterns into a transition plausibility matrix that regularizes\nthe optimal transport objective. By embedding these contextual constraints,\nContextFlow generates trajectories that are not only statistically consistent\nbut also biologically meaningful, making it a generalizable framework for\nmodeling spatiotemporal dynamics from longitudinal, spatially resolved omics\ndata. Evaluated on three datasets, ContextFlow consistently outperforms\nstate-of-the-art flow matching methods across multiple quantitative and\nqualitative metrics of inference accuracy and biological coherence. Our code is\navailable at: \\href{https://github.com/santanurathod/ContextFlow}{ContextFlow}"
                },
                "authors": [
                    {
                        "name": "Santanu Subhash Rathod"
                    },
                    {
                        "name": "Francesco Ceccarelli"
                    },
                    {
                        "name": "Sean B. Holden"
                    },
                    {
                        "name": "Pietro Li"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Jovan Tanevski"
                    }
                ],
                "author_detail": {
                    "name": "Jovan Tanevski"
                },
                "author": "Jovan Tanevski",
                "arxiv_comment": "26 pages, 9 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26495v2",
                "updated": "2025-10-03T12:46:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    46,
                    22,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-30T16:39:17Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    39,
                    17,
                    1,
                    273,
                    0
                ],
                "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!"
                },
                "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models - Qwen-3 (235B) with 77.77% and Mistral (24B) with 79.96% -\nfall far short of reliable operational safety, while GPT models plateau in the\n62-73% range, Phi achieves only mid-level scores (48-70%), and Gemma and\nLlama-3 collapse to 39.53% and 23.84%, respectively. While operational safety\nis a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41% and Qwen-3 (30B) by 27%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models - Qwen-3 (235B) with 77.77% and Mistral (24B) with 79.96% -\nfall far short of reliable operational safety, while GPT models plateau in the\n62-73% range, Phi achieves only mid-level scores (48-70%), and Gemma and\nLlama-3 collapse to 39.53% and 23.84%, respectively. While operational safety\nis a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41% and Qwen-3 (30B) by 27%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents."
                },
                "authors": [
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Varun Gumma"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Seok Min Lim"
                    },
                    {
                        "name": "Chuan Li"
                    },
                    {
                        "name": "Amir Zadeh"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03923v2",
                "updated": "2025-10-03T12:29:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    29,
                    9,
                    4,
                    276,
                    0
                ],
                "published": "2024-03-06T18:33:51Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    18,
                    33,
                    51,
                    2,
                    66,
                    0
                ],
                "title": "Did Translation Models Get More Robust Without Anyone Even Noticing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Did Translation Models Get More Robust Without Anyone Even Noticing?"
                },
                "summary": "Neural machine translation (MT) models achieve strong results across a\nvariety of settings, but it is widely believed that they are highly sensitive\nto \"noisy\" inputs, such as spelling errors, abbreviations, and other formatting\nissues. In this paper, we revisit this insight in light of recent multilingual\nMT models and large language models (LLMs) applied to machine translation.\nSomewhat surprisingly, we show through controlled experiments that these models\nare far more robust to many kinds of noise than previous models, even when they\nperform similarly on clean data. This is notable because, even though LLMs have\nmore parameters and more complex training processes than past models, none of\nthe open ones we consider use any techniques specifically designed to encourage\nrobustness. Next, we show that similar trends hold for social media translation\nexperiments -- LLMs are more robust to social media text. We include an\nanalysis of the circumstances in which source correction techniques can be used\nto mitigate the effects of noise. Altogether, we show that robustness to many\ntypes of noise has increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural machine translation (MT) models achieve strong results across a\nvariety of settings, but it is widely believed that they are highly sensitive\nto \"noisy\" inputs, such as spelling errors, abbreviations, and other formatting\nissues. In this paper, we revisit this insight in light of recent multilingual\nMT models and large language models (LLMs) applied to machine translation.\nSomewhat surprisingly, we show through controlled experiments that these models\nare far more robust to many kinds of noise than previous models, even when they\nperform similarly on clean data. This is notable because, even though LLMs have\nmore parameters and more complex training processes than past models, none of\nthe open ones we consider use any techniques specifically designed to encourage\nrobustness. Next, we show that similar trends hold for social media translation\nexperiments -- LLMs are more robust to social media text. We include an\nanalysis of the circumstances in which source correction techniques can be used\nto mitigate the effects of noise. Altogether, we show that robustness to many\ntypes of noise has increased."
                },
                "authors": [
                    {
                        "name": "Ben Peters"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr F. T. Martins"
                },
                "author": "Andr F. T. Martins",
                "arxiv_comment": "ACL 2025 (Main) camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02934v1",
                "updated": "2025-10-03T12:25:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    25,
                    28,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:25:28Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    25,
                    28,
                    4,
                    276,
                    0
                ],
                "title": "Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic\n  Internal Representation Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic\n  Internal Representation Selection"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation and are increasingly integrated into the software development\nprocess. However, ensuring the correctness of LLM-generated code remains a\ncritical concern. Prior work has shown that the internal representations of\nLLMs encode meaningful signals for assessing code correctness. Nevertheless,\nthe existing methods rely on representations from pre-selected/fixed layers and\ntoken positions, which could limit its generalizability across diverse model\narchitectures and tasks. In this work, we introduce AUTOPROBE, a novel\nmodel-agnostic approach that dynamically selects the most informative internal\nrepresentations for code correctness assessment. AUTOPROBE employs an\nattention-based mechanism to learn importance scores for hidden states,\nenabling it to focus on the most relevant features. These weighted\nrepresentations are then aggregated and passed to a probing classifier to\npredict code correctness across multiple dimensions, including compilability,\nfunctionality, and security. To evaluate the performance of AUTOPROBE, we\nconduct extensive experiments across multiple benchmarks and code LLMs. Our\nexperimental results show that AUTOPROBE consistently outperforms the\nbaselines. For security assessment, AUTOPROBE surpasses the state-of-the-art\nwhite-box approach by 18%. For compilability and functionality assessment,\nAUTOPROBE demonstrates its highest robustness to code complexity, with the\nperformance higher than the other approaches by up to 19% and 111%,\nrespectively. These findings highlight that dynamically selecting important\ninternal signals enables AUTOPROBE to serve as a robust and generalizable\nsolution for assessing the correctness of code generated by various LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation and are increasingly integrated into the software development\nprocess. However, ensuring the correctness of LLM-generated code remains a\ncritical concern. Prior work has shown that the internal representations of\nLLMs encode meaningful signals for assessing code correctness. Nevertheless,\nthe existing methods rely on representations from pre-selected/fixed layers and\ntoken positions, which could limit its generalizability across diverse model\narchitectures and tasks. In this work, we introduce AUTOPROBE, a novel\nmodel-agnostic approach that dynamically selects the most informative internal\nrepresentations for code correctness assessment. AUTOPROBE employs an\nattention-based mechanism to learn importance scores for hidden states,\nenabling it to focus on the most relevant features. These weighted\nrepresentations are then aggregated and passed to a probing classifier to\npredict code correctness across multiple dimensions, including compilability,\nfunctionality, and security. To evaluate the performance of AUTOPROBE, we\nconduct extensive experiments across multiple benchmarks and code LLMs. Our\nexperimental results show that AUTOPROBE consistently outperforms the\nbaselines. For security assessment, AUTOPROBE surpasses the state-of-the-art\nwhite-box approach by 18%. For compilability and functionality assessment,\nAUTOPROBE demonstrates its highest robustness to code complexity, with the\nperformance higher than the other approaches by up to 19% and 111%,\nrespectively. These findings highlight that dynamically selecting important\ninternal signals enables AUTOPROBE to serve as a robust and generalizable\nsolution for assessing the correctness of code generated by various LLMs."
                },
                "authors": [
                    {
                        "name": "Thanh Trong Vu"
                    },
                    {
                        "name": "Tuan-Dung Bui"
                    },
                    {
                        "name": "Thu-Trang Nguyen"
                    },
                    {
                        "name": "Son Nguyen"
                    },
                    {
                        "name": "Hieu Dinh Vo"
                    }
                ],
                "author_detail": {
                    "name": "Hieu Dinh Vo"
                },
                "author": "Hieu Dinh Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01252v2",
                "updated": "2025-10-03T12:24:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    24,
                    5,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-24T11:10:16Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    10,
                    16,
                    2,
                    267,
                    0
                ],
                "title": "GPT and Prejudice: A Sparse Approach to Understanding Learned\n  Representations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT and Prejudice: A Sparse Approach to Understanding Learned\n  Representations in Large Language Models"
                },
                "summary": "As large language models (LLMs) are increasingly trained on massive,\nuncurated corpora, understanding both model representations and the data they\ninternalize has become a major challenge. In this work, we show that pairing\nLLMs with sparse autoencoders (SAEs) enables interpretation not only of model\nbehavior but also of the deeper structures, themes, and biases embedded in the\ntraining data. We train a GPT-style transformer model exclusively on the novels\nof Jane Austen, a corpus rich in social constructs and narrative patterns. We\nthen apply SAEs to hidden states across multiple layers, uncovering sparse,\ninterpretable features that reflect the key narratives and concepts present in\nthe corpus, including gender, class, and societal duty. Our findings\ndemonstrate that LLMs combined with SAEs can act as scalable probes into\ncomplex datasets, offering a new path for corpus exploration, bias discovery,\nand model interpretability at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly trained on massive,\nuncurated corpora, understanding both model representations and the data they\ninternalize has become a major challenge. In this work, we show that pairing\nLLMs with sparse autoencoders (SAEs) enables interpretation not only of model\nbehavior but also of the deeper structures, themes, and biases embedded in the\ntraining data. We train a GPT-style transformer model exclusively on the novels\nof Jane Austen, a corpus rich in social constructs and narrative patterns. We\nthen apply SAEs to hidden states across multiple layers, uncovering sparse,\ninterpretable features that reflect the key narratives and concepts present in\nthe corpus, including gender, class, and societal duty. Our findings\ndemonstrate that LLMs combined with SAEs can act as scalable probes into\ncomplex datasets, offering a new path for corpus exploration, bias discovery,\nand model interpretability at scale."
                },
                "authors": [
                    {
                        "name": "Mariam Mahran"
                    },
                    {
                        "name": "Katharina Simbeck"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Simbeck"
                },
                "author": "Katharina Simbeck",
                "arxiv_comment": "Preprint. Draft version, subject to revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00410v2",
                "updated": "2025-10-03T12:15:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    15,
                    38,
                    4,
                    276,
                    0
                ],
                "published": "2025-08-01T08:09:14Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    9,
                    14,
                    4,
                    213,
                    0
                ],
                "title": "Co-rewarding: Stable Self-supervised RL for Eliciting Reasoning in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-rewarding: Stable Self-supervised RL for Eliciting Reasoning in Large\n  Language Models"
                },
                "summary": "While reinforcement learning with verifiable rewards (RLVR) is effective to\nimprove the reasoning ability of large language models (LLMs), its reliance on\nhuman-annotated labels leads to the scaling up dilemma, especially for complex\ntasks. Recent self-rewarding methods investigate a label-free alternative to\nunlock the reasoning capabilities of LLMs, yet they frequently encounter the\nnon-negligible training collapse issue, as the single-view supervision signal\neasily forms the self-consistent illusion, yielding the reward hacking.\nInspired by the success of self-supervised learning, we propose\n\\textit{Co-rewarding}, a novel self-supervised RL framework that improves\ntraining stability by seeking complementary supervision from another views.\nSpecifically, we instantiate Co-rewarding in two ways: (1)\n\\textit{Co-rewarding-I} is a data-side instantiation that derives reward\nsignals from contrastive agreement across semantically analogous questions; and\n(2) \\textit{Co-rewarding-II} is a model-side instantiation that maintains a\nslowly-updated reference teacher with pseudo labels to realize\nself-distillation. Intuitively, such instantiations introduce different levels\nof discrepancy to increase the difficulty of training collapse on trivial\nreasoning solutions. Empirically, Co-rewarding exhibits stable training across\nvarious setups, and outperforms other self-rewarding baselines by $+3.31\\%$\nimprovements on average on multiple mathematical reasoning benchmarks,\nespecially by $+7.49\\%$ on Llama-3.2-3B-Instruct. Notably, Co-rewarding reaches\nor even surpasses RLVR with ground-truth (GT) label in several cases, such as a\nPass@$1$ of $94.01\\%$ on GSM8K with Qwen3-8B-Base remarkably higher than GT.\nOur code is publicly available at https://github.com/tmlr-group/Co-rewarding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning with verifiable rewards (RLVR) is effective to\nimprove the reasoning ability of large language models (LLMs), its reliance on\nhuman-annotated labels leads to the scaling up dilemma, especially for complex\ntasks. Recent self-rewarding methods investigate a label-free alternative to\nunlock the reasoning capabilities of LLMs, yet they frequently encounter the\nnon-negligible training collapse issue, as the single-view supervision signal\neasily forms the self-consistent illusion, yielding the reward hacking.\nInspired by the success of self-supervised learning, we propose\n\\textit{Co-rewarding}, a novel self-supervised RL framework that improves\ntraining stability by seeking complementary supervision from another views.\nSpecifically, we instantiate Co-rewarding in two ways: (1)\n\\textit{Co-rewarding-I} is a data-side instantiation that derives reward\nsignals from contrastive agreement across semantically analogous questions; and\n(2) \\textit{Co-rewarding-II} is a model-side instantiation that maintains a\nslowly-updated reference teacher with pseudo labels to realize\nself-distillation. Intuitively, such instantiations introduce different levels\nof discrepancy to increase the difficulty of training collapse on trivial\nreasoning solutions. Empirically, Co-rewarding exhibits stable training across\nvarious setups, and outperforms other self-rewarding baselines by $+3.31\\%$\nimprovements on average on multiple mathematical reasoning benchmarks,\nespecially by $+7.49\\%$ on Llama-3.2-3B-Instruct. Notably, Co-rewarding reaches\nor even surpasses RLVR with ground-truth (GT) label in several cases, such as a\nPass@$1$ of $94.01\\%$ on GSM8K with Qwen3-8B-Base remarkably higher than GT.\nOur code is publicly available at https://github.com/tmlr-group/Co-rewarding."
                },
                "authors": [
                    {
                        "name": "Zizhuo Zhang"
                    },
                    {
                        "name": "Jianing Zhu"
                    },
                    {
                        "name": "Xinmu Ge"
                    },
                    {
                        "name": "Zihua Zhao"
                    },
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Jiangchao Yao"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02931v1",
                "updated": "2025-10-03T12:11:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    11,
                    47,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:11:47Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    11,
                    47,
                    4,
                    276,
                    0
                ],
                "title": "The Role of Asteroseismology in Understanding Mass Loss in Red Giants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Asteroseismology in Understanding Mass Loss in Red Giants"
                },
                "summary": "Red giant stars play a key role in advancing our understanding of stellar\nmass loss. However, its initial mass and the amount of mass lost during this\nphase remain uncertain. In this study, we investigate the asteroseismic\nsignatures of mass loss and the parameters that influence it. We examine six\nstars identified as red giant branch (RGB) stars in the APOKASC-2 catalog.\nAssuming these stars are on their first ascent of the RGB, we construct\ninterior models. The resulting model ages are significantly older than the age\nof the Galaxy, indicating that these stars are likely experiencing mass loss\nand evolving toward the red clump (RC) phase. The minimum possible initial\nmasses are estimated using the mass-metallicity diagram, from which we infer\nthat the minimum mass lost by these stars ranges from $0.1$-$0.3{\\rm\nM}_{\\odot}$. Models constructed with an initial minimum mass yield the maximum\npossible age of the star. The ages of these models fall within the range of\n9-9.5Gyr. For two stars, asteroseismic parameters confirm RC classification.\nDue to degeneracies in the HR diagram, distinguishing between internal\nstructure models is challenging; however, asteroseismic constraints provide\nclear differentiation. Although mass-loss and mass-conservation models have\nsimilar $M$, $R$, and $T_{\\rm eff}$ values, $\\Delta\\nu$s determined from the\n$l$=0 modes in the mass-loss models are 5-10$\\%$ higher than observed. This\ndiscrepancy may arise from differences in internal structure. Finally,\nevolutionary model grids are used to examine how initial mass and $Z$ affect\nmass loss. Mass loss increases with increasing metallicity and decreases with\nincreasing initial mass, regardless of the adopted value of $\\eta$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red giant stars play a key role in advancing our understanding of stellar\nmass loss. However, its initial mass and the amount of mass lost during this\nphase remain uncertain. In this study, we investigate the asteroseismic\nsignatures of mass loss and the parameters that influence it. We examine six\nstars identified as red giant branch (RGB) stars in the APOKASC-2 catalog.\nAssuming these stars are on their first ascent of the RGB, we construct\ninterior models. The resulting model ages are significantly older than the age\nof the Galaxy, indicating that these stars are likely experiencing mass loss\nand evolving toward the red clump (RC) phase. The minimum possible initial\nmasses are estimated using the mass-metallicity diagram, from which we infer\nthat the minimum mass lost by these stars ranges from $0.1$-$0.3{\\rm\nM}_{\\odot}$. Models constructed with an initial minimum mass yield the maximum\npossible age of the star. The ages of these models fall within the range of\n9-9.5Gyr. For two stars, asteroseismic parameters confirm RC classification.\nDue to degeneracies in the HR diagram, distinguishing between internal\nstructure models is challenging; however, asteroseismic constraints provide\nclear differentiation. Although mass-loss and mass-conservation models have\nsimilar $M$, $R$, and $T_{\\rm eff}$ values, $\\Delta\\nu$s determined from the\n$l$=0 modes in the mass-loss models are 5-10$\\%$ higher than observed. This\ndiscrepancy may arise from differences in internal structure. Finally,\nevolutionary model grids are used to examine how initial mass and $Z$ affect\nmass loss. Mass loss increases with increasing metallicity and decreases with\nincreasing initial mass, regardless of the adopted value of $\\eta$."
                },
                "authors": [
                    {
                        "name": "Sibel rtel"
                    },
                    {
                        "name": "Mutlu Yldz"
                    }
                ],
                "author_detail": {
                    "name": "Mutlu Yldz"
                },
                "author": "Mutlu Yldz",
                "arxiv_comment": "13 pages and 9 gigures. Accepted by MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14031v2",
                "updated": "2025-10-03T12:08:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    8,
                    27,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-20T07:29:41Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    29,
                    41,
                    1,
                    140,
                    0
                ],
                "title": "Reading.help: Supporting EFL Readers with Proactive and On-Demand\n  Explanation of English Grammar and Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading.help: Supporting EFL Readers with Proactive and On-Demand\n  Explanation of English Grammar and Semantics"
                },
                "summary": "A large portion of texts is written in English, but readers who see English\nas a Foreign Language (EFL) often struggle to read texts accurately and\nswiftly. EFL readers seek help from professional teachers and mentors, which is\nlimited and costly. In this paper, we explore how an intelligent reading tool\ncan assist EFL readers. We conducted a case study with EFL readers in South\nKorea. We at first developed an LLM-based reading tool based on prior\nliterature. We then revised the tool based on the feedback from a study with 15\nSouth Korean EFL readers. The final tool, named Reading.help, helps EFL readers\ncomprehend complex sentences and paragraphs with on-demand and proactive\nexplanations. We finally evaluated the tool with 5 EFL readers and 2 EFL\neducation professionals. Our findings suggest Reading.help could potentially\nhelp EFL readers self-learn English when they do not have access to external\nsupport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large portion of texts is written in English, but readers who see English\nas a Foreign Language (EFL) often struggle to read texts accurately and\nswiftly. EFL readers seek help from professional teachers and mentors, which is\nlimited and costly. In this paper, we explore how an intelligent reading tool\ncan assist EFL readers. We conducted a case study with EFL readers in South\nKorea. We at first developed an LLM-based reading tool based on prior\nliterature. We then revised the tool based on the feedback from a study with 15\nSouth Korean EFL readers. The final tool, named Reading.help, helps EFL readers\ncomprehend complex sentences and paragraphs with on-demand and proactive\nexplanations. We finally evaluated the tool with 5 EFL readers and 2 EFL\neducation professionals. Our findings suggest Reading.help could potentially\nhelp EFL readers self-learn English when they do not have access to external\nsupport."
                },
                "authors": [
                    {
                        "name": "Sunghyo Chung"
                    },
                    {
                        "name": "Hyeon Jeon"
                    },
                    {
                        "name": "Sungbok Shin"
                    },
                    {
                        "name": "Md Naimul Hoque"
                    }
                ],
                "author_detail": {
                    "name": "Md Naimul Hoque"
                },
                "author": "Md Naimul Hoque",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02919v1",
                "updated": "2025-10-03T11:46:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    46,
                    4,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T11:46:04Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    46,
                    4,
                    4,
                    276,
                    0
                ],
                "title": "Self-Reflective Generation at Test Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Reflective Generation at Test Time"
                },
                "summary": "Large language models (LLMs) increasingly solve complex reasoning tasks via\nlong chain-of-thought, but their forward-only autoregressive generation process\nis fragile; early token errors can cascade, which creates a clear need for\nself-reflection mechanisms. However, existing self-reflection either performs\nrevisions over full drafts or learns self-correction via expensive training,\nboth fundamentally reactive and inefficient. To address this, we propose\nSelf-Reflective Generation at Test Time (SRGen), a lightweight test-time\nframework that reflects before generating at uncertain points. During token\ngeneration, SRGen utilizes dynamic entropy thresholding to identify\nhigh-uncertainty tokens. For each identified token, it trains a specific\ncorrective vector, which fully exploits the already generated context for a\nself-reflective generation to correct the token probability distribution. By\nretrospectively analyzing the partial output, this self-reflection enables more\ntrustworthy decisions, thereby significantly reducing the probability of errors\nat highly uncertain points. Evaluated on challenging mathematical reasoning\nbenchmarks and a diverse set of LLMs, SRGen can consistently strengthen model\nreasoning: improvements in single-pass quality also translate into stronger\nself-consistency voting. Especially, on AIME2024 with\nDeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on\nPass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a\nplug-and-play method that integrates reflection into the generation process for\nreliable LLM reasoning, achieving consistent gains with bounded overhead and\nbroad composability with other training-time (e.g., RLHF) and test-time (e.g.,\nSLOT) techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly solve complex reasoning tasks via\nlong chain-of-thought, but their forward-only autoregressive generation process\nis fragile; early token errors can cascade, which creates a clear need for\nself-reflection mechanisms. However, existing self-reflection either performs\nrevisions over full drafts or learns self-correction via expensive training,\nboth fundamentally reactive and inefficient. To address this, we propose\nSelf-Reflective Generation at Test Time (SRGen), a lightweight test-time\nframework that reflects before generating at uncertain points. During token\ngeneration, SRGen utilizes dynamic entropy thresholding to identify\nhigh-uncertainty tokens. For each identified token, it trains a specific\ncorrective vector, which fully exploits the already generated context for a\nself-reflective generation to correct the token probability distribution. By\nretrospectively analyzing the partial output, this self-reflection enables more\ntrustworthy decisions, thereby significantly reducing the probability of errors\nat highly uncertain points. Evaluated on challenging mathematical reasoning\nbenchmarks and a diverse set of LLMs, SRGen can consistently strengthen model\nreasoning: improvements in single-pass quality also translate into stronger\nself-consistency voting. Especially, on AIME2024 with\nDeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on\nPass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a\nplug-and-play method that integrates reflection into the generation process for\nreliable LLM reasoning, achieving consistent gains with bounded overhead and\nbroad composability with other training-time (e.g., RLHF) and test-time (e.g.,\nSLOT) techniques."
                },
                "authors": [
                    {
                        "name": "Jian Mu"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Zhiyong Wang"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Shuang Qiu"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Zhongxiang Dai"
                    },
                    {
                        "name": "Yao Shu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Shu"
                },
                "author": "Yao Shu",
                "arxiv_comment": "24 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02917v1",
                "updated": "2025-10-03T11:44:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    44,
                    21,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T11:44:21Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    44,
                    21,
                    4,
                    276,
                    0
                ],
                "title": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse\n  Autoencoders"
                },
                "summary": "As Large Language Models become integral to software development, with\nsubstantial portions of AI-suggested code entering production, understanding\ntheir internal correctness mechanisms becomes critical for safe deployment. We\napply sparse autoencoders to decompose LLM representations, identifying\ndirections that correspond to code correctness. We select predictor directions\nusing t-statistics and steering directions through separation scores from base\nmodel representations, then analyze their mechanistic properties through\nsteering, attention analysis, and weight orthogonalization. We find that code\ncorrectness directions in LLMs reliably predict incorrect code, while\ncorrection capabilities, though statistically significant, involve tradeoffs\nbetween fixing errors and preserving correct code. Mechanistically, successful\ncode generation depends on attending to test cases rather than problem\ndescriptions. Moreover, directions identified in base models retain their\neffectiveness after instruction-tuning, suggesting code correctness mechanisms\nlearned during pre-training are repurposed during fine-tuning. Our mechanistic\ninsights suggest three practical applications: prompting strategies should\nprioritize test examples over elaborate problem descriptions, predictor\ndirections can serve as error alarms for developer review, and these same\npredictors can guide selective steering, intervening only when errors are\nanticipated to prevent the code corruption from constant steering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models become integral to software development, with\nsubstantial portions of AI-suggested code entering production, understanding\ntheir internal correctness mechanisms becomes critical for safe deployment. We\napply sparse autoencoders to decompose LLM representations, identifying\ndirections that correspond to code correctness. We select predictor directions\nusing t-statistics and steering directions through separation scores from base\nmodel representations, then analyze their mechanistic properties through\nsteering, attention analysis, and weight orthogonalization. We find that code\ncorrectness directions in LLMs reliably predict incorrect code, while\ncorrection capabilities, though statistically significant, involve tradeoffs\nbetween fixing errors and preserving correct code. Mechanistically, successful\ncode generation depends on attending to test cases rather than problem\ndescriptions. Moreover, directions identified in base models retain their\neffectiveness after instruction-tuning, suggesting code correctness mechanisms\nlearned during pre-training are repurposed during fine-tuning. Our mechanistic\ninsights suggest three practical applications: prompting strategies should\nprioritize test examples over elaborate problem descriptions, predictor\ndirections can serve as error alarms for developer review, and these same\npredictors can guide selective steering, intervening only when errors are\nanticipated to prevent the code corruption from constant steering."
                },
                "authors": [
                    {
                        "name": "Kriz Tahimic"
                    },
                    {
                        "name": "Charibeth Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Charibeth Cheng"
                },
                "author": "Charibeth Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23799v2",
                "updated": "2025-10-03T11:34:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    34,
                    59,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-28T10:49:22Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    10,
                    49,
                    22,
                    6,
                    271,
                    0
                ],
                "title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector\n  Refinement"
                },
                "summary": "Steering has emerged as a promising approach in controlling large language\nmodels (LLMs) without modifying model parameters. However, most existing\nsteering methods rely on large-scale datasets to learn clear behavioral\ninformation, which limits their applicability in many real-world scenarios. The\nsteering vectors extracted from small dataset often contain task-irrelevant\nnoising features, which degrades their effectiveness. To refine the steering\nvectors learned from limited data, we introduce Refinement of Steering Vector\nvia Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise\nand augment the steering vectors. In our framework, we first remove\ntask-irrelevant features according to their semantics provided by SAEs, and\nthen enrich task-relevant features missing from the small dataset through their\nsemantic similarity to the identified relevant features. Extensive experiments\ndemonstrate that the proposed SAE-RSV substantially outperforms all the\nbaseline methods including supervised fine-tuning. Our findings show that\neffective steering vector can be constructed from limited training data by\nrefining the original steering vector through SAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering has emerged as a promising approach in controlling large language\nmodels (LLMs) without modifying model parameters. However, most existing\nsteering methods rely on large-scale datasets to learn clear behavioral\ninformation, which limits their applicability in many real-world scenarios. The\nsteering vectors extracted from small dataset often contain task-irrelevant\nnoising features, which degrades their effectiveness. To refine the steering\nvectors learned from limited data, we introduce Refinement of Steering Vector\nvia Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise\nand augment the steering vectors. In our framework, we first remove\ntask-irrelevant features according to their semantics provided by SAEs, and\nthen enrich task-relevant features missing from the small dataset through their\nsemantic similarity to the identified relevant features. Extensive experiments\ndemonstrate that the proposed SAE-RSV substantially outperforms all the\nbaseline methods including supervised fine-tuning. Our findings show that\neffective steering vector can be constructed from limited training data by\nrefining the original steering vector through SAEs."
                },
                "authors": [
                    {
                        "name": "Anyi Wang"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Ninghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ninghao Liu"
                },
                "author": "Ninghao Liu",
                "arxiv_comment": "19 pages, 11 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02912v1",
                "updated": "2025-10-03T11:33:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    33,
                    40,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T11:33:40Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    33,
                    40,
                    4,
                    276,
                    0
                ],
                "title": "Don't Just Chase \"Highlighted Tokens\" in MLLMs: Revisiting Visual\n  Holistic Context Retention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Just Chase \"Highlighted Tokens\" in MLLMs: Revisiting Visual\n  Holistic Context Retention"
                },
                "summary": "Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)\nsuffer from considerable computational overhead due to their reliance on\nmassive visual tokens. Recent studies have explored token pruning to alleviate\nthis problem, which typically uses text-vision cross-attention or\n[\\texttt{CLS}] attention to assess and discard redundant visual tokens. In this\nwork, we identify a critical limitation of such attention-first pruning\napproaches, i.e., they tend to preserve semantically similar tokens, resulting\nin pronounced performance drops under high pruning ratios. To this end, we\npropose {HoloV}, a simple yet effective, plug-and-play visual token pruning\nframework for efficient inference. Distinct from previous attention-first\nschemes, HoloV rethinks token retention from a holistic perspective. By\nadaptively distributing the pruning budget across different spatial crops,\nHoloV ensures that the retained tokens capture the global visual context rather\nthan isolated salient features. This strategy minimizes representational\ncollapse and maintains task-relevant information even under aggressive pruning.\nExperimental results demonstrate that our HoloV achieves superior performance\nacross various tasks, MLLM architectures, and pruning ratios compared to SOTA\nmethods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\\% of the\noriginal performance after pruning 88.9\\% of visual tokens, achieving superior\nefficiency-accuracy trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)\nsuffer from considerable computational overhead due to their reliance on\nmassive visual tokens. Recent studies have explored token pruning to alleviate\nthis problem, which typically uses text-vision cross-attention or\n[\\texttt{CLS}] attention to assess and discard redundant visual tokens. In this\nwork, we identify a critical limitation of such attention-first pruning\napproaches, i.e., they tend to preserve semantically similar tokens, resulting\nin pronounced performance drops under high pruning ratios. To this end, we\npropose {HoloV}, a simple yet effective, plug-and-play visual token pruning\nframework for efficient inference. Distinct from previous attention-first\nschemes, HoloV rethinks token retention from a holistic perspective. By\nadaptively distributing the pruning budget across different spatial crops,\nHoloV ensures that the retained tokens capture the global visual context rather\nthan isolated salient features. This strategy minimizes representational\ncollapse and maintains task-relevant information even under aggressive pruning.\nExperimental results demonstrate that our HoloV achieves superior performance\nacross various tasks, MLLM architectures, and pruning ratios compared to SOTA\nmethods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\\% of the\noriginal performance after pruning 88.9\\% of visual tokens, achieving superior\nefficiency-accuracy trade-offs."
                },
                "authors": [
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Di Lu"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "arxiv_comment": "Accepted by NeurIPS 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04404v2",
                "updated": "2025-10-03T11:24:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    24,
                    43,
                    4,
                    276,
                    0
                ],
                "published": "2025-07-06T14:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    14,
                    35,
                    43,
                    6,
                    187,
                    0
                ],
                "title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers"
                },
                "summary": "Large language models (LLMs) excel at natural language understanding and\ngeneration but remain vulnerable to factual errors, limiting their reliability\nin knowledge-intensive tasks. While decoding-time strategies provide a\npromising efficient solution without training, existing methods typically treat\ntoken-level and layer-level signals in isolation, overlooking the joint\ndynamics between them. In this work, we introduce a token-aware,\nlayer-localized contrastive decoding method that aligns specific token types\nwith their most influential transformer layers to improve factual generation.\nThrough empirical attention analysis, we identify two key patterns: punctuation\ntokens receive dominant attention in early layers, while conceptual tokens\ngovern semantic reasoning in intermediate layers. By selectively suppressing\nattention to these token types at their respective depths, we achieve the\ninduction of controlled factual degradation and derive contrastive signals to\nguide the final factual decoding. Our method requires no additional training or\nmodel modification, and experiments demonstrate that our method consistently\nimproves factuality across multiple LLMs and various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at natural language understanding and\ngeneration but remain vulnerable to factual errors, limiting their reliability\nin knowledge-intensive tasks. While decoding-time strategies provide a\npromising efficient solution without training, existing methods typically treat\ntoken-level and layer-level signals in isolation, overlooking the joint\ndynamics between them. In this work, we introduce a token-aware,\nlayer-localized contrastive decoding method that aligns specific token types\nwith their most influential transformer layers to improve factual generation.\nThrough empirical attention analysis, we identify two key patterns: punctuation\ntokens receive dominant attention in early layers, while conceptual tokens\ngovern semantic reasoning in intermediate layers. By selectively suppressing\nattention to these token types at their respective depths, we achieve the\ninduction of controlled factual degradation and derive contrastive signals to\nguide the final factual decoding. Our method requires no additional training or\nmodel modification, and experiments demonstrate that our method consistently\nimproves factuality across multiple LLMs and various benchmarks."
                },
                "authors": [
                    {
                        "name": "Jingze Zhu"
                    },
                    {
                        "name": "Yongliang Wu"
                    },
                    {
                        "name": "Wenbo Zhu"
                    },
                    {
                        "name": "Jiawang Cao"
                    },
                    {
                        "name": "Yanqiang Zheng"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Jonas Fischer"
                    },
                    {
                        "name": "Xinting Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xinting Hu"
                },
                "author": "Xinting Hu",
                "arxiv_comment": "The submission was made before undergoing the required review by the\n  co-authors' affiliated institutions. We are withdrawing the paper to allow\n  for the completion of the institutional review process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17052v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17052v5",
                "updated": "2025-10-03T11:22:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    22,
                    35,
                    4,
                    276,
                    0
                ],
                "published": "2024-12-22T15:05:30Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    15,
                    5,
                    30,
                    6,
                    357,
                    0
                ],
                "title": "ViLBias: Detecting and Reasoning about Bias in Multimodal Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViLBias: Detecting and Reasoning about Bias in Multimodal Content"
                },
                "summary": "Detecting bias in multimodal news requires models that reason over\ntext--image pairs, not just classify text. In response, we present ViLBias, a\nVQA-style benchmark and framework for detecting and reasoning about bias in\nmultimodal news. The dataset comprises 40,945 text--image pairs from diverse\noutlets, each annotated with a bias label and concise rationale using a\ntwo-stage LLM-as-annotator pipeline with hierarchical majority voting and\nhuman-in-the-loop validation. We evaluate Small Language Models (SLMs), Large\nLanguage Models (LLMs), and Vision--Language Models (VLMs) across closed-ended\nclassification and open-ended reasoning (oVQA), and compare parameter-efficient\ntuning strategies. Results show that incorporating images alongside text\nimproves detection accuracy by 3--5\\%, and that LLMs/VLMs better capture subtle\nframing and text--image inconsistencies than SLMs. Parameter-efficient methods\n(LoRA/QLoRA/Adapters) recover 97--99\\% of full fine-tuning performance with\n$<5\\%$ trainable parameters. For oVQA, reasoning accuracy spans 52--79\\% and\nfaithfulness 68--89\\%, both improved by instruction tuning; closed accuracy\ncorrelates strongly with reasoning ($r = 0.91$). ViLBias offers a scalable\nbenchmark and strong baselines for multimodal bias detection and rationale\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting bias in multimodal news requires models that reason over\ntext--image pairs, not just classify text. In response, we present ViLBias, a\nVQA-style benchmark and framework for detecting and reasoning about bias in\nmultimodal news. The dataset comprises 40,945 text--image pairs from diverse\noutlets, each annotated with a bias label and concise rationale using a\ntwo-stage LLM-as-annotator pipeline with hierarchical majority voting and\nhuman-in-the-loop validation. We evaluate Small Language Models (SLMs), Large\nLanguage Models (LLMs), and Vision--Language Models (VLMs) across closed-ended\nclassification and open-ended reasoning (oVQA), and compare parameter-efficient\ntuning strategies. Results show that incorporating images alongside text\nimproves detection accuracy by 3--5\\%, and that LLMs/VLMs better capture subtle\nframing and text--image inconsistencies than SLMs. Parameter-efficient methods\n(LoRA/QLoRA/Adapters) recover 97--99\\% of full fine-tuning performance with\n$<5\\%$ trainable parameters. For oVQA, reasoning accuracy spans 52--79\\% and\nfaithfulness 68--89\\%, both improved by instruction tuning; closed accuracy\ncorrelates strongly with reasoning ($r = 0.91$). ViLBias offers a scalable\nbenchmark and strong baselines for multimodal bias detection and rationale\nquality."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Caesar Saleh"
                    },
                    {
                        "name": "Azib Farooq"
                    },
                    {
                        "name": "Emrul Hasan"
                    },
                    {
                        "name": "Franklin Ogidi"
                    },
                    {
                        "name": "Maximus Powers"
                    },
                    {
                        "name": "Veronica Chatrath"
                    },
                    {
                        "name": "Marcelo Lotif"
                    },
                    {
                        "name": "Karanpal Sekhon"
                    },
                    {
                        "name": "Roya Javadi"
                    },
                    {
                        "name": "Haad Zahid"
                    },
                    {
                        "name": "Anam Zahid"
                    },
                    {
                        "name": "Vahid Reza Khazaie"
                    },
                    {
                        "name": "Zhenyu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Yu"
                },
                "author": "Zhenyu Yu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17052v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17052v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02906v1",
                "updated": "2025-10-03T11:19:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    19,
                    31,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T11:19:31Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    19,
                    31,
                    4,
                    276,
                    0
                ],
                "title": "FinReflectKG -- MultiHop: Financial QA Benchmark for Reasoning with\n  Knowledge Graph Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinReflectKG -- MultiHop: Financial QA Benchmark for Reasoning with\n  Knowledge Graph Evidence"
                },
                "summary": "Multi-hop reasoning over financial disclosures is often a retrieval problem\nbefore it becomes a reasoning or generation problem: relevant facts are\ndispersed across sections, filings, companies, and years, and LLMs often expend\nexcessive tokens navigating noisy context. Without precise Knowledge Graph\n(KG)-guided selection of relevant context, even strong reasoning models either\nfail to answer or consume excessive tokens, whereas KG-linked evidence enables\nmodels to focus their reasoning on composing already retrieved facts. We\npresent FinReflectKG - MultiHop, a benchmark built on FinReflectKG, a\ntemporally indexed financial KG that links audited triples to source chunks\nfrom S&P 100 filings (2022-2024). Mining frequent 2-3 hop subgraph patterns\nacross sectors (via GICS taxonomy), we generate financial analyst style\nquestions with exact supporting evidence from the KG. A two-phase pipeline\nfirst creates QA pairs via pattern-specific prompts, followed by a\nmulti-criteria quality control evaluation to ensure QA validity. We then\nevaluate three controlled retrieval scenarios: (S1) precise KG-linked paths;\n(S2) text-only page windows centered on relevant text spans; and (S3) relevant\npage windows with randomizations and distractors. Across both reasoning and\nnon-reasoning models, KG-guided precise retrieval yields substantial gains on\nthe FinReflectKG - MultiHop QA benchmark dataset, boosting correctness scores\nby approximately 24 percent while reducing token utilization by approximately\n84.5 percent compared to the page window setting, which reflects the\ntraditional vector retrieval paradigm. Spanning intra-document, inter-year, and\ncross-company scopes, our work underscores the pivotal role of knowledge graphs\nin efficiently connecting evidence for multi-hop financial QA. We also release\na curated subset of the benchmark (555 QA Pairs) to catalyze further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop reasoning over financial disclosures is often a retrieval problem\nbefore it becomes a reasoning or generation problem: relevant facts are\ndispersed across sections, filings, companies, and years, and LLMs often expend\nexcessive tokens navigating noisy context. Without precise Knowledge Graph\n(KG)-guided selection of relevant context, even strong reasoning models either\nfail to answer or consume excessive tokens, whereas KG-linked evidence enables\nmodels to focus their reasoning on composing already retrieved facts. We\npresent FinReflectKG - MultiHop, a benchmark built on FinReflectKG, a\ntemporally indexed financial KG that links audited triples to source chunks\nfrom S&P 100 filings (2022-2024). Mining frequent 2-3 hop subgraph patterns\nacross sectors (via GICS taxonomy), we generate financial analyst style\nquestions with exact supporting evidence from the KG. A two-phase pipeline\nfirst creates QA pairs via pattern-specific prompts, followed by a\nmulti-criteria quality control evaluation to ensure QA validity. We then\nevaluate three controlled retrieval scenarios: (S1) precise KG-linked paths;\n(S2) text-only page windows centered on relevant text spans; and (S3) relevant\npage windows with randomizations and distractors. Across both reasoning and\nnon-reasoning models, KG-guided precise retrieval yields substantial gains on\nthe FinReflectKG - MultiHop QA benchmark dataset, boosting correctness scores\nby approximately 24 percent while reducing token utilization by approximately\n84.5 percent compared to the page window setting, which reflects the\ntraditional vector retrieval paradigm. Spanning intra-document, inter-year, and\ncross-company scopes, our work underscores the pivotal role of knowledge graphs\nin efficiently connecting evidence for multi-hop financial QA. We also release\na curated subset of the benchmark (555 QA Pairs) to catalyze further research."
                },
                "authors": [
                    {
                        "name": "Abhinav Arun"
                    },
                    {
                        "name": "Reetu Raj Harsh"
                    },
                    {
                        "name": "Bhaskarjit Sarmah"
                    },
                    {
                        "name": "Stefano Pasquali"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Pasquali"
                },
                "author": "Stefano Pasquali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19886v2",
                "updated": "2025-10-03T11:11:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    11,
                    10,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-24T05:21:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    5,
                    21,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "Diffusion-aided Task-oriented Semantic Communications with Model\n  Inversion Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-aided Task-oriented Semantic Communications with Model\n  Inversion Attack"
                },
                "summary": "Semantic communication enhances transmission efficiency by conveying semantic\ninformation rather than raw input symbol sequences. Task-oriented semantic\ncommunication is a variant that tries to retains only task-specific\ninformation, thus achieving greater bandwidth savings. However, these\nneural-based communication systems are vulnerable to model inversion attacks,\nwhere adversaries try to infer sensitive input information from eavesdropped\ntransmitted data. The key challenge, therefore, lies in preserving privacy\nwhile ensuring transmission correctness and robustness. While prior studies\ntypically assume that adversaries aim to fully reconstruct the raw input in\ntask-oriented settings, there exist scenarios where pixel-level metrics such as\nPSNR or SSIM are low, yet the adversary's outputs still suffice to accomplish\nthe downstream task, indicating leakage of sensitive information. We therefore\nadopt the attacker's task accuracy as a more appropriate metric for evaluating\nattack effectiveness. To optimize the gap between the legitimate receiver's\naccuracy and the adversary's accuracy, we propose DiffSem, a diffusion-aided\nframework for task-oriented semantic communication. DiffSem integrates a\ntransmitter-side self-noising mechanism that adaptively regulates semantic\ncontent while compensating for channel noise, and a receiver-side diffusion\nU-Net that enhances task performance and can be optionally strengthened by\nself-referential label embeddings. Our experiments demonstrate that DiffSem\nenables the legitimate receiver to achieve higher accuracy, thereby validating\nthe superior performance of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic communication enhances transmission efficiency by conveying semantic\ninformation rather than raw input symbol sequences. Task-oriented semantic\ncommunication is a variant that tries to retains only task-specific\ninformation, thus achieving greater bandwidth savings. However, these\nneural-based communication systems are vulnerable to model inversion attacks,\nwhere adversaries try to infer sensitive input information from eavesdropped\ntransmitted data. The key challenge, therefore, lies in preserving privacy\nwhile ensuring transmission correctness and robustness. While prior studies\ntypically assume that adversaries aim to fully reconstruct the raw input in\ntask-oriented settings, there exist scenarios where pixel-level metrics such as\nPSNR or SSIM are low, yet the adversary's outputs still suffice to accomplish\nthe downstream task, indicating leakage of sensitive information. We therefore\nadopt the attacker's task accuracy as a more appropriate metric for evaluating\nattack effectiveness. To optimize the gap between the legitimate receiver's\naccuracy and the adversary's accuracy, we propose DiffSem, a diffusion-aided\nframework for task-oriented semantic communication. DiffSem integrates a\ntransmitter-side self-noising mechanism that adaptively regulates semantic\ncontent while compensating for channel noise, and a receiver-side diffusion\nU-Net that enhances task performance and can be optionally strengthened by\nself-referential label embeddings. Our experiments demonstrate that DiffSem\nenables the legitimate receiver to achieve higher accuracy, thereby validating\nthe superior performance of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Xuesong Wang"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Xingyan Shi"
                    },
                    {
                        "name": "Zhaoqian Liu"
                    },
                    {
                        "name": "Shenghao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Shenghao Yang"
                },
                "author": "Shenghao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06428v3",
                "updated": "2025-10-03T10:59:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    59,
                    41,
                    4,
                    276,
                    0
                ],
                "published": "2023-03-11T15:33:12Z",
                "published_parsed": [
                    2023,
                    3,
                    11,
                    15,
                    33,
                    12,
                    5,
                    70,
                    0
                ],
                "title": "Universal trade-off between irreversibility and intrinsic timescale in\n  thermal relaxation with applications to thermodynamic inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal trade-off between irreversibility and intrinsic timescale in\n  thermal relaxation with applications to thermodynamic inference"
                },
                "summary": "We establish a general lower bound for the entropy production rate (EPR)\nbased on the Kullback-Leibler divergence and the Logarithmic-Sobolev constant\nthat characterizes the time-scale of relaxation. This bound can be considered\nas an enhanced second law of thermodynamics. When applied to thermal\nrelaxation, it reveals a universal trade-off relation between the dissipation\nrate and the intrinsic relaxation timescale. From this relation, a\nthermodynamic upper bound on the relaxation time between two given states\nemerges, acting as an inverse speed limit over the entire time region. We also\nobtain a quantum version of this upper bound, which is always tighter than its\nclassical counterpart, incorporating an additional term due to decoherence.\nRemarkably, we further demonstrate that the trade-off relation remains valid\nfor any generally non-Markovian coarse-grained relaxation dynamics,\nhighlighting its significant applications in thermodynamic inference. This\ntrade-off relation is a new tool in inferring EPRs in molecular dynamics\nsimulations and practical experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We establish a general lower bound for the entropy production rate (EPR)\nbased on the Kullback-Leibler divergence and the Logarithmic-Sobolev constant\nthat characterizes the time-scale of relaxation. This bound can be considered\nas an enhanced second law of thermodynamics. When applied to thermal\nrelaxation, it reveals a universal trade-off relation between the dissipation\nrate and the intrinsic relaxation timescale. From this relation, a\nthermodynamic upper bound on the relaxation time between two given states\nemerges, acting as an inverse speed limit over the entire time region. We also\nobtain a quantum version of this upper bound, which is always tighter than its\nclassical counterpart, incorporating an additional term due to decoherence.\nRemarkably, we further demonstrate that the trade-off relation remains valid\nfor any generally non-Markovian coarse-grained relaxation dynamics,\nhighlighting its significant applications in thermodynamic inference. This\ntrade-off relation is a new tool in inferring EPRs in molecular dynamics\nsimulations and practical experiments."
                },
                "authors": [
                    {
                        "name": "Ruicheng Bao"
                    },
                    {
                        "name": "Chaoqun Du"
                    },
                    {
                        "name": "Zhiyu Cao"
                    },
                    {
                        "name": "Zhonghuai Hou"
                    }
                ],
                "author_detail": {
                    "name": "Zhonghuai Hou"
                },
                "author": "Zhonghuai Hou",
                "arxiv_comment": "5 + 9 pages, 4 figures. Results applied to coarse-grained dynamics\n  are added. To appear in PRE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02892v1",
                "updated": "2025-10-03T10:59:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    59,
                    26,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T10:59:26Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    59,
                    26,
                    4,
                    276,
                    0
                ],
                "title": "RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative\n  Reinforcement Learning"
                },
                "summary": "Reinforcement learning (RL) is central to improving reasoning in large\nlanguage models (LLMs) but typically requires ground-truth rewards. Test-Time\nReinforcement Learning (TTRL) removes this need by using majority-vote rewards,\nbut relies on heavy online RL and incurs substantial computational cost. We\npropose RoiRL: Reasoning with offline iterative Reinforcement Learning, a\nfamily of lightweight offline learning alternatives that can target the same\nregularized optimal policies. Unlike TTRL, RoiRL eliminates the need to\nmaintain a reference model and instead optimizes weighted log-likelihood\nobjectives, enabling stable training with significantly lower memory and\ncompute requirements. Experimental results show that RoiRL trains to 2.5x\nfaster and consistently outperforms TTRL on reasoning benchmarks, establishing\na scalable path to self-improving LLMs without labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is central to improving reasoning in large\nlanguage models (LLMs) but typically requires ground-truth rewards. Test-Time\nReinforcement Learning (TTRL) removes this need by using majority-vote rewards,\nbut relies on heavy online RL and incurs substantial computational cost. We\npropose RoiRL: Reasoning with offline iterative Reinforcement Learning, a\nfamily of lightweight offline learning alternatives that can target the same\nregularized optimal policies. Unlike TTRL, RoiRL eliminates the need to\nmaintain a reference model and instead optimizes weighted log-likelihood\nobjectives, enabling stable training with significantly lower memory and\ncompute requirements. Experimental results show that RoiRL trains to 2.5x\nfaster and consistently outperforms TTRL on reasoning benchmarks, establishing\na scalable path to self-improving LLMs without labels."
                },
                "authors": [
                    {
                        "name": "Aleksei Arzhantsev"
                    },
                    {
                        "name": "Otmane Sakhi"
                    },
                    {
                        "name": "Flavian Vasile"
                    }
                ],
                "author_detail": {
                    "name": "Flavian Vasile"
                },
                "author": "Flavian Vasile",
                "arxiv_comment": "Accepted to the Efficient Reasoning Workshop at NeuRIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01925v2",
                "updated": "2025-10-03T10:55:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    55,
                    18,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T11:42:17Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    42,
                    17,
                    3,
                    275,
                    0
                ],
                "title": "Enhancing Large Language Model Reasoning with Reward Models: An\n  Analytical Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Model Reasoning with Reward Models: An\n  Analytical Survey"
                },
                "summary": "Reward models (RMs) play a critical role in enhancing the reasoning\nperformance of LLMs. For example, they can provide training signals to finetune\nLLMs during reinforcement learning (RL) and help select the best answer from\nmultiple candidates during inference. In this paper, we provide a systematic\nintroduction to RMs, along with a comprehensive survey of their applications in\nLLM reasoning. We first review fundamental concepts of RMs, including their\narchitectures, training methodologies, and evaluation techniques. Then, we\nexplore their key applications: (1) guiding generation and selecting optimal\noutputs during LLM inference, (2) facilitating data synthesis and iterative\nself-improvement for LLMs, and (3) providing training signals in RL-based\nfinetuning. Finally, we discuss critical open questions regarding the\nselection, generalization, evaluation, and enhancement of RMs, based on\nexisting research and our own empirical findings. Our analysis aims to provide\nactionable insights for the effective deployment and advancement of RMs for LLM\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) play a critical role in enhancing the reasoning\nperformance of LLMs. For example, they can provide training signals to finetune\nLLMs during reinforcement learning (RL) and help select the best answer from\nmultiple candidates during inference. In this paper, we provide a systematic\nintroduction to RMs, along with a comprehensive survey of their applications in\nLLM reasoning. We first review fundamental concepts of RMs, including their\narchitectures, training methodologies, and evaluation techniques. Then, we\nexplore their key applications: (1) guiding generation and selecting optimal\noutputs during LLM inference, (2) facilitating data synthesis and iterative\nself-improvement for LLMs, and (3) providing training signals in RL-based\nfinetuning. Finally, we discuss critical open questions regarding the\nselection, generalization, evaluation, and enhancement of RMs, based on\nexisting research and our own empirical findings. Our analysis aims to provide\nactionable insights for the effective deployment and advancement of RMs for LLM\nreasoning."
                },
                "authors": [
                    {
                        "name": "Qiyuan Liu"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Xuhong Chen"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Ning Miao"
                    }
                ],
                "author_detail": {
                    "name": "Ning Miao"
                },
                "author": "Ning Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21710v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21710v3",
                "updated": "2025-10-03T10:38:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    38,
                    39,
                    4,
                    276,
                    0
                ],
                "published": "2025-03-27T17:21:47Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    21,
                    47,
                    3,
                    86,
                    0
                ],
                "title": "Enhancing repository-level software repair via repository-aware\n  knowledge graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing repository-level software repair via repository-aware\n  knowledge graphs"
                },
                "summary": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which\nprimarily rely on large language models (LLMs), are hindered by semantic\nambiguities, limited understanding of structural context, and insufficient\nreasoning capabilities. To address these limitations, we propose KGCompass with\ntwo innovations: (1) a novel repository-aware knowledge graph (KG) that\naccurately links repository artifacts (issues and pull requests) and codebase\nentities (files, classes, and functions), allowing us to effectively narrow\ndown the vast search space to only 20 most relevant functions with accurate\ncandidate fault locations and contextual information, and (2) a path-guided\nrepair mechanism that leverages KG-mined entity paths, tracing through which\nallows us to augment LLMs with relevant contextual information to generate\nprecise patches along with their explanations. Experimental results in the\nSWE-bench Lite demonstrate that KGCompass achieves state-of-the-art single-LLM\nrepair performance (58.3%) and function-level fault location accuracy (56.0%)\nacross open-source approaches with a single repair model, costing only $0.2 per\nrepair. Among the bugs that KGCompass successfully localizes, 89.7% lack\nexplicit location hints in the issue and are found only through multi-hop graph\ntraversal, where pure LLMs struggle to locate bugs accurately. Relative to\npure-LLM baselines, KGCompass lifts the resolved rate by 50.8% on Claude-4\nSonnet, 30.2% on Claude-3.5 Sonnet, 115.7% on DeepSeek-V3, and 156.4% on\nQwen2.5 Max. These consistent improvements demonstrate that this graph-guided\nrepair framework delivers model-agnostic, cost-efficient repair and sets a\nstrong new baseline for repository-level repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which\nprimarily rely on large language models (LLMs), are hindered by semantic\nambiguities, limited understanding of structural context, and insufficient\nreasoning capabilities. To address these limitations, we propose KGCompass with\ntwo innovations: (1) a novel repository-aware knowledge graph (KG) that\naccurately links repository artifacts (issues and pull requests) and codebase\nentities (files, classes, and functions), allowing us to effectively narrow\ndown the vast search space to only 20 most relevant functions with accurate\ncandidate fault locations and contextual information, and (2) a path-guided\nrepair mechanism that leverages KG-mined entity paths, tracing through which\nallows us to augment LLMs with relevant contextual information to generate\nprecise patches along with their explanations. Experimental results in the\nSWE-bench Lite demonstrate that KGCompass achieves state-of-the-art single-LLM\nrepair performance (58.3%) and function-level fault location accuracy (56.0%)\nacross open-source approaches with a single repair model, costing only $0.2 per\nrepair. Among the bugs that KGCompass successfully localizes, 89.7% lack\nexplicit location hints in the issue and are found only through multi-hop graph\ntraversal, where pure LLMs struggle to locate bugs accurately. Relative to\npure-LLM baselines, KGCompass lifts the resolved rate by 50.8% on Claude-4\nSonnet, 30.2% on Claude-3.5 Sonnet, 115.7% on DeepSeek-V3, and 156.4% on\nQwen2.5 Max. These consistent improvements demonstrate that this graph-guided\nrepair framework delivers model-agnostic, cost-efficient repair and sets a\nstrong new baseline for repository-level repair."
                },
                "authors": [
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Jiadong Ren"
                    },
                    {
                        "name": "Shunfu Jin"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Haoye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Haoye Tian"
                },
                "author": "Haoye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21710v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21710v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01918v2",
                "updated": "2025-10-03T10:19:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    19,
                    23,
                    4,
                    276,
                    0
                ],
                "published": "2025-08-03T21:03:22Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    21,
                    3,
                    22,
                    6,
                    215,
                    0
                ],
                "title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and\n  Retrieval for the Punjabi Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and\n  Retrieval for the Punjabi Language"
                },
                "summary": "Despite rapid advances in large language models (LLMs), low-resource\nlanguages remain excluded from NLP, limiting digital access for millions. We\npresent PunGPT2, the first fully open-source Punjabi generative model suite,\ntrained on a 35GB corpus covering literature, religious texts, news, social\ndiscourse, etc. PunGPT2 captures Punjabi's syntactic and morphological richness\nthrough a tokenizer optimized for Gurmukhi and Shahmukhi scripts. We introduce\nPun-RAG, a retrieval-augmented framework integrating PunGPT2 with a FAISS\nretriever over a curated Punjabi knowledge base, and Pun-Instruct, an\ninstruction-tuned variant using QLoRA for robust zero-shot summarization,\ntranslation, and question answering. Our key innovation, Quantum-RAG, fuses\nsparse, dense, and quantum kernel embeddings for efficient, context-aware\nretrieval with low memory overhead, marking the first practical\nquantum-inspired retrieval in a low-resource LLM. Our models outperform\nmultilingual baselines (mBERT, mT5, MuRIL, BLOOM) on FLORES-200, IndicGenBench,\nand a new PunjabiEval suite. Quantum-RAG yields +7.4 Recall@10 over FAISS and\n+3.5 BLEU over mT5 on PunjabiEval. We publicly release all training scripts,\nhyperparameters, evaluation pipelines, the 35GB Punjabi corpus, the PunjabiEval\nbenchmark, and all model weights, establishing new state-of-the-art results for\nPunjabi language generation and retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid advances in large language models (LLMs), low-resource\nlanguages remain excluded from NLP, limiting digital access for millions. We\npresent PunGPT2, the first fully open-source Punjabi generative model suite,\ntrained on a 35GB corpus covering literature, religious texts, news, social\ndiscourse, etc. PunGPT2 captures Punjabi's syntactic and morphological richness\nthrough a tokenizer optimized for Gurmukhi and Shahmukhi scripts. We introduce\nPun-RAG, a retrieval-augmented framework integrating PunGPT2 with a FAISS\nretriever over a curated Punjabi knowledge base, and Pun-Instruct, an\ninstruction-tuned variant using QLoRA for robust zero-shot summarization,\ntranslation, and question answering. Our key innovation, Quantum-RAG, fuses\nsparse, dense, and quantum kernel embeddings for efficient, context-aware\nretrieval with low memory overhead, marking the first practical\nquantum-inspired retrieval in a low-resource LLM. Our models outperform\nmultilingual baselines (mBERT, mT5, MuRIL, BLOOM) on FLORES-200, IndicGenBench,\nand a new PunjabiEval suite. Quantum-RAG yields +7.4 Recall@10 over FAISS and\n+3.5 BLEU over mT5 on PunjabiEval. We publicly release all training scripts,\nhyperparameters, evaluation pipelines, the 35GB Punjabi corpus, the PunjabiEval\nbenchmark, and all model weights, establishing new state-of-the-art results for\nPunjabi language generation and retrieval."
                },
                "authors": [
                    {
                        "name": "Jaskaranjeet Singh"
                    },
                    {
                        "name": "Rakesh Thakur"
                    }
                ],
                "author_detail": {
                    "name": "Rakesh Thakur"
                },
                "author": "Rakesh Thakur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00679v2",
                "updated": "2025-10-03T10:02:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    2,
                    18,
                    4,
                    276,
                    0
                ],
                "published": "2023-12-01T16:01:46Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    16,
                    1,
                    46,
                    4,
                    335,
                    0
                ],
                "title": "Euclid preparation. XLI. Galaxy power spectrum modelling in real space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid preparation. XLI. Galaxy power spectrum modelling in real space"
                },
                "summary": "We investigate the accuracy of the perturbative galaxy bias expansion in view\nof the forthcoming analysis of the Euclid spectroscopic galaxy samples. We\ncompare the performance of an Eulerian galaxy bias expansion, using\nstate-of-art prescriptions from the effective field theory of large-scale\nstructure (EFTofLSS), against a hybrid approach based on Lagrangian\nperturbation theory and high-resolution simulations. These models are\nbenchmarked against comoving snapshots of the Flagship I N-body simulation at\n$z=(0.9,1.2,1.5,1.8)$, which have been populated with H$\\alpha$ galaxies\nleading to catalogues of millions of objects within a volume of about\n$58\\,h^{-3}\\,{\\rm Gpc}^3$. Our analysis suggests that both models can be used\nto provide a robust inference of the parameters $(h, \\omega_{\\rm c})$ in the\nredshift range under consideration, with comparable constraining power. We\nadditionally determine the range of validity of the EFTofLSS model in terms of\nscale cuts and model degrees of freedom. From these tests, it emerges that the\nstandard third-order Eulerian bias expansion can accurately describe the full\nshape of the real-space galaxy power spectrum up to the maximum wavenumber\n$k_{\\rm max}=0.45\\,h\\,{\\rm Mpc}^{-1}$, even with a measurement precision well\nbelow the percent level. In particular, this is true for a configuration with\nsix free nuisance parameters, including local and non-local bias parameters, a\nmatter counterterm, and a correction to the shot-noise contribution. Fixing\neither tidal bias parameters to physically-motivated relations still leads to\nunbiased cosmological constraints. We finally repeat our analysis assuming a\nvolume that matches the expected footprint of Euclid, but without considering\nobservational effects, as purity and completeness, showing that we can get\nconsistent cosmological constraints over this range of scales and redshifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the accuracy of the perturbative galaxy bias expansion in view\nof the forthcoming analysis of the Euclid spectroscopic galaxy samples. We\ncompare the performance of an Eulerian galaxy bias expansion, using\nstate-of-art prescriptions from the effective field theory of large-scale\nstructure (EFTofLSS), against a hybrid approach based on Lagrangian\nperturbation theory and high-resolution simulations. These models are\nbenchmarked against comoving snapshots of the Flagship I N-body simulation at\n$z=(0.9,1.2,1.5,1.8)$, which have been populated with H$\\alpha$ galaxies\nleading to catalogues of millions of objects within a volume of about\n$58\\,h^{-3}\\,{\\rm Gpc}^3$. Our analysis suggests that both models can be used\nto provide a robust inference of the parameters $(h, \\omega_{\\rm c})$ in the\nredshift range under consideration, with comparable constraining power. We\nadditionally determine the range of validity of the EFTofLSS model in terms of\nscale cuts and model degrees of freedom. From these tests, it emerges that the\nstandard third-order Eulerian bias expansion can accurately describe the full\nshape of the real-space galaxy power spectrum up to the maximum wavenumber\n$k_{\\rm max}=0.45\\,h\\,{\\rm Mpc}^{-1}$, even with a measurement precision well\nbelow the percent level. In particular, this is true for a configuration with\nsix free nuisance parameters, including local and non-local bias parameters, a\nmatter counterterm, and a correction to the shot-noise contribution. Fixing\neither tidal bias parameters to physically-motivated relations still leads to\nunbiased cosmological constraints. We finally repeat our analysis assuming a\nvolume that matches the expected footprint of Euclid, but without considering\nobservational effects, as purity and completeness, showing that we can get\nconsistent cosmological constraints over this range of scales and redshifts."
                },
                "authors": [
                    {
                        "name": "Euclid Collaboration"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "C. Moretti"
                    },
                    {
                        "name": "M. Zennaro"
                    },
                    {
                        "name": "A. Moradinezhad Dizgah"
                    },
                    {
                        "name": "M. Crocce"
                    },
                    {
                        "name": "E. Sefusatti"
                    },
                    {
                        "name": "I. Ferrero"
                    },
                    {
                        "name": "K. Pardede"
                    },
                    {
                        "name": "A. Eggemeier"
                    },
                    {
                        "name": "A. Barreira"
                    },
                    {
                        "name": "R. E. Angulo"
                    },
                    {
                        "name": "M. Marinucci"
                    },
                    {
                        "name": "B. Camacho Quevedo"
                    },
                    {
                        "name": "S. de la Torre"
                    },
                    {
                        "name": "D. Alkhanishvili"
                    },
                    {
                        "name": "M. Biagetti"
                    },
                    {
                        "name": "M. -A. Breton"
                    },
                    {
                        "name": "E. Castorina"
                    },
                    {
                        "name": "G. D'Amico"
                    },
                    {
                        "name": "V. Desjacques"
                    },
                    {
                        "name": "M. Guidi"
                    },
                    {
                        "name": "M. Krcher"
                    },
                    {
                        "name": "A. Oddo"
                    },
                    {
                        "name": "M. Pellejero Ibanez"
                    },
                    {
                        "name": "C. Porciani"
                    },
                    {
                        "name": "A. Pugno"
                    },
                    {
                        "name": "J. Salvalaggio"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "Z. Vlah"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "R. Bender"
                    },
                    {
                        "name": "C. Bodendorf"
                    },
                    {
                        "name": "D. Bonino"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "V. F. Cardone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "L. Corcione"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "A. M. Di Giorgio"
                    },
                    {
                        "name": "J. Dinis"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "A. Ealet"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "P. Fosalba"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "B. R. Granett"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "L. Guzzo"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihnen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "M. Kilbinger"
                    },
                    {
                        "name": "T. Kitching"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "K. Markovic"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "E. Munari"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "J. E. Pollack"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "E. Rossetti"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "M. Seiffert"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "C. Surace"
                    },
                    {
                        "name": "P. Tallada-Cresp"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "E. A. Valentijn"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "J. Zoubian"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "E. Bozzo"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "D. Di Ferdinando"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "M. Wiesmann"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "V. Allevato"
                    },
                    {
                        "name": "S. Anselmi"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "F. Bernardeau"
                    },
                    {
                        "name": "A. Blanchard"
                    },
                    {
                        "name": "S. Borgani"
                    },
                    {
                        "name": "S. Bruton"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "A. Cappi"
                    },
                    {
                        "name": "C. S. Carvalho"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "T. Castro"
                    },
                    {
                        "name": "G. Caas-Herrera"
                    },
                    {
                        "name": "K. C. Chambers"
                    },
                    {
                        "name": "S. Contarini"
                    },
                    {
                        "name": "A. R. Cooray"
                    },
                    {
                        "name": "J. Coupon"
                    },
                    {
                        "name": "S. Davini"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "G. Desprez"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "H. Dole"
                    },
                    {
                        "name": "A. Daz-Snchez"
                    },
                    {
                        "name": "J. A. Escartin Vigo"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "P. G. Ferreira"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "K. Ganga"
                    },
                    {
                        "name": "J. Garca-Bellido"
                    },
                    {
                        "name": "F. Giacomini"
                    },
                    {
                        "name": "G. Gozaliasl"
                    },
                    {
                        "name": "A. Hall"
                    },
                    {
                        "name": "S. Ili"
                    },
                    {
                        "name": "S. Joudaki"
                    },
                    {
                        "name": "J. J. E. Kajava"
                    },
                    {
                        "name": "V. Kansal"
                    },
                    {
                        "name": "C. C. Kirkpatrick"
                    },
                    {
                        "name": "L. Legrand"
                    },
                    {
                        "name": "A. Loureiro"
                    },
                    {
                        "name": "J. Macias-Perez"
                    },
                    {
                        "name": "M. Magliocchetti"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "R. Maoli"
                    },
                    {
                        "name": "C. J. A. P. Martins"
                    },
                    {
                        "name": "S. Matthew"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "M. Migliaccio"
                    },
                    {
                        "name": "P. Monaco"
                    },
                    {
                        "name": "G. Morgante"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "Nicholas A. Walton"
                    },
                    {
                        "name": "L. Patrizii"
                    },
                    {
                        "name": "V. Popa"
                    },
                    {
                        "name": "D. Potter"
                    },
                    {
                        "name": "A. Pourtsidou"
                    },
                    {
                        "name": "M. Pntinen"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "P. -F. Rocci"
                    },
                    {
                        "name": "A. G. Snchez"
                    },
                    {
                        "name": "M. Sahln"
                    },
                    {
                        "name": "A. Schneider"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "P. Simon"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "R. Teyssier"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "S. Tosi"
                    },
                    {
                        "name": "A. Troja"
                    },
                    {
                        "name": "M. Tucci"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "D. Vergani"
                    },
                    {
                        "name": "G. Verza"
                    },
                    {
                        "name": "P. Vielzeuf"
                    }
                ],
                "author_detail": {
                    "name": "P. Vielzeuf"
                },
                "author": "P. Vielzeuf",
                "arxiv_affiliation": "Aix-Marseille Universit, CNRS/IN2P3, CPPM, Marseille, France",
                "arxiv_doi": "10.1051/0004-6361/202348939",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202348939",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.00679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "40 pages, 20 figures",
                "arxiv_journal_ref": "A&A 687, A216 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02858v1",
                "updated": "2025-10-03T09:53:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    53,
                    42,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:53:42Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    53,
                    42,
                    4,
                    276,
                    0
                ],
                "title": "PINNGraPE: Physics Informed Neural Network for Gravitational wave\n  Parameter Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINNGraPE: Physics Informed Neural Network for Gravitational wave\n  Parameter Estimation"
                },
                "summary": "Weakly-modelled searches for gravitational waves are essential for ensuring\nthat all potential sources are accounted for in detection efforts, as they make\nminimal assumptions regarding source morphology. While these searches primarily\ntarget generic transient sources, they are also highly effective at identifying\na broad range of compact binary coalescences, demonstrated by the\nweakly-modelled search algorithm coherent WaveBurst being the first to detect\nGW150914. Despite their ability to detect compact binaries with diverse\nproperties, the accurate estimation of source parameters from their output\nremains to be a challenging task. To overcome this, we leverage\nphysics-informed neural networks, which serve as a powerful tool for parameter\nestimation by applying physical constraints through the universal differential\nequation governing a compact binary system. With this approach, we rapidly\ninfer the mass parameters of binary black hole merger systems to within 7% from\nonly the time-frequency representation of the gravitational wave signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly-modelled searches for gravitational waves are essential for ensuring\nthat all potential sources are accounted for in detection efforts, as they make\nminimal assumptions regarding source morphology. While these searches primarily\ntarget generic transient sources, they are also highly effective at identifying\na broad range of compact binary coalescences, demonstrated by the\nweakly-modelled search algorithm coherent WaveBurst being the first to detect\nGW150914. Despite their ability to detect compact binaries with diverse\nproperties, the accurate estimation of source parameters from their output\nremains to be a challenging task. To overcome this, we leverage\nphysics-informed neural networks, which serve as a powerful tool for parameter\nestimation by applying physical constraints through the universal differential\nequation governing a compact binary system. With this approach, we rapidly\ninfer the mass parameters of binary black hole merger systems to within 7% from\nonly the time-frequency representation of the gravitational wave signal."
                },
                "authors": [
                    {
                        "name": "Leigh Smith"
                    },
                    {
                        "name": "Matteo Scialpi"
                    },
                    {
                        "name": "Francesco di Clemente"
                    },
                    {
                        "name": "Micha Bejger"
                    }
                ],
                "author_detail": {
                    "name": "Micha Bejger"
                },
                "author": "Micha Bejger",
                "arxiv_comment": "4 pages, 2 figures. Proceedings contribution to Journal of Physics:\n  Conference Series for the 24th International Conference on General Relativity\n  and Gravitation (GR24) and the 16th Edoardo Amaldi Conference on\n  Gravitational Waves (Amaldi16)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02851v1",
                "updated": "2025-10-03T09:40:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    40,
                    39,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:40:39Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    40,
                    39,
                    4,
                    276,
                    0
                ],
                "title": "Action Deviation-Aware Inference for Low-Latency Wireless Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action Deviation-Aware Inference for Low-Latency Wireless Robots"
                },
                "summary": "To support latency-sensitive AI applications ranging from autonomous driving\nto industrial robot manipulation, 6G envisions distributed ML, connecting\ndistributed computational resources in edge and cloud over hyper-reliable\nlow-latency communication (HRLLC). In this setting, speculative decoding can\nfacilitate collaborative inference of models distributively deployed: an\non-device draft model locally generates drafts and a remote server-based target\nmodel verifies and corrects them, resulting lower latency. However, unlike\nautoregressive text generation, behavior cloning policies, typically used for\nembodied AI applications like robot manipulation and autonomous driving, cannot\nparallelize verification and correction for multiple drafts as each action\ndepends on observation which needs to be updated by a previous action. To this\nend, we propose Action Deviation-Aware Hybrid Inference, wherein the draft\nmodel estimates an action's need for verification and correction by the target\nmodel and selectively skips communication and computation for server\noperations. Action deviation shows a strong correlation with action's rejection\nprobability by the target model, enabling selective skipping. We derive the\npath deviation threshold that balances the transmission rate and the inference\nperformance, and we empirically show that action deviation-aware hybrid\ninference reduces uplink transmission and server operation by 40%, while\nlowering end-to-end latency by 33.32% relative to hybrid inference without\nskipping and achieving task success rate up to 97.03% of that of target model\nonly inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support latency-sensitive AI applications ranging from autonomous driving\nto industrial robot manipulation, 6G envisions distributed ML, connecting\ndistributed computational resources in edge and cloud over hyper-reliable\nlow-latency communication (HRLLC). In this setting, speculative decoding can\nfacilitate collaborative inference of models distributively deployed: an\non-device draft model locally generates drafts and a remote server-based target\nmodel verifies and corrects them, resulting lower latency. However, unlike\nautoregressive text generation, behavior cloning policies, typically used for\nembodied AI applications like robot manipulation and autonomous driving, cannot\nparallelize verification and correction for multiple drafts as each action\ndepends on observation which needs to be updated by a previous action. To this\nend, we propose Action Deviation-Aware Hybrid Inference, wherein the draft\nmodel estimates an action's need for verification and correction by the target\nmodel and selectively skips communication and computation for server\noperations. Action deviation shows a strong correlation with action's rejection\nprobability by the target model, enabling selective skipping. We derive the\npath deviation threshold that balances the transmission rate and the inference\nperformance, and we empirically show that action deviation-aware hybrid\ninference reduces uplink transmission and server operation by 40%, while\nlowering end-to-end latency by 33.32% relative to hybrid inference without\nskipping and achieving task success rate up to 97.03% of that of target model\nonly inference."
                },
                "authors": [
                    {
                        "name": "Jeyoung Park"
                    },
                    {
                        "name": "Yeonsub Lim"
                    },
                    {
                        "name": "Seungeun Oh"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Jinho Choi"
                    },
                    {
                        "name": "Seong-Lyun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Lyun Kim"
                },
                "author": "Seong-Lyun Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11274v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11274v4",
                "updated": "2025-10-03T09:38:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    38,
                    3,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-16T14:08:04Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    8,
                    4,
                    4,
                    136,
                    0
                ],
                "title": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning"
                },
                "summary": "While reasoning models demonstrate exceptional performance on complex tasks,\nthey often exhibit tendencies of overthinking on simple problems. This\nphenomenon not only leads to excessive computational resource consumption but\nalso significantly degrades user experience. To address this challenge, we\npropose SelfBudgeter - a novel user-friendly adaptive controllable reasoning\nframework that incorporates a budget estimation mechanism prior to reasoning.\nThe framework adopts a dual-phase training paradigm: during the cold-start\nphase, the model learns to predict token budgets before executing reasoning in\na standardized format; in the reinforcement learning phase, the model is\ntrained to autonomously plan budgets based on problem difficulty and strictly\nadhere to them when generating responses. Since the model outputs budget\nestimates at the initial stage, users can immediately anticipate waiting\nduration, enabling flexible decisions on whether to interrupt or continue the\ngeneration process. Notably, our method supports manual control of reasoning\nlength through pre-filled budget fields. Experimental results demonstrate that\nSelfBudgeter can dynamically allocate budgets according to problem complexity,\nyielding an average response length compression of 61% for the 1.5B model on\nGSM8K, MATH500, and AIME2025, and 48% for the 7B model, while maintaining\nnearly undiminished accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reasoning models demonstrate exceptional performance on complex tasks,\nthey often exhibit tendencies of overthinking on simple problems. This\nphenomenon not only leads to excessive computational resource consumption but\nalso significantly degrades user experience. To address this challenge, we\npropose SelfBudgeter - a novel user-friendly adaptive controllable reasoning\nframework that incorporates a budget estimation mechanism prior to reasoning.\nThe framework adopts a dual-phase training paradigm: during the cold-start\nphase, the model learns to predict token budgets before executing reasoning in\na standardized format; in the reinforcement learning phase, the model is\ntrained to autonomously plan budgets based on problem difficulty and strictly\nadhere to them when generating responses. Since the model outputs budget\nestimates at the initial stage, users can immediately anticipate waiting\nduration, enabling flexible decisions on whether to interrupt or continue the\ngeneration process. Notably, our method supports manual control of reasoning\nlength through pre-filled budget fields. Experimental results demonstrate that\nSelfBudgeter can dynamically allocate budgets according to problem complexity,\nyielding an average response length compression of 61% for the 1.5B model on\nGSM8K, MATH500, and AIME2025, and 48% for the 7B model, while maintaining\nnearly undiminished accuracy."
                },
                "authors": [
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kai Jia"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11274v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11274v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02850v1",
                "updated": "2025-10-03T09:37:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    37,
                    59,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:37:59Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    37,
                    59,
                    4,
                    276,
                    0
                ],
                "title": "Reward Model Routing in Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Model Routing in Alignment"
                },
                "summary": "Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become\nthe standard paradigm for aligning large language models (LLMs). However, most\npipelines rely on a single reward model (RM), limiting alignment quality and\nrisking overfitting. Recent work explores RM routing--dynamically selecting an\nRM from a candidate pool to exploit complementary strengths while maintaining\n$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient\nexploration. We propose BayesianRouter, a hybrid routing framework that\ncombines offline RM strengths learning with online Bayesian selection. In the\noffline stage, a multi-task router is trained on preference data to estimate\nper-RM reliability. In the online stage, a Bayesian Thompson sampling router\nperforms per-query RM selection, initializing RM-specific weight vectors with\noffline embeddings as Gaussian priors and adaptively updating their posteriors\nwith online rewards to adapt to the evolving policy distribution. Extensive\nexperiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and\nreasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently\noutperforms individual RMs, RM ensembling, and existing routing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become\nthe standard paradigm for aligning large language models (LLMs). However, most\npipelines rely on a single reward model (RM), limiting alignment quality and\nrisking overfitting. Recent work explores RM routing--dynamically selecting an\nRM from a candidate pool to exploit complementary strengths while maintaining\n$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient\nexploration. We propose BayesianRouter, a hybrid routing framework that\ncombines offline RM strengths learning with online Bayesian selection. In the\noffline stage, a multi-task router is trained on preference data to estimate\nper-RM reliability. In the online stage, a Bayesian Thompson sampling router\nperforms per-query RM selection, initializing RM-specific weight vectors with\noffline embeddings as Gaussian priors and adaptively updating their posteriors\nwith online rewards to adapt to the evolving policy distribution. Extensive\nexperiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and\nreasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently\noutperforms individual RMs, RM ensembling, and existing routing methods."
                },
                "authors": [
                    {
                        "name": "Xinle Wu"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02848v1",
                "updated": "2025-10-03T09:36:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    36,
                    55,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:36:55Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    36,
                    55,
                    4,
                    276,
                    0
                ],
                "title": "Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating\n  and Dynamic Pacing Zero-shot Text-to-Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating\n  and Dynamic Pacing Zero-shot Text-to-Speech"
                },
                "summary": "Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling\nmodels to synthesize speech from text using short, limited-context prompts.\nThese prompts serve as voice exemplars, allowing the model to mimic speaker\nidentity, prosody, and other traits without extensive speaker-specific data.\nAlthough recent approaches incorporating language models, diffusion, and flow\nmatching have proven their effectiveness in zero-shot TTS, they still encounter\nchallenges such as unreliable synthesis caused by token repetition or\nunexpected content transfer, along with slow inference and substantial\ncomputational overhead. Moreover, temporal diversity-crucial for enhancing the\nnaturalness of synthesized speech-remains largely underexplored. To address\nthese challenges, we propose Flamed-TTS, a novel zero-shot TTS framework that\nemphasizes low computational cost, low latency, and high speech fidelity\nalongside rich temporal diversity. To achieve this, we reformulate the flow\nmatching training paradigm and incorporate both discrete and continuous\nrepresentations corresponding to different attributes of speech. Experimental\nresults demonstrate that Flamed-TTS surpasses state-of-the-art models in terms\nof intelligibility, naturalness, speaker similarity, acoustic characteristics\npreservation, and dynamic pace. Notably, Flamed-TTS achieves the best WER of 4%\ncompared to the leading zero-shot TTS baselines, while maintaining low latency\nin inference and high fidelity in generated speech. Code and audio samples are\navailable at our demo page https://flamed-tts.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling\nmodels to synthesize speech from text using short, limited-context prompts.\nThese prompts serve as voice exemplars, allowing the model to mimic speaker\nidentity, prosody, and other traits without extensive speaker-specific data.\nAlthough recent approaches incorporating language models, diffusion, and flow\nmatching have proven their effectiveness in zero-shot TTS, they still encounter\nchallenges such as unreliable synthesis caused by token repetition or\nunexpected content transfer, along with slow inference and substantial\ncomputational overhead. Moreover, temporal diversity-crucial for enhancing the\nnaturalness of synthesized speech-remains largely underexplored. To address\nthese challenges, we propose Flamed-TTS, a novel zero-shot TTS framework that\nemphasizes low computational cost, low latency, and high speech fidelity\nalongside rich temporal diversity. To achieve this, we reformulate the flow\nmatching training paradigm and incorporate both discrete and continuous\nrepresentations corresponding to different attributes of speech. Experimental\nresults demonstrate that Flamed-TTS surpasses state-of-the-art models in terms\nof intelligibility, naturalness, speaker similarity, acoustic characteristics\npreservation, and dynamic pace. Notably, Flamed-TTS achieves the best WER of 4%\ncompared to the leading zero-shot TTS baselines, while maintaining low latency\nin inference and high fidelity in generated speech. Code and audio samples are\navailable at our demo page https://flamed-tts.github.io."
                },
                "authors": [
                    {
                        "name": "Hieu-Nghia Huynh-Nguyen"
                    },
                    {
                        "name": "Huynh Nguyen Dang"
                    },
                    {
                        "name": "Ngoc-Son Nguyen"
                    },
                    {
                        "name": "Van Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Van Nguyen"
                },
                "author": "Van Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02837v1",
                "updated": "2025-10-03T09:19:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    19,
                    15,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:19:15Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    19,
                    15,
                    4,
                    276,
                    0
                ],
                "title": "Beyond the Final Answer: Evaluating the Reasoning Trajectories of\n  Tool-Augmented Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Final Answer: Evaluating the Reasoning Trajectories of\n  Tool-Augmented Agents"
                },
                "summary": "Although recent tool-augmented benchmarks incorporate complex user requests\nand diverse tools, the evaluation methods for most of them remain limited to\nanswer matching. However, as the number of steps required to resolve a user\nrequest increases, a proper evaluation of an agent's performance must go beyond\nthe final answer to also assess the problem-solving trajectory, including\npreviously ignored aspects such as efficiency, hallucination, and adaptivity.\nThe most straightforward method for evaluating these aspects is to compare an\nagent's trajectory with the ground-truth trajectory, but this approach is\nfundamentally limited since annotating all valid ground-truth trajectories is\nprohibitively expensive. However, a simple LLM-based evaluator struggles to\nassess trajectories in detail without ground truth. To effectively evaluate the\nagents in this manner, we introduce TRACE, a framework for the\nmulti-dimensional evaluation of tool-augmented LLM agent performance. By\nincorporating an evidence bank, which accumulates knowledge gathered from\npreceding reasoning steps, TRACE enables a multi-faceted analysis and\nevaluation of an agent's reasoning trajectory effectively. To validate our\nframework, we develop a new meta-evaluation dataset by augmenting existing\nbenchmarks with diverse and flawed trajectories, each labeled with\nmulti-faceted performance scores. Our results confirm that TRACE accurately\nevaluates these complex behaviors in a scalable and cost-effective manner, even\nwith small open-source LLMs. Furthermore, we apply our method to evaluate the\ntrajectories that agents produce while solving tool-augmented tasks, presenting\npreviously unreported observations and their corresponding insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent tool-augmented benchmarks incorporate complex user requests\nand diverse tools, the evaluation methods for most of them remain limited to\nanswer matching. However, as the number of steps required to resolve a user\nrequest increases, a proper evaluation of an agent's performance must go beyond\nthe final answer to also assess the problem-solving trajectory, including\npreviously ignored aspects such as efficiency, hallucination, and adaptivity.\nThe most straightforward method for evaluating these aspects is to compare an\nagent's trajectory with the ground-truth trajectory, but this approach is\nfundamentally limited since annotating all valid ground-truth trajectories is\nprohibitively expensive. However, a simple LLM-based evaluator struggles to\nassess trajectories in detail without ground truth. To effectively evaluate the\nagents in this manner, we introduce TRACE, a framework for the\nmulti-dimensional evaluation of tool-augmented LLM agent performance. By\nincorporating an evidence bank, which accumulates knowledge gathered from\npreceding reasoning steps, TRACE enables a multi-faceted analysis and\nevaluation of an agent's reasoning trajectory effectively. To validate our\nframework, we develop a new meta-evaluation dataset by augmenting existing\nbenchmarks with diverse and flawed trajectories, each labeled with\nmulti-faceted performance scores. Our results confirm that TRACE accurately\nevaluates these complex behaviors in a scalable and cost-effective manner, even\nwith small open-source LLMs. Furthermore, we apply our method to evaluate the\ntrajectories that agents produce while solving tool-augmented tasks, presenting\npreviously unreported observations and their corresponding insights."
                },
                "authors": [
                    {
                        "name": "Wonjoong Kim"
                    },
                    {
                        "name": "Sangwu Park"
                    },
                    {
                        "name": "Yeonjun In"
                    },
                    {
                        "name": "Sein Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Chanyoung Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanyoung Park"
                },
                "author": "Chanyoung Park",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.03223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03223v1",
                "updated": "2025-10-03T17:56:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    56,
                    33,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:56:33Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    56,
                    33,
                    4,
                    276,
                    0
                ],
                "title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention\n  Alignment"
                },
                "summary": "To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining."
                },
                "authors": [
                    {
                        "name": "Hongxiang Zhang"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Tianyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhang"
                },
                "author": "Tianyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03219v1",
                "updated": "2025-10-03T17:54:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    54,
                    15,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:54:15Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    54,
                    15,
                    4,
                    276,
                    0
                ],
                "title": "TPM-Based Continuous Remote Attestation and Integrity Verification for\n  5G VNFs on Kubernetes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPM-Based Continuous Remote Attestation and Integrity Verification for\n  5G VNFs on Kubernetes"
                },
                "summary": "In the rapidly evolving landscape of 5G technology, the adoption of\ncloud-based infrastructure for the deployment of 5G services has become\nincreasingly common. Using a service-based architecture, critical 5G\ncomponents, such as the Access and Mobility Management Function (AMF), Session\nManagement Function (SMF), and User Plane Function (UPF), now run as\ncontainerized pods on Kubernetes clusters. Although this approach improves\nscalability, flexibility, and resilience, it also introduces new security\nchallenges, particularly to ensure the integrity and trustworthiness of these\ncomponents. Current 5G security specifications (for example, 3GPP TS 33.501)\nfocus on communication security and assume that network functions remain\ntrustworthy after authentication, consequently lacking mechanisms to\ncontinuously validate the integrity of NVFs at runtime. To close this gap, and\nto align with Zero Trust principles of 'never trust, always verify', we present\na TPM 2.0-based continuous remote attestation solution for core 5G components\ndeployed on Kubernetes. Our approach uses the Linux Integrity Measurement\nArchitecture (IMA) and a Trusted Platform Module (TPM) to provide\nhardware-based runtime validation. We integrate the open-source Keylime\nframework with a custom IMA template that isolates pod-level measurements,\nallowing per-pod integrity verification. A prototype on a k3s cluster\n(consisting of 1 master, 2 worker nodes) was implemented to attest to core\nfunctions, including AMF, SMF and UPF. The experimental results show that the\nsystem detects unauthorized modifications in real time, labels each pod's trust\nstate, and generates detailed audit logs. This work provides hardware-based\ncontinuous attestation for cloud native and edge deployments, strengthening the\nresilience of 5G as critical infrastructure in multi-vendor and\nmission-critical scenarios of 5G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of 5G technology, the adoption of\ncloud-based infrastructure for the deployment of 5G services has become\nincreasingly common. Using a service-based architecture, critical 5G\ncomponents, such as the Access and Mobility Management Function (AMF), Session\nManagement Function (SMF), and User Plane Function (UPF), now run as\ncontainerized pods on Kubernetes clusters. Although this approach improves\nscalability, flexibility, and resilience, it also introduces new security\nchallenges, particularly to ensure the integrity and trustworthiness of these\ncomponents. Current 5G security specifications (for example, 3GPP TS 33.501)\nfocus on communication security and assume that network functions remain\ntrustworthy after authentication, consequently lacking mechanisms to\ncontinuously validate the integrity of NVFs at runtime. To close this gap, and\nto align with Zero Trust principles of 'never trust, always verify', we present\na TPM 2.0-based continuous remote attestation solution for core 5G components\ndeployed on Kubernetes. Our approach uses the Linux Integrity Measurement\nArchitecture (IMA) and a Trusted Platform Module (TPM) to provide\nhardware-based runtime validation. We integrate the open-source Keylime\nframework with a custom IMA template that isolates pod-level measurements,\nallowing per-pod integrity verification. A prototype on a k3s cluster\n(consisting of 1 master, 2 worker nodes) was implemented to attest to core\nfunctions, including AMF, SMF and UPF. The experimental results show that the\nsystem detects unauthorized modifications in real time, labels each pod's trust\nstate, and generates detailed audit logs. This work provides hardware-based\ncontinuous attestation for cloud native and edge deployments, strengthening the\nresilience of 5G as critical infrastructure in multi-vendor and\nmission-critical scenarios of 5G."
                },
                "authors": [
                    {
                        "name": "Al Nahian Bin Emran"
                    },
                    {
                        "name": "Rajendra Upadhyay"
                    },
                    {
                        "name": "Rajendra Paudyal"
                    },
                    {
                        "name": "Lisa Donnan"
                    },
                    {
                        "name": "Duminda Wijesekera"
                    }
                ],
                "author_detail": {
                    "name": "Duminda Wijesekera"
                },
                "author": "Duminda Wijesekera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03217v1",
                "updated": "2025-10-03T17:53:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    53,
                    28,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:53:28Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    53,
                    28,
                    4,
                    276,
                    0
                ],
                "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic\n  Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic\n  Program Repair"
                },
                "summary": "Agentic Automated Program Repair (APR) is increasingly tackling complex,\nrepository-level bugs in industry, but ultimately agent-generated patches still\nneed to be reviewed by a human before committing them to ensure they address\nthe bug. Showing unlikely patches to developers can lead to substantial noise,\nwasting valuable developer time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such noise: bug\nabstention and patch validation policies. Bug abstention excludes bugs that the\nagentic APR system is unlikely to fix. Patch validation rejects patches that\nare unlikely to be a good fix for the given bug. We evaluate both policies on\nthree sets of bugs from Google's codebase, and their candidate patches\ngenerated by an internal agentic APR system. On a set of 174 human-reported\nbugs, removing bugs and patch trajectories rejected by our policies can raise\nsuccess rates by up to 13 percentage points and 15 percentage points,\nrespectively, and by up to 39 percentage points in combination. On null pointer\nexceptions and sanitizer-reported bugs with machine-generated bug reports,\npatch validation also improves average single-sample success rates. This\ntwo-policy approach provides a practical path to the reliable, industrial-scale\ndeployment of agentic APR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Automated Program Repair (APR) is increasingly tackling complex,\nrepository-level bugs in industry, but ultimately agent-generated patches still\nneed to be reviewed by a human before committing them to ensure they address\nthe bug. Showing unlikely patches to developers can lead to substantial noise,\nwasting valuable developer time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such noise: bug\nabstention and patch validation policies. Bug abstention excludes bugs that the\nagentic APR system is unlikely to fix. Patch validation rejects patches that\nare unlikely to be a good fix for the given bug. We evaluate both policies on\nthree sets of bugs from Google's codebase, and their candidate patches\ngenerated by an internal agentic APR system. On a set of 174 human-reported\nbugs, removing bugs and patch trajectories rejected by our policies can raise\nsuccess rates by up to 13 percentage points and 15 percentage points,\nrespectively, and by up to 39 percentage points in combination. On null pointer\nexceptions and sanitizer-reported bugs with machine-generated bug reports,\npatch validation also improves average single-sample success rates. This\ntwo-policy approach provides a practical path to the reliable, industrial-scale\ndeployment of agentic APR systems."
                },
                "authors": [
                    {
                        "name": "Jos Cambronero"
                    },
                    {
                        "name": "Michele Tufano"
                    },
                    {
                        "name": "Sherry Shi"
                    },
                    {
                        "name": "Renyao Wei"
                    },
                    {
                        "name": "Grant Uy"
                    },
                    {
                        "name": "Runxiang Cheng"
                    },
                    {
                        "name": "Chin-Jung Liu"
                    },
                    {
                        "name": "Shiying Pan"
                    },
                    {
                        "name": "Satish Chandra"
                    },
                    {
                        "name": "Pat Rondon"
                    }
                ],
                "author_detail": {
                    "name": "Pat Rondon"
                },
                "author": "Pat Rondon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03216v1",
                "updated": "2025-10-03T17:53:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    53,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:53:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    53,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image\n  Segmentation"
                },
                "summary": "For equitable deployment of AI tools in hospitals and healthcare facilities,\nwe need Deep Segmentation Networks that offer high performance and can be\ntrained on cost-effective GPUs with limited memory and large batch sizes. In\nthis work, we propose Wave-GMS, a lightweight and efficient multi-scale\ngenerative model for medical image segmentation. Wave-GMS has a substantially\nsmaller number of trainable parameters, does not require loading\nmemory-intensive pretrained vision foundation models, and supports training\nwith large batch sizes on GPUs with limited memory. We conducted extensive\nexperiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument,\nand HAM10000), demonstrating that Wave-GMS achieves state-of-the-art\nsegmentation performance with superior cross-domain generalizability, while\nrequiring only ~2.6M trainable parameters. Code is available at\nhttps://github.com/ATPLab-LUMS/Wave-GMS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For equitable deployment of AI tools in hospitals and healthcare facilities,\nwe need Deep Segmentation Networks that offer high performance and can be\ntrained on cost-effective GPUs with limited memory and large batch sizes. In\nthis work, we propose Wave-GMS, a lightweight and efficient multi-scale\ngenerative model for medical image segmentation. Wave-GMS has a substantially\nsmaller number of trainable parameters, does not require loading\nmemory-intensive pretrained vision foundation models, and supports training\nwith large batch sizes on GPUs with limited memory. We conducted extensive\nexperiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument,\nand HAM10000), demonstrating that Wave-GMS achieves state-of-the-art\nsegmentation performance with superior cross-domain generalizability, while\nrequiring only ~2.6M trainable parameters. Code is available at\nhttps://github.com/ATPLab-LUMS/Wave-GMS."
                },
                "authors": [
                    {
                        "name": "Talha Ahmed"
                    },
                    {
                        "name": "Nehal Ahmed Shaikh"
                    },
                    {
                        "name": "Hassan Mohy-ud-Din"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Mohy-ud-Din"
                },
                "author": "Hassan Mohy-ud-Din",
                "arxiv_comment": "5 pages, 1 figure, 4 tables; Submitted to IEEE Conference for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03215v1",
                "updated": "2025-10-03T17:52:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models"
                },
                "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Zihan Min"
                    },
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Jichao Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07714v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07714v3",
                "updated": "2025-10-03T17:50:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    50,
                    53,
                    4,
                    276,
                    0
                ],
                "published": "2024-06-11T20:48:28Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    20,
                    48,
                    28,
                    1,
                    163,
                    0
                ],
                "title": "LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLAMAFUZZ: Large Language Model Enhanced Greybox Fuzzing"
                },
                "summary": "Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in\nprograms. However, randomized mutation strategies have limited the fuzzer's\nperformance on structured data. Specialized fuzzers can handle complex\nstructured data, but require additional efforts in grammar and suffer from low\nthroughput.\n  In this paper, we explore the potential of utilizing the Large Language Model\nto enhance greybox fuzzing for structured data. We utilize the pre-trained\nknowledge of LLM about data conversion and format to generate new valid inputs.\nWe further fine-tuned it with paired mutation seeds to learn structured format\nand mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ,\nintegrates the power of LLM to understand and mutate structured data to\nfuzzing. We conduct experiments on the standard bug-based benchmark Magma and a\nwide variety of real-world programs. LLAMAFUZZ outperforms our top competitor\nby 41 bugs on average. We also identified 47 unique bugs across all trials.\nMoreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and\nbug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in\nreal-world program sets on average. We also demonstrate a case study to explain\nhow LLMs enhance the fuzzing process in terms of code coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Greybox fuzzing has achieved success in revealing bugs and vulnerabilities in\nprograms. However, randomized mutation strategies have limited the fuzzer's\nperformance on structured data. Specialized fuzzers can handle complex\nstructured data, but require additional efforts in grammar and suffer from low\nthroughput.\n  In this paper, we explore the potential of utilizing the Large Language Model\nto enhance greybox fuzzing for structured data. We utilize the pre-trained\nknowledge of LLM about data conversion and format to generate new valid inputs.\nWe further fine-tuned it with paired mutation seeds to learn structured format\nand mutation strategies effectively. Our LLM-based fuzzer, LLAMAFUZZ,\nintegrates the power of LLM to understand and mutate structured data to\nfuzzing. We conduct experiments on the standard bug-based benchmark Magma and a\nwide variety of real-world programs. LLAMAFUZZ outperforms our top competitor\nby 41 bugs on average. We also identified 47 unique bugs across all trials.\nMoreover, LLAMAFUZZ demonstrated consistent performance on both bug trigger and\nbug reached. Compared to AFL++, LLAMAFUZZ achieved 27.19% more branches in\nreal-world program sets on average. We also demonstrate a case study to explain\nhow LLMs enhance the fuzzing process in terms of code coverage."
                },
                "authors": [
                    {
                        "name": "Hongxiang Zhang"
                    },
                    {
                        "name": "Yuyang Rong"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07714v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07714v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21634v2",
                "updated": "2025-10-03T17:43:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    43,
                    57,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-25T21:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    49,
                    43,
                    3,
                    268,
                    0
                ],
                "title": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G\n  Open RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G\n  Open RANs"
                },
                "summary": "The evolution toward 6G networks is being accelerated by the Open Radio\nAccess Network (O-RAN) paradigm -- an open, interoperable architecture that\nenables intelligent, modular applications across public telecom and private\nenterprise domains. While this openness creates unprecedented opportunities for\ninnovation, it also expands the attack surface, demanding resilient, low-cost,\nand autonomous security solutions. Legacy defenses remain largely reactive,\nlabor-intensive, and inadequate for the scale and complexity of next-generation\nsystems. Current O-RAN applications focus mainly on network optimization or\npassive threat detection, with limited capability for closed-loop, automated\nresponse.\n  To address this critical gap, we present an agentic AI framework for fully\nautomated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM\norchestrates security workflows through a modular multi-agent system powered by\nLarge Language Models (LLMs). The framework features a Threat Analysis Agent\nfor real-time data triage, a Threat Classification Agent that uses\nRetrieval-Augmented Generation (RAG) to map anomalies to specific\ncountermeasures, and a Threat Response Agent that safely operationalizes\nmitigation actions via O-RAN control interfaces. Grounded in trusted knowledge\nbases such as the MITRE FiGHT framework and 3GPP specifications, and equipped\nwith robust safety guardrails, MobiLLM provides a blueprint for trustworthy\nAI-driven network security. Initial evaluations demonstrate that MobiLLM can\neffectively identify and orchestrate complex mitigation strategies,\nsignificantly reducing response latency and showcasing the feasibility of\nautonomous security operations in 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution toward 6G networks is being accelerated by the Open Radio\nAccess Network (O-RAN) paradigm -- an open, interoperable architecture that\nenables intelligent, modular applications across public telecom and private\nenterprise domains. While this openness creates unprecedented opportunities for\ninnovation, it also expands the attack surface, demanding resilient, low-cost,\nand autonomous security solutions. Legacy defenses remain largely reactive,\nlabor-intensive, and inadequate for the scale and complexity of next-generation\nsystems. Current O-RAN applications focus mainly on network optimization or\npassive threat detection, with limited capability for closed-loop, automated\nresponse.\n  To address this critical gap, we present an agentic AI framework for fully\nautomated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM\norchestrates security workflows through a modular multi-agent system powered by\nLarge Language Models (LLMs). The framework features a Threat Analysis Agent\nfor real-time data triage, a Threat Classification Agent that uses\nRetrieval-Augmented Generation (RAG) to map anomalies to specific\ncountermeasures, and a Threat Response Agent that safely operationalizes\nmitigation actions via O-RAN control interfaces. Grounded in trusted knowledge\nbases such as the MITRE FiGHT framework and 3GPP specifications, and equipped\nwith robust safety guardrails, MobiLLM provides a blueprint for trustworthy\nAI-driven network security. Initial evaluations demonstrate that MobiLLM can\neffectively identify and orchestrate complex mitigation strategies,\nsignificantly reducing response latency and showcasing the feasibility of\nautonomous security operations in 6G."
                },
                "authors": [
                    {
                        "name": "Prakhar Sharma"
                    },
                    {
                        "name": "Haohuang Wen"
                    },
                    {
                        "name": "Vinod Yegneswaran"
                    },
                    {
                        "name": "Ashish Gehani"
                    },
                    {
                        "name": "Phillip Porras"
                    },
                    {
                        "name": "Zhiqiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lin"
                },
                "author": "Zhiqiang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00430v2",
                "updated": "2025-10-03T17:42:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    42,
                    59,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-31T07:17:48Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    7,
                    17,
                    48,
                    5,
                    151,
                    0
                ],
                "title": "MIRROR: Modular Internal Processing for Personalized Safety in LLM\n  Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRROR: Modular Internal Processing for Personalized Safety in LLM\n  Dialogue"
                },
                "summary": "Large language models frequently generate harmful recommendations in personal\nmulti-turn dialogue by ignoring user-specific safety context, exhibiting\nsycophantic agreement, and compromising user safety for larger group\npreferences. We introduce MIRROR, a modular production-focused architecture\nthat prevents these failures through a persistent, bounded internal state that\npreserves personal conversational information across conversational turns. Our\ndual-component design inspired by Dual Process Theory separates immediate\nresponse generation (Talker) from asynchronous deliberative processing\n(Thinker), which synthesizes parallel reasoning threads between turns with\nmarginal latency. On the CuRaTe personalized safety benchmark, MIRROR-augmented\nmodels achieve a 21% relative improvement (69% to 84%) across seven diverse\nfrontier models, with open-source Llama 4 and Mistral 3 variants surpassing\nboth GPT-4o and Claude 3.7 Sonnet at only \\$0.0028 to \\$0.0172 additional cost\nper turn, narrowing the gap between affordable open-source models to frontier\nsystems in the safety space. The modular architecture enables flexible\ndeployment: full internal processing for affordable models or single-component\nconfigurations for expensive systems, democratizing access to safer,\npersonalized AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models frequently generate harmful recommendations in personal\nmulti-turn dialogue by ignoring user-specific safety context, exhibiting\nsycophantic agreement, and compromising user safety for larger group\npreferences. We introduce MIRROR, a modular production-focused architecture\nthat prevents these failures through a persistent, bounded internal state that\npreserves personal conversational information across conversational turns. Our\ndual-component design inspired by Dual Process Theory separates immediate\nresponse generation (Talker) from asynchronous deliberative processing\n(Thinker), which synthesizes parallel reasoning threads between turns with\nmarginal latency. On the CuRaTe personalized safety benchmark, MIRROR-augmented\nmodels achieve a 21% relative improvement (69% to 84%) across seven diverse\nfrontier models, with open-source Llama 4 and Mistral 3 variants surpassing\nboth GPT-4o and Claude 3.7 Sonnet at only \\$0.0028 to \\$0.0172 additional cost\nper turn, narrowing the gap between affordable open-source models to frontier\nsystems in the safety space. The modular architecture enables flexible\ndeployment: full internal processing for affordable models or single-component\nconfigurations for expensive systems, democratizing access to safer,\npersonalized AI."
                },
                "authors": [
                    {
                        "name": "Nicole Hsing"
                    }
                ],
                "author_detail": {
                    "name": "Nicole Hsing"
                },
                "author": "Nicole Hsing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03204v1",
                "updated": "2025-10-03T17:41:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    41,
                    30,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:41:30Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    41,
                    30,
                    4,
                    276,
                    0
                ],
                "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents"
                },
                "summary": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure."
                },
                "authors": [
                    {
                        "name": "Imene Kerboua"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Xing Han L"
                    },
                    {
                        "name": "Lo Boisvert"
                    },
                    {
                        "name": "Massimo Caccia"
                    },
                    {
                        "name": "Jrmy Espinas"
                    },
                    {
                        "name": "Alexandre Aussem"
                    },
                    {
                        "name": "Vronique Eglin"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Lacoste"
                },
                "author": "Alexandre Lacoste",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03203v1",
                "updated": "2025-10-03T17:40:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    40,
                    29,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:40:29Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    40,
                    29,
                    4,
                    276,
                    0
                ],
                "title": "OpenZL: A Graph-Based Model for Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenZL: A Graph-Based Model for Compression"
                },
                "summary": "Research in general-purpose lossless compression over the last decade has\nlargely found improvements in compression ratio that come at great cost to\nresource utilization and processing throughput. However, most production\nworkloads require high throughput and low resource utilization, so most\nresearch systems have seen little adoption. Instead, real world improvements in\ncompression are increasingly often realized by building application-specific\ncompressors which can exploit knowledge about the structure and semantics of\nthe data being compressed. These systems easily outperform even the best\ngeneric compressors, but application-specific compression schemes are not\nwithout drawbacks. They are inherently limited in applicability and are\ndifficult to maintain and deploy.\n  We show that these challenges can be overcome with a new way of thinking\nabout compression. We propose the ``graph model'' of compression, a new\ntheoretical framework for representing compression as a directed acyclic graph\nof modular codecs. This motivates OpenZL, an implementation of this model that\ncompresses data into a self-describing wire format, any configuration of which\ncan be decompressed by a universal decoder. OpenZL's design enables rapid\ndevelopment of tailored compressors with minimal code, its universal decoder\neliminates deployment lag, and its investment in a well-vetted standard\ncomponent library minimizes security risks. Experimental results demonstrate\nthat OpenZL achieves superior compression ratios and speeds compared to\nstate-of-the-art general-purpose compressors on a variety of real-world\ndatasets. Internal deployments at Meta have also shown consistent improvements\nin size and/or speed, with development timelines reduced from months to days.\nOpenZL thus represents an advance in practical, scalable, and maintainable data\ncompression for modern data-intensive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in general-purpose lossless compression over the last decade has\nlargely found improvements in compression ratio that come at great cost to\nresource utilization and processing throughput. However, most production\nworkloads require high throughput and low resource utilization, so most\nresearch systems have seen little adoption. Instead, real world improvements in\ncompression are increasingly often realized by building application-specific\ncompressors which can exploit knowledge about the structure and semantics of\nthe data being compressed. These systems easily outperform even the best\ngeneric compressors, but application-specific compression schemes are not\nwithout drawbacks. They are inherently limited in applicability and are\ndifficult to maintain and deploy.\n  We show that these challenges can be overcome with a new way of thinking\nabout compression. We propose the ``graph model'' of compression, a new\ntheoretical framework for representing compression as a directed acyclic graph\nof modular codecs. This motivates OpenZL, an implementation of this model that\ncompresses data into a self-describing wire format, any configuration of which\ncan be decompressed by a universal decoder. OpenZL's design enables rapid\ndevelopment of tailored compressors with minimal code, its universal decoder\neliminates deployment lag, and its investment in a well-vetted standard\ncomponent library minimizes security risks. Experimental results demonstrate\nthat OpenZL achieves superior compression ratios and speeds compared to\nstate-of-the-art general-purpose compressors on a variety of real-world\ndatasets. Internal deployments at Meta have also shown consistent improvements\nin size and/or speed, with development timelines reduced from months to days.\nOpenZL thus represents an advance in practical, scalable, and maintainable data\ncompression for modern data-intensive applications."
                },
                "authors": [
                    {
                        "name": "Yann Collet"
                    },
                    {
                        "name": "Nick Terrell"
                    },
                    {
                        "name": "W. Felix Handte"
                    },
                    {
                        "name": "Danielle Rozenblit"
                    },
                    {
                        "name": "Victor Zhang"
                    },
                    {
                        "name": "Kevin Zhang"
                    },
                    {
                        "name": "Yaelle Goldschlag"
                    },
                    {
                        "name": "Jennifer Lee"
                    },
                    {
                        "name": "Daniel Riegel"
                    },
                    {
                        "name": "Stan Angelov"
                    },
                    {
                        "name": "Nadav Rotem"
                    }
                ],
                "author_detail": {
                    "name": "Nadav Rotem"
                },
                "author": "Nadav Rotem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10150v2",
                "updated": "2025-10-03T17:36:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    36,
                    51,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-11T20:10:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    20,
                    10,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "When Large Language Models are Reliable for Judging Empathic\n  Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Large Language Models are Reliable for Judging Empathic\n  Communication"
                },
                "summary": "Large language models (LLMs) excel at generating empathic responses in\ntext-based conversations. But, how reliably do they judge the nuances of\nempathic communication? We investigate this question by comparing how experts,\ncrowdworkers, and LLMs annotate empathic communication across four evaluative\nframeworks drawn from psychology, natural language processing, and\ncommunications applied to 200 real-world conversations where one speaker shares\na personal problem and the other offers support. Drawing on 3,150 expert\nannotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess\ninter-rater reliability between these three annotator groups. We find that\nexpert agreement is high but varies across the frameworks' sub-components\ndepending on their clarity, complexity, and subjectivity. We show that expert\nagreement offers a more informative benchmark for contextualizing LLM\nperformance than standard classification metrics. Across all four frameworks,\nLLMs consistently approach this expert level benchmark and exceed the\nreliability of crowdworkers. These results demonstrate how LLMs, when validated\non specific tasks with appropriate benchmarks, can support transparency and\noversight in emotionally sensitive applications including their use as\nconversational companions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at generating empathic responses in\ntext-based conversations. But, how reliably do they judge the nuances of\nempathic communication? We investigate this question by comparing how experts,\ncrowdworkers, and LLMs annotate empathic communication across four evaluative\nframeworks drawn from psychology, natural language processing, and\ncommunications applied to 200 real-world conversations where one speaker shares\na personal problem and the other offers support. Drawing on 3,150 expert\nannotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess\ninter-rater reliability between these three annotator groups. We find that\nexpert agreement is high but varies across the frameworks' sub-components\ndepending on their clarity, complexity, and subjectivity. We show that expert\nagreement offers a more informative benchmark for contextualizing LLM\nperformance than standard classification metrics. Across all four frameworks,\nLLMs consistently approach this expert level benchmark and exceed the\nreliability of crowdworkers. These results demonstrate how LLMs, when validated\non specific tasks with appropriate benchmarks, can support transparency and\noversight in emotionally sensitive applications including their use as\nconversational companions."
                },
                "authors": [
                    {
                        "name": "Aakriti Kumar"
                    },
                    {
                        "name": "Nalin Poungpeth"
                    },
                    {
                        "name": "Diyi Yang"
                    },
                    {
                        "name": "Erina Farrell"
                    },
                    {
                        "name": "Bruce Lambert"
                    },
                    {
                        "name": "Matthew Groh"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Groh"
                },
                "author": "Matthew Groh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14052v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14052v4",
                "updated": "2025-10-03T17:35:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    52,
                    4,
                    276,
                    0
                ],
                "published": "2025-08-07T22:15:22Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    22,
                    15,
                    22,
                    3,
                    219,
                    0
                ],
                "title": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial\n  Question Answering"
                },
                "summary": "Accurate information retrieval (IR) is critical in the financial domain,\nwhere investors must identify relevant information from large collections of\ndocuments. Traditional IR methods -- whether sparse or dense -- often fall\nshort in retrieval accuracy, as it requires not only capturing semantic\nsimilarity but also performing fine-grained reasoning over document structure\nand domain-specific knowledge. Recent advances in large language models (LLMs)\nhave opened up new opportunities for retrieval with multi-step reasoning, where\nthe model ranks passages through iterative reasoning about which information is\nmost relevant to a given query. However, there exists no benchmark to evaluate\nsuch capabilities in the financial domain. To address this gap, we introduce\nFinAgentBench, the first large-scale benchmark for evaluating retrieval with\nmulti-step reasoning in finance -- a setting we term agentic retrieval. The\nbenchmark consists of 26K expert-annotated examples on S&P-500 listed firms and\nassesses whether LLM agents can (1) identify the most relevant document type\namong candidates, and (2) pinpoint the key passage within the selected\ndocument. Our evaluation framework explicitly separates these two reasoning\nsteps to address context limitations. This design enables to provide a\nquantitative basis for understanding retrieval-centric LLM behavior in finance.\nWe evaluate a suite of state-of-the-art models and further demonstrated how\ntargeted fine-tuning can significantly improve agentic retrieval performance.\nOur benchmark provides a foundation for studying retrieval-centric LLM behavior\nin complex, domain-specific tasks for finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate information retrieval (IR) is critical in the financial domain,\nwhere investors must identify relevant information from large collections of\ndocuments. Traditional IR methods -- whether sparse or dense -- often fall\nshort in retrieval accuracy, as it requires not only capturing semantic\nsimilarity but also performing fine-grained reasoning over document structure\nand domain-specific knowledge. Recent advances in large language models (LLMs)\nhave opened up new opportunities for retrieval with multi-step reasoning, where\nthe model ranks passages through iterative reasoning about which information is\nmost relevant to a given query. However, there exists no benchmark to evaluate\nsuch capabilities in the financial domain. To address this gap, we introduce\nFinAgentBench, the first large-scale benchmark for evaluating retrieval with\nmulti-step reasoning in finance -- a setting we term agentic retrieval. The\nbenchmark consists of 26K expert-annotated examples on S&P-500 listed firms and\nassesses whether LLM agents can (1) identify the most relevant document type\namong candidates, and (2) pinpoint the key passage within the selected\ndocument. Our evaluation framework explicitly separates these two reasoning\nsteps to address context limitations. This design enables to provide a\nquantitative basis for understanding retrieval-centric LLM behavior in finance.\nWe evaluate a suite of state-of-the-art models and further demonstrated how\ntargeted fine-tuning can significantly improve agentic retrieval performance.\nOur benchmark provides a foundation for studying retrieval-centric LLM behavior\nin complex, domain-specific tasks for finance."
                },
                "authors": [
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Jihoon Kwon"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Chaewoon Kim"
                    },
                    {
                        "name": "Minjae Kim"
                    },
                    {
                        "name": "Juneha Hwang"
                    },
                    {
                        "name": "Jaeseon Ha"
                    },
                    {
                        "name": "Hojun Choi"
                    },
                    {
                        "name": "Suyeol Yun"
                    },
                    {
                        "name": "Yongjin Kim"
                    },
                    {
                        "name": "Yongjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yongjae Lee"
                },
                "author": "Yongjae Lee",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14052v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14052v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03199v1",
                "updated": "2025-10-03T17:35:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    45,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:35:45Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    45,
                    4,
                    276,
                    0
                ],
                "title": "Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference\n  Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference\n  Scaling"
                },
                "summary": "LLM inference often generates a batch of candidates for a prompt and selects\none via strategies like majority voting or Best-of- N (BoN). For difficult\ntasks, this single-shot selection often underperforms. Consequently,\nevaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,\nand only the best of them is used when computing regret. Motivated by this, we\nstudy inference scaling in the more general Pass@$k$ inference setting, and\nprove that neither majority voting nor BoN exhibits the desirable scaling with\n$k$ and the sampling budget $N$. Combining the advantages of majority voting\nand BoN, we propose a new inference strategy called Best-of-Majority (BoM),\nwith a pivotal step that restricts the candidates to the responses with high\nfrequency in the $N$ samples before selecting the top-$k$ rewards. We prove\nthat when the sampling budget is $N=\\tilde\\Omega(C^*)$, the regret of BoM is\n$O(\\epsilon_{\\mathrm{opt}}+\\sqrt{\\epsilon_{\\mathrm{RM}}^2C^*/k})$, where $C^*$\nis the coverage coefficient, $\\epsilon_{\\mathrm{RM}}$ is the estimation error\nof the reward model, and $\\epsilon_{\\mathrm{opt}}$ is the estimation error of\nreward at the optimal response. We further establish a matching lower bound,\ncertifying that our algorithm is minimax optimal. Beyond optimality, BoM has a\nkey advantage: unlike majority voting and BoN, its performance does not degrade\nwhen increasing $N$. Experimental results of inference on math problems show\nBoM outperforming both majority voting and BoN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference often generates a batch of candidates for a prompt and selects\none via strategies like majority voting or Best-of- N (BoN). For difficult\ntasks, this single-shot selection often underperforms. Consequently,\nevaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,\nand only the best of them is used when computing regret. Motivated by this, we\nstudy inference scaling in the more general Pass@$k$ inference setting, and\nprove that neither majority voting nor BoN exhibits the desirable scaling with\n$k$ and the sampling budget $N$. Combining the advantages of majority voting\nand BoN, we propose a new inference strategy called Best-of-Majority (BoM),\nwith a pivotal step that restricts the candidates to the responses with high\nfrequency in the $N$ samples before selecting the top-$k$ rewards. We prove\nthat when the sampling budget is $N=\\tilde\\Omega(C^*)$, the regret of BoM is\n$O(\\epsilon_{\\mathrm{opt}}+\\sqrt{\\epsilon_{\\mathrm{RM}}^2C^*/k})$, where $C^*$\nis the coverage coefficient, $\\epsilon_{\\mathrm{RM}}$ is the estimation error\nof the reward model, and $\\epsilon_{\\mathrm{opt}}$ is the estimation error of\nreward at the optimal response. We further establish a matching lower bound,\ncertifying that our algorithm is minimax optimal. Beyond optimality, BoM has a\nkey advantage: unlike majority voting and BoN, its performance does not degrade\nwhen increasing $N$. Experimental results of inference on math problems show\nBoM outperforming both majority voting and BoN."
                },
                "authors": [
                    {
                        "name": "Qiwei Di"
                    },
                    {
                        "name": "Kaixuan Ji"
                    },
                    {
                        "name": "Xuheng Li"
                    },
                    {
                        "name": "Heyang Zhao"
                    },
                    {
                        "name": "Quanquan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Quanquan Gu"
                },
                "author": "Quanquan Gu",
                "arxiv_comment": "29 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03195v1",
                "updated": "2025-10-03T17:30:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    30,
                    56,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:30:56Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    30,
                    56,
                    4,
                    276,
                    0
                ],
                "title": "Can LLMs Hit Moving Targets? Tracking Evolving Signals in Corporate\n  Disclosures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Hit Moving Targets? Tracking Evolving Signals in Corporate\n  Disclosures"
                },
                "summary": "Moving targets -- managers' strategic shifting of key performance metrics\nwhen the original targets become difficult to achieve -- have been shown to\npredict subsequent stock underperformance. However, our work reveals that the\nmethod employed in that study exhibits two key limitations that hinder the\naccuracy -- noise in the extracted targets and loss of contextual information\n-- both of which stem primarily from the use of a named entity recognition\n(NER). To address these two limitations, we propose an LLM-based target\nextraction} method with a newly defined metric that better captures semantic\ncontext. This approach preserves semantic context beyond simple entity\nrecognition and yields consistently higher predictive power than the original\napproach. Overall, our approach enhances the granularity and accuracy of\nfinancial text-based performance prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moving targets -- managers' strategic shifting of key performance metrics\nwhen the original targets become difficult to achieve -- have been shown to\npredict subsequent stock underperformance. However, our work reveals that the\nmethod employed in that study exhibits two key limitations that hinder the\naccuracy -- noise in the extracted targets and loss of contextual information\n-- both of which stem primarily from the use of a named entity recognition\n(NER). To address these two limitations, we propose an LLM-based target\nextraction} method with a newly defined metric that better captures semantic\ncontext. This approach preserves semantic context beyond simple entity\nrecognition and yields consistently higher predictive power than the original\napproach. Overall, our approach enhances the granularity and accuracy of\nfinancial text-based performance prediction."
                },
                "authors": [
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Jihoon Kwon"
                    },
                    {
                        "name": "Minjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Minjae Kim"
                },
                "author": "Minjae Kim",
                "arxiv_comment": "8 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03194v1",
                "updated": "2025-10-03T17:30:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    30,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:30:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    30,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoDA: Agentic Systems for Collaborative Data Visualization"
                },
                "summary": "Deep research has revolutionized data analysis, yet data scientists still\ndevote substantial time to manually crafting visualizations, highlighting the\nneed for robust automation from natural language queries. However, current\nsystems struggle with complex datasets containing multiple files and iterative\nrefinement. Existing approaches, including simple single- or multi-agent\nsystems, often oversimplify the task, focusing on initial query parsing while\nfailing to robustly manage data complexity, code errors, or final visualization\nquality. In this paper, we reframe this challenge as a collaborative\nmulti-agent problem. We introduce CoDA, a multi-agent system that employs\nspecialized LLM agents for metadata analysis, task planning, code generation,\nand self-reflection. We formalize this pipeline, demonstrating how\nmetadata-focused analysis bypasses token limits and quality-driven refinement\nensures robustness. Extensive evaluations show CoDA achieves substantial gains\nin the overall score, outperforming competitive baselines by up to 41.5%. This\nwork demonstrates that the future of visualization automation lies not in\nisolated code generation but in integrated, collaborative agentic workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep research has revolutionized data analysis, yet data scientists still\ndevote substantial time to manually crafting visualizations, highlighting the\nneed for robust automation from natural language queries. However, current\nsystems struggle with complex datasets containing multiple files and iterative\nrefinement. Existing approaches, including simple single- or multi-agent\nsystems, often oversimplify the task, focusing on initial query parsing while\nfailing to robustly manage data complexity, code errors, or final visualization\nquality. In this paper, we reframe this challenge as a collaborative\nmulti-agent problem. We introduce CoDA, a multi-agent system that employs\nspecialized LLM agents for metadata analysis, task planning, code generation,\nand self-reflection. We formalize this pipeline, demonstrating how\nmetadata-focused analysis bypasses token limits and quality-driven refinement\nensures robustness. Extensive evaluations show CoDA achieves substantial gains\nin the overall score, outperforming competitive baselines by up to 41.5%. This\nwork demonstrates that the future of visualization automation lies not in\nisolated code generation but in integrated, collaborative agentic workflows."
                },
                "authors": [
                    {
                        "name": "Zichen Chen"
                    },
                    {
                        "name": "Jiefeng Chen"
                    },
                    {
                        "name": "Sercan . Arik"
                    },
                    {
                        "name": "Misha Sra"
                    },
                    {
                        "name": "Tomas Pfister"
                    },
                    {
                        "name": "Jinsung Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Jinsung Yoon"
                },
                "author": "Jinsung Yoon",
                "arxiv_comment": "31 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24015v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24015v3",
                "updated": "2025-10-03T17:26:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    26,
                    56,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-30T16:19:38Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    19,
                    38,
                    0,
                    181,
                    0
                ],
                "title": "Hierarchical Knowledge Injection for Improving LLM-based Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Knowledge Injection for Improving LLM-based Program Repair"
                },
                "summary": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Esteban Parra"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "arxiv_comment": "Accepted at IEEE/ACM Automated Software Engineering (ASE) 2025\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24015v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24015v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01903v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01903v3",
                "updated": "2025-10-03T17:11:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    11,
                    21,
                    4,
                    276,
                    0
                ],
                "published": "2024-04-02T12:44:44Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    12,
                    44,
                    44,
                    1,
                    93,
                    0
                ],
                "title": "Understanding How CodeLLMs (Mis)Predict Types with Activation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding How CodeLLMs (Mis)Predict Types with Activation Steering"
                },
                "summary": "Large Language Models (LLMs) are widely used by software engineers for\nprogramming tasks. However, research shows that LLMs often lack a deep\nunderstanding of program semantics. Even minor changes to syntax, such as\nrenaming variables, can significantly degrade performance across various tasks.\nIn this work, we examine the task of type prediction: given a partially typed\nprogram, can a model predict a missing type annotations such that the resulting\nprogram is more typed? We construct a dataset of adversarial examples where\nmodels initially predict the correct types, but begin to fail after\nsemantically irrelevant edits. This is problematic, as models should ideally\ngeneralize across different syntactic forms of semantically equivalent code.\nThis lack of robustness suggests that models may have a shallow understanding\nof code semantics. Despite this, we provide evidence that LLMs do, in fact,\nlearn robust mechanisms for type prediction-though these mechanisms often fail\nto activate in adversarial scenarios. By using activation steering, a method\nthat manipulates a model's internal activations to guide it toward using latent\nknowledge, we restore accurate predictions on adversarial inputs. We show that\nsteering successfully activates a type prediction mechanism that is shared by\nboth Python and TypeScript, and is more effective than prompting with\nin-context examples. Across five different models, our comprehensive evaluation\ndemonstrates that LLMs can learn generalizable representations of code\nsemantics that transfer across programming languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used by software engineers for\nprogramming tasks. However, research shows that LLMs often lack a deep\nunderstanding of program semantics. Even minor changes to syntax, such as\nrenaming variables, can significantly degrade performance across various tasks.\nIn this work, we examine the task of type prediction: given a partially typed\nprogram, can a model predict a missing type annotations such that the resulting\nprogram is more typed? We construct a dataset of adversarial examples where\nmodels initially predict the correct types, but begin to fail after\nsemantically irrelevant edits. This is problematic, as models should ideally\ngeneralize across different syntactic forms of semantically equivalent code.\nThis lack of robustness suggests that models may have a shallow understanding\nof code semantics. Despite this, we provide evidence that LLMs do, in fact,\nlearn robust mechanisms for type prediction-though these mechanisms often fail\nto activate in adversarial scenarios. By using activation steering, a method\nthat manipulates a model's internal activations to guide it toward using latent\nknowledge, we restore accurate predictions on adversarial inputs. We show that\nsteering successfully activates a type prediction mechanism that is shared by\nboth Python and TypeScript, and is more effective than prompting with\nin-context examples. Across five different models, our comprehensive evaluation\ndemonstrates that LLMs can learn generalizable representations of code\nsemantics that transfer across programming languages."
                },
                "authors": [
                    {
                        "name": "Francesca Lucchetti"
                    },
                    {
                        "name": "Arjun Guha"
                    }
                ],
                "author_detail": {
                    "name": "Arjun Guha"
                },
                "author": "Arjun Guha",
                "arxiv_comment": "40 pages, 67 figures. To be published at BlackBoxNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01903v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01903v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03185v1",
                "updated": "2025-10-03T17:09:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    9,
                    3,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:09:03Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    9,
                    3,
                    4,
                    276,
                    0
                ],
                "title": "PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning"
                },
                "summary": "Benchmarks for competition-style reasoning have advanced evaluation in\nmathematics and programming, yet physics remains comparatively explored. Most\nexisting physics benchmarks evaluate only final answers, which fail to capture\nreasoning processes, while recent stepwise methods rely on heuristic\nLLM-as-judge scoring or restrictive linear assumptions, limiting reliability\nand diagnostic validity. We introduce PRISM-Physics, a process-level evaluation\nframework and benchmark for complex physics reasoning problems. Solutions are\nrepresented as directed acyclic graphs (DAGs) of formulas, explicitly encoding\ncausal dependencies among intermediate steps to enable fine-grained,\ninterpretable, and theoretically grounded scoring. We prove the optimality of\nthe DAG representation and the corresponding scoring policy. Combining with a\nfully rule-based method for symbolic formula equivalence matching that we\ndeveloped, we ensure consistent validation across diverse formulations without\nheuristic judgments. Results show that our evaluation framework is more aligned\nwith human experts' scoring. Experiments on state-of-the-art LLMs reveal\npersistent reasoning failures in physics, while step-level scoring offers both\ndiagnostic insight and rich signals for later training. By combining structural\nrigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides\na principled foundation for advancing process-level evaluation and guiding the\ndevelopment of models with deeper scientific reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks for competition-style reasoning have advanced evaluation in\nmathematics and programming, yet physics remains comparatively explored. Most\nexisting physics benchmarks evaluate only final answers, which fail to capture\nreasoning processes, while recent stepwise methods rely on heuristic\nLLM-as-judge scoring or restrictive linear assumptions, limiting reliability\nand diagnostic validity. We introduce PRISM-Physics, a process-level evaluation\nframework and benchmark for complex physics reasoning problems. Solutions are\nrepresented as directed acyclic graphs (DAGs) of formulas, explicitly encoding\ncausal dependencies among intermediate steps to enable fine-grained,\ninterpretable, and theoretically grounded scoring. We prove the optimality of\nthe DAG representation and the corresponding scoring policy. Combining with a\nfully rule-based method for symbolic formula equivalence matching that we\ndeveloped, we ensure consistent validation across diverse formulations without\nheuristic judgments. Results show that our evaluation framework is more aligned\nwith human experts' scoring. Experiments on state-of-the-art LLMs reveal\npersistent reasoning failures in physics, while step-level scoring offers both\ndiagnostic insight and rich signals for later training. By combining structural\nrigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides\na principled foundation for advancing process-level evaluation and guiding the\ndevelopment of models with deeper scientific reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Qinwei Ma"
                    },
                    {
                        "name": "Jingzhe Shi"
                    },
                    {
                        "name": "Shirley Wu"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Si-Yuan Chen"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Ludwig Schmidt"
                    },
                    {
                        "name": "James Zou"
                    }
                ],
                "author_detail": {
                    "name": "James Zou"
                },
                "author": "James Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01443v2",
                "updated": "2025-10-03T17:04:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    4,
                    35,
                    4,
                    276,
                    0
                ],
                "published": "2025-08-02T17:11:40Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    17,
                    11,
                    40,
                    5,
                    214,
                    0
                ],
                "title": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial\n  Perspective"
                },
                "summary": "There is a growing interest in leveraging multiple large language models\n(LLMs) for automated code optimization. However, industrial platforms deploying\nmultiple LLMs face a critical challenge: prompts optimized for one LLM often\nfail with others, requiring expensive model-specific prompt engineering. This\ncross-model prompt engineering bottleneck severely limits the practical\ndeployment of multi-LLM systems in production environments. We introduce\nMeta-Prompted Code Optimization (MPCO), a framework that automatically\ngenerates high-quality, task-specific prompts across diverse LLMs while\nmaintaining industrial efficiency requirements. MPCO leverages metaprompting to\ndynamically synthesize context-aware optimization prompts by integrating\nproject metadata, task requirements, and LLM-specific contexts. It is an\nessential part of the ARTEMIS code optimization platform for automated\nvalidation and scaling. Our comprehensive evaluation on five real-world\ncodebases with 366 hours of runtime benchmarking demonstrates MPCO's\neffectiveness: it achieves overall performance improvements up to 19.06% with\nthe best statistical rank across all systems compared to baseline methods.\nAnalysis shows that 96% of the top-performing optimizations stem from\nmeaningful edits. Through systematic ablation studies and meta-prompter\nsensitivity analysis, we identify that comprehensive context integration is\nessential for effective meta-prompting and that major LLMs can serve\neffectively as meta-prompters, providing actionable insights for industrial\npractitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in leveraging multiple large language models\n(LLMs) for automated code optimization. However, industrial platforms deploying\nmultiple LLMs face a critical challenge: prompts optimized for one LLM often\nfail with others, requiring expensive model-specific prompt engineering. This\ncross-model prompt engineering bottleneck severely limits the practical\ndeployment of multi-LLM systems in production environments. We introduce\nMeta-Prompted Code Optimization (MPCO), a framework that automatically\ngenerates high-quality, task-specific prompts across diverse LLMs while\nmaintaining industrial efficiency requirements. MPCO leverages metaprompting to\ndynamically synthesize context-aware optimization prompts by integrating\nproject metadata, task requirements, and LLM-specific contexts. It is an\nessential part of the ARTEMIS code optimization platform for automated\nvalidation and scaling. Our comprehensive evaluation on five real-world\ncodebases with 366 hours of runtime benchmarking demonstrates MPCO's\neffectiveness: it achieves overall performance improvements up to 19.06% with\nthe best statistical rank across all systems compared to baseline methods.\nAnalysis shows that 96% of the top-performing optimizations stem from\nmeaningful edits. Through systematic ablation studies and meta-prompter\nsensitivity analysis, we identify that comprehensive context integration is\nessential for effective meta-prompting and that major LLMs can serve\neffectively as meta-prompters, providing actionable insights for industrial\npractitioners."
                },
                "authors": [
                    {
                        "name": "Jingzhi Gong"
                    },
                    {
                        "name": "Rafail Giavrimis"
                    },
                    {
                        "name": "Paul Brookes"
                    },
                    {
                        "name": "Vardan Voskanyan"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Mari Ashiga"
                    },
                    {
                        "name": "Matthew Truscott"
                    },
                    {
                        "name": "Mike Basios"
                    },
                    {
                        "name": "Leslie Kanthan"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang",
                "arxiv_comment": "Accepted by ASE'25 Industry Showcase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03178v1",
                "updated": "2025-10-03T16:53:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    53,
                    13,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:53:13Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    53,
                    13,
                    4,
                    276,
                    0
                ],
                "title": "When Names Disappear: Revealing What LLMs Actually Understand About Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Names Disappear: Revealing What LLMs Actually Understand About Code"
                },
                "summary": "Large Language Models (LLMs) achieve strong results on code tasks, but how\nthey derive program meaning remains unclear. We argue that code communicates\nthrough two channels: structural semantics, which define formal behavior, and\nhuman-interpretable naming, which conveys intent. Removing the naming channel\nseverely degrades intent-level tasks such as summarization, where models\nregress to line-by-line descriptions. Surprisingly, we also observe consistent\nreductions on execution tasks that should depend only on structure, revealing\nthat current benchmarks reward memorization of naming patterns rather than\ngenuine semantic reasoning. To disentangle these effects, we introduce a suite\nof semantics-preserving obfuscations and show that they expose identifier\nleakage across both summarization and execution. Building on these insights, we\nrelease ClassEval-Obf, an obfuscation-enhanced benchmark that systematically\nsuppresses naming cues while preserving behavior. Our results demonstrate that\nClassEval-Obf reduces inflated performance gaps, weakens memorization\nshortcuts, and provides a more reliable basis for assessing LLMs' code\nunderstanding and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve strong results on code tasks, but how\nthey derive program meaning remains unclear. We argue that code communicates\nthrough two channels: structural semantics, which define formal behavior, and\nhuman-interpretable naming, which conveys intent. Removing the naming channel\nseverely degrades intent-level tasks such as summarization, where models\nregress to line-by-line descriptions. Surprisingly, we also observe consistent\nreductions on execution tasks that should depend only on structure, revealing\nthat current benchmarks reward memorization of naming patterns rather than\ngenuine semantic reasoning. To disentangle these effects, we introduce a suite\nof semantics-preserving obfuscations and show that they expose identifier\nleakage across both summarization and execution. Building on these insights, we\nrelease ClassEval-Obf, an obfuscation-enhanced benchmark that systematically\nsuppresses naming cues while preserving behavior. Our results demonstrate that\nClassEval-Obf reduces inflated performance gaps, weakens memorization\nshortcuts, and provides a more reliable basis for assessing LLMs' code\nunderstanding and generalization."
                },
                "authors": [
                    {
                        "name": "Cuong Chi Le"
                    },
                    {
                        "name": "Minh V. T. Pham"
                    },
                    {
                        "name": "Cuong Duc Van"
                    },
                    {
                        "name": "Hoang N. Phan"
                    },
                    {
                        "name": "Huy N. Phan"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Tien N. Nguyen"
                },
                "author": "Tien N. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03174v1",
                "updated": "2025-10-03T16:48:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    48,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:48:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    48,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Topic Modeling as Long-Form Generation: Can Long-Context LLMs\n  revolutionize NTM via Zero-Shot Prompting?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic Modeling as Long-Form Generation: Can Long-Context LLMs\n  revolutionize NTM via Zero-Shot Prompting?"
                },
                "summary": "Traditional topic models such as neural topic models rely on inference and\ngeneration networks to learn latent topic distributions. This paper explores a\nnew paradigm for topic modeling in the era of large language models, framing TM\nas a long-form generation task whose definition is updated in this paradigm. We\npropose a simple but practical approach to implement LLM-based topic model\ntasks out of the box (sample a data subset, generate topics and representative\ntext with our prompt, text assignment with keyword match). We then investigate\nwhether the long-form generation paradigm can beat NTMs via zero-shot\nprompting. We conduct a systematic comparison between NTMs and LLMs in terms of\ntopic quality and empirically examine the claim that \"a majority of NTMs are\noutdated.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional topic models such as neural topic models rely on inference and\ngeneration networks to learn latent topic distributions. This paper explores a\nnew paradigm for topic modeling in the era of large language models, framing TM\nas a long-form generation task whose definition is updated in this paradigm. We\npropose a simple but practical approach to implement LLM-based topic model\ntasks out of the box (sample a data subset, generate topics and representative\ntext with our prompt, text assignment with keyword match). We then investigate\nwhether the long-form generation paradigm can beat NTMs via zero-shot\nprompting. We conduct a systematic comparison between NTMs and LLMs in terms of\ntopic quality and empirically examine the claim that \"a majority of NTMs are\noutdated.\""
                },
                "authors": [
                    {
                        "name": "Xuan Xu"
                    },
                    {
                        "name": "Haolun Li"
                    },
                    {
                        "name": "Zhongliang Yang"
                    },
                    {
                        "name": "Beilin Chu"
                    },
                    {
                        "name": "Jia Song"
                    },
                    {
                        "name": "Moxuan Xu"
                    },
                    {
                        "name": "Linna Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Linna Zhou"
                },
                "author": "Linna Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24827v2",
                "updated": "2025-10-03T16:46:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    46,
                    28,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-29T14:13:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    13,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "Putnam-like dataset summary: LLMs as mathematical competition\n  contestants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putnam-like dataset summary: LLMs as mathematical competition\n  contestants"
                },
                "summary": "In this paper we summarize the results of the Putnam-like benchmark published\nby Google DeepMind. This dataset consists of 96 original problems in the spirit\nof the Putnam Competition and 576 solutions of LLMs. We analyse the performance\nof models on this set of problems to verify their ability to solve problems\nfrom mathematical contests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we summarize the results of the Putnam-like benchmark published\nby Google DeepMind. This dataset consists of 96 original problems in the spirit\nof the Putnam Competition and 576 solutions of LLMs. We analyse the performance\nof models on this set of problems to verify their ability to solve problems\nfrom mathematical contests."
                },
                "authors": [
                    {
                        "name": "Bartosz Bieganowski"
                    },
                    {
                        "name": "Daniel Strzelecki"
                    },
                    {
                        "name": "Robert Skiba"
                    },
                    {
                        "name": "Mateusz Topolewski"
                    }
                ],
                "author_detail": {
                    "name": "Mateusz Topolewski"
                },
                "author": "Mateusz Topolewski",
                "arxiv_comment": "11 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03165v1",
                "updated": "2025-10-03T16:36:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    36,
                    9,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:36:09Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    36,
                    9,
                    4,
                    276,
                    0
                ],
                "title": "FTTE: Federated Learning on Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FTTE: Federated Learning on Resource-Constrained Devices"
                },
                "summary": "Federated learning (FL) enables collaborative model training across\ndistributed devices while preserving data privacy, but deployment on\nresource-constrained edge nodes remains challenging due to limited memory,\nenergy, and communication bandwidth. Traditional synchronous and asynchronous\nFL approaches further suffer from straggler induced delays and slow convergence\nin heterogeneous, large scale networks. We present FTTE (Federated Tiny\nTraining Engine),a novel semi-asynchronous FL framework that uniquely employs\nsparse parameter updates and a staleness-weighted aggregation based on both age\nand variance of client updates. Extensive experiments across diverse models and\ndata distributions - including up to 500 clients and 90% stragglers -\ndemonstrate that FTTE not only achieves 81% faster convergence, 80% lower\non-device memory usage, and 69% communication payload reduction than\nsynchronous FL (eg.FedAVG), but also consistently reaches comparable or higher\ntarget accuracy than semi-asynchronous (eg.FedBuff) in challenging regimes.\nThese results establish FTTE as the first practical and scalable solution for\nreal-world FL deployments on heterogeneous and predominantly\nresource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) enables collaborative model training across\ndistributed devices while preserving data privacy, but deployment on\nresource-constrained edge nodes remains challenging due to limited memory,\nenergy, and communication bandwidth. Traditional synchronous and asynchronous\nFL approaches further suffer from straggler induced delays and slow convergence\nin heterogeneous, large scale networks. We present FTTE (Federated Tiny\nTraining Engine),a novel semi-asynchronous FL framework that uniquely employs\nsparse parameter updates and a staleness-weighted aggregation based on both age\nand variance of client updates. Extensive experiments across diverse models and\ndata distributions - including up to 500 clients and 90% stragglers -\ndemonstrate that FTTE not only achieves 81% faster convergence, 80% lower\non-device memory usage, and 69% communication payload reduction than\nsynchronous FL (eg.FedAVG), but also consistently reaches comparable or higher\ntarget accuracy than semi-asynchronous (eg.FedBuff) in challenging regimes.\nThese results establish FTTE as the first practical and scalable solution for\nreal-world FL deployments on heterogeneous and predominantly\nresource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Irene Tenison"
                    },
                    {
                        "name": "Anna Murphy"
                    },
                    {
                        "name": "Charles Beauville"
                    },
                    {
                        "name": "Lalana Kagal"
                    }
                ],
                "author_detail": {
                    "name": "Lalana Kagal"
                },
                "author": "Lalana Kagal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03160v1",
                "updated": "2025-10-03T16:32:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    32,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:32:02Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    32,
                    2,
                    4,
                    276,
                    0
                ],
                "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus"
                },
                "summary": "Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs."
                },
                "authors": [
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Wenhui Dong"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Xiang Zheng"
                    },
                    {
                        "name": "Zhonghao Zhang"
                    },
                    {
                        "name": "Zian Zhou"
                    },
                    {
                        "name": "Yunzhi Guan"
                    },
                    {
                        "name": "Liukun Xu"
                    },
                    {
                        "name": "Wei Peng"
                    },
                    {
                        "name": "Zhaoyang Gong"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Dachuan Li"
                    },
                    {
                        "name": "Xiaosheng Ma"
                    },
                    {
                        "name": "Yuli Ma"
                    },
                    {
                        "name": "Jianing Ni"
                    },
                    {
                        "name": "Changjiang Jiang"
                    },
                    {
                        "name": "Lixia Tian"
                    },
                    {
                        "name": "Qixin Chen"
                    },
                    {
                        "name": "Kaishun Xia"
                    },
                    {
                        "name": "Pingping Liu"
                    },
                    {
                        "name": "Tongshun Zhang"
                    },
                    {
                        "name": "Zhiqiang Liu"
                    },
                    {
                        "name": "Zhongan Bi"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Tiansheng Sun"
                    },
                    {
                        "name": "Caifeng Shan"
                    }
                ],
                "author_detail": {
                    "name": "Caifeng Shan"
                },
                "author": "Caifeng Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03153v1",
                "updated": "2025-10-03T16:25:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    25,
                    48,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:25:48Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    25,
                    48,
                    4,
                    276,
                    0
                ],
                "title": "Improving Cooperation in Collaborative Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Cooperation in Collaborative Embodied AI"
                },
                "summary": "The integration of Large Language Models (LLMs) into multiagent systems has\nopened new possibilities for collaborative reasoning and cooperation with AI\nagents. This paper explores different prompting methods and evaluates their\neffectiveness in enhancing agent collaborative behaviour and decision-making.\nWe enhance CoELA, a framework designed for building Collaborative Embodied\nAgents that leverage LLMs for multi-agent communication, reasoning, and task\ncoordination in shared virtual spaces. Through systematic experimentation, we\nexamine different LLMs and prompt engineering strategies to identify optimised\ncombinations that maximise collaboration performance. Furthermore, we extend\nour research by integrating speech capabilities, enabling seamless\ncollaborative voice-based interactions. Our findings highlight the\neffectiveness of prompt optimisation in enhancing collaborative agent\nperformance; for example, our best combination improved the efficiency of the\nsystem running with Gemma3 by 22% compared to the original CoELA system. In\naddition, the speech integration provides a more engaging user interface for\niterative system development and demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into multiagent systems has\nopened new possibilities for collaborative reasoning and cooperation with AI\nagents. This paper explores different prompting methods and evaluates their\neffectiveness in enhancing agent collaborative behaviour and decision-making.\nWe enhance CoELA, a framework designed for building Collaborative Embodied\nAgents that leverage LLMs for multi-agent communication, reasoning, and task\ncoordination in shared virtual spaces. Through systematic experimentation, we\nexamine different LLMs and prompt engineering strategies to identify optimised\ncombinations that maximise collaboration performance. Furthermore, we extend\nour research by integrating speech capabilities, enabling seamless\ncollaborative voice-based interactions. Our findings highlight the\neffectiveness of prompt optimisation in enhancing collaborative agent\nperformance; for example, our best combination improved the efficiency of the\nsystem running with Gemma3 by 22% compared to the original CoELA system. In\naddition, the speech integration provides a more engaging user interface for\niterative system development and demonstrations."
                },
                "authors": [
                    {
                        "name": "Hima Jacob Leven Suprabha"
                    },
                    {
                        "name": "Laxmi Nag Laxminarayan Nagesh"
                    },
                    {
                        "name": "Ajith Nair"
                    },
                    {
                        "name": "Alvin Reuben Amal Selvaster"
                    },
                    {
                        "name": "Ayan Khan"
                    },
                    {
                        "name": "Raghuram Damarla"
                    },
                    {
                        "name": "Sanju Hannah Samuel"
                    },
                    {
                        "name": "Sreenithi Saravana Perumal"
                    },
                    {
                        "name": "Titouan Puech"
                    },
                    {
                        "name": "Venkataramireddy Marella"
                    },
                    {
                        "name": "Vishal Sonar"
                    },
                    {
                        "name": "Alessandro Suglia"
                    },
                    {
                        "name": "Oliver Lemon"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Lemon"
                },
                "author": "Oliver Lemon",
                "arxiv_comment": "In proceedings of UKCI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01456v2",
                "updated": "2025-10-03T16:22:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    22,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-01T20:54:49Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    20,
                    54,
                    49,
                    2,
                    274,
                    0
                ],
                "title": "SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for\n  Diffusion"
                },
                "summary": "Out-of-distribution (OOD) detection is essential for reliable deployment of\nmachine learning systems in vision, robotics, reinforcement learning, and\nbeyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator\nfor Diffusion (SCOPED), a fast and general-purpose OOD detection method for\ndiffusion models that reduces the number of forward passes on the trained model\nby an order of magnitude compared to prior methods, outperforming most\ndiffusion-based baselines and closely approaching the accuracy of the strongest\nones. SCOPED is computed from a single diffusion model trained once on a\ndiverse dataset, and combines the Jacobian trace and squared norm of the\nmodel's score function into a single test statistic. Rather than thresholding\non a fixed value, we estimate the in-distribution density of SCOPED scores\nusing kernel density estimation, enabling a flexible, unsupervised test that,\nin the simplest case, only requires a single forward pass and one\nJacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.\nOn four vision benchmarks, SCOPED achieves competitive or state-of-the-art\nprecision-recall scores despite its low computational cost. The same method\ngeneralizes to robotic control tasks with shared state and action spaces,\nidentifying distribution shifts across reward functions and training regimes.\nThese results position SCOPED as a practical foundation for fast and reliable\nOOD detection in real-world domains, including perceptual artifacts in vision,\noutlier detection in autoregressive models, exploration in reinforcement\nlearning, and dataset curation for unsupervised training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is essential for reliable deployment of\nmachine learning systems in vision, robotics, reinforcement learning, and\nbeyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator\nfor Diffusion (SCOPED), a fast and general-purpose OOD detection method for\ndiffusion models that reduces the number of forward passes on the trained model\nby an order of magnitude compared to prior methods, outperforming most\ndiffusion-based baselines and closely approaching the accuracy of the strongest\nones. SCOPED is computed from a single diffusion model trained once on a\ndiverse dataset, and combines the Jacobian trace and squared norm of the\nmodel's score function into a single test statistic. Rather than thresholding\non a fixed value, we estimate the in-distribution density of SCOPED scores\nusing kernel density estimation, enabling a flexible, unsupervised test that,\nin the simplest case, only requires a single forward pass and one\nJacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.\nOn four vision benchmarks, SCOPED achieves competitive or state-of-the-art\nprecision-recall scores despite its low computational cost. The same method\ngeneralizes to robotic control tasks with shared state and action spaces,\nidentifying distribution shifts across reward functions and training regimes.\nThese results position SCOPED as a practical foundation for fast and reliable\nOOD detection in real-world domains, including perceptual artifacts in vision,\noutlier detection in autoregressive models, exploration in reinforcement\nlearning, and dataset curation for unsupervised training."
                },
                "authors": [
                    {
                        "name": "Brett Barkley"
                    },
                    {
                        "name": "Preston Culbertson"
                    },
                    {
                        "name": "David Fridovich-Keil"
                    }
                ],
                "author_detail": {
                    "name": "David Fridovich-Keil"
                },
                "author": "David Fridovich-Keil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09901v2",
                "updated": "2025-10-03T16:12:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    12,
                    51,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-15T02:09:18Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    2,
                    9,
                    18,
                    3,
                    135,
                    0
                ],
                "title": "Comparing Exploration-Exploitation Strategies of LLMs and Humans:\n  Insights from Standard Multi-armed Bandit Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Exploration-Exploitation Strategies of LLMs and Humans:\n  Insights from Standard Multi-armed Bandit Experiments"
                },
                "summary": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making settings. A natural\nquestion is then whether LLMs exhibit similar decision-making behavior to\nhumans, and can achieve comparable (or superior) performance. In this work, we\nfocus on the exploration-exploitation (E&E) tradeoff, a fundamental aspect of\ndynamic decision-making under uncertainty. We employ canonical multi-armed\nbandit (MAB) experiments introduced in the cognitive science and psychiatry\nliterature to conduct a comparative study of the E&E strategies of LLMs,\nhumans, and MAB algorithms. We use interpretable choice models to capture the\nE&E strategies of the agents and investigate how enabling thinking traces,\nthrough both prompting strategies and thinking models, shapes LLM\ndecision-making. We find that enabling thinking in LLMs shifts their behavior\ntoward more human-like behavior, characterized by a mix of random and directed\nexploration. In a simple stationary setting, thinking-enabled LLMs exhibit\nsimilar levels of random and directed exploration compared to humans. However,\nin more complex, non-stationary environments, LLMs struggle to match human\nadaptability, particularly in effective directed exploration, despite achieving\nsimilar regret in certain scenarios. Our findings highlight both the promise\nand limits of LLMs as simulators of human behavior and tools for automated\ndecision-making and point to potential areas for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to simulate or automate\nhuman behavior in complex sequential decision-making settings. A natural\nquestion is then whether LLMs exhibit similar decision-making behavior to\nhumans, and can achieve comparable (or superior) performance. In this work, we\nfocus on the exploration-exploitation (E&E) tradeoff, a fundamental aspect of\ndynamic decision-making under uncertainty. We employ canonical multi-armed\nbandit (MAB) experiments introduced in the cognitive science and psychiatry\nliterature to conduct a comparative study of the E&E strategies of LLMs,\nhumans, and MAB algorithms. We use interpretable choice models to capture the\nE&E strategies of the agents and investigate how enabling thinking traces,\nthrough both prompting strategies and thinking models, shapes LLM\ndecision-making. We find that enabling thinking in LLMs shifts their behavior\ntoward more human-like behavior, characterized by a mix of random and directed\nexploration. In a simple stationary setting, thinking-enabled LLMs exhibit\nsimilar levels of random and directed exploration compared to humans. However,\nin more complex, non-stationary environments, LLMs struggle to match human\nadaptability, particularly in effective directed exploration, despite achieving\nsimilar regret in certain scenarios. Our findings highlight both the promise\nand limits of LLMs as simulators of human behavior and tools for automated\ndecision-making and point to potential areas for improvement."
                },
                "authors": [
                    {
                        "name": "Ziyuan Zhang"
                    },
                    {
                        "name": "Darcy Wang"
                    },
                    {
                        "name": "Ningyuan Chen"
                    },
                    {
                        "name": "Rodrigo Mansur"
                    },
                    {
                        "name": "Vahid Sarhangian"
                    }
                ],
                "author_detail": {
                    "name": "Vahid Sarhangian"
                },
                "author": "Vahid Sarhangian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03136v1",
                "updated": "2025-10-03T16:07:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    7,
                    15,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T16:07:15Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    16,
                    7,
                    15,
                    4,
                    276,
                    0
                ],
                "title": "Beyond the Final Layer: Intermediate Representations for Better\n  Multilingual Calibration in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Final Layer: Intermediate Representations for Better\n  Multilingual Calibration in Large Language Models"
                },
                "summary": "Confidence calibration, the alignment of a model's predicted confidence with\nits actual accuracy, is crucial for the reliable deployment of Large Language\nModels (LLMs). However, this critical property remains largely under-explored\nin multilingual contexts. In this work, we conduct the first large-scale,\nsystematic studies of multilingual calibration across six model families and\nover 100 languages, revealing that non-English languages suffer from\nsystematically worse calibration. To diagnose this, we investigate the model's\ninternal representations and find that the final layer, biased by\nEnglish-centric training, provides a poor signal for multilingual confidence.\nIn contrast, our layer-wise analysis uncovers a key insight that\nlate-intermediate layers consistently offer a more reliable and\nbetter-calibrated signal. Building on this, we introduce a suite of\ntraining-free methods, including Language-Aware Confidence Ensemble (LACE),\nwhich adaptively selects an optimal ensemble of layers for each specific\nlanguage. Our study highlights the hidden costs of English-centric alignment\nand offer a new path toward building more globally equitable and trustworthy\nLLMs by looking beyond the final layer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence calibration, the alignment of a model's predicted confidence with\nits actual accuracy, is crucial for the reliable deployment of Large Language\nModels (LLMs). However, this critical property remains largely under-explored\nin multilingual contexts. In this work, we conduct the first large-scale,\nsystematic studies of multilingual calibration across six model families and\nover 100 languages, revealing that non-English languages suffer from\nsystematically worse calibration. To diagnose this, we investigate the model's\ninternal representations and find that the final layer, biased by\nEnglish-centric training, provides a poor signal for multilingual confidence.\nIn contrast, our layer-wise analysis uncovers a key insight that\nlate-intermediate layers consistently offer a more reliable and\nbetter-calibrated signal. Building on this, we introduce a suite of\ntraining-free methods, including Language-Aware Confidence Ensemble (LACE),\nwhich adaptively selects an optimal ensemble of layers for each specific\nlanguage. Our study highlights the hidden costs of English-centric alignment\nand offer a new path toward building more globally equitable and trustworthy\nLLMs by looking beyond the final layer."
                },
                "authors": [
                    {
                        "name": "Ej Zhou"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Chengzu Li"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Ivan Vuli"
                    },
                    {
                        "name": "Anna Korhonen"
                    }
                ],
                "author_detail": {
                    "name": "Anna Korhonen"
                },
                "author": "Anna Korhonen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07052v2",
                "updated": "2025-10-03T15:56:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    56,
                    43,
                    4,
                    276,
                    0
                ],
                "published": "2025-04-09T17:12:49Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    12,
                    49,
                    2,
                    99,
                    0
                ],
                "title": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model\n  Reasoning"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved their reasoning abilities, particularly through techniques involving\nsearch and backtracking. Backtracking naturally scales test-time compute by\nenabling sequential, linearized exploration via long chain-of-thought (CoT)\ngeneration. However, this is not the only strategy for scaling test\ntime-compute: parallel sampling with best-of-N selection provides an\nalternative that generates diverse solutions simultaneously. Despite the\ngrowing adoption of sequential search, its advantages over parallel\nsampling-especially under a fixed compute budget-remain poorly understood. In\nthis paper, we systematically compare these two approaches on two challenging\nreasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential\nsearch underperforms parallel sampling on CountDown but outperforms it on\nSudoku, suggesting that backtracking is not universally beneficial. We identify\ntwo factors that can cause backtracking to degrade performance: (1) training on\nfixed search traces can lock models intro suboptimal strategies, and (2)\nexplicit CoT supervision can discourage implicit (non verbalized) reasoning.\nExtending our analysis to reinforcement learning (RL), we show that models with\nbacktracking capabilities benefit significantly from RL fine-tuning, while\nmodels without backtracking see limited, mixed gains. Together, these findings\nchallenge the assumption that backtracking universally enhances LLM reasoning,\ninstead revealing a complex interaction between task structure, training data,\nmodel scale, and learning paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nimproved their reasoning abilities, particularly through techniques involving\nsearch and backtracking. Backtracking naturally scales test-time compute by\nenabling sequential, linearized exploration via long chain-of-thought (CoT)\ngeneration. However, this is not the only strategy for scaling test\ntime-compute: parallel sampling with best-of-N selection provides an\nalternative that generates diverse solutions simultaneously. Despite the\ngrowing adoption of sequential search, its advantages over parallel\nsampling-especially under a fixed compute budget-remain poorly understood. In\nthis paper, we systematically compare these two approaches on two challenging\nreasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential\nsearch underperforms parallel sampling on CountDown but outperforms it on\nSudoku, suggesting that backtracking is not universally beneficial. We identify\ntwo factors that can cause backtracking to degrade performance: (1) training on\nfixed search traces can lock models intro suboptimal strategies, and (2)\nexplicit CoT supervision can discourage implicit (non verbalized) reasoning.\nExtending our analysis to reinforcement learning (RL), we show that models with\nbacktracking capabilities benefit significantly from RL fine-tuning, while\nmodels without backtracking see limited, mixed gains. Together, these findings\nchallenge the assumption that backtracking universally enhances LLM reasoning,\ninstead revealing a complex interaction between task structure, training data,\nmodel scale, and learning paradigm."
                },
                "authors": [
                    {
                        "name": "Tian Qin"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    },
                    {
                        "name": "Samy Jelassi"
                    },
                    {
                        "name": "Eran Malach"
                    }
                ],
                "author_detail": {
                    "name": "Eran Malach"
                },
                "author": "Eran Malach",
                "arxiv_comment": "COLM 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22811v2",
                "updated": "2025-10-03T15:53:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    53,
                    5,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-28T19:40:34Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    19,
                    40,
                    34,
                    2,
                    148,
                    0
                ],
                "title": "Highly Efficient and Effective LLMs with Multi-Boolean Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Highly Efficient and Effective LLMs with Multi-Boolean Architectures"
                },
                "summary": "Weight binarization has emerged as a promising strategy to reduce the\ncomplexity of large language models (LLMs). Existing approaches fall into\npost-training binarization, which is simple but causes severe performance loss,\nand training-aware methods, which depend on full-precision latent weights,\nadding complexity and limiting efficiency. We propose a novel framework that\nrepresents LLMs with multi-kernel Boolean parameters and, for the first time,\nenables direct finetuning LMMs in the Boolean domain, eliminating the need for\nlatent weights. This enhances representational capacity and dramatically\nreduces complexity during both finetuning and inference. Extensive experiments\nacross diverse LLMs show our method outperforms recent ultra low-bit\nquantization and binarization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight binarization has emerged as a promising strategy to reduce the\ncomplexity of large language models (LLMs). Existing approaches fall into\npost-training binarization, which is simple but causes severe performance loss,\nand training-aware methods, which depend on full-precision latent weights,\nadding complexity and limiting efficiency. We propose a novel framework that\nrepresents LLMs with multi-kernel Boolean parameters and, for the first time,\nenables direct finetuning LMMs in the Boolean domain, eliminating the need for\nlatent weights. This enhances representational capacity and dramatically\nreduces complexity during both finetuning and inference. Extensive experiments\nacross diverse LLMs show our method outperforms recent ultra low-bit\nquantization and binarization techniques."
                },
                "authors": [
                    {
                        "name": "Ba-Hien Tran"
                    },
                    {
                        "name": "Van Minh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Van Minh Nguyen"
                },
                "author": "Van Minh Nguyen",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03120v1",
                "updated": "2025-10-03T15:49:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    49,
                    9,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:49:09Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    49,
                    9,
                    4,
                    276,
                    0
                ],
                "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?"
                },
                "summary": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation)."
                },
                "authors": [
                    {
                        "name": "Zhaojun Sun"
                    },
                    {
                        "name": "Xuzhou Zhu"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v2",
                "updated": "2025-10-03T15:37:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    37,
                    19,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures; Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03102v1",
                "updated": "2025-10-03T15:31:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    31,
                    11,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:31:11Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    31,
                    11,
                    4,
                    276,
                    0
                ],
                "title": "Semantic Similarity in Radiology Reports via LLMs and NER",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Similarity in Radiology Reports via LLMs and NER"
                },
                "summary": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at:\n\\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\\_reports}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at:\n\\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\\_reports}"
                },
                "authors": [
                    {
                        "name": "Beth Pearson"
                    },
                    {
                        "name": "Ahmed Adnan"
                    },
                    {
                        "name": "Zahraa Abdallah"
                    }
                ],
                "author_detail": {
                    "name": "Zahraa Abdallah"
                },
                "author": "Zahraa Abdallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03093v1",
                "updated": "2025-10-03T15:23:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    23,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T15:23:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    23,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better\n  Scaling than CoT Prompting?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better\n  Scaling than CoT Prompting?"
                },
                "summary": "Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based\nmodels, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,\nwhere the model is guided to first transcribe the speech and then translate it.\nCoT typically outperforms direct prompting primarily because it can exploit\nabundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)\ndatasets to explicitly model its steps. In this paper, we systematically\ncompare CoT and Direct prompting under increasing amounts of S2TT data. To this\nend, we pseudo-label an ASR corpus by translating its transcriptions into six\nEuropean languages, and train LLM-based S2TT systems with both prompting\nstrategies at different data scales. Our results show that Direct improves more\nconsistently as the amount of data increases, suggesting that it may become a\nmore effective approach as larger S2TT resources are created.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based\nmodels, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,\nwhere the model is guided to first transcribe the speech and then translate it.\nCoT typically outperforms direct prompting primarily because it can exploit\nabundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)\ndatasets to explicitly model its steps. In this paper, we systematically\ncompare CoT and Direct prompting under increasing amounts of S2TT data. To this\nend, we pseudo-label an ASR corpus by translating its transcriptions into six\nEuropean languages, and train LLM-based S2TT systems with both prompting\nstrategies at different data scales. Our results show that Direct improves more\nconsistently as the amount of data increases, suggesting that it may become a\nmore effective approach as larger S2TT resources are created."
                },
                "authors": [
                    {
                        "name": "Oriol Pareras"
                    },
                    {
                        "name": "Gerard I. Gllego"
                    },
                    {
                        "name": "Federico Costa"
                    },
                    {
                        "name": "Cristina Espaa-Bonet"
                    },
                    {
                        "name": "Javier Hernando"
                    }
                ],
                "author_detail": {
                    "name": "Javier Hernando"
                },
                "author": "Javier Hernando",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04632v2",
                "updated": "2025-10-03T15:20:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    20,
                    24,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-05T05:04:44Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    4,
                    44,
                    3,
                    156,
                    0
                ],
                "title": "Risk-Sensitive Agent Compositions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk-Sensitive Agent Compositions"
                },
                "summary": "From software development to robot control, modern agentic systems decompose\ncomplex objectives into a sequence of subtasks and choose a set of specialized\nAI agents to complete them. We formalize agentic workflows as directed acyclic\ngraphs, called agent graphs, where edges represent AI agents and paths\ncorrespond to feasible compositions of agents. Real-world deployment requires\nselecting agent compositions that not only maximize task success but also\nminimize violations of safety, fairness, and privacy requirements which demands\na careful analysis of the low-probability (tail) behaviors of compositions of\nagents. In this work, we consider risk minimization over the set of feasible\nagent compositions and seek to minimize the value-at-risk of the loss\ndistribution of the agent composition where the loss quantifies violations of\nthese requirements. We introduce an efficient algorithm which traverses the\nagent graph and finds a near-optimal composition of agents. It uses a dynamic\nprogramming approach to approximate the value-at-risk of agent compositions by\nexploiting a union bound. Furthermore, we prove that the approximation is\nnear-optimal asymptotically for a broad class of practical loss functions. To\nevaluate our framework, we consider a suite of video game-like control\nbenchmarks that require composing several agents trained with reinforcement\nlearning and demonstrate our algorithm's effectiveness in approximating the\nvalue-at-risk and identifying the optimal agent composition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From software development to robot control, modern agentic systems decompose\ncomplex objectives into a sequence of subtasks and choose a set of specialized\nAI agents to complete them. We formalize agentic workflows as directed acyclic\ngraphs, called agent graphs, where edges represent AI agents and paths\ncorrespond to feasible compositions of agents. Real-world deployment requires\nselecting agent compositions that not only maximize task success but also\nminimize violations of safety, fairness, and privacy requirements which demands\na careful analysis of the low-probability (tail) behaviors of compositions of\nagents. In this work, we consider risk minimization over the set of feasible\nagent compositions and seek to minimize the value-at-risk of the loss\ndistribution of the agent composition where the loss quantifies violations of\nthese requirements. We introduce an efficient algorithm which traverses the\nagent graph and finds a near-optimal composition of agents. It uses a dynamic\nprogramming approach to approximate the value-at-risk of agent compositions by\nexploiting a union bound. Furthermore, we prove that the approximation is\nnear-optimal asymptotically for a broad class of practical loss functions. To\nevaluate our framework, we consider a suite of video game-like control\nbenchmarks that require composing several agents trained with reinforcement\nlearning and demonstrate our algorithm's effectiveness in approximating the\nvalue-at-risk and identifying the optimal agent composition."
                },
                "authors": [
                    {
                        "name": "Guruprerana Shabadi"
                    },
                    {
                        "name": "Rajeev Alur"
                    }
                ],
                "author_detail": {
                    "name": "Rajeev Alur"
                },
                "author": "Rajeev Alur",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21692v2",
                "updated": "2025-10-03T15:19:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    19,
                    6,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-26T18:22:53Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    22,
                    53,
                    3,
                    177,
                    0
                ],
                "title": "Frequency-stable nanophotonic microcavities via integrated thermometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency-stable nanophotonic microcavities via integrated thermometry"
                },
                "summary": "Field-deployable integrated photonic devices co-packaged with electronics\nwill enable important applications such as optical interconnects, quantum\ninformation processing, precision measurements, spectroscopy, and microwave\ngeneration. Significant progress has been made over the past two decades on\nincreasing the functional complexity of photonic chips. However, a critical\nchallenge that remains is the lack of scalable techniques to overcome thermal\nperturbations arising from the environment and co-packaged electronics. Here,\nwe demonstrate a fully integrated scheme to monitor and stabilize the\ntemperature of a high-Q microresonator on a Si-based chip, which can serve as a\nphotonic frequency reference. Our approach relies on a thin-film metallic\nresistor placed directly above the microcavity, acting as an integrated\nresistance thermometer, enabling unique mapping of the cavity's absolute\nresonance wavelength to the thermometer's electrical resistance. Following a\none-time calibration, the microresonator can be accurately and repeatably tuned\nto any desired absolute resonance wavelength using thermometry alone with a\nroot-mean squared wavelength error of <0.8 pm over a timespan of days. We\nfrequency-lock a distributed feedback (DFB) laser to the microresonator and\ndemonstrate a 48x reduction in its frequency drift, resulting in its center\nwavelength staying within +-0.5 pm of the mean over the duration of 50 hours in\nthe presence of significant ambient fluctuations, outperforming many commercial\nDFB and wavelength-locker-based laser systems. Finally, we stabilize a soliton\nmode-locked Kerr comb without the need for photodetection, paving the way for\nKerr-comb-based photonic devices that can potentially operate in the desired\nmode-locked state indefinitely.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Field-deployable integrated photonic devices co-packaged with electronics\nwill enable important applications such as optical interconnects, quantum\ninformation processing, precision measurements, spectroscopy, and microwave\ngeneration. Significant progress has been made over the past two decades on\nincreasing the functional complexity of photonic chips. However, a critical\nchallenge that remains is the lack of scalable techniques to overcome thermal\nperturbations arising from the environment and co-packaged electronics. Here,\nwe demonstrate a fully integrated scheme to monitor and stabilize the\ntemperature of a high-Q microresonator on a Si-based chip, which can serve as a\nphotonic frequency reference. Our approach relies on a thin-film metallic\nresistor placed directly above the microcavity, acting as an integrated\nresistance thermometer, enabling unique mapping of the cavity's absolute\nresonance wavelength to the thermometer's electrical resistance. Following a\none-time calibration, the microresonator can be accurately and repeatably tuned\nto any desired absolute resonance wavelength using thermometry alone with a\nroot-mean squared wavelength error of <0.8 pm over a timespan of days. We\nfrequency-lock a distributed feedback (DFB) laser to the microresonator and\ndemonstrate a 48x reduction in its frequency drift, resulting in its center\nwavelength staying within +-0.5 pm of the mean over the duration of 50 hours in\nthe presence of significant ambient fluctuations, outperforming many commercial\nDFB and wavelength-locker-based laser systems. Finally, we stabilize a soliton\nmode-locked Kerr comb without the need for photodetection, paving the way for\nKerr-comb-based photonic devices that can potentially operate in the desired\nmode-locked state indefinitely."
                },
                "authors": [
                    {
                        "name": "Sai Kanth Dacha"
                    },
                    {
                        "name": "Yun Zhao"
                    },
                    {
                        "name": "Karl J. McNulty"
                    },
                    {
                        "name": "Gaurang R. Bhatt"
                    },
                    {
                        "name": "Michal Lipson"
                    },
                    {
                        "name": "Alexander L. Gaeta"
                    }
                ],
                "author_detail": {
                    "name": "Alexander L. Gaeta"
                },
                "author": "Alexander L. Gaeta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11400v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11400v2",
                "updated": "2025-10-03T15:11:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    11,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-17T03:34:31Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    3,
                    34,
                    31,
                    0,
                    48,
                    0
                ],
                "title": "On the Diminishing Returns of Complex Robust RAG Training in the Era of\n  Powerful LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Diminishing Returns of Complex Robust RAG Training in the Era of\n  Powerful LLMs"
                },
                "summary": "Retrieval-augmented generation (RAG) systems traditionally employ\nsophisticated training strategies to enhance robustness against retrieval\nnoise. In this work, we investigate a critical question: does the benefit of\nthese complex robust training methods diminish as language models become more\npowerful? Through systematic evaluation across multiple model scales and\nquestion-answering datasets, our analysis reveals a consistent trend: \\emph{the\nmarginal robustness benefit of sophisticated training strategies decreases\nsubstantially as model capacity increases.} While smaller models show\nsignificant performance improvements from complex document selection and\nadversarial objectives, more capable models achieve comparable or even superior\nperformance with simpler training approaches. Further investigation\ndemonstrates that stronger models naturally exhibit better confidence\ncalibration, cross-dataset generalization capability, and more effective\nattention patterns, even under simple training regimes. These findings suggest\nthat as foundation models evolve, the engineering effort invested in complex\nrobust training may yield diminishing returns, indicating that simplified RAG\npipelines could suffice for powerful models while maintaining competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems traditionally employ\nsophisticated training strategies to enhance robustness against retrieval\nnoise. In this work, we investigate a critical question: does the benefit of\nthese complex robust training methods diminish as language models become more\npowerful? Through systematic evaluation across multiple model scales and\nquestion-answering datasets, our analysis reveals a consistent trend: \\emph{the\nmarginal robustness benefit of sophisticated training strategies decreases\nsubstantially as model capacity increases.} While smaller models show\nsignificant performance improvements from complex document selection and\nadversarial objectives, more capable models achieve comparable or even superior\nperformance with simpler training approaches. Further investigation\ndemonstrates that stronger models naturally exhibit better confidence\ncalibration, cross-dataset generalization capability, and more effective\nattention patterns, even under simple training regimes. These findings suggest\nthat as foundation models evolve, the engineering effort invested in complex\nrobust training may yield diminishing returns, indicating that simplified RAG\npipelines could suffice for powerful models while maintaining competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Hanxing Ding"
                    },
                    {
                        "name": "Shuchang Tao"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Zihao Wei"
                    },
                    {
                        "name": "Liwei Chen"
                    },
                    {
                        "name": "Kun Xu"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted at SIGIR-AP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11400v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11400v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22860v2",
                "updated": "2025-10-03T14:50:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    50,
                    37,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-28T20:47:02Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    20,
                    47,
                    2,
                    2,
                    148,
                    0
                ],
                "title": "Permissioned LLMs: Enforcing Access Control in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Permissioned LLMs: Enforcing Access Control in Large Language Models"
                },
                "summary": "In enterprise settings, organizational data is segregated, siloed and\ncarefully protected by elaborate access control frameworks. These access\ncontrol structures can completely break down if an LLM fine-tuned on the siloed\ndata serves requests, for downstream tasks, from individuals with disparate\naccess privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs\nthat superimpose the organizational data access control structures on query\nresponses they generate. We formalize abstractions underpinning the means to\ndetermine whether access control enforcement happens correctly over LLM query\nresponses. Our formalism introduces the notion of a relevant response that can\nbe used to prove whether a PermLLM mechanism has been implemented correctly. We\nalso introduce a novel metric, called access advantage, to empirically evaluate\nthe efficacy of a PermLLM mechanism. We introduce three novel PermLLM\nmechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired\naccess control. We furthermore present two instantiations of access\nadvantage--(i) Domain Distinguishability Index (DDI) based on Membership\nInference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility\nevaluation. We demonstrate the efficacy of our PermLLM mechanisms through\nextensive experiments on five public datasets (GPQA, RCV1, SimpleQA, WMDP, and\nPubMedQA), in addition to evaluating the validity of DDI and UGI metrics\nthemselves for quantifying access control in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In enterprise settings, organizational data is segregated, siloed and\ncarefully protected by elaborate access control frameworks. These access\ncontrol structures can completely break down if an LLM fine-tuned on the siloed\ndata serves requests, for downstream tasks, from individuals with disparate\naccess privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs\nthat superimpose the organizational data access control structures on query\nresponses they generate. We formalize abstractions underpinning the means to\ndetermine whether access control enforcement happens correctly over LLM query\nresponses. Our formalism introduces the notion of a relevant response that can\nbe used to prove whether a PermLLM mechanism has been implemented correctly. We\nalso introduce a novel metric, called access advantage, to empirically evaluate\nthe efficacy of a PermLLM mechanism. We introduce three novel PermLLM\nmechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired\naccess control. We furthermore present two instantiations of access\nadvantage--(i) Domain Distinguishability Index (DDI) based on Membership\nInference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility\nevaluation. We demonstrate the efficacy of our PermLLM mechanisms through\nextensive experiments on five public datasets (GPQA, RCV1, SimpleQA, WMDP, and\nPubMedQA), in addition to evaluating the validity of DDI and UGI metrics\nthemselves for quantifying access control in LLMs."
                },
                "authors": [
                    {
                        "name": "Bargav Jayaraman"
                    },
                    {
                        "name": "Virendra J. Marathe"
                    },
                    {
                        "name": "Hamid Mozaffari"
                    },
                    {
                        "name": "William F. Shen"
                    },
                    {
                        "name": "Krishnaram Kenthapadi"
                    }
                ],
                "author_detail": {
                    "name": "Krishnaram Kenthapadi"
                },
                "author": "Krishnaram Kenthapadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21345v2",
                "updated": "2025-10-03T14:35:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    35,
                    40,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-17T15:17:48Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    17,
                    48,
                    2,
                    260,
                    0
                ],
                "title": "Neuromorphic Deployment of Spiking Neural Networks for Cognitive Load\n  Classification in Air Traffic Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Deployment of Spiking Neural Networks for Cognitive Load\n  Classification in Air Traffic Control"
                },
                "summary": "This paper presents a neuromorphic system for cognitive load classification\nin a real-world setting, an Air Traffic Control (ATC) task, using a hardware\nimplementation of Spiking Neural Networks (SNNs). Electroencephalogram (EEG)\nand eye-tracking features, extracted from an open-source dataset, were used to\ntrain and evaluate both conventional machine learning models and SNNs. Among\nthe SNN architectures explored, a minimalistic, single-layer model trained with\na biologically inspired delta-rule learning algorithm achieved competitive\nperformance (80.6%). To enable deployment on neuromorphic hardware, the model\nwas quantized and implemented on the mixed-signal DYNAP-SE chip. Despite\nhardware constraints and analog variability, the chip-deployed SNN maintained a\nclassification accuracy of up to 73.5% using spike-based input. These results\ndemonstrate the feasibility of event-driven neuromorphic systems for\nultra-low-power, embedded cognitive state monitoring in dynamic real-world\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a neuromorphic system for cognitive load classification\nin a real-world setting, an Air Traffic Control (ATC) task, using a hardware\nimplementation of Spiking Neural Networks (SNNs). Electroencephalogram (EEG)\nand eye-tracking features, extracted from an open-source dataset, were used to\ntrain and evaluate both conventional machine learning models and SNNs. Among\nthe SNN architectures explored, a minimalistic, single-layer model trained with\na biologically inspired delta-rule learning algorithm achieved competitive\nperformance (80.6%). To enable deployment on neuromorphic hardware, the model\nwas quantized and implemented on the mixed-signal DYNAP-SE chip. Despite\nhardware constraints and analog variability, the chip-deployed SNN maintained a\nclassification accuracy of up to 73.5% using spike-based input. These results\ndemonstrate the feasibility of event-driven neuromorphic systems for\nultra-low-power, embedded cognitive state monitoring in dynamic real-world\nscenarios."
                },
                "authors": [
                    {
                        "name": "Jiahui An"
                    },
                    {
                        "name": "Chonghao Cai"
                    },
                    {
                        "name": "Olympia Gallou"
                    },
                    {
                        "name": "Sara Irina Fabrikant"
                    },
                    {
                        "name": "Giacomo Indiveri"
                    },
                    {
                        "name": "Elisa Donati"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Donati"
                },
                "author": "Elisa Donati",
                "arxiv_comment": "Preprint version. Accepted at ACM/IEEE ICONS 2025 (to appear in\n  Proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03050v1",
                "updated": "2025-10-03T14:33:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    33,
                    6,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T14:33:06Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    33,
                    6,
                    4,
                    276,
                    0
                ],
                "title": "Refactoring Towards Microservices: Preparing the Ground for Service\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refactoring Towards Microservices: Preparing the Ground for Service\n  Extraction"
                },
                "summary": "As organizations increasingly transition from monolithic systems to\nmicroservices, they aim to achieve higher availability, automatic scaling,\nsimplified infrastructure management, enhanced collaboration, and streamlined\ndeployments. However, this migration process remains largely manual and\nlabour-intensive. While existing literature offers various strategies for\ndecomposing monoliths, these approaches primarily focus on architecture-level\nguidance, often overlooking the code-level challenges and dependencies that\ndevelopers must address during the migration. This article introduces a\ncatalogue of seven refactorings specifically designed to support the transition\nto a microservices architecture with a focus on handling dependencies. The\ncatalogue provides developers with a systematic guide that consolidates\nrefactorings identified in the literature and addresses the critical gap in\nsystematizing the process at the code level. By offering a structured,\nstep-by-step approach, this work simplifies the migration process and lays the\ngroundwork for its potential automation, empowering developers to implement\nthese changes efficiently and effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As organizations increasingly transition from monolithic systems to\nmicroservices, they aim to achieve higher availability, automatic scaling,\nsimplified infrastructure management, enhanced collaboration, and streamlined\ndeployments. However, this migration process remains largely manual and\nlabour-intensive. While existing literature offers various strategies for\ndecomposing monoliths, these approaches primarily focus on architecture-level\nguidance, often overlooking the code-level challenges and dependencies that\ndevelopers must address during the migration. This article introduces a\ncatalogue of seven refactorings specifically designed to support the transition\nto a microservices architecture with a focus on handling dependencies. The\ncatalogue provides developers with a systematic guide that consolidates\nrefactorings identified in the literature and addresses the critical gap in\nsystematizing the process at the code level. By offering a structured,\nstep-by-step approach, this work simplifies the migration process and lays the\ngroundwork for its potential automation, empowering developers to implement\nthese changes efficiently and effectively."
                },
                "authors": [
                    {
                        "name": "Rita Peixoto"
                    },
                    {
                        "name": "Filipe F. Correia"
                    },
                    {
                        "name": "Thatiane Rosa"
                    },
                    {
                        "name": "Eduardo Guerra"
                    },
                    {
                        "name": "Alfredo Goldman"
                    }
                ],
                "author_detail": {
                    "name": "Alfredo Goldman"
                },
                "author": "Alfredo Goldman",
                "arxiv_comment": "Accepted for publication in the EuroPLoP 2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03038v1",
                "updated": "2025-10-03T14:20:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    20,
                    45,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T14:20:45Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    20,
                    45,
                    4,
                    276,
                    0
                ],
                "title": "CHORD: Customizing Hybrid-precision On-device Model for Sequential\n  Recommendation with Device-cloud Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHORD: Customizing Hybrid-precision On-device Model for Sequential\n  Recommendation with Device-cloud Collaboration"
                },
                "summary": "With the advancement of mobile device capabilities, deploying reranking\nmodels directly on devices has become feasible, enabling real-time contextual\nrecommendations. When migrating models from cloud to devices, resource\nheterogeneity inevitably necessitates model compression. Recent quantization\nmethods show promise for efficient deployment, yet they overlook\ndevice-specific user interests, resulting in compromised recommendation\naccuracy. While on-device finetuning captures personalized user preference, it\nimposes additional computational burden through local retraining. To address\nthese challenges, we propose a framework for \\underline{\\textbf{C}}ustomizing\n\\underline{\\textbf{H}}ybrid-precision \\underline{\\textbf{O}}n-device model for\nsequential \\underline{\\textbf{R}}ecommendation with\n\\underline{\\textbf{D}}evice-cloud collaboration (\\textbf{CHORD}), leveraging\nchannel-wise mixed-precision quantization to simultaneously achieve\npersonalization and resource-adaptive deployment. CHORD distributes randomly\ninitialized models across heterogeneous devices and identifies user-specific\ncritical parameters through auxiliary hypernetwork modules on the cloud. Our\nparameter sensitivity analysis operates across multiple granularities (layer,\nfilter, and element levels), enabling precise mapping from user profiles to\nquantization strategy. Through on-device mixed-precision quantization, CHORD\ndelivers dynamic model adaptation and accelerated inference without\nbackpropagation, eliminating costly retraining cycles. We minimize\ncommunication overhead by encoding quantization strategies using only 2 bits\nper channel instead of 32-bit weights. Experiments on three real-world datasets\nwith two popular backbones (SASRec and Caser) demonstrate the accuracy,\nefficiency, and adaptivity of CHORD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of mobile device capabilities, deploying reranking\nmodels directly on devices has become feasible, enabling real-time contextual\nrecommendations. When migrating models from cloud to devices, resource\nheterogeneity inevitably necessitates model compression. Recent quantization\nmethods show promise for efficient deployment, yet they overlook\ndevice-specific user interests, resulting in compromised recommendation\naccuracy. While on-device finetuning captures personalized user preference, it\nimposes additional computational burden through local retraining. To address\nthese challenges, we propose a framework for \\underline{\\textbf{C}}ustomizing\n\\underline{\\textbf{H}}ybrid-precision \\underline{\\textbf{O}}n-device model for\nsequential \\underline{\\textbf{R}}ecommendation with\n\\underline{\\textbf{D}}evice-cloud collaboration (\\textbf{CHORD}), leveraging\nchannel-wise mixed-precision quantization to simultaneously achieve\npersonalization and resource-adaptive deployment. CHORD distributes randomly\ninitialized models across heterogeneous devices and identifies user-specific\ncritical parameters through auxiliary hypernetwork modules on the cloud. Our\nparameter sensitivity analysis operates across multiple granularities (layer,\nfilter, and element levels), enabling precise mapping from user profiles to\nquantization strategy. Through on-device mixed-precision quantization, CHORD\ndelivers dynamic model adaptation and accelerated inference without\nbackpropagation, eliminating costly retraining cycles. We minimize\ncommunication overhead by encoding quantization strategies using only 2 bits\nper channel instead of 32-bit weights. Experiments on three real-world datasets\nwith two popular backbones (SASRec and Caser) demonstrate the accuracy,\nefficiency, and adaptivity of CHORD."
                },
                "authors": [
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Kairui Fu"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Wenyan Fan"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_doi": "10.1145/3746027.3755632",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755632",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.03038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted by ACM MM'25",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03029v1",
                "updated": "2025-10-03T14:09:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    9,
                    55,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T14:09:55Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    14,
                    9,
                    55,
                    4,
                    276,
                    0
                ],
                "title": "Investigating The Smells of LLM Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating The Smells of LLM Generated Code"
                },
                "summary": "Context: Large Language Models (LLMs) are increasingly being used to generate\nprogram code. Much research has been reported on the functional correctness of\ngenerated code, but there is far less on code quality.\n  Objectives: In this study, we propose a scenario-based method of evaluating\nthe quality of LLM-generated code to identify the weakest scenarios in which\nthe quality of LLM generated code should be improved.\n  Methods: The method measures code smells, an important indicator of code\nquality, and compares them with a baseline formed from reference solutions of\nprofessionally written code. The test dataset is divided into various subsets\naccording to the topics of the code and complexity of the coding tasks to\nrepresent different scenarios of using LLMs for code generation. We will also\npresent an automated test system for this purpose and report experiments with\nthe Java programs generated in response to prompts given to four\nstate-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.\n  Results: We find that LLM-generated code has a higher incidence of code\nsmells compared to reference solutions. Falcon performed the least badly, with\na smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)\nand finally Codex (84.97%). The average smell increase across all LLMs was\n63.34%, comprising 73.35% for implementation smells and 21.42% for design\nsmells. We also found that the increase in code smells is greater for more\ncomplex coding tasks and for more advanced topics, such as those involving\nobject-orientated concepts.\n  Conclusion: In terms of code smells, LLM's performances on various coding\ntask complexities and topics are highly correlated to the quality of human\nwritten code in the corresponding scenarios. However, the quality of LLM\ngenerated code is noticeably poorer than human written code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Large Language Models (LLMs) are increasingly being used to generate\nprogram code. Much research has been reported on the functional correctness of\ngenerated code, but there is far less on code quality.\n  Objectives: In this study, we propose a scenario-based method of evaluating\nthe quality of LLM-generated code to identify the weakest scenarios in which\nthe quality of LLM generated code should be improved.\n  Methods: The method measures code smells, an important indicator of code\nquality, and compares them with a baseline formed from reference solutions of\nprofessionally written code. The test dataset is divided into various subsets\naccording to the topics of the code and complexity of the coding tasks to\nrepresent different scenarios of using LLMs for code generation. We will also\npresent an automated test system for this purpose and report experiments with\nthe Java programs generated in response to prompts given to four\nstate-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.\n  Results: We find that LLM-generated code has a higher incidence of code\nsmells compared to reference solutions. Falcon performed the least badly, with\na smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)\nand finally Codex (84.97%). The average smell increase across all LLMs was\n63.34%, comprising 73.35% for implementation smells and 21.42% for design\nsmells. We also found that the increase in code smells is greater for more\ncomplex coding tasks and for more advanced topics, such as those involving\nobject-orientated concepts.\n  Conclusion: In terms of code smells, LLM's performances on various coding\ntask complexities and topics are highly correlated to the quality of human\nwritten code in the corresponding scenarios. However, the quality of LLM\ngenerated code is noticeably poorer than human written code."
                },
                "authors": [
                    {
                        "name": "Debalina Ghosh Paul"
                    },
                    {
                        "name": "Hong Zhu"
                    },
                    {
                        "name": "Ian Bayley"
                    }
                ],
                "author_detail": {
                    "name": "Ian Bayley"
                },
                "author": "Ian Bayley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03012v1",
                "updated": "2025-10-03T13:56:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    56,
                    18,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T13:56:18Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    56,
                    18,
                    4,
                    276,
                    0
                ],
                "title": "PocketSR: The Super-Resolution Expert in Your Pocket Mobiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PocketSR: The Super-Resolution Expert in Your Pocket Mobiles"
                },
                "summary": "Real-world image super-resolution (RealSR) aims to enhance the visual quality\nof in-the-wild images, such as those captured by mobile phones. While existing\nmethods leveraging large generative models demonstrate impressive results, the\nhigh computational cost and latency make them impractical for edge deployment.\nIn this paper, we introduce PocketSR, an ultra-lightweight, single-step model\nthat brings generative modeling capabilities to RealSR while maintaining high\nfidelity. To achieve this, we design LiteED, a highly efficient alternative to\nthe original computationally intensive VAE in SD, reducing parameters by 97.5%\nwhile preserving high-quality encoding and decoding. Additionally, we propose\nonline annealing pruning for the U-Net, which progressively shifts generative\npriors from heavy modules to lightweight counterparts, ensuring effective\nknowledge transfer and further optimizing efficiency. To mitigate the loss of\nprior knowledge during pruning, we incorporate a multi-layer feature\ndistillation loss. Through an in-depth analysis of each design component, we\nprovide valuable insights for future research. PocketSR, with a model size of\n146M parameters, processes 4K images in just 0.8 seconds, achieving a\nremarkable speedup over previous methods. Notably, it delivers performance on\npar with state-of-the-art single-step and even multi-step RealSR models, making\nit a highly practical solution for edge-device applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (RealSR) aims to enhance the visual quality\nof in-the-wild images, such as those captured by mobile phones. While existing\nmethods leveraging large generative models demonstrate impressive results, the\nhigh computational cost and latency make them impractical for edge deployment.\nIn this paper, we introduce PocketSR, an ultra-lightweight, single-step model\nthat brings generative modeling capabilities to RealSR while maintaining high\nfidelity. To achieve this, we design LiteED, a highly efficient alternative to\nthe original computationally intensive VAE in SD, reducing parameters by 97.5%\nwhile preserving high-quality encoding and decoding. Additionally, we propose\nonline annealing pruning for the U-Net, which progressively shifts generative\npriors from heavy modules to lightweight counterparts, ensuring effective\nknowledge transfer and further optimizing efficiency. To mitigate the loss of\nprior knowledge during pruning, we incorporate a multi-layer feature\ndistillation loss. Through an in-depth analysis of each design component, we\nprovide valuable insights for future research. PocketSR, with a model size of\n146M parameters, processes 4K images in just 0.8 seconds, achieving a\nremarkable speedup over previous methods. Notably, it delivers performance on\npar with state-of-the-art single-step and even multi-step RealSR models, making\nit a highly practical solution for edge-device applications."
                },
                "authors": [
                    {
                        "name": "Haoze Sun"
                    },
                    {
                        "name": "Linfeng Jiang"
                    },
                    {
                        "name": "Fan Li"
                    },
                    {
                        "name": "Renjing Pei"
                    },
                    {
                        "name": "Zhixin Wang"
                    },
                    {
                        "name": "Yong Guo"
                    },
                    {
                        "name": "Jiaqi Xu"
                    },
                    {
                        "name": "Haoyu Chen"
                    },
                    {
                        "name": "Jin Han"
                    },
                    {
                        "name": "Fenglong Song"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Wenbo Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenbo Li"
                },
                "author": "Wenbo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09669v2",
                "updated": "2025-10-03T13:54:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    54,
                    9,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-11T12:39:48Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    39,
                    48,
                    2,
                    162,
                    0
                ],
                "title": "Query-Level Uncertainty in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Level Uncertainty in Large Language Models"
                },
                "summary": "It is important for Large Language Models (LLMs) to be aware of the boundary\nof their knowledge, distinguishing queries they can confidently answer from\nthose that lie beyond their capabilities. Such awareness enables models to\nperform adaptive inference, such as invoking retrieval-augmented generation\n(RAG), engaging in slow and deep thinking, or abstaining from answering when\nappropriate. These mechanisms are key to developing efficient and trustworthy\nAI. In this work, we propose a method to detect knowledge boundaries via\nQuery-Level Uncertainty, which estimates if a model is capable of answering a\ngiven query before generating any tokens, thus avoiding the generation cost. To\nthis end, we propose a novel, training-free method called Internal Confidence,\nwhich leverages self-evaluations across layers and tokens to provide a reliable\nsignal of uncertainty. Empirical studies on both factual question answering and\nmathematical reasoning tasks demonstrate that our Internal Confidence\noutperforms several baselines in quality of confidence while being\ncomputationally cheaper. Furthermore, we demonstrate its benefits in adaptive\ninference settings, showing that for RAG and model cascading it reduces\ninference costs while preserving overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is important for Large Language Models (LLMs) to be aware of the boundary\nof their knowledge, distinguishing queries they can confidently answer from\nthose that lie beyond their capabilities. Such awareness enables models to\nperform adaptive inference, such as invoking retrieval-augmented generation\n(RAG), engaging in slow and deep thinking, or abstaining from answering when\nappropriate. These mechanisms are key to developing efficient and trustworthy\nAI. In this work, we propose a method to detect knowledge boundaries via\nQuery-Level Uncertainty, which estimates if a model is capable of answering a\ngiven query before generating any tokens, thus avoiding the generation cost. To\nthis end, we propose a novel, training-free method called Internal Confidence,\nwhich leverages self-evaluations across layers and tokens to provide a reliable\nsignal of uncertainty. Empirical studies on both factual question answering and\nmathematical reasoning tasks demonstrate that our Internal Confidence\noutperforms several baselines in quality of confidence while being\ncomputationally cheaper. Furthermore, we demonstrate its benefits in adaptive\ninference settings, showing that for RAG and model cascading it reduces\ninference costs while preserving overall performance."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gerard de Melo"
                    },
                    {
                        "name": "Fabian M. Suchanek"
                    },
                    {
                        "name": "Gal Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gal Varoquaux"
                },
                "author": "Gal Varoquaux",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02093v2",
                "updated": "2025-10-03T13:39:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    39,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-02T08:45:29Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    8,
                    45,
                    29,
                    1,
                    245,
                    0
                ],
                "title": "Better by Comparison: Retrieval-Augmented Contrastive Reasoning for\n  Automatic Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better by Comparison: Retrieval-Augmented Contrastive Reasoning for\n  Automatic Prompt Optimization"
                },
                "summary": "Automatic prompt optimization has recently emerged as a strategy for\nimproving the quality of prompts used in Large Language Models (LLMs), with the\ngoal of generating more accurate and useful responses. However, most prior work\nfocuses on direct prompt refinement or model fine-tuning, overlooking the\npotential of leveraging LLMs' inherent reasoning capability to learn from\ncontrasting examples. In this paper, we present Contrastive Reasoning Prompt\nOptimization (CRPO), a novel framework that formulates prompt optimization as a\nretrieval-augmented reasoning process. Our approach retrieves top k reference\nprompt-response pairs from the HelpSteer2 dataset, an open source collection\nwhere each response is annotated for helpfulness, correctness, coherence,\ncomplexity, and verbosity, and constructs two complementary optimization\nparadigms: (1) tiered contrastive reasoning, where the LLM compares high-,\nmedium-, and low-quality exemplars (both prompts and responses) to refine its\nown generation through reflective reasoning, and (2) multi-metric contrastive\nreasoning, where the LLM analyzes the best exemplars along each evaluation\ndimension and integrates their strengths into an optimized prompt. By\nexplicitly contrasting high and low quality exemplars, CRPO enables the model\nto deduce why certain prompts succeed while others fail, thereby achieving more\nrobust and interpretable optimization. Experimental results on the HelpSteer2\nbenchmark demonstrate that CRPO significantly outperforms baselines. Our\nfindings highlight the promise of contrastive, retrieval-augmented reasoning\nfor advancing automatic prompt optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic prompt optimization has recently emerged as a strategy for\nimproving the quality of prompts used in Large Language Models (LLMs), with the\ngoal of generating more accurate and useful responses. However, most prior work\nfocuses on direct prompt refinement or model fine-tuning, overlooking the\npotential of leveraging LLMs' inherent reasoning capability to learn from\ncontrasting examples. In this paper, we present Contrastive Reasoning Prompt\nOptimization (CRPO), a novel framework that formulates prompt optimization as a\nretrieval-augmented reasoning process. Our approach retrieves top k reference\nprompt-response pairs from the HelpSteer2 dataset, an open source collection\nwhere each response is annotated for helpfulness, correctness, coherence,\ncomplexity, and verbosity, and constructs two complementary optimization\nparadigms: (1) tiered contrastive reasoning, where the LLM compares high-,\nmedium-, and low-quality exemplars (both prompts and responses) to refine its\nown generation through reflective reasoning, and (2) multi-metric contrastive\nreasoning, where the LLM analyzes the best exemplars along each evaluation\ndimension and integrates their strengths into an optimized prompt. By\nexplicitly contrasting high and low quality exemplars, CRPO enables the model\nto deduce why certain prompts succeed while others fail, thereby achieving more\nrobust and interpretable optimization. Experimental results on the HelpSteer2\nbenchmark demonstrate that CRPO significantly outperforms baselines. Our\nfindings highlight the promise of contrastive, retrieval-augmented reasoning\nfor advancing automatic prompt optimization."
                },
                "authors": [
                    {
                        "name": "Juhyeon Lee"
                    },
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Hyunjin An"
                    },
                    {
                        "name": "Seunghyun Lee"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02999v1",
                "updated": "2025-10-03T13:38:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    38,
                    56,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T13:38:56Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    38,
                    56,
                    4,
                    276,
                    0
                ],
                "title": "Untargeted Jailbreak Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Untargeted Jailbreak Attack"
                },
                "summary": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs),\nsuch as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize\nadversarial suffixes to align the LLM output with a predefined target response.\nHowever, by restricting the optimization objective as inducing a predefined\ntarget, these methods inherently constrain the adversarial search space, which\nlimit their overall attack efficacy. Furthermore, existing methods typically\nrequire a large number of optimization iterations to fulfill the large gap\nbetween the fixed target and the original model response, resulting in low\nattack efficiency.\n  To overcome the limitations of targeted jailbreak attacks, we propose the\nfirst gradient-based untargeted jailbreak attack (UJA), aiming to elicit an\nunsafe response without enforcing any predefined patterns. Specifically, we\nformulate an untargeted attack objective to maximize the unsafety probability\nof the LLM response, which can be quantified using a judge model. Since the\nobjective is non-differentiable, we further decompose it into two\ndifferentiable sub-objectives for optimizing an optimal harmful response and\nthe corresponding adversarial prompt, with a theoretical analysis to validate\nthe decomposition. In contrast to targeted jailbreak attacks, UJA's\nunrestricted objective significantly expands the search space, enabling a more\nflexible and efficient exploration of LLM vulnerabilities.Extensive evaluations\ndemonstrate that \\textsc{UJA} can achieve over 80\\% attack success rates\nagainst recent safety-aligned LLMs with only 100 optimization iterations,\noutperforming the state-of-the-art gradient-based attacks such as I-GCG and\nCOLD-Attack by over 20\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs),\nsuch as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize\nadversarial suffixes to align the LLM output with a predefined target response.\nHowever, by restricting the optimization objective as inducing a predefined\ntarget, these methods inherently constrain the adversarial search space, which\nlimit their overall attack efficacy. Furthermore, existing methods typically\nrequire a large number of optimization iterations to fulfill the large gap\nbetween the fixed target and the original model response, resulting in low\nattack efficiency.\n  To overcome the limitations of targeted jailbreak attacks, we propose the\nfirst gradient-based untargeted jailbreak attack (UJA), aiming to elicit an\nunsafe response without enforcing any predefined patterns. Specifically, we\nformulate an untargeted attack objective to maximize the unsafety probability\nof the LLM response, which can be quantified using a judge model. Since the\nobjective is non-differentiable, we further decompose it into two\ndifferentiable sub-objectives for optimizing an optimal harmful response and\nthe corresponding adversarial prompt, with a theoretical analysis to validate\nthe decomposition. In contrast to targeted jailbreak attacks, UJA's\nunrestricted objective significantly expands the search space, enabling a more\nflexible and efficient exploration of LLM vulnerabilities.Extensive evaluations\ndemonstrate that \\textsc{UJA} can achieve over 80\\% attack success rates\nagainst recent safety-aligned LLMs with only 100 optimization iterations,\noutperforming the state-of-the-art gradient-based attacks such as I-GCG and\nCOLD-Attack by over 20\\%."
                },
                "authors": [
                    {
                        "name": "Xinzhe Huang"
                    },
                    {
                        "name": "Wenjing Hu"
                    },
                    {
                        "name": "Tianhang Zheng"
                    },
                    {
                        "name": "Kedong Xiu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02995v1",
                "updated": "2025-10-03T13:35:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    35,
                    45,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T13:35:45Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    35,
                    45,
                    4,
                    276,
                    0
                ],
                "title": "AudioToolAgent: An Agentic Framework for Audio-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioToolAgent: An Agentic Framework for Audio-Language Models"
                },
                "summary": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks\nbut lack multi-step reasoning and tool-calling found in recent Large Language\nModels (LLMs). This paper presents AudioToolAgent, a framework that coordinates\naudio-language models as tools via a central LLM agent that accesses tool\nadapters for audio question answering and speech-to-text. The agent selects\ntools, asks follow-up questions, and compares outputs for verification.\nExperiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to\n74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling\nfor shapley values across 374 configurations identifies effective agent-tool\ncombinations. The modular design allows integration of new tools and eliminates\nthe use of data and training costs. Code and reproduction materials are\navailable at: github.com/GLJS/AudioToolAgent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks\nbut lack multi-step reasoning and tool-calling found in recent Large Language\nModels (LLMs). This paper presents AudioToolAgent, a framework that coordinates\naudio-language models as tools via a central LLM agent that accesses tool\nadapters for audio question answering and speech-to-text. The agent selects\ntools, asks follow-up questions, and compares outputs for verification.\nExperiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to\n74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling\nfor shapley values across 374 configurations identifies effective agent-tool\ncombinations. The modular design allows integration of new tools and eliminates\nthe use of data and training costs. Code and reproduction materials are\navailable at: github.com/GLJS/AudioToolAgent"
                },
                "authors": [
                    {
                        "name": "Gijs Wijngaard"
                    },
                    {
                        "name": "Elia Formisano"
                    },
                    {
                        "name": "Michel Dumontier"
                    }
                ],
                "author_detail": {
                    "name": "Michel Dumontier"
                },
                "author": "Michel Dumontier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07044v2",
                "updated": "2025-10-03T13:29:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    29,
                    21,
                    4,
                    276,
                    0
                ],
                "published": "2025-03-10T08:32:33Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    32,
                    33,
                    0,
                    69,
                    0
                ],
                "title": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and\n  Robust Data Science Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Adaptive and\n  Robust Data Science Automation"
                },
                "summary": "Existing large language model (LLM) agents for automating data science show\npromise, but they remain constrained by narrow task scopes, limited\ngeneralization across tasks and models, and over-reliance on state-of-the-art\n(SOTA) LLMs. We introduce DatawiseAgent, a notebook-centric LLM agent framework\nfor adaptive and robust data science automation. Inspired by how human data\nscientists work in computational notebooks, DatawiseAgent introduces a unified\ninteraction representation and a multi-stage architecture based on finite-state\ntransducers (FSTs). This design enables flexible long-horizon planning,\nprogressive solution development, and robust recovery from execution failures.\nExtensive experiments across diverse data science scenarios and models show\nthat DatawiseAgent consistently achieves SOTA performance by surpassing strong\nbaselines such as AutoGen and TaskWeaver, demonstrating superior effectiveness\nand adaptability. Further evaluations reveal graceful performance degradation\nunder weaker or smaller models, underscoring the robustness and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) agents for automating data science show\npromise, but they remain constrained by narrow task scopes, limited\ngeneralization across tasks and models, and over-reliance on state-of-the-art\n(SOTA) LLMs. We introduce DatawiseAgent, a notebook-centric LLM agent framework\nfor adaptive and robust data science automation. Inspired by how human data\nscientists work in computational notebooks, DatawiseAgent introduces a unified\ninteraction representation and a multi-stage architecture based on finite-state\ntransducers (FSTs). This design enables flexible long-horizon planning,\nprogressive solution development, and robust recovery from execution failures.\nExtensive experiments across diverse data science scenarios and models show\nthat DatawiseAgent consistently achieves SOTA performance by surpassing strong\nbaselines such as AutoGen and TaskWeaver, demonstrating superior effectiveness\nand adaptability. Further evaluations reveal graceful performance degradation\nunder weaker or smaller models, underscoring the robustness and scalability."
                },
                "authors": [
                    {
                        "name": "Ziming You"
                    },
                    {
                        "name": "Yumiao Zhang"
                    },
                    {
                        "name": "Dexuan Xu"
                    },
                    {
                        "name": "Yiwei Lou"
                    },
                    {
                        "name": "Yandong Yan"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Huaming Zhang"
                    },
                    {
                        "name": "Yu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Huang"
                },
                "author": "Yu Huang",
                "arxiv_comment": "The camera-ready version for EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02987v1",
                "updated": "2025-10-03T13:25:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    25,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T13:25:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    13,
                    25,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via\n  Text-to-Image-to-Text Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via\n  Text-to-Image-to-Text Consistency"
                },
                "summary": "With the rapid advancement of large multimodal models (LMMs), recent\ntext-to-image (T2I) models can generate high-quality images and demonstrate\ngreat alignment to short prompts. However, they still struggle to effectively\nunderstand and follow long and detailed prompts, displaying inconsistent\ngeneration. To address this challenge, we introduce LPG-Bench, a comprehensive\nbenchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench\nfeatures 200 meticulously crafted prompts with an average length of over 250\nwords, approaching the input capacity of several leading commercial models.\nUsing these prompts, we generate 2,600 images from 13 state-of-the-art models\nand further perform comprehensive human-ranked annotations. Based on LPG-Bench,\nwe observe that state-of-the-art T2I alignment evaluation metrics exhibit poor\nconsistency with human preferences on long-prompt-based image generation. To\naddress the gap, we introduce a novel zero-shot metric based on\ntext-to-image-to-text consistency, termed TIT, for evaluating\nlong-prompt-generated images. The core concept of TIT is to quantify T2I\nalignment by directly comparing the consistency between the raw prompt and the\nLMM-produced description on the generated image, which includes an efficient\nscore-based instantiation TIT-Score and a large-language-model (LLM) based\ninstantiation TIT-Score-LLM. Extensive experiments demonstrate that our\nframework achieves superior alignment with human judgment compared to\nCLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute\nimprovement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT\nmethods together offer a deeper perspective to benchmark and foster the\ndevelopment of T2I models. All resources will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large multimodal models (LMMs), recent\ntext-to-image (T2I) models can generate high-quality images and demonstrate\ngreat alignment to short prompts. However, they still struggle to effectively\nunderstand and follow long and detailed prompts, displaying inconsistent\ngeneration. To address this challenge, we introduce LPG-Bench, a comprehensive\nbenchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench\nfeatures 200 meticulously crafted prompts with an average length of over 250\nwords, approaching the input capacity of several leading commercial models.\nUsing these prompts, we generate 2,600 images from 13 state-of-the-art models\nand further perform comprehensive human-ranked annotations. Based on LPG-Bench,\nwe observe that state-of-the-art T2I alignment evaluation metrics exhibit poor\nconsistency with human preferences on long-prompt-based image generation. To\naddress the gap, we introduce a novel zero-shot metric based on\ntext-to-image-to-text consistency, termed TIT, for evaluating\nlong-prompt-generated images. The core concept of TIT is to quantify T2I\nalignment by directly comparing the consistency between the raw prompt and the\nLMM-produced description on the generated image, which includes an efficient\nscore-based instantiation TIT-Score and a large-language-model (LLM) based\ninstantiation TIT-Score-LLM. Extensive experiments demonstrate that our\nframework achieves superior alignment with human judgment compared to\nCLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute\nimprovement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT\nmethods together offer a deeper perspective to benchmark and foster the\ndevelopment of T2I models. All resources will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Juntong Wang"
                    },
                    {
                        "name": "Huiyu Duan"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Ziheng Jia"
                    },
                    {
                        "name": "Guangtao Zhai"
                    },
                    {
                        "name": "Xiongkuo Min"
                    }
                ],
                "author_detail": {
                    "name": "Xiongkuo Min"
                },
                "author": "Xiongkuo Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02967v1",
                "updated": "2025-10-03T12:57:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    57,
                    13,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:57:13Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    57,
                    13,
                    4,
                    276,
                    0
                ],
                "title": "Grounding Large Language Models in Clinical Evidence: A\n  Retrieval-Augmented Generation System for Querying UK NICE Clinical\n  Guidelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Large Language Models in Clinical Evidence: A\n  Retrieval-Augmented Generation System for Querying UK NICE Clinical\n  Guidelines"
                },
                "summary": "This paper presents the development and evaluation of a Retrieval-Augmented\nGeneration (RAG) system for querying the United Kingdom's National Institute\nfor Health and Care Excellence (NICE) clinical guidelines using Large Language\nModels (LLMs). The extensive length and volume of these guidelines can impede\ntheir utilisation within a time-constrained healthcare system, a challenge this\nproject addresses through the creation of a system capable of providing users\nwith precisely matched information in response to natural language queries. The\nsystem's retrieval architecture, composed of a hybrid embedding mechanism, was\nevaluated against a database of 10,195 text chunks derived from three hundred\nguidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)\nof 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten\nretrieved chunks, when evaluated on 7901 queries.\n  The most significant impact of the RAG system was observed during the\ngeneration phase. When evaluated on a manually curated dataset of seventy\nquestion-answer pairs, RAG-enhanced models showed substantial gains in\nperformance. Faithfulness, the measure of whether an answer is supported by the\nsource text, was increased by 64.7 percentage points to 99.5% for the\nRAG-enhanced O4-Mini model and significantly outperformed the medical-focused\nMeditron3-8B LLM, which scored 43%. This, combined with a perfect Context\nPrecision score of 1 for all RAG-enhanced models, confirms the system's ability\nto prevent information fabrication by grounding its answers in relevant source\nmaterial. This study thus establishes RAG as an effective, reliable, and\nscalable approach for applying generative AI in healthcare, enabling\ncost-effective access to medical guidelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and evaluation of a Retrieval-Augmented\nGeneration (RAG) system for querying the United Kingdom's National Institute\nfor Health and Care Excellence (NICE) clinical guidelines using Large Language\nModels (LLMs). The extensive length and volume of these guidelines can impede\ntheir utilisation within a time-constrained healthcare system, a challenge this\nproject addresses through the creation of a system capable of providing users\nwith precisely matched information in response to natural language queries. The\nsystem's retrieval architecture, composed of a hybrid embedding mechanism, was\nevaluated against a database of 10,195 text chunks derived from three hundred\nguidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)\nof 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten\nretrieved chunks, when evaluated on 7901 queries.\n  The most significant impact of the RAG system was observed during the\ngeneration phase. When evaluated on a manually curated dataset of seventy\nquestion-answer pairs, RAG-enhanced models showed substantial gains in\nperformance. Faithfulness, the measure of whether an answer is supported by the\nsource text, was increased by 64.7 percentage points to 99.5% for the\nRAG-enhanced O4-Mini model and significantly outperformed the medical-focused\nMeditron3-8B LLM, which scored 43%. This, combined with a perfect Context\nPrecision score of 1 for all RAG-enhanced models, confirms the system's ability\nto prevent information fabrication by grounding its answers in relevant source\nmaterial. This study thus establishes RAG as an effective, reliable, and\nscalable approach for applying generative AI in healthcare, enabling\ncost-effective access to medical guidelines."
                },
                "authors": [
                    {
                        "name": "Matthew Lewis"
                    },
                    {
                        "name": "Samuel Thio"
                    },
                    {
                        "name": "Richard JB Dobson"
                    },
                    {
                        "name": "Spiros Denaxas"
                    }
                ],
                "author_detail": {
                    "name": "Spiros Denaxas"
                },
                "author": "Spiros Denaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02964v1",
                "updated": "2025-10-03T12:53:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    53,
                    45,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:53:45Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    53,
                    45,
                    4,
                    276,
                    0
                ],
                "title": "External Data Extraction Attacks against Retrieval-Augmented Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External Data Extraction Attacks against Retrieval-Augmented Large\n  Language Models"
                },
                "summary": "In recent years, RAG has emerged as a key paradigm for enhancing large\nlanguage models (LLMs). By integrating externally retrieved information, RAG\nalleviates issues like outdated knowledge and, crucially, insufficient domain\nexpertise. While effective, RAG introduces new risks of external data\nextraction attacks (EDEAs), where sensitive or copyrighted data in its\nknowledge base may be extracted verbatim. These risks are particularly acute\nwhen RAG is used to customize specialized LLM applications with private\nknowledge bases. Despite initial studies exploring these risks, they often lack\na formalized framework, robust attack performance, and comprehensive\nevaluation, leaving critical questions about real-world EDEA feasibility\nunanswered.\n  In this paper, we present the first comprehensive study to formalize EDEAs\nagainst retrieval-augmented LLMs. We first formally define EDEAs and propose a\nunified framework decomposing their design into three components: extraction\ninstruction, jailbreak operator, and retrieval trigger, under which prior\nattacks can be considered instances within our framework. Guided by this\nframework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction\naTtack. Specifically, SECRET incorporates (1) an adaptive optimization process\nusing LLMs as optimizers to generate specialized jailbreak prompts for EDEAs,\nand (2) cluster-focused triggering, an adaptive strategy that alternates\nbetween global exploration and local exploitation to efficiently generate\neffective retrieval triggers. Extensive evaluations across 4 models reveal that\nSECRET significantly outperforms previous attacks, and is highly effective\nagainst all 16 tested RAG instances. Notably, SECRET successfully extracts 35%\nof the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas\nother attacks yield 0% extraction. Our findings call for attention to this\nemerging threat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, RAG has emerged as a key paradigm for enhancing large\nlanguage models (LLMs). By integrating externally retrieved information, RAG\nalleviates issues like outdated knowledge and, crucially, insufficient domain\nexpertise. While effective, RAG introduces new risks of external data\nextraction attacks (EDEAs), where sensitive or copyrighted data in its\nknowledge base may be extracted verbatim. These risks are particularly acute\nwhen RAG is used to customize specialized LLM applications with private\nknowledge bases. Despite initial studies exploring these risks, they often lack\na formalized framework, robust attack performance, and comprehensive\nevaluation, leaving critical questions about real-world EDEA feasibility\nunanswered.\n  In this paper, we present the first comprehensive study to formalize EDEAs\nagainst retrieval-augmented LLMs. We first formally define EDEAs and propose a\nunified framework decomposing their design into three components: extraction\ninstruction, jailbreak operator, and retrieval trigger, under which prior\nattacks can be considered instances within our framework. Guided by this\nframework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction\naTtack. Specifically, SECRET incorporates (1) an adaptive optimization process\nusing LLMs as optimizers to generate specialized jailbreak prompts for EDEAs,\nand (2) cluster-focused triggering, an adaptive strategy that alternates\nbetween global exploration and local exploitation to efficiently generate\neffective retrieval triggers. Extensive evaluations across 4 models reveal that\nSECRET significantly outperforms previous attacks, and is highly effective\nagainst all 16 tested RAG instances. Notably, SECRET successfully extracts 35%\nof the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas\nother attacks yield 0% extraction. Our findings call for attention to this\nemerging threat."
                },
                "authors": [
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Yifei Chen"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Leyi Qi"
                    },
                    {
                        "name": "Boheng Li"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02962v1",
                "updated": "2025-10-03T12:53:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    53,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:53:02Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    53,
                    2,
                    4,
                    276,
                    0
                ],
                "title": "Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in\n  Large Language Models via Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in\n  Large Language Models via Watermarking"
                },
                "summary": "Large Language Models (LLMs) are increasingly fine-tuned on smaller,\ndomain-specific datasets to improve downstream performance. These datasets\noften contain proprietary or copyrighted material, raising the need for\nreliable safeguards against unauthorized use. Existing membership inference\nattacks (MIAs) and dataset-inference methods typically require access to\ninternal signals such as logits, while current black-box approaches often rely\non handcrafted prompts or a clean reference dataset for calibration, both of\nwhich limit practical applicability. Watermarking is a promising alternative,\nbut prior techniques can degrade text quality or reduce task performance. We\npropose TRACE, a practical framework for fully black-box detection of\ncopyrighted dataset usage in LLM fine-tuning. \\texttt{TRACE} rewrites datasets\nwith distortion-free watermarks guided by a private key, ensuring both text\nquality and downstream utility. At detection time, we exploit the radioactivity\neffect of fine-tuning on watermarked data and introduce an entropy-gated\nprocedure that selectively scores high-uncertainty tokens, substantially\namplifying detection power. Across diverse datasets and model families, TRACE\nconsistently achieves significant detections (p<0.05), often with extremely\nstrong statistical evidence. Furthermore, it supports multi-dataset attribution\nand remains robust even after continued pretraining on large non-watermarked\ncorpora. These results establish TRACE as a practical route to reliable\nblack-box verification of copyrighted dataset usage. We will make our code\navailable at: https://github.com/NusIoraPrivacy/TRACE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly fine-tuned on smaller,\ndomain-specific datasets to improve downstream performance. These datasets\noften contain proprietary or copyrighted material, raising the need for\nreliable safeguards against unauthorized use. Existing membership inference\nattacks (MIAs) and dataset-inference methods typically require access to\ninternal signals such as logits, while current black-box approaches often rely\non handcrafted prompts or a clean reference dataset for calibration, both of\nwhich limit practical applicability. Watermarking is a promising alternative,\nbut prior techniques can degrade text quality or reduce task performance. We\npropose TRACE, a practical framework for fully black-box detection of\ncopyrighted dataset usage in LLM fine-tuning. \\texttt{TRACE} rewrites datasets\nwith distortion-free watermarks guided by a private key, ensuring both text\nquality and downstream utility. At detection time, we exploit the radioactivity\neffect of fine-tuning on watermarked data and introduce an entropy-gated\nprocedure that selectively scores high-uncertainty tokens, substantially\namplifying detection power. Across diverse datasets and model families, TRACE\nconsistently achieves significant detections (p<0.05), often with extremely\nstrong statistical evidence. Furthermore, it supports multi-dataset attribution\nand remains robust even after continued pretraining on large non-watermarked\ncorpora. These results establish TRACE as a practical route to reliable\nblack-box verification of copyrighted dataset usage. We will make our code\navailable at: https://github.com/NusIoraPrivacy/TRACE."
                },
                "authors": [
                    {
                        "name": "Jingqi Zhang"
                    },
                    {
                        "name": "Ruibo Chen"
                    },
                    {
                        "name": "Yingqing Yang"
                    },
                    {
                        "name": "Peihua Mai"
                    },
                    {
                        "name": "Heng Huang"
                    },
                    {
                        "name": "Yan Pang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Pang"
                },
                "author": "Yan Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24183v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24183v3",
                "updated": "2025-10-03T12:52:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    52,
                    4,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-30T03:51:06Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    3,
                    51,
                    6,
                    4,
                    150,
                    0
                ],
                "title": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation"
                },
                "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while even exceeding the\nperformance of 671B DeepSeek-R1 on RTLLM. We have released our model, training\ncode, and dataset to facilitate research in EDA and LLM communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while even exceeding the\nperformance of 671B DeepSeek-R1 on RTLLM. We have released our model, training\ncode, and dataset to facilitate research in EDA and LLM communities."
                },
                "authors": [
                    {
                        "name": "Yaoyu Zhu"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Hanqi Lyu"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Chongxiao Li"
                    },
                    {
                        "name": "Wenxuan Shi"
                    },
                    {
                        "name": "Yutong Wu"
                    },
                    {
                        "name": "Jianan Mu"
                    },
                    {
                        "name": "Jinghua Wang"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yunji Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Chen"
                },
                "author": "Yunji Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24183v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24183v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02958v1",
                "updated": "2025-10-03T12:50:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    50,
                    3,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:50:03Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    50,
                    3,
                    4,
                    276,
                    0
                ],
                "title": "Sequence-Based Deep Learning for Handover Optimization in Dense Urban\n  Cellular Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-Based Deep Learning for Handover Optimization in Dense Urban\n  Cellular Network"
                },
                "summary": "Efficient handover management remains a critical challenge in dense urban\ncellular networks, where high cell density, user mobility, and diverse service\ndemands increase the likelihood of unnecessary handovers and ping-pong effects.\nThis paper leverages a real-world, multi-operator drive-test dataset of 30,925\nlabelled records collected within a 2 km area around Sunway City to investigate\nsequence-based deep learning approaches for handover detection and avoidance.\nWe formulate handover prediction as a sequence problem and evaluate Gated\nRecurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer\narchitectures under Reference Signal Received Power (RSRP)-only and all-feature\nsettings. The integration of multi-dimensional features significantly enhanced\nhandover performance in dense urban cellular networks. The proposed GRU-based\nmodel achieved a remarkable 98% reduction in ping-pong handovers, alongside a\n46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only\napproach which yielded a 22.19% reduction. Furthermore, the model demonstrated\na 46% improvement in Time of Stay (ToS), indicating more stable user\nconnections. With an inference time of just 0.91 seconds, the solution proves\nhighly efficient and well-suited for real-time edge deployment scenarios.\nCompared to the conventional 3GPP A3 algorithm, these improvements demonstrate\nsignificant gains in mobility robustness and user Quality of Experience (QoE)\nimprovement. The dataset is released to foster reproducibility and further\nresearch in intelligent mobility management for 5G and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient handover management remains a critical challenge in dense urban\ncellular networks, where high cell density, user mobility, and diverse service\ndemands increase the likelihood of unnecessary handovers and ping-pong effects.\nThis paper leverages a real-world, multi-operator drive-test dataset of 30,925\nlabelled records collected within a 2 km area around Sunway City to investigate\nsequence-based deep learning approaches for handover detection and avoidance.\nWe formulate handover prediction as a sequence problem and evaluate Gated\nRecurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer\narchitectures under Reference Signal Received Power (RSRP)-only and all-feature\nsettings. The integration of multi-dimensional features significantly enhanced\nhandover performance in dense urban cellular networks. The proposed GRU-based\nmodel achieved a remarkable 98% reduction in ping-pong handovers, alongside a\n46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only\napproach which yielded a 22.19% reduction. Furthermore, the model demonstrated\na 46% improvement in Time of Stay (ToS), indicating more stable user\nconnections. With an inference time of just 0.91 seconds, the solution proves\nhighly efficient and well-suited for real-time edge deployment scenarios.\nCompared to the conventional 3GPP A3 algorithm, these improvements demonstrate\nsignificant gains in mobility robustness and user Quality of Experience (QoE)\nimprovement. The dataset is released to foster reproducibility and further\nresearch in intelligent mobility management for 5G and beyond."
                },
                "authors": [
                    {
                        "name": "Muhammad Kabeer"
                    },
                    {
                        "name": "Rosdiadee Nordin"
                    },
                    {
                        "name": "Mehran Behjati"
                    },
                    {
                        "name": "Lau Sian Lun"
                    }
                ],
                "author_detail": {
                    "name": "Lau Sian Lun"
                },
                "author": "Lau Sian Lun",
                "arxiv_comment": "6 pages, 6 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13761v2",
                "updated": "2025-10-03T12:48:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    48,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-17T07:16:12Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    16,
                    12,
                    2,
                    260,
                    0
                ],
                "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both episode-level problem solving and step-level code generation. This is\nmotivated by our key insight that the success of an intermediate tool call is a\nstrong predictor of the final answer's correctness. Finally, THOR incorporates\na self-correction mechanism that leverages immediate tool feedback to\ndynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both episode-level problem solving and step-level code generation. This is\nmotivated by our key insight that the success of an intermediate tool call is a\nstrong predictor of the final answer's correctness. Finally, THOR incorporates\na self-correction mechanism that leverages immediate tool feedback to\ndynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR."
                },
                "authors": [
                    {
                        "name": "Qikai Chang"
                    },
                    {
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Jiefeng Ma"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Quan Liu"
                    },
                    {
                        "name": "Jianqing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianqing Gao"
                },
                "author": "Jianqing Gao",
                "arxiv_comment": "22 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02956v1",
                "updated": "2025-10-03T12:48:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    48,
                    11,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:48:11Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    48,
                    11,
                    4,
                    276,
                    0
                ],
                "title": "Confidence and Dispersity as Signals: Unsupervised Model Evaluation and\n  Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence and Dispersity as Signals: Unsupervised Model Evaluation and\n  Ranking"
                },
                "summary": "Assessing model generalization under distribution shift is essential for\nreal-world deployment, particularly when labeled test data is unavailable. This\npaper presents a unified and practical framework for unsupervised model\nevaluation and ranking in two common deployment settings: (1) estimating the\naccuracy of a fixed model on multiple unlabeled test sets (dataset-centric\nevaluation), and (2) ranking a set of candidate models on a single unlabeled\ntest set (model-centric evaluation). We demonstrate that two intrinsic\nproperties of model predictions, namely confidence (which reflects prediction\ncertainty) and dispersity (which captures the diversity of predicted classes),\ntogether provide strong and complementary signals for generalization. We\nsystematically benchmark a set of confidence-based, dispersity-based, and\nhybrid metrics across a wide range of model architectures, datasets, and\ndistribution shift types. Our results show that hybrid metrics consistently\noutperform single-aspect metrics on both dataset-centric and model-centric\nevaluation settings. In particular, the nuclear norm of the prediction matrix\nprovides robust and accurate performance across tasks, including real-world\ndatasets, and maintains reliability under moderate class imbalance. These\nfindings offer a practical and generalizable basis for unsupervised model\nassessment in deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing model generalization under distribution shift is essential for\nreal-world deployment, particularly when labeled test data is unavailable. This\npaper presents a unified and practical framework for unsupervised model\nevaluation and ranking in two common deployment settings: (1) estimating the\naccuracy of a fixed model on multiple unlabeled test sets (dataset-centric\nevaluation), and (2) ranking a set of candidate models on a single unlabeled\ntest set (model-centric evaluation). We demonstrate that two intrinsic\nproperties of model predictions, namely confidence (which reflects prediction\ncertainty) and dispersity (which captures the diversity of predicted classes),\ntogether provide strong and complementary signals for generalization. We\nsystematically benchmark a set of confidence-based, dispersity-based, and\nhybrid metrics across a wide range of model architectures, datasets, and\ndistribution shift types. Our results show that hybrid metrics consistently\noutperform single-aspect metrics on both dataset-centric and model-centric\nevaluation settings. In particular, the nuclear norm of the prediction matrix\nprovides robust and accurate performance across tasks, including real-world\ndatasets, and maintains reliability under moderate class imbalance. These\nfindings offer a practical and generalizable basis for unsupervised model\nassessment in deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Weijian Deng"
                    },
                    {
                        "name": "Weijie Tu"
                    },
                    {
                        "name": "Ibrahim Radwan"
                    },
                    {
                        "name": "Mohammad Abu Alsheikh"
                    },
                    {
                        "name": "Stephen Gould"
                    },
                    {
                        "name": "Liang Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zheng"
                },
                "author": "Liang Zheng",
                "arxiv_comment": "15 pages, 11 figures, extension of ICML'23 work: Confidence and\n  Dispersity Speak: Characterizing Prediction Matrix for Unsupervised Accuracy\n  Estimation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26495v2",
                "updated": "2025-10-03T12:46:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    46,
                    22,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-30T16:39:17Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    39,
                    17,
                    1,
                    273,
                    0
                ],
                "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!"
                },
                "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models - Qwen-3 (235B) with 77.77% and Mistral (24B) with 79.96% -\nfall far short of reliable operational safety, while GPT models plateau in the\n62-73% range, Phi achieves only mid-level scores (48-70%), and Gemma and\nLlama-3 collapse to 39.53% and 23.84%, respectively. While operational safety\nis a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41% and Qwen-3 (30B) by 27%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models - Qwen-3 (235B) with 77.77% and Mistral (24B) with 79.96% -\nfall far short of reliable operational safety, while GPT models plateau in the\n62-73% range, Phi achieves only mid-level scores (48-70%), and Gemma and\nLlama-3 collapse to 39.53% and 23.84%, respectively. While operational safety\nis a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41% and Qwen-3 (30B) by 27%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents."
                },
                "authors": [
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Varun Gumma"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Seok Min Lim"
                    },
                    {
                        "name": "Chuan Li"
                    },
                    {
                        "name": "Amir Zadeh"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03923v2",
                "updated": "2025-10-03T12:29:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    29,
                    9,
                    4,
                    276,
                    0
                ],
                "published": "2024-03-06T18:33:51Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    18,
                    33,
                    51,
                    2,
                    66,
                    0
                ],
                "title": "Did Translation Models Get More Robust Without Anyone Even Noticing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Did Translation Models Get More Robust Without Anyone Even Noticing?"
                },
                "summary": "Neural machine translation (MT) models achieve strong results across a\nvariety of settings, but it is widely believed that they are highly sensitive\nto \"noisy\" inputs, such as spelling errors, abbreviations, and other formatting\nissues. In this paper, we revisit this insight in light of recent multilingual\nMT models and large language models (LLMs) applied to machine translation.\nSomewhat surprisingly, we show through controlled experiments that these models\nare far more robust to many kinds of noise than previous models, even when they\nperform similarly on clean data. This is notable because, even though LLMs have\nmore parameters and more complex training processes than past models, none of\nthe open ones we consider use any techniques specifically designed to encourage\nrobustness. Next, we show that similar trends hold for social media translation\nexperiments -- LLMs are more robust to social media text. We include an\nanalysis of the circumstances in which source correction techniques can be used\nto mitigate the effects of noise. Altogether, we show that robustness to many\ntypes of noise has increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural machine translation (MT) models achieve strong results across a\nvariety of settings, but it is widely believed that they are highly sensitive\nto \"noisy\" inputs, such as spelling errors, abbreviations, and other formatting\nissues. In this paper, we revisit this insight in light of recent multilingual\nMT models and large language models (LLMs) applied to machine translation.\nSomewhat surprisingly, we show through controlled experiments that these models\nare far more robust to many kinds of noise than previous models, even when they\nperform similarly on clean data. This is notable because, even though LLMs have\nmore parameters and more complex training processes than past models, none of\nthe open ones we consider use any techniques specifically designed to encourage\nrobustness. Next, we show that similar trends hold for social media translation\nexperiments -- LLMs are more robust to social media text. We include an\nanalysis of the circumstances in which source correction techniques can be used\nto mitigate the effects of noise. Altogether, we show that robustness to many\ntypes of noise has increased."
                },
                "authors": [
                    {
                        "name": "Ben Peters"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "Andr F. T. Martins"
                },
                "author": "Andr F. T. Martins",
                "arxiv_comment": "ACL 2025 (Main) camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02934v1",
                "updated": "2025-10-03T12:25:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    25,
                    28,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T12:25:28Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    25,
                    28,
                    4,
                    276,
                    0
                ],
                "title": "Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic\n  Internal Representation Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic\n  Internal Representation Selection"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation and are increasingly integrated into the software development\nprocess. However, ensuring the correctness of LLM-generated code remains a\ncritical concern. Prior work has shown that the internal representations of\nLLMs encode meaningful signals for assessing code correctness. Nevertheless,\nthe existing methods rely on representations from pre-selected/fixed layers and\ntoken positions, which could limit its generalizability across diverse model\narchitectures and tasks. In this work, we introduce AUTOPROBE, a novel\nmodel-agnostic approach that dynamically selects the most informative internal\nrepresentations for code correctness assessment. AUTOPROBE employs an\nattention-based mechanism to learn importance scores for hidden states,\nenabling it to focus on the most relevant features. These weighted\nrepresentations are then aggregated and passed to a probing classifier to\npredict code correctness across multiple dimensions, including compilability,\nfunctionality, and security. To evaluate the performance of AUTOPROBE, we\nconduct extensive experiments across multiple benchmarks and code LLMs. Our\nexperimental results show that AUTOPROBE consistently outperforms the\nbaselines. For security assessment, AUTOPROBE surpasses the state-of-the-art\nwhite-box approach by 18%. For compilability and functionality assessment,\nAUTOPROBE demonstrates its highest robustness to code complexity, with the\nperformance higher than the other approaches by up to 19% and 111%,\nrespectively. These findings highlight that dynamically selecting important\ninternal signals enables AUTOPROBE to serve as a robust and generalizable\nsolution for assessing the correctness of code generated by various LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation and are increasingly integrated into the software development\nprocess. However, ensuring the correctness of LLM-generated code remains a\ncritical concern. Prior work has shown that the internal representations of\nLLMs encode meaningful signals for assessing code correctness. Nevertheless,\nthe existing methods rely on representations from pre-selected/fixed layers and\ntoken positions, which could limit its generalizability across diverse model\narchitectures and tasks. In this work, we introduce AUTOPROBE, a novel\nmodel-agnostic approach that dynamically selects the most informative internal\nrepresentations for code correctness assessment. AUTOPROBE employs an\nattention-based mechanism to learn importance scores for hidden states,\nenabling it to focus on the most relevant features. These weighted\nrepresentations are then aggregated and passed to a probing classifier to\npredict code correctness across multiple dimensions, including compilability,\nfunctionality, and security. To evaluate the performance of AUTOPROBE, we\nconduct extensive experiments across multiple benchmarks and code LLMs. Our\nexperimental results show that AUTOPROBE consistently outperforms the\nbaselines. For security assessment, AUTOPROBE surpasses the state-of-the-art\nwhite-box approach by 18%. For compilability and functionality assessment,\nAUTOPROBE demonstrates its highest robustness to code complexity, with the\nperformance higher than the other approaches by up to 19% and 111%,\nrespectively. These findings highlight that dynamically selecting important\ninternal signals enables AUTOPROBE to serve as a robust and generalizable\nsolution for assessing the correctness of code generated by various LLMs."
                },
                "authors": [
                    {
                        "name": "Thanh Trong Vu"
                    },
                    {
                        "name": "Tuan-Dung Bui"
                    },
                    {
                        "name": "Thu-Trang Nguyen"
                    },
                    {
                        "name": "Son Nguyen"
                    },
                    {
                        "name": "Hieu Dinh Vo"
                    }
                ],
                "author_detail": {
                    "name": "Hieu Dinh Vo"
                },
                "author": "Hieu Dinh Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01252v2",
                "updated": "2025-10-03T12:24:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    24,
                    5,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-24T11:10:16Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    11,
                    10,
                    16,
                    2,
                    267,
                    0
                ],
                "title": "GPT and Prejudice: A Sparse Approach to Understanding Learned\n  Representations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT and Prejudice: A Sparse Approach to Understanding Learned\n  Representations in Large Language Models"
                },
                "summary": "As large language models (LLMs) are increasingly trained on massive,\nuncurated corpora, understanding both model representations and the data they\ninternalize has become a major challenge. In this work, we show that pairing\nLLMs with sparse autoencoders (SAEs) enables interpretation not only of model\nbehavior but also of the deeper structures, themes, and biases embedded in the\ntraining data. We train a GPT-style transformer model exclusively on the novels\nof Jane Austen, a corpus rich in social constructs and narrative patterns. We\nthen apply SAEs to hidden states across multiple layers, uncovering sparse,\ninterpretable features that reflect the key narratives and concepts present in\nthe corpus, including gender, class, and societal duty. Our findings\ndemonstrate that LLMs combined with SAEs can act as scalable probes into\ncomplex datasets, offering a new path for corpus exploration, bias discovery,\nand model interpretability at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly trained on massive,\nuncurated corpora, understanding both model representations and the data they\ninternalize has become a major challenge. In this work, we show that pairing\nLLMs with sparse autoencoders (SAEs) enables interpretation not only of model\nbehavior but also of the deeper structures, themes, and biases embedded in the\ntraining data. We train a GPT-style transformer model exclusively on the novels\nof Jane Austen, a corpus rich in social constructs and narrative patterns. We\nthen apply SAEs to hidden states across multiple layers, uncovering sparse,\ninterpretable features that reflect the key narratives and concepts present in\nthe corpus, including gender, class, and societal duty. Our findings\ndemonstrate that LLMs combined with SAEs can act as scalable probes into\ncomplex datasets, offering a new path for corpus exploration, bias discovery,\nand model interpretability at scale."
                },
                "authors": [
                    {
                        "name": "Mariam Mahran"
                    },
                    {
                        "name": "Katharina Simbeck"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Simbeck"
                },
                "author": "Katharina Simbeck",
                "arxiv_comment": "Preprint. Draft version, subject to revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00410v2",
                "updated": "2025-10-03T12:15:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    15,
                    38,
                    4,
                    276,
                    0
                ],
                "published": "2025-08-01T08:09:14Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    9,
                    14,
                    4,
                    213,
                    0
                ],
                "title": "Co-rewarding: Stable Self-supervised RL for Eliciting Reasoning in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-rewarding: Stable Self-supervised RL for Eliciting Reasoning in Large\n  Language Models"
                },
                "summary": "While reinforcement learning with verifiable rewards (RLVR) is effective to\nimprove the reasoning ability of large language models (LLMs), its reliance on\nhuman-annotated labels leads to the scaling up dilemma, especially for complex\ntasks. Recent self-rewarding methods investigate a label-free alternative to\nunlock the reasoning capabilities of LLMs, yet they frequently encounter the\nnon-negligible training collapse issue, as the single-view supervision signal\neasily forms the self-consistent illusion, yielding the reward hacking.\nInspired by the success of self-supervised learning, we propose\n\\textit{Co-rewarding}, a novel self-supervised RL framework that improves\ntraining stability by seeking complementary supervision from another views.\nSpecifically, we instantiate Co-rewarding in two ways: (1)\n\\textit{Co-rewarding-I} is a data-side instantiation that derives reward\nsignals from contrastive agreement across semantically analogous questions; and\n(2) \\textit{Co-rewarding-II} is a model-side instantiation that maintains a\nslowly-updated reference teacher with pseudo labels to realize\nself-distillation. Intuitively, such instantiations introduce different levels\nof discrepancy to increase the difficulty of training collapse on trivial\nreasoning solutions. Empirically, Co-rewarding exhibits stable training across\nvarious setups, and outperforms other self-rewarding baselines by $+3.31\\%$\nimprovements on average on multiple mathematical reasoning benchmarks,\nespecially by $+7.49\\%$ on Llama-3.2-3B-Instruct. Notably, Co-rewarding reaches\nor even surpasses RLVR with ground-truth (GT) label in several cases, such as a\nPass@$1$ of $94.01\\%$ on GSM8K with Qwen3-8B-Base remarkably higher than GT.\nOur code is publicly available at https://github.com/tmlr-group/Co-rewarding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning with verifiable rewards (RLVR) is effective to\nimprove the reasoning ability of large language models (LLMs), its reliance on\nhuman-annotated labels leads to the scaling up dilemma, especially for complex\ntasks. Recent self-rewarding methods investigate a label-free alternative to\nunlock the reasoning capabilities of LLMs, yet they frequently encounter the\nnon-negligible training collapse issue, as the single-view supervision signal\neasily forms the self-consistent illusion, yielding the reward hacking.\nInspired by the success of self-supervised learning, we propose\n\\textit{Co-rewarding}, a novel self-supervised RL framework that improves\ntraining stability by seeking complementary supervision from another views.\nSpecifically, we instantiate Co-rewarding in two ways: (1)\n\\textit{Co-rewarding-I} is a data-side instantiation that derives reward\nsignals from contrastive agreement across semantically analogous questions; and\n(2) \\textit{Co-rewarding-II} is a model-side instantiation that maintains a\nslowly-updated reference teacher with pseudo labels to realize\nself-distillation. Intuitively, such instantiations introduce different levels\nof discrepancy to increase the difficulty of training collapse on trivial\nreasoning solutions. Empirically, Co-rewarding exhibits stable training across\nvarious setups, and outperforms other self-rewarding baselines by $+3.31\\%$\nimprovements on average on multiple mathematical reasoning benchmarks,\nespecially by $+7.49\\%$ on Llama-3.2-3B-Instruct. Notably, Co-rewarding reaches\nor even surpasses RLVR with ground-truth (GT) label in several cases, such as a\nPass@$1$ of $94.01\\%$ on GSM8K with Qwen3-8B-Base remarkably higher than GT.\nOur code is publicly available at https://github.com/tmlr-group/Co-rewarding."
                },
                "authors": [
                    {
                        "name": "Zizhuo Zhang"
                    },
                    {
                        "name": "Jianing Zhu"
                    },
                    {
                        "name": "Xinmu Ge"
                    },
                    {
                        "name": "Zihua Zhao"
                    },
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Jiangchao Yao"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14031v2",
                "updated": "2025-10-03T12:08:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    12,
                    8,
                    27,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-20T07:29:41Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    29,
                    41,
                    1,
                    140,
                    0
                ],
                "title": "Reading.help: Supporting EFL Readers with Proactive and On-Demand\n  Explanation of English Grammar and Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading.help: Supporting EFL Readers with Proactive and On-Demand\n  Explanation of English Grammar and Semantics"
                },
                "summary": "A large portion of texts is written in English, but readers who see English\nas a Foreign Language (EFL) often struggle to read texts accurately and\nswiftly. EFL readers seek help from professional teachers and mentors, which is\nlimited and costly. In this paper, we explore how an intelligent reading tool\ncan assist EFL readers. We conducted a case study with EFL readers in South\nKorea. We at first developed an LLM-based reading tool based on prior\nliterature. We then revised the tool based on the feedback from a study with 15\nSouth Korean EFL readers. The final tool, named Reading.help, helps EFL readers\ncomprehend complex sentences and paragraphs with on-demand and proactive\nexplanations. We finally evaluated the tool with 5 EFL readers and 2 EFL\neducation professionals. Our findings suggest Reading.help could potentially\nhelp EFL readers self-learn English when they do not have access to external\nsupport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large portion of texts is written in English, but readers who see English\nas a Foreign Language (EFL) often struggle to read texts accurately and\nswiftly. EFL readers seek help from professional teachers and mentors, which is\nlimited and costly. In this paper, we explore how an intelligent reading tool\ncan assist EFL readers. We conducted a case study with EFL readers in South\nKorea. We at first developed an LLM-based reading tool based on prior\nliterature. We then revised the tool based on the feedback from a study with 15\nSouth Korean EFL readers. The final tool, named Reading.help, helps EFL readers\ncomprehend complex sentences and paragraphs with on-demand and proactive\nexplanations. We finally evaluated the tool with 5 EFL readers and 2 EFL\neducation professionals. Our findings suggest Reading.help could potentially\nhelp EFL readers self-learn English when they do not have access to external\nsupport."
                },
                "authors": [
                    {
                        "name": "Sunghyo Chung"
                    },
                    {
                        "name": "Hyeon Jeon"
                    },
                    {
                        "name": "Sungbok Shin"
                    },
                    {
                        "name": "Md Naimul Hoque"
                    }
                ],
                "author_detail": {
                    "name": "Md Naimul Hoque"
                },
                "author": "Md Naimul Hoque",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02926v1",
                "updated": "2025-10-03T11:54:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    54,
                    41,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T11:54:41Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    54,
                    41,
                    4,
                    276,
                    0
                ],
                "title": "Scalable Quantum Optimisation using HADOF: Hamiltonian\n  Auto-Decomposition Optimisation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Quantum Optimisation using HADOF: Hamiltonian\n  Auto-Decomposition Optimisation Framework"
                },
                "summary": "Quantum Annealing (QA) and QAOA are promising quantum optimisation algorithms\nused for finding approximate solutions to combinatorial problems on near-term\nNISQ systems. Many NP-hard problems can be reformulated as Quadratic\nUnconstrained Binary Optimisation (QUBO), which maps naturally onto quantum\nHamiltonians. However, the limited qubit counts of current NISQ devices\nrestrict practical deployment of such algorithms. In this study, we present the\nHamiltonian Auto-Decomposition Optimisation Framework (HADOF), which leverages\nan iterative strategy to automatically divide the Quadratic Unconstrained\nBinary Optimisation (QUBO) Hamiltonian into sub-Hamiltonians which can be\noptimised separately using Hamiltonian based optimisers such as QAOA, QA or\nSimulated Annealing (SA) and aggregated into a global solution. We compare\nHADOF with Simulated Annealing (SA) and the CPLEX exact solver, showing\nscalability to problem sizes far exceeding available qubits while maintaining\ncompetitive accuracy and runtime. Furthermore, we realise HADOF for a toy\nproblem on an IBM quantum computer, showing promise for practical applications\nof quantum optimisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Annealing (QA) and QAOA are promising quantum optimisation algorithms\nused for finding approximate solutions to combinatorial problems on near-term\nNISQ systems. Many NP-hard problems can be reformulated as Quadratic\nUnconstrained Binary Optimisation (QUBO), which maps naturally onto quantum\nHamiltonians. However, the limited qubit counts of current NISQ devices\nrestrict practical deployment of such algorithms. In this study, we present the\nHamiltonian Auto-Decomposition Optimisation Framework (HADOF), which leverages\nan iterative strategy to automatically divide the Quadratic Unconstrained\nBinary Optimisation (QUBO) Hamiltonian into sub-Hamiltonians which can be\noptimised separately using Hamiltonian based optimisers such as QAOA, QA or\nSimulated Annealing (SA) and aggregated into a global solution. We compare\nHADOF with Simulated Annealing (SA) and the CPLEX exact solver, showing\nscalability to problem sizes far exceeding available qubits while maintaining\ncompetitive accuracy and runtime. Furthermore, we realise HADOF for a toy\nproblem on an IBM quantum computer, showing promise for practical applications\nof quantum optimisation."
                },
                "authors": [
                    {
                        "name": "Namasi G Sankar"
                    },
                    {
                        "name": "Georgios Miliotis"
                    },
                    {
                        "name": "Simon Caton"
                    }
                ],
                "author_detail": {
                    "name": "Simon Caton"
                },
                "author": "Simon Caton",
                "arxiv_comment": "Sankar, N., Miliotis, G. and Caton, S. Scalable Quantum Optimisation\n  using HADOF: Hamiltonian Auto-Decomposition Optimisation Framework. In 3rd\n  International Workshop on AI for Quantum and Quantum for AI (AIQxQIA 2025),\n  at the 28th European Conference on Artificial Intelligence (ECAI), October\n  25-30, 2025, Bologna, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02919v1",
                "updated": "2025-10-03T11:46:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    46,
                    4,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T11:46:04Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    46,
                    4,
                    4,
                    276,
                    0
                ],
                "title": "Self-Reflective Generation at Test Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Reflective Generation at Test Time"
                },
                "summary": "Large language models (LLMs) increasingly solve complex reasoning tasks via\nlong chain-of-thought, but their forward-only autoregressive generation process\nis fragile; early token errors can cascade, which creates a clear need for\nself-reflection mechanisms. However, existing self-reflection either performs\nrevisions over full drafts or learns self-correction via expensive training,\nboth fundamentally reactive and inefficient. To address this, we propose\nSelf-Reflective Generation at Test Time (SRGen), a lightweight test-time\nframework that reflects before generating at uncertain points. During token\ngeneration, SRGen utilizes dynamic entropy thresholding to identify\nhigh-uncertainty tokens. For each identified token, it trains a specific\ncorrective vector, which fully exploits the already generated context for a\nself-reflective generation to correct the token probability distribution. By\nretrospectively analyzing the partial output, this self-reflection enables more\ntrustworthy decisions, thereby significantly reducing the probability of errors\nat highly uncertain points. Evaluated on challenging mathematical reasoning\nbenchmarks and a diverse set of LLMs, SRGen can consistently strengthen model\nreasoning: improvements in single-pass quality also translate into stronger\nself-consistency voting. Especially, on AIME2024 with\nDeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on\nPass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a\nplug-and-play method that integrates reflection into the generation process for\nreliable LLM reasoning, achieving consistent gains with bounded overhead and\nbroad composability with other training-time (e.g., RLHF) and test-time (e.g.,\nSLOT) techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly solve complex reasoning tasks via\nlong chain-of-thought, but their forward-only autoregressive generation process\nis fragile; early token errors can cascade, which creates a clear need for\nself-reflection mechanisms. However, existing self-reflection either performs\nrevisions over full drafts or learns self-correction via expensive training,\nboth fundamentally reactive and inefficient. To address this, we propose\nSelf-Reflective Generation at Test Time (SRGen), a lightweight test-time\nframework that reflects before generating at uncertain points. During token\ngeneration, SRGen utilizes dynamic entropy thresholding to identify\nhigh-uncertainty tokens. For each identified token, it trains a specific\ncorrective vector, which fully exploits the already generated context for a\nself-reflective generation to correct the token probability distribution. By\nretrospectively analyzing the partial output, this self-reflection enables more\ntrustworthy decisions, thereby significantly reducing the probability of errors\nat highly uncertain points. Evaluated on challenging mathematical reasoning\nbenchmarks and a diverse set of LLMs, SRGen can consistently strengthen model\nreasoning: improvements in single-pass quality also translate into stronger\nself-consistency voting. Especially, on AIME2024 with\nDeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on\nPass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a\nplug-and-play method that integrates reflection into the generation process for\nreliable LLM reasoning, achieving consistent gains with bounded overhead and\nbroad composability with other training-time (e.g., RLHF) and test-time (e.g.,\nSLOT) techniques."
                },
                "authors": [
                    {
                        "name": "Jian Mu"
                    },
                    {
                        "name": "Qixin Zhang"
                    },
                    {
                        "name": "Zhiyong Wang"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Shuang Qiu"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Zhongxiang Dai"
                    },
                    {
                        "name": "Yao Shu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Shu"
                },
                "author": "Yao Shu",
                "arxiv_comment": "24 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02917v1",
                "updated": "2025-10-03T11:44:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    44,
                    21,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T11:44:21Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    44,
                    21,
                    4,
                    276,
                    0
                ],
                "title": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse\n  Autoencoders"
                },
                "summary": "As Large Language Models become integral to software development, with\nsubstantial portions of AI-suggested code entering production, understanding\ntheir internal correctness mechanisms becomes critical for safe deployment. We\napply sparse autoencoders to decompose LLM representations, identifying\ndirections that correspond to code correctness. We select predictor directions\nusing t-statistics and steering directions through separation scores from base\nmodel representations, then analyze their mechanistic properties through\nsteering, attention analysis, and weight orthogonalization. We find that code\ncorrectness directions in LLMs reliably predict incorrect code, while\ncorrection capabilities, though statistically significant, involve tradeoffs\nbetween fixing errors and preserving correct code. Mechanistically, successful\ncode generation depends on attending to test cases rather than problem\ndescriptions. Moreover, directions identified in base models retain their\neffectiveness after instruction-tuning, suggesting code correctness mechanisms\nlearned during pre-training are repurposed during fine-tuning. Our mechanistic\ninsights suggest three practical applications: prompting strategies should\nprioritize test examples over elaborate problem descriptions, predictor\ndirections can serve as error alarms for developer review, and these same\npredictors can guide selective steering, intervening only when errors are\nanticipated to prevent the code corruption from constant steering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models become integral to software development, with\nsubstantial portions of AI-suggested code entering production, understanding\ntheir internal correctness mechanisms becomes critical for safe deployment. We\napply sparse autoencoders to decompose LLM representations, identifying\ndirections that correspond to code correctness. We select predictor directions\nusing t-statistics and steering directions through separation scores from base\nmodel representations, then analyze their mechanistic properties through\nsteering, attention analysis, and weight orthogonalization. We find that code\ncorrectness directions in LLMs reliably predict incorrect code, while\ncorrection capabilities, though statistically significant, involve tradeoffs\nbetween fixing errors and preserving correct code. Mechanistically, successful\ncode generation depends on attending to test cases rather than problem\ndescriptions. Moreover, directions identified in base models retain their\neffectiveness after instruction-tuning, suggesting code correctness mechanisms\nlearned during pre-training are repurposed during fine-tuning. Our mechanistic\ninsights suggest three practical applications: prompting strategies should\nprioritize test examples over elaborate problem descriptions, predictor\ndirections can serve as error alarms for developer review, and these same\npredictors can guide selective steering, intervening only when errors are\nanticipated to prevent the code corruption from constant steering."
                },
                "authors": [
                    {
                        "name": "Kriz Tahimic"
                    },
                    {
                        "name": "Charibeth Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Charibeth Cheng"
                },
                "author": "Charibeth Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23799v2",
                "updated": "2025-10-03T11:34:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    34,
                    59,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-28T10:49:22Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    10,
                    49,
                    22,
                    6,
                    271,
                    0
                ],
                "title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector\n  Refinement"
                },
                "summary": "Steering has emerged as a promising approach in controlling large language\nmodels (LLMs) without modifying model parameters. However, most existing\nsteering methods rely on large-scale datasets to learn clear behavioral\ninformation, which limits their applicability in many real-world scenarios. The\nsteering vectors extracted from small dataset often contain task-irrelevant\nnoising features, which degrades their effectiveness. To refine the steering\nvectors learned from limited data, we introduce Refinement of Steering Vector\nvia Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise\nand augment the steering vectors. In our framework, we first remove\ntask-irrelevant features according to their semantics provided by SAEs, and\nthen enrich task-relevant features missing from the small dataset through their\nsemantic similarity to the identified relevant features. Extensive experiments\ndemonstrate that the proposed SAE-RSV substantially outperforms all the\nbaseline methods including supervised fine-tuning. Our findings show that\neffective steering vector can be constructed from limited training data by\nrefining the original steering vector through SAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering has emerged as a promising approach in controlling large language\nmodels (LLMs) without modifying model parameters. However, most existing\nsteering methods rely on large-scale datasets to learn clear behavioral\ninformation, which limits their applicability in many real-world scenarios. The\nsteering vectors extracted from small dataset often contain task-irrelevant\nnoising features, which degrades their effectiveness. To refine the steering\nvectors learned from limited data, we introduce Refinement of Steering Vector\nvia Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise\nand augment the steering vectors. In our framework, we first remove\ntask-irrelevant features according to their semantics provided by SAEs, and\nthen enrich task-relevant features missing from the small dataset through their\nsemantic similarity to the identified relevant features. Extensive experiments\ndemonstrate that the proposed SAE-RSV substantially outperforms all the\nbaseline methods including supervised fine-tuning. Our findings show that\neffective steering vector can be constructed from limited training data by\nrefining the original steering vector through SAEs."
                },
                "authors": [
                    {
                        "name": "Anyi Wang"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Ninghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ninghao Liu"
                },
                "author": "Ninghao Liu",
                "arxiv_comment": "19 pages, 11 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04404v2",
                "updated": "2025-10-03T11:24:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    24,
                    43,
                    4,
                    276,
                    0
                ],
                "published": "2025-07-06T14:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    14,
                    35,
                    43,
                    6,
                    187,
                    0
                ],
                "title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers"
                },
                "summary": "Large language models (LLMs) excel at natural language understanding and\ngeneration but remain vulnerable to factual errors, limiting their reliability\nin knowledge-intensive tasks. While decoding-time strategies provide a\npromising efficient solution without training, existing methods typically treat\ntoken-level and layer-level signals in isolation, overlooking the joint\ndynamics between them. In this work, we introduce a token-aware,\nlayer-localized contrastive decoding method that aligns specific token types\nwith their most influential transformer layers to improve factual generation.\nThrough empirical attention analysis, we identify two key patterns: punctuation\ntokens receive dominant attention in early layers, while conceptual tokens\ngovern semantic reasoning in intermediate layers. By selectively suppressing\nattention to these token types at their respective depths, we achieve the\ninduction of controlled factual degradation and derive contrastive signals to\nguide the final factual decoding. Our method requires no additional training or\nmodel modification, and experiments demonstrate that our method consistently\nimproves factuality across multiple LLMs and various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at natural language understanding and\ngeneration but remain vulnerable to factual errors, limiting their reliability\nin knowledge-intensive tasks. While decoding-time strategies provide a\npromising efficient solution without training, existing methods typically treat\ntoken-level and layer-level signals in isolation, overlooking the joint\ndynamics between them. In this work, we introduce a token-aware,\nlayer-localized contrastive decoding method that aligns specific token types\nwith their most influential transformer layers to improve factual generation.\nThrough empirical attention analysis, we identify two key patterns: punctuation\ntokens receive dominant attention in early layers, while conceptual tokens\ngovern semantic reasoning in intermediate layers. By selectively suppressing\nattention to these token types at their respective depths, we achieve the\ninduction of controlled factual degradation and derive contrastive signals to\nguide the final factual decoding. Our method requires no additional training or\nmodel modification, and experiments demonstrate that our method consistently\nimproves factuality across multiple LLMs and various benchmarks."
                },
                "authors": [
                    {
                        "name": "Jingze Zhu"
                    },
                    {
                        "name": "Yongliang Wu"
                    },
                    {
                        "name": "Wenbo Zhu"
                    },
                    {
                        "name": "Jiawang Cao"
                    },
                    {
                        "name": "Yanqiang Zheng"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Jonas Fischer"
                    },
                    {
                        "name": "Xinting Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xinting Hu"
                },
                "author": "Xinting Hu",
                "arxiv_comment": "The submission was made before undergoing the required review by the\n  co-authors' affiliated institutions. We are withdrawing the paper to allow\n  for the completion of the institutional review process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17052v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17052v5",
                "updated": "2025-10-03T11:22:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    22,
                    35,
                    4,
                    276,
                    0
                ],
                "published": "2024-12-22T15:05:30Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    15,
                    5,
                    30,
                    6,
                    357,
                    0
                ],
                "title": "ViLBias: Detecting and Reasoning about Bias in Multimodal Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViLBias: Detecting and Reasoning about Bias in Multimodal Content"
                },
                "summary": "Detecting bias in multimodal news requires models that reason over\ntext--image pairs, not just classify text. In response, we present ViLBias, a\nVQA-style benchmark and framework for detecting and reasoning about bias in\nmultimodal news. The dataset comprises 40,945 text--image pairs from diverse\noutlets, each annotated with a bias label and concise rationale using a\ntwo-stage LLM-as-annotator pipeline with hierarchical majority voting and\nhuman-in-the-loop validation. We evaluate Small Language Models (SLMs), Large\nLanguage Models (LLMs), and Vision--Language Models (VLMs) across closed-ended\nclassification and open-ended reasoning (oVQA), and compare parameter-efficient\ntuning strategies. Results show that incorporating images alongside text\nimproves detection accuracy by 3--5\\%, and that LLMs/VLMs better capture subtle\nframing and text--image inconsistencies than SLMs. Parameter-efficient methods\n(LoRA/QLoRA/Adapters) recover 97--99\\% of full fine-tuning performance with\n$<5\\%$ trainable parameters. For oVQA, reasoning accuracy spans 52--79\\% and\nfaithfulness 68--89\\%, both improved by instruction tuning; closed accuracy\ncorrelates strongly with reasoning ($r = 0.91$). ViLBias offers a scalable\nbenchmark and strong baselines for multimodal bias detection and rationale\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting bias in multimodal news requires models that reason over\ntext--image pairs, not just classify text. In response, we present ViLBias, a\nVQA-style benchmark and framework for detecting and reasoning about bias in\nmultimodal news. The dataset comprises 40,945 text--image pairs from diverse\noutlets, each annotated with a bias label and concise rationale using a\ntwo-stage LLM-as-annotator pipeline with hierarchical majority voting and\nhuman-in-the-loop validation. We evaluate Small Language Models (SLMs), Large\nLanguage Models (LLMs), and Vision--Language Models (VLMs) across closed-ended\nclassification and open-ended reasoning (oVQA), and compare parameter-efficient\ntuning strategies. Results show that incorporating images alongside text\nimproves detection accuracy by 3--5\\%, and that LLMs/VLMs better capture subtle\nframing and text--image inconsistencies than SLMs. Parameter-efficient methods\n(LoRA/QLoRA/Adapters) recover 97--99\\% of full fine-tuning performance with\n$<5\\%$ trainable parameters. For oVQA, reasoning accuracy spans 52--79\\% and\nfaithfulness 68--89\\%, both improved by instruction tuning; closed accuracy\ncorrelates strongly with reasoning ($r = 0.91$). ViLBias offers a scalable\nbenchmark and strong baselines for multimodal bias detection and rationale\nquality."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Caesar Saleh"
                    },
                    {
                        "name": "Azib Farooq"
                    },
                    {
                        "name": "Emrul Hasan"
                    },
                    {
                        "name": "Franklin Ogidi"
                    },
                    {
                        "name": "Maximus Powers"
                    },
                    {
                        "name": "Veronica Chatrath"
                    },
                    {
                        "name": "Marcelo Lotif"
                    },
                    {
                        "name": "Karanpal Sekhon"
                    },
                    {
                        "name": "Roya Javadi"
                    },
                    {
                        "name": "Haad Zahid"
                    },
                    {
                        "name": "Anam Zahid"
                    },
                    {
                        "name": "Vahid Reza Khazaie"
                    },
                    {
                        "name": "Zhenyu Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Yu"
                },
                "author": "Zhenyu Yu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17052v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17052v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02906v1",
                "updated": "2025-10-03T11:19:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    19,
                    31,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T11:19:31Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    11,
                    19,
                    31,
                    4,
                    276,
                    0
                ],
                "title": "FinReflectKG -- MultiHop: Financial QA Benchmark for Reasoning with\n  Knowledge Graph Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinReflectKG -- MultiHop: Financial QA Benchmark for Reasoning with\n  Knowledge Graph Evidence"
                },
                "summary": "Multi-hop reasoning over financial disclosures is often a retrieval problem\nbefore it becomes a reasoning or generation problem: relevant facts are\ndispersed across sections, filings, companies, and years, and LLMs often expend\nexcessive tokens navigating noisy context. Without precise Knowledge Graph\n(KG)-guided selection of relevant context, even strong reasoning models either\nfail to answer or consume excessive tokens, whereas KG-linked evidence enables\nmodels to focus their reasoning on composing already retrieved facts. We\npresent FinReflectKG - MultiHop, a benchmark built on FinReflectKG, a\ntemporally indexed financial KG that links audited triples to source chunks\nfrom S&P 100 filings (2022-2024). Mining frequent 2-3 hop subgraph patterns\nacross sectors (via GICS taxonomy), we generate financial analyst style\nquestions with exact supporting evidence from the KG. A two-phase pipeline\nfirst creates QA pairs via pattern-specific prompts, followed by a\nmulti-criteria quality control evaluation to ensure QA validity. We then\nevaluate three controlled retrieval scenarios: (S1) precise KG-linked paths;\n(S2) text-only page windows centered on relevant text spans; and (S3) relevant\npage windows with randomizations and distractors. Across both reasoning and\nnon-reasoning models, KG-guided precise retrieval yields substantial gains on\nthe FinReflectKG - MultiHop QA benchmark dataset, boosting correctness scores\nby approximately 24 percent while reducing token utilization by approximately\n84.5 percent compared to the page window setting, which reflects the\ntraditional vector retrieval paradigm. Spanning intra-document, inter-year, and\ncross-company scopes, our work underscores the pivotal role of knowledge graphs\nin efficiently connecting evidence for multi-hop financial QA. We also release\na curated subset of the benchmark (555 QA Pairs) to catalyze further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop reasoning over financial disclosures is often a retrieval problem\nbefore it becomes a reasoning or generation problem: relevant facts are\ndispersed across sections, filings, companies, and years, and LLMs often expend\nexcessive tokens navigating noisy context. Without precise Knowledge Graph\n(KG)-guided selection of relevant context, even strong reasoning models either\nfail to answer or consume excessive tokens, whereas KG-linked evidence enables\nmodels to focus their reasoning on composing already retrieved facts. We\npresent FinReflectKG - MultiHop, a benchmark built on FinReflectKG, a\ntemporally indexed financial KG that links audited triples to source chunks\nfrom S&P 100 filings (2022-2024). Mining frequent 2-3 hop subgraph patterns\nacross sectors (via GICS taxonomy), we generate financial analyst style\nquestions with exact supporting evidence from the KG. A two-phase pipeline\nfirst creates QA pairs via pattern-specific prompts, followed by a\nmulti-criteria quality control evaluation to ensure QA validity. We then\nevaluate three controlled retrieval scenarios: (S1) precise KG-linked paths;\n(S2) text-only page windows centered on relevant text spans; and (S3) relevant\npage windows with randomizations and distractors. Across both reasoning and\nnon-reasoning models, KG-guided precise retrieval yields substantial gains on\nthe FinReflectKG - MultiHop QA benchmark dataset, boosting correctness scores\nby approximately 24 percent while reducing token utilization by approximately\n84.5 percent compared to the page window setting, which reflects the\ntraditional vector retrieval paradigm. Spanning intra-document, inter-year, and\ncross-company scopes, our work underscores the pivotal role of knowledge graphs\nin efficiently connecting evidence for multi-hop financial QA. We also release\na curated subset of the benchmark (555 QA Pairs) to catalyze further research."
                },
                "authors": [
                    {
                        "name": "Abhinav Arun"
                    },
                    {
                        "name": "Reetu Raj Harsh"
                    },
                    {
                        "name": "Bhaskarjit Sarmah"
                    },
                    {
                        "name": "Stefano Pasquali"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Pasquali"
                },
                "author": "Stefano Pasquali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02892v1",
                "updated": "2025-10-03T10:59:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    59,
                    26,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T10:59:26Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    59,
                    26,
                    4,
                    276,
                    0
                ],
                "title": "RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative\n  Reinforcement Learning"
                },
                "summary": "Reinforcement learning (RL) is central to improving reasoning in large\nlanguage models (LLMs) but typically requires ground-truth rewards. Test-Time\nReinforcement Learning (TTRL) removes this need by using majority-vote rewards,\nbut relies on heavy online RL and incurs substantial computational cost. We\npropose RoiRL: Reasoning with offline iterative Reinforcement Learning, a\nfamily of lightweight offline learning alternatives that can target the same\nregularized optimal policies. Unlike TTRL, RoiRL eliminates the need to\nmaintain a reference model and instead optimizes weighted log-likelihood\nobjectives, enabling stable training with significantly lower memory and\ncompute requirements. Experimental results show that RoiRL trains to 2.5x\nfaster and consistently outperforms TTRL on reasoning benchmarks, establishing\na scalable path to self-improving LLMs without labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is central to improving reasoning in large\nlanguage models (LLMs) but typically requires ground-truth rewards. Test-Time\nReinforcement Learning (TTRL) removes this need by using majority-vote rewards,\nbut relies on heavy online RL and incurs substantial computational cost. We\npropose RoiRL: Reasoning with offline iterative Reinforcement Learning, a\nfamily of lightweight offline learning alternatives that can target the same\nregularized optimal policies. Unlike TTRL, RoiRL eliminates the need to\nmaintain a reference model and instead optimizes weighted log-likelihood\nobjectives, enabling stable training with significantly lower memory and\ncompute requirements. Experimental results show that RoiRL trains to 2.5x\nfaster and consistently outperforms TTRL on reasoning benchmarks, establishing\na scalable path to self-improving LLMs without labels."
                },
                "authors": [
                    {
                        "name": "Aleksei Arzhantsev"
                    },
                    {
                        "name": "Otmane Sakhi"
                    },
                    {
                        "name": "Flavian Vasile"
                    }
                ],
                "author_detail": {
                    "name": "Flavian Vasile"
                },
                "author": "Flavian Vasile",
                "arxiv_comment": "Accepted to the Efficient Reasoning Workshop at NeuRIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01925v2",
                "updated": "2025-10-03T10:55:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    55,
                    18,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T11:42:17Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    42,
                    17,
                    3,
                    275,
                    0
                ],
                "title": "Enhancing Large Language Model Reasoning with Reward Models: An\n  Analytical Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Model Reasoning with Reward Models: An\n  Analytical Survey"
                },
                "summary": "Reward models (RMs) play a critical role in enhancing the reasoning\nperformance of LLMs. For example, they can provide training signals to finetune\nLLMs during reinforcement learning (RL) and help select the best answer from\nmultiple candidates during inference. In this paper, we provide a systematic\nintroduction to RMs, along with a comprehensive survey of their applications in\nLLM reasoning. We first review fundamental concepts of RMs, including their\narchitectures, training methodologies, and evaluation techniques. Then, we\nexplore their key applications: (1) guiding generation and selecting optimal\noutputs during LLM inference, (2) facilitating data synthesis and iterative\nself-improvement for LLMs, and (3) providing training signals in RL-based\nfinetuning. Finally, we discuss critical open questions regarding the\nselection, generalization, evaluation, and enhancement of RMs, based on\nexisting research and our own empirical findings. Our analysis aims to provide\nactionable insights for the effective deployment and advancement of RMs for LLM\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) play a critical role in enhancing the reasoning\nperformance of LLMs. For example, they can provide training signals to finetune\nLLMs during reinforcement learning (RL) and help select the best answer from\nmultiple candidates during inference. In this paper, we provide a systematic\nintroduction to RMs, along with a comprehensive survey of their applications in\nLLM reasoning. We first review fundamental concepts of RMs, including their\narchitectures, training methodologies, and evaluation techniques. Then, we\nexplore their key applications: (1) guiding generation and selecting optimal\noutputs during LLM inference, (2) facilitating data synthesis and iterative\nself-improvement for LLMs, and (3) providing training signals in RL-based\nfinetuning. Finally, we discuss critical open questions regarding the\nselection, generalization, evaluation, and enhancement of RMs, based on\nexisting research and our own empirical findings. Our analysis aims to provide\nactionable insights for the effective deployment and advancement of RMs for LLM\nreasoning."
                },
                "authors": [
                    {
                        "name": "Qiyuan Liu"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Xuhong Chen"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Ning Miao"
                    }
                ],
                "author_detail": {
                    "name": "Ning Miao"
                },
                "author": "Ning Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21710v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21710v3",
                "updated": "2025-10-03T10:38:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    38,
                    39,
                    4,
                    276,
                    0
                ],
                "published": "2025-03-27T17:21:47Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    21,
                    47,
                    3,
                    86,
                    0
                ],
                "title": "Enhancing repository-level software repair via repository-aware\n  knowledge graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing repository-level software repair via repository-aware\n  knowledge graphs"
                },
                "summary": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which\nprimarily rely on large language models (LLMs), are hindered by semantic\nambiguities, limited understanding of structural context, and insufficient\nreasoning capabilities. To address these limitations, we propose KGCompass with\ntwo innovations: (1) a novel repository-aware knowledge graph (KG) that\naccurately links repository artifacts (issues and pull requests) and codebase\nentities (files, classes, and functions), allowing us to effectively narrow\ndown the vast search space to only 20 most relevant functions with accurate\ncandidate fault locations and contextual information, and (2) a path-guided\nrepair mechanism that leverages KG-mined entity paths, tracing through which\nallows us to augment LLMs with relevant contextual information to generate\nprecise patches along with their explanations. Experimental results in the\nSWE-bench Lite demonstrate that KGCompass achieves state-of-the-art single-LLM\nrepair performance (58.3%) and function-level fault location accuracy (56.0%)\nacross open-source approaches with a single repair model, costing only $0.2 per\nrepair. Among the bugs that KGCompass successfully localizes, 89.7% lack\nexplicit location hints in the issue and are found only through multi-hop graph\ntraversal, where pure LLMs struggle to locate bugs accurately. Relative to\npure-LLM baselines, KGCompass lifts the resolved rate by 50.8% on Claude-4\nSonnet, 30.2% on Claude-3.5 Sonnet, 115.7% on DeepSeek-V3, and 156.4% on\nQwen2.5 Max. These consistent improvements demonstrate that this graph-guided\nrepair framework delivers model-agnostic, cost-efficient repair and sets a\nstrong new baseline for repository-level repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level software repair faces challenges in bridging semantic gaps\nbetween issue descriptions and code patches. Existing approaches, which\nprimarily rely on large language models (LLMs), are hindered by semantic\nambiguities, limited understanding of structural context, and insufficient\nreasoning capabilities. To address these limitations, we propose KGCompass with\ntwo innovations: (1) a novel repository-aware knowledge graph (KG) that\naccurately links repository artifacts (issues and pull requests) and codebase\nentities (files, classes, and functions), allowing us to effectively narrow\ndown the vast search space to only 20 most relevant functions with accurate\ncandidate fault locations and contextual information, and (2) a path-guided\nrepair mechanism that leverages KG-mined entity paths, tracing through which\nallows us to augment LLMs with relevant contextual information to generate\nprecise patches along with their explanations. Experimental results in the\nSWE-bench Lite demonstrate that KGCompass achieves state-of-the-art single-LLM\nrepair performance (58.3%) and function-level fault location accuracy (56.0%)\nacross open-source approaches with a single repair model, costing only $0.2 per\nrepair. Among the bugs that KGCompass successfully localizes, 89.7% lack\nexplicit location hints in the issue and are found only through multi-hop graph\ntraversal, where pure LLMs struggle to locate bugs accurately. Relative to\npure-LLM baselines, KGCompass lifts the resolved rate by 50.8% on Claude-4\nSonnet, 30.2% on Claude-3.5 Sonnet, 115.7% on DeepSeek-V3, and 156.4% on\nQwen2.5 Max. These consistent improvements demonstrate that this graph-guided\nrepair framework delivers model-agnostic, cost-efficient repair and sets a\nstrong new baseline for repository-level repair."
                },
                "authors": [
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Jiadong Ren"
                    },
                    {
                        "name": "Shunfu Jin"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Haoye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Haoye Tian"
                },
                "author": "Haoye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21710v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21710v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.15238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.15238v2",
                "updated": "2025-10-03T10:31:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    31,
                    37,
                    4,
                    276,
                    0
                ],
                "published": "2023-11-26T08:31:57Z",
                "published_parsed": [
                    2023,
                    11,
                    26,
                    8,
                    31,
                    57,
                    6,
                    330,
                    0
                ],
                "title": "A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning\n  with General Function Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning\n  with General Function Approximation"
                },
                "summary": "The exploration-exploitation dilemma has been a central challenge in\nreinforcement learning (RL) with complex model classes. In this paper, we\npropose a new algorithm, Monotonic Q-Learning with Upper Confidence Bound\n(MQL-UCB) for RL with general function approximation. Our key algorithmic\ndesign includes (1) a general deterministic policy-switching strategy that\nachieves low switching cost, (2) a monotonic value function structure with\ncarefully controlled function class complexity, and (3) a variance-weighted\nregression scheme that exploits historical trajectories with high data\nefficiency. MQL-UCB achieves minimax optimal regret of $\\tilde{O}(d\\sqrt{HK})$\nwhen $K$ is sufficiently large and near-optimal policy switching cost of\n$\\tilde{O}(dH)$, with $d$ being the eluder dimension of the function class, $H$\nbeing the planning horizon, and $K$ being the number of episodes.\n  Our work sheds light on designing provably sample-efficient and\ndeployment-efficient Q-learning with nonlinear function approximation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exploration-exploitation dilemma has been a central challenge in\nreinforcement learning (RL) with complex model classes. In this paper, we\npropose a new algorithm, Monotonic Q-Learning with Upper Confidence Bound\n(MQL-UCB) for RL with general function approximation. Our key algorithmic\ndesign includes (1) a general deterministic policy-switching strategy that\nachieves low switching cost, (2) a monotonic value function structure with\ncarefully controlled function class complexity, and (3) a variance-weighted\nregression scheme that exploits historical trajectories with high data\nefficiency. MQL-UCB achieves minimax optimal regret of $\\tilde{O}(d\\sqrt{HK})$\nwhen $K$ is sufficiently large and near-optimal policy switching cost of\n$\\tilde{O}(dH)$, with $d$ being the eluder dimension of the function class, $H$\nbeing the planning horizon, and $K$ being the number of episodes.\n  Our work sheds light on designing provably sample-efficient and\ndeployment-efficient Q-learning with nonlinear function approximation."
                },
                "authors": [
                    {
                        "name": "Heyang Zhao"
                    },
                    {
                        "name": "Jiafan He"
                    },
                    {
                        "name": "Quanquan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Quanquan Gu"
                },
                "author": "Quanquan Gu",
                "arxiv_comment": "46 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.15238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.15238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01918v2",
                "updated": "2025-10-03T10:19:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    19,
                    23,
                    4,
                    276,
                    0
                ],
                "published": "2025-08-03T21:03:22Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    21,
                    3,
                    22,
                    6,
                    215,
                    0
                ],
                "title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and\n  Retrieval for the Punjabi Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and\n  Retrieval for the Punjabi Language"
                },
                "summary": "Despite rapid advances in large language models (LLMs), low-resource\nlanguages remain excluded from NLP, limiting digital access for millions. We\npresent PunGPT2, the first fully open-source Punjabi generative model suite,\ntrained on a 35GB corpus covering literature, religious texts, news, social\ndiscourse, etc. PunGPT2 captures Punjabi's syntactic and morphological richness\nthrough a tokenizer optimized for Gurmukhi and Shahmukhi scripts. We introduce\nPun-RAG, a retrieval-augmented framework integrating PunGPT2 with a FAISS\nretriever over a curated Punjabi knowledge base, and Pun-Instruct, an\ninstruction-tuned variant using QLoRA for robust zero-shot summarization,\ntranslation, and question answering. Our key innovation, Quantum-RAG, fuses\nsparse, dense, and quantum kernel embeddings for efficient, context-aware\nretrieval with low memory overhead, marking the first practical\nquantum-inspired retrieval in a low-resource LLM. Our models outperform\nmultilingual baselines (mBERT, mT5, MuRIL, BLOOM) on FLORES-200, IndicGenBench,\nand a new PunjabiEval suite. Quantum-RAG yields +7.4 Recall@10 over FAISS and\n+3.5 BLEU over mT5 on PunjabiEval. We publicly release all training scripts,\nhyperparameters, evaluation pipelines, the 35GB Punjabi corpus, the PunjabiEval\nbenchmark, and all model weights, establishing new state-of-the-art results for\nPunjabi language generation and retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid advances in large language models (LLMs), low-resource\nlanguages remain excluded from NLP, limiting digital access for millions. We\npresent PunGPT2, the first fully open-source Punjabi generative model suite,\ntrained on a 35GB corpus covering literature, religious texts, news, social\ndiscourse, etc. PunGPT2 captures Punjabi's syntactic and morphological richness\nthrough a tokenizer optimized for Gurmukhi and Shahmukhi scripts. We introduce\nPun-RAG, a retrieval-augmented framework integrating PunGPT2 with a FAISS\nretriever over a curated Punjabi knowledge base, and Pun-Instruct, an\ninstruction-tuned variant using QLoRA for robust zero-shot summarization,\ntranslation, and question answering. Our key innovation, Quantum-RAG, fuses\nsparse, dense, and quantum kernel embeddings for efficient, context-aware\nretrieval with low memory overhead, marking the first practical\nquantum-inspired retrieval in a low-resource LLM. Our models outperform\nmultilingual baselines (mBERT, mT5, MuRIL, BLOOM) on FLORES-200, IndicGenBench,\nand a new PunjabiEval suite. Quantum-RAG yields +7.4 Recall@10 over FAISS and\n+3.5 BLEU over mT5 on PunjabiEval. We publicly release all training scripts,\nhyperparameters, evaluation pipelines, the 35GB Punjabi corpus, the PunjabiEval\nbenchmark, and all model weights, establishing new state-of-the-art results for\nPunjabi language generation and retrieval."
                },
                "authors": [
                    {
                        "name": "Jaskaranjeet Singh"
                    },
                    {
                        "name": "Rakesh Thakur"
                    }
                ],
                "author_detail": {
                    "name": "Rakesh Thakur"
                },
                "author": "Rakesh Thakur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02854v1",
                "updated": "2025-10-03T09:43:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    43,
                    51,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:43:51Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    43,
                    51,
                    4,
                    276,
                    0
                ],
                "title": "C2|Q>: A Robust Framework for Bridging Classical and Quantum Software\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C2|Q>: A Robust Framework for Bridging Classical and Quantum Software\n  Development"
                },
                "summary": "Quantum Software Engineering (QSE) is emerging as a critical discipline to\nmake quantum computing accessible to a broader developer community; however,\nmost quantum development environments still require developers to engage with\nlow-level details across the software stack - including problem encoding,\ncircuit construction, algorithm configuration, hardware selection, and result\ninterpretation - making them difficult for classical software engineers to use.\nTo bridge this gap, we present C2|Q>: a hardware-agnostic quantum software\ndevelopment framework that translates classical specifications (code) into\nquantum-executable programs while preserving methodological rigor. The\nframework applies modular software engineering principles by classifying the\nworkflow into three core modules: an encoder that classifies problems, produces\nQuantum-Compatible Formats (QCFs), and constructs quantum circuits, a\ndeployment module that generates circuits and recommends hardware based on\nfidelity, runtime, and cost, and a decoder that interprets quantum outputs into\nclassical solutions. In evaluation, the encoder module achieved a 93.8%\ncompletion rate, the hardware recommendation module consistently selected the\nappropriate quantum devices for workloads scaling up to 56 qubits, and the full\nC2|Q>: workflow successfully processed classical specifications (434 Python\nsnippets and 100 JSON inputs) with completion rates of 93.8% and 100%,\nrespectively. For case study problems executed on publicly available NISQ\nhardware, C2|Q>: reduced the required implementation effort by nearly 40X\ncompared to manual implementations using low-level quantum software development\nkits (SDKs), with empirical runs limited to small- and medium-sized instances\nconsistent with current NISQ capabilities. The open-source implementation of\nC2|Q>: is available at https://github.com/C2-Q/C2Q",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Software Engineering (QSE) is emerging as a critical discipline to\nmake quantum computing accessible to a broader developer community; however,\nmost quantum development environments still require developers to engage with\nlow-level details across the software stack - including problem encoding,\ncircuit construction, algorithm configuration, hardware selection, and result\ninterpretation - making them difficult for classical software engineers to use.\nTo bridge this gap, we present C2|Q>: a hardware-agnostic quantum software\ndevelopment framework that translates classical specifications (code) into\nquantum-executable programs while preserving methodological rigor. The\nframework applies modular software engineering principles by classifying the\nworkflow into three core modules: an encoder that classifies problems, produces\nQuantum-Compatible Formats (QCFs), and constructs quantum circuits, a\ndeployment module that generates circuits and recommends hardware based on\nfidelity, runtime, and cost, and a decoder that interprets quantum outputs into\nclassical solutions. In evaluation, the encoder module achieved a 93.8%\ncompletion rate, the hardware recommendation module consistently selected the\nappropriate quantum devices for workloads scaling up to 56 qubits, and the full\nC2|Q>: workflow successfully processed classical specifications (434 Python\nsnippets and 100 JSON inputs) with completion rates of 93.8% and 100%,\nrespectively. For case study problems executed on publicly available NISQ\nhardware, C2|Q>: reduced the required implementation effort by nearly 40X\ncompared to manual implementations using low-level quantum software development\nkits (SDKs), with empirical runs limited to small- and medium-sized instances\nconsistent with current NISQ capabilities. The open-source implementation of\nC2|Q>: is available at https://github.com/C2-Q/C2Q"
                },
                "authors": [
                    {
                        "name": "Boshuai Ye"
                    },
                    {
                        "name": "Arif Ali Khan"
                    },
                    {
                        "name": "Teemu Pihkakoski"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Muhammad Azeem Akbar"
                    },
                    {
                        "name": "Matti Silveri"
                    },
                    {
                        "name": "Lauri Malmi"
                    }
                ],
                "author_detail": {
                    "name": "Lauri Malmi"
                },
                "author": "Lauri Malmi",
                "arxiv_comment": "46 pages, 8 images, 14 tables, Manuscript submitted to a Journal\n  (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11274v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11274v4",
                "updated": "2025-10-03T09:38:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    38,
                    3,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-16T14:08:04Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    8,
                    4,
                    4,
                    136,
                    0
                ],
                "title": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning"
                },
                "summary": "While reasoning models demonstrate exceptional performance on complex tasks,\nthey often exhibit tendencies of overthinking on simple problems. This\nphenomenon not only leads to excessive computational resource consumption but\nalso significantly degrades user experience. To address this challenge, we\npropose SelfBudgeter - a novel user-friendly adaptive controllable reasoning\nframework that incorporates a budget estimation mechanism prior to reasoning.\nThe framework adopts a dual-phase training paradigm: during the cold-start\nphase, the model learns to predict token budgets before executing reasoning in\na standardized format; in the reinforcement learning phase, the model is\ntrained to autonomously plan budgets based on problem difficulty and strictly\nadhere to them when generating responses. Since the model outputs budget\nestimates at the initial stage, users can immediately anticipate waiting\nduration, enabling flexible decisions on whether to interrupt or continue the\ngeneration process. Notably, our method supports manual control of reasoning\nlength through pre-filled budget fields. Experimental results demonstrate that\nSelfBudgeter can dynamically allocate budgets according to problem complexity,\nyielding an average response length compression of 61% for the 1.5B model on\nGSM8K, MATH500, and AIME2025, and 48% for the 7B model, while maintaining\nnearly undiminished accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reasoning models demonstrate exceptional performance on complex tasks,\nthey often exhibit tendencies of overthinking on simple problems. This\nphenomenon not only leads to excessive computational resource consumption but\nalso significantly degrades user experience. To address this challenge, we\npropose SelfBudgeter - a novel user-friendly adaptive controllable reasoning\nframework that incorporates a budget estimation mechanism prior to reasoning.\nThe framework adopts a dual-phase training paradigm: during the cold-start\nphase, the model learns to predict token budgets before executing reasoning in\na standardized format; in the reinforcement learning phase, the model is\ntrained to autonomously plan budgets based on problem difficulty and strictly\nadhere to them when generating responses. Since the model outputs budget\nestimates at the initial stage, users can immediately anticipate waiting\nduration, enabling flexible decisions on whether to interrupt or continue the\ngeneration process. Notably, our method supports manual control of reasoning\nlength through pre-filled budget fields. Experimental results demonstrate that\nSelfBudgeter can dynamically allocate budgets according to problem complexity,\nyielding an average response length compression of 61% for the 1.5B model on\nGSM8K, MATH500, and AIME2025, and 48% for the 7B model, while maintaining\nnearly undiminished accuracy."
                },
                "authors": [
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kai Jia"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11274v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11274v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02850v1",
                "updated": "2025-10-03T09:37:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    37,
                    59,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:37:59Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    37,
                    59,
                    4,
                    276,
                    0
                ],
                "title": "Reward Model Routing in Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Model Routing in Alignment"
                },
                "summary": "Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become\nthe standard paradigm for aligning large language models (LLMs). However, most\npipelines rely on a single reward model (RM), limiting alignment quality and\nrisking overfitting. Recent work explores RM routing--dynamically selecting an\nRM from a candidate pool to exploit complementary strengths while maintaining\n$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient\nexploration. We propose BayesianRouter, a hybrid routing framework that\ncombines offline RM strengths learning with online Bayesian selection. In the\noffline stage, a multi-task router is trained on preference data to estimate\nper-RM reliability. In the online stage, a Bayesian Thompson sampling router\nperforms per-query RM selection, initializing RM-specific weight vectors with\noffline embeddings as Gaussian priors and adaptively updating their posteriors\nwith online rewards to adapt to the evolving policy distribution. Extensive\nexperiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and\nreasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently\noutperforms individual RMs, RM ensembling, and existing routing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become\nthe standard paradigm for aligning large language models (LLMs). However, most\npipelines rely on a single reward model (RM), limiting alignment quality and\nrisking overfitting. Recent work explores RM routing--dynamically selecting an\nRM from a candidate pool to exploit complementary strengths while maintaining\n$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient\nexploration. We propose BayesianRouter, a hybrid routing framework that\ncombines offline RM strengths learning with online Bayesian selection. In the\noffline stage, a multi-task router is trained on preference data to estimate\nper-RM reliability. In the online stage, a Bayesian Thompson sampling router\nperforms per-query RM selection, initializing RM-specific weight vectors with\noffline embeddings as Gaussian priors and adaptively updating their posteriors\nwith online rewards to adapt to the evolving policy distribution. Extensive\nexperiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and\nreasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently\noutperforms individual RMs, RM ensembling, and existing routing methods."
                },
                "authors": [
                    {
                        "name": "Xinle Wu"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02838v1",
                "updated": "2025-10-03T09:23:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    23,
                    56,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:23:56Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    23,
                    56,
                    4,
                    276,
                    0
                ],
                "title": "TridentServe: A Stage-level Serving System for Diffusion Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TridentServe: A Stage-level Serving System for Diffusion Pipelines"
                },
                "summary": "Diffusion pipelines, renowned for their powerful visual generation\ncapabilities, have seen widespread adoption in generative vision tasks (e.g.,\ntext-to-image/video). These pipelines typically follow an\nencode--diffuse--decode three-stage architecture. Current serving systems\ndeploy diffusion pipelines within a static, manual, and pipeline-level\nparadigm, allocating the same resources to every request and stage. However,\nthrough an in-depth analysis, we find that such a paradigm is inefficient due\nto the discrepancy in resource needs across the three stages of each request,\nas well as across different requests. Following the analysis, we propose the\ndynamic stage-level serving paradigm and develop TridentServe, a brand new\ndiffusion serving system. TridentServe automatically, dynamically derives the\nplacement plan (i.e., how each stage resides) for pipeline deployment and the\ndispatch plan (i.e., how the requests are routed) for request processing,\nco-optimizing the resource allocation for both model and requests. Extensive\nexperiments show that TridentServe consistently improves SLO attainment and\nreduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works\nacross a variety of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion pipelines, renowned for their powerful visual generation\ncapabilities, have seen widespread adoption in generative vision tasks (e.g.,\ntext-to-image/video). These pipelines typically follow an\nencode--diffuse--decode three-stage architecture. Current serving systems\ndeploy diffusion pipelines within a static, manual, and pipeline-level\nparadigm, allocating the same resources to every request and stage. However,\nthrough an in-depth analysis, we find that such a paradigm is inefficient due\nto the discrepancy in resource needs across the three stages of each request,\nas well as across different requests. Following the analysis, we propose the\ndynamic stage-level serving paradigm and develop TridentServe, a brand new\ndiffusion serving system. TridentServe automatically, dynamically derives the\nplacement plan (i.e., how each stage resides) for pipeline deployment and the\ndispatch plan (i.e., how the requests are routed) for request processing,\nco-optimizing the resource allocation for both model and requests. Extensive\nexperiments show that TridentServe consistently improves SLO attainment and\nreduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works\nacross a variety of workloads."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Hao Yuan"
                    },
                    {
                        "name": "Hanke Zhang"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02837v1",
                "updated": "2025-10-03T09:19:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    19,
                    15,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:19:15Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    19,
                    15,
                    4,
                    276,
                    0
                ],
                "title": "Beyond the Final Answer: Evaluating the Reasoning Trajectories of\n  Tool-Augmented Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Final Answer: Evaluating the Reasoning Trajectories of\n  Tool-Augmented Agents"
                },
                "summary": "Although recent tool-augmented benchmarks incorporate complex user requests\nand diverse tools, the evaluation methods for most of them remain limited to\nanswer matching. However, as the number of steps required to resolve a user\nrequest increases, a proper evaluation of an agent's performance must go beyond\nthe final answer to also assess the problem-solving trajectory, including\npreviously ignored aspects such as efficiency, hallucination, and adaptivity.\nThe most straightforward method for evaluating these aspects is to compare an\nagent's trajectory with the ground-truth trajectory, but this approach is\nfundamentally limited since annotating all valid ground-truth trajectories is\nprohibitively expensive. However, a simple LLM-based evaluator struggles to\nassess trajectories in detail without ground truth. To effectively evaluate the\nagents in this manner, we introduce TRACE, a framework for the\nmulti-dimensional evaluation of tool-augmented LLM agent performance. By\nincorporating an evidence bank, which accumulates knowledge gathered from\npreceding reasoning steps, TRACE enables a multi-faceted analysis and\nevaluation of an agent's reasoning trajectory effectively. To validate our\nframework, we develop a new meta-evaluation dataset by augmenting existing\nbenchmarks with diverse and flawed trajectories, each labeled with\nmulti-faceted performance scores. Our results confirm that TRACE accurately\nevaluates these complex behaviors in a scalable and cost-effective manner, even\nwith small open-source LLMs. Furthermore, we apply our method to evaluate the\ntrajectories that agents produce while solving tool-augmented tasks, presenting\npreviously unreported observations and their corresponding insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent tool-augmented benchmarks incorporate complex user requests\nand diverse tools, the evaluation methods for most of them remain limited to\nanswer matching. However, as the number of steps required to resolve a user\nrequest increases, a proper evaluation of an agent's performance must go beyond\nthe final answer to also assess the problem-solving trajectory, including\npreviously ignored aspects such as efficiency, hallucination, and adaptivity.\nThe most straightforward method for evaluating these aspects is to compare an\nagent's trajectory with the ground-truth trajectory, but this approach is\nfundamentally limited since annotating all valid ground-truth trajectories is\nprohibitively expensive. However, a simple LLM-based evaluator struggles to\nassess trajectories in detail without ground truth. To effectively evaluate the\nagents in this manner, we introduce TRACE, a framework for the\nmulti-dimensional evaluation of tool-augmented LLM agent performance. By\nincorporating an evidence bank, which accumulates knowledge gathered from\npreceding reasoning steps, TRACE enables a multi-faceted analysis and\nevaluation of an agent's reasoning trajectory effectively. To validate our\nframework, we develop a new meta-evaluation dataset by augmenting existing\nbenchmarks with diverse and flawed trajectories, each labeled with\nmulti-faceted performance scores. Our results confirm that TRACE accurately\nevaluates these complex behaviors in a scalable and cost-effective manner, even\nwith small open-source LLMs. Furthermore, we apply our method to evaluate the\ntrajectories that agents produce while solving tool-augmented tasks, presenting\npreviously unreported observations and their corresponding insights."
                },
                "authors": [
                    {
                        "name": "Wonjoong Kim"
                    },
                    {
                        "name": "Sangwu Park"
                    },
                    {
                        "name": "Yeonjun In"
                    },
                    {
                        "name": "Sein Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Chanyoung Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanyoung Park"
                },
                "author": "Chanyoung Park",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02836v1",
                "updated": "2025-10-03T09:18:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    18,
                    36,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:18:36Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    18,
                    36,
                    4,
                    276,
                    0
                ],
                "title": "VR as a \"Drop-In\" Well-being Tool for Knowledge Workers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VR as a \"Drop-In\" Well-being Tool for Knowledge Workers"
                },
                "summary": "Virtual Reality (VR) is increasingly being used to support workplace\nwell-being, but many interventions focus narrowly on a single activity or goal.\nOur work explores how VR can meet the diverse physical and mental needs of\nknowledge workers. We developed Tranquil Loom, a VR app offering stretching,\nguided meditation, and open exploration across four environments. The app\nincludes an AI assistant that suggests activities based on users' emotional\nstates. We conducted a two-phase mixed-methods study: (1) interviews with 10\nknowledge workers to guide the app's design, and (2) deployment with 35\nparticipants gathering usage data, well-being measures, and interviews. Results\nshowed increases in mindfulness and reductions in anxiety. Participants enjoyed\nboth structured and open-ended activities, often using the app playfully. While\nAI suggestions were used infrequently, they prompted ideas for future\npersonalization. Overall, participants viewed VR as a flexible, ``drop-in''\ntool, highlighting its value for situational rather than prescriptive\nwell-being support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) is increasingly being used to support workplace\nwell-being, but many interventions focus narrowly on a single activity or goal.\nOur work explores how VR can meet the diverse physical and mental needs of\nknowledge workers. We developed Tranquil Loom, a VR app offering stretching,\nguided meditation, and open exploration across four environments. The app\nincludes an AI assistant that suggests activities based on users' emotional\nstates. We conducted a two-phase mixed-methods study: (1) interviews with 10\nknowledge workers to guide the app's design, and (2) deployment with 35\nparticipants gathering usage data, well-being measures, and interviews. Results\nshowed increases in mindfulness and reductions in anxiety. Participants enjoyed\nboth structured and open-ended activities, often using the app playfully. While\nAI suggestions were used infrequently, they prompted ideas for future\npersonalization. Overall, participants viewed VR as a flexible, ``drop-in''\ntool, highlighting its value for situational rather than prescriptive\nwell-being support."
                },
                "authors": [
                    {
                        "name": "Sophia Ppali"
                    },
                    {
                        "name": "Haris Psallidopoulos"
                    },
                    {
                        "name": "Marios Constantinides"
                    },
                    {
                        "name": "Fotis Liarokapis"
                    }
                ],
                "author_detail": {
                    "name": "Fotis Liarokapis"
                },
                "author": "Fotis Liarokapis",
                "arxiv_comment": "11 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02833v1",
                "updated": "2025-10-03T09:10:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    10,
                    27,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:10:27Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    10,
                    27,
                    4,
                    276,
                    0
                ],
                "title": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs"
                },
                "summary": "Despite substantial efforts in safety alignment, recent research indicates\nthat Large Language Models (LLMs) remain highly susceptible to jailbreak\nattacks. Among these attacks, finetuning-based ones that compromise LLMs'\nsafety alignment via fine-tuning stand out due to its stable jailbreak\nperformance. In particular, a recent study indicates that fine-tuning with as\nfew as 10 harmful question-answer (QA) pairs can lead to successful\njailbreaking across various harmful questions. However, such malicious\nfine-tuning attacks are readily detectable and hence thwarted by moderation\nmodels. In this paper, we demonstrate that LLMs can be jailbroken by\nfine-tuning with only 10 benign QA pairs; our attack exploits the increased\nsensitivity of LLMs to fine-tuning data after being overfitted. Specifically,\nour fine-tuning process starts with overfitting an LLM via fine-tuning with\nbenign QA pairs involving identical refusal answers. Further fine-tuning is\nthen performed with standard benign answers, causing the overfitted LLM to\nforget the refusal attitude and thus provide compliant answers regardless of\nthe harmfulness of a question. We implement our attack on the ten LLMs and\ncompare it with five existing baselines. Experiments demonstrate that our\nmethod achieves significant advantages in both attack effectiveness and attack\nstealth. Our findings expose previously unreported security vulnerabilities in\ncurrent LLMs and provide a new perspective on understanding how LLMs' security\nis compromised, even with benign fine-tuning. Our code is available at\nhttps://github.com/ZHIXINXIE/tenBenign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial efforts in safety alignment, recent research indicates\nthat Large Language Models (LLMs) remain highly susceptible to jailbreak\nattacks. Among these attacks, finetuning-based ones that compromise LLMs'\nsafety alignment via fine-tuning stand out due to its stable jailbreak\nperformance. In particular, a recent study indicates that fine-tuning with as\nfew as 10 harmful question-answer (QA) pairs can lead to successful\njailbreaking across various harmful questions. However, such malicious\nfine-tuning attacks are readily detectable and hence thwarted by moderation\nmodels. In this paper, we demonstrate that LLMs can be jailbroken by\nfine-tuning with only 10 benign QA pairs; our attack exploits the increased\nsensitivity of LLMs to fine-tuning data after being overfitted. Specifically,\nour fine-tuning process starts with overfitting an LLM via fine-tuning with\nbenign QA pairs involving identical refusal answers. Further fine-tuning is\nthen performed with standard benign answers, causing the overfitted LLM to\nforget the refusal attitude and thus provide compliant answers regardless of\nthe harmfulness of a question. We implement our attack on the ten LLMs and\ncompare it with five existing baselines. Experiments demonstrate that our\nmethod achieves significant advantages in both attack effectiveness and attack\nstealth. Our findings expose previously unreported security vulnerabilities in\ncurrent LLMs and provide a new perspective on understanding how LLMs' security\nis compromised, even with benign fine-tuning. Our code is available at\nhttps://github.com/ZHIXINXIE/tenBenign."
                },
                "authors": [
                    {
                        "name": "Zhixin Xie"
                    },
                    {
                        "name": "Xurui Song"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02830v1",
                "updated": "2025-10-03T09:09:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    9,
                    35,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T09:09:35Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    9,
                    35,
                    4,
                    276,
                    0
                ],
                "title": "Evaluating Large Language Models for IUCN Red List Species Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for IUCN Red List Species Information"
                },
                "summary": "Large Language Models (LLMs) are rapidly being adopted in conservation to\naddress the biodiversity crisis, yet their reliability for species evaluation\nis uncertain. This study systematically validates five leading models on 21,955\nspecies across four core IUCN Red List assessment components: taxonomy,\nconservation status, distribution, and threats. A critical paradox was\nrevealed: models excelled at taxonomic classification (94.9%) but consistently\nfailed at conservation reasoning (27.2% for status assessment). This\nknowledge-reasoning gap, evident across all models, suggests inherent\narchitectural constraints, not just data limitations. Furthermore, models\nexhibited systematic biases favoring charismatic vertebrates, potentially\namplifying existing conservation inequities. These findings delineate clear\nboundaries for responsible LLM deployment: they are powerful tools for\ninformation retrieval but require human oversight for judgment-based decisions.\nA hybrid approach is recommended, where LLMs augment expert capacity while\nhuman experts retain sole authority over risk assessment and policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are rapidly being adopted in conservation to\naddress the biodiversity crisis, yet their reliability for species evaluation\nis uncertain. This study systematically validates five leading models on 21,955\nspecies across four core IUCN Red List assessment components: taxonomy,\nconservation status, distribution, and threats. A critical paradox was\nrevealed: models excelled at taxonomic classification (94.9%) but consistently\nfailed at conservation reasoning (27.2% for status assessment). This\nknowledge-reasoning gap, evident across all models, suggests inherent\narchitectural constraints, not just data limitations. Furthermore, models\nexhibited systematic biases favoring charismatic vertebrates, potentially\namplifying existing conservation inequities. These findings delineate clear\nboundaries for responsible LLM deployment: they are powerful tools for\ninformation retrieval but require human oversight for judgment-based decisions.\nA hybrid approach is recommended, where LLMs augment expert capacity while\nhuman experts retain sole authority over risk assessment and policy."
                },
                "authors": [
                    {
                        "name": "Shinya Uryu"
                    }
                ],
                "author_detail": {
                    "name": "Shinya Uryu"
                },
                "author": "Shinya Uryu",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21700v2",
                "updated": "2025-10-03T09:07:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    9,
                    7,
                    13,
                    4,
                    276,
                    0
                ],
                "published": "2025-04-30T14:44:24Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    14,
                    44,
                    24,
                    2,
                    120,
                    0
                ],
                "title": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs"
                },
                "summary": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are fundamental actors in the modern IT landscape\ndominated by AI solutions. However, security threats associated with them might\nprevent their reliable adoption in critical application scenarios such as\ngovernment organizations and medical institutions. For this reason, commercial\nLLMs typically undergo a sophisticated censoring mechanism to eliminate any\nharmful output they could possibly produce. In response to this, LLM\nJailbreaking is a significant threat to such protections, and many previous\napproaches have already demonstrated its effectiveness across diverse domains.\nExisting jailbreak proposals mostly adopt a generate-and-test strategy to craft\nmalicious input. To improve the comprehension of censoring mechanisms and\ndesign a targeted jailbreak attack, we propose an Explainable-AI solution that\ncomparatively analyzes the behavior of censored and uncensored models to derive\nunique exploitable alignment patterns. Then, we propose XBreaking, a novel\njailbreak attack that exploits these unique patterns to break the security\nconstraints of LLMs by targeted noise injection. Our thorough experimental\ncampaign returns important insights about the censoring mechanisms and\ndemonstrates the effectiveness and performance of our attack."
                },
                "authors": [
                    {
                        "name": "Marco Arazzi"
                    },
                    {
                        "name": "Vignesh Kumar Kembu"
                    },
                    {
                        "name": "Antonino Nocera"
                    },
                    {
                        "name": "Vinod P"
                    }
                ],
                "author_detail": {
                    "name": "Vinod P"
                },
                "author": "Vinod P",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16076v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16076v2",
                "updated": "2025-10-03T08:59:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    8,
                    59,
                    31,
                    4,
                    276,
                    0
                ],
                "published": "2025-07-21T21:23:29Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    21,
                    23,
                    29,
                    0,
                    202,
                    0
                ],
                "title": "The Prompt Makes the Person(a): A Systematic Evaluation of\n  Sociodemographic Persona Prompting for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Makes the Person(a): A Systematic Evaluation of\n  Sociodemographic Persona Prompting for Large Language Models"
                },
                "summary": "Persona prompting is increasingly used in large language models (LLMs) to\nsimulate views of various sociodemographic groups. However, how a persona\nprompt is formulated can significantly affect outcomes, raising concerns about\nthe fidelity of such simulations. Using five open-source LLMs, we\nsystematically examine how different persona prompt strategies, specifically\nrole adoption formats and demographic priming strategies, influence LLM\nsimulations across 15 intersectional demographic groups in both open- and\nclosed-ended tasks. Our findings show that LLMs struggle to simulate\nmarginalized groups but that the choice of demographic priming and role\nadoption strategy significantly impacts their portrayal. Specifically, we find\nthat prompting in an interview-style format and name-based priming can help\nreduce stereotyping and improve alignment. Surprisingly, smaller models like\nOLMo-2-7B outperform larger ones such as Llama-3.3-70B. Our findings offer\nactionable guidance for designing sociodemographic persona prompts in LLM-based\nsimulation studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona prompting is increasingly used in large language models (LLMs) to\nsimulate views of various sociodemographic groups. However, how a persona\nprompt is formulated can significantly affect outcomes, raising concerns about\nthe fidelity of such simulations. Using five open-source LLMs, we\nsystematically examine how different persona prompt strategies, specifically\nrole adoption formats and demographic priming strategies, influence LLM\nsimulations across 15 intersectional demographic groups in both open- and\nclosed-ended tasks. Our findings show that LLMs struggle to simulate\nmarginalized groups but that the choice of demographic priming and role\nadoption strategy significantly impacts their portrayal. Specifically, we find\nthat prompting in an interview-style format and name-based priming can help\nreduce stereotyping and improve alignment. Surprisingly, smaller models like\nOLMo-2-7B outperform larger ones such as Llama-3.3-70B. Our findings offer\nactionable guidance for designing sociodemographic persona prompts in LLM-based\nsimulation studies."
                },
                "authors": [
                    {
                        "name": "Marlene Lutz"
                    },
                    {
                        "name": "Indira Sen"
                    },
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Elisa Rogers"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "arxiv_comment": "Accepted to EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16076v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16076v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02816v1",
                "updated": "2025-10-03T08:48:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    8,
                    48,
                    4,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T08:48:04Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    8,
                    48,
                    4,
                    4,
                    276,
                    0
                ],
                "title": "NCV: A Node-Wise Consistency Verification Approach for Low-Cost\n  Structured Error Localization in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NCV: A Node-Wise Consistency Verification Approach for Low-Cost\n  Structured Error Localization in LLM Reasoning"
                },
                "summary": "Verifying multi-step reasoning in large language models is difficult due to\nimprecise error localization and high token costs. Existing methods either\nassess entire reasoning chains, suffering attention dilution, or rely on\nexpensive multi-sampling. We introduce Node-wise Consistency Verification\n(NCV), a training-free framework that recasts verification as lightweight\nbinary consistency checks at the node level. By decomposing the chain of\nthought into interconnected verification nodes, NCV precisely localizes errors\nand avoids unnecessary long-form generation. Experiments demonstrate that our\napproach enhances interpretability and efficiency, presenting a scalable\nsolution for reliable LLM reasoning verification. On public datasets, NCV\nachieves a 10\\% to 25\\% improvement in F1 scores over baselines while utilizing\n$6\\times$~$58\\times$ fewer tokens than traditional methods like CoT-based\nverifiers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying multi-step reasoning in large language models is difficult due to\nimprecise error localization and high token costs. Existing methods either\nassess entire reasoning chains, suffering attention dilution, or rely on\nexpensive multi-sampling. We introduce Node-wise Consistency Verification\n(NCV), a training-free framework that recasts verification as lightweight\nbinary consistency checks at the node level. By decomposing the chain of\nthought into interconnected verification nodes, NCV precisely localizes errors\nand avoids unnecessary long-form generation. Experiments demonstrate that our\napproach enhances interpretability and efficiency, presenting a scalable\nsolution for reliable LLM reasoning verification. On public datasets, NCV\nachieves a 10\\% to 25\\% improvement in F1 scores over baselines while utilizing\n$6\\times$~$58\\times$ fewer tokens than traditional methods like CoT-based\nverifiers."
                },
                "authors": [
                    {
                        "name": "Yulong Zhang"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Peilin Li"
                    },
                    {
                        "name": "Yuqin Dai Zhiyuan Zhao"
                    },
                    {
                        "name": "Lingyong Fang"
                    },
                    {
                        "name": "Ziniu Liu"
                    },
                    {
                        "name": "Ru Zhang"
                    },
                    {
                        "name": "Huijia Zhu"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01761v2",
                "updated": "2025-10-03T08:40:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    8,
                    40,
                    59,
                    4,
                    276,
                    0
                ],
                "published": "2025-05-03T09:30:26Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    9,
                    30,
                    26,
                    5,
                    123,
                    0
                ],
                "title": "Same evaluation, more tokens: On the effect of input length for machine\n  translation evaluation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Same evaluation, more tokens: On the effect of input length for machine\n  translation evaluation using Large Language Models"
                },
                "summary": "Accurately evaluating machine-translated text remains a long-standing\nchallenge, particularly for long documents. Recent work has shown that large\nlanguage models (LLMs) can serve as reliable and interpretable sentence-level\ntranslation evaluators via MQM error span annotations. With modern LLMs\nsupporting larger context windows, a natural question arises: can we feed\nentire document translations into an LLM for quality assessment? Ideally,\nevaluation should be invariant to text length, producing consistent error spans\nregardless of input granularity. However, our analysis shows that text length\nsignificantly impacts evaluation: longer texts lead to fewer error spans and\nreduced system ranking accuracy. To address this limitation, we evaluate\nseveral strategies, including granularity-aligned prompting, Focus Sentence\nPrompting (FSP), and a fine-tuning approach to better align LLMs with the\nevaluation task. The latter two methods largely mitigate this length bias,\nmaking LLMs more reliable for long-form translation evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately evaluating machine-translated text remains a long-standing\nchallenge, particularly for long documents. Recent work has shown that large\nlanguage models (LLMs) can serve as reliable and interpretable sentence-level\ntranslation evaluators via MQM error span annotations. With modern LLMs\nsupporting larger context windows, a natural question arises: can we feed\nentire document translations into an LLM for quality assessment? Ideally,\nevaluation should be invariant to text length, producing consistent error spans\nregardless of input granularity. However, our analysis shows that text length\nsignificantly impacts evaluation: longer texts lead to fewer error spans and\nreduced system ranking accuracy. To address this limitation, we evaluate\nseveral strategies, including granularity-aligned prompting, Focus Sentence\nPrompting (FSP), and a fine-tuning approach to better align LLMs with the\nevaluation task. The latter two methods largely mitigate this length bias,\nmaking LLMs more reliable for long-form translation evaluation."
                },
                "authors": [
                    {
                        "name": "Tobias Domhan"
                    },
                    {
                        "name": "Dawei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Zhu"
                },
                "author": "Dawei Zhu",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02810v1",
                "updated": "2025-10-03T08:33:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    8,
                    33,
                    7,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T08:33:07Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    8,
                    33,
                    7,
                    4,
                    276,
                    0
                ],
                "title": "Dissecting Transformers: A CLEAR Perspective towards Green AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Transformers: A CLEAR Perspective towards Green AI"
                },
                "summary": "The rapid adoption of Large Language Models (LLMs) has raised significant\nenvironmental concerns. Unlike the one-time cost of training, LLM inference\noccurs continuously at a global scale and now dominates the AI energy\nfootprint. Yet, most sustainability studies report only coarse, model-level\nmetrics due to the lack of fine-grained measurement methods, treating energy\nefficiency more as an afterthought than as a primary objective. We present the\nfirst fine-grained empirical analysis of inference energy across core\ncomponents of transformer architecture. We propose a novel methodology,\nComponent-Level Energy Assessment via Repeated sampling (CLEAR), to overcome\ntemporal mismatch between microsecond scale component execution and monitoring\nof millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models\nspanning four distinct architecture types and consistently keep component-wise\nenergy variance below 9.5\\% while capturing more than 90\\% of the model's total\nenergy as individual components. Our empirical analysis reveals that Attention\nblocks consume significantly more energy per floating-point operation (FLOP),\nindicating that energy consumption is not proportionally aligned with FLOP\ncounts. This shows that FLOPs alone fail to capture the true energy cost at a\ncomponent level. Our findings establish detailed component-level energy\nbaselines and provide insight as an initial step to build energy-efficient\ntransformer models through component-level optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Large Language Models (LLMs) has raised significant\nenvironmental concerns. Unlike the one-time cost of training, LLM inference\noccurs continuously at a global scale and now dominates the AI energy\nfootprint. Yet, most sustainability studies report only coarse, model-level\nmetrics due to the lack of fine-grained measurement methods, treating energy\nefficiency more as an afterthought than as a primary objective. We present the\nfirst fine-grained empirical analysis of inference energy across core\ncomponents of transformer architecture. We propose a novel methodology,\nComponent-Level Energy Assessment via Repeated sampling (CLEAR), to overcome\ntemporal mismatch between microsecond scale component execution and monitoring\nof millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models\nspanning four distinct architecture types and consistently keep component-wise\nenergy variance below 9.5\\% while capturing more than 90\\% of the model's total\nenergy as individual components. Our empirical analysis reveals that Attention\nblocks consume significantly more energy per floating-point operation (FLOP),\nindicating that energy consumption is not proportionally aligned with FLOP\ncounts. This shows that FLOPs alone fail to capture the true energy cost at a\ncomponent level. Our findings establish detailed component-level energy\nbaselines and provide insight as an initial step to build energy-efficient\ntransformer models through component-level optimizations."
                },
                "authors": [
                    {
                        "name": "Hemang Jain"
                    },
                    {
                        "name": "Shailender Goyal"
                    },
                    {
                        "name": "Divyansh Pandey"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Vaidhyanathan"
                },
                "author": "Karthik Vaidhyanathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18070v2",
                "updated": "2025-10-03T08:26:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    8,
                    26,
                    29,
                    4,
                    276,
                    0
                ],
                "published": "2025-04-25T04:47:34Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    47,
                    34,
                    4,
                    115,
                    0
                ],
                "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths"
                },
                "summary": "Retrieval Augmented Generation (RAG) has become the standard approach for\nequipping Large Language Models (LLMs) with up-to-date knowledge. However,\nstandard RAG, relying on independent passage retrieval, often fails to capture\nthe interconnected nature of information required for complex, multi-hop\nreasoning. While structured RAG methods attempt to address this using knowledge\ngraphs built from triples, we argue that the inherent context loss of triples\n(context collapse) limits the fidelity of the knowledge representation. We\nintroduce PropRAG, a novel RAG framework that shifts from triples to\ncontext-rich propositions and introduces an efficient, LLM-free online beam\nsearch over proposition paths to discover multi-step reasoning chains. By\ncoupling a higher-fidelity knowledge representation with explicit path\ndiscovery, PropRAG achieves state-of-the-art zero-shot Recall@5 and F1 scores\non 2Wiki, HotpotQA, and MuSiQue, advancing non-parametric knowledge integration\nby improving evidence retrieval through richer representation and efficient\nreasoning path discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has become the standard approach for\nequipping Large Language Models (LLMs) with up-to-date knowledge. However,\nstandard RAG, relying on independent passage retrieval, often fails to capture\nthe interconnected nature of information required for complex, multi-hop\nreasoning. While structured RAG methods attempt to address this using knowledge\ngraphs built from triples, we argue that the inherent context loss of triples\n(context collapse) limits the fidelity of the knowledge representation. We\nintroduce PropRAG, a novel RAG framework that shifts from triples to\ncontext-rich propositions and introduces an efficient, LLM-free online beam\nsearch over proposition paths to discover multi-step reasoning chains. By\ncoupling a higher-fidelity knowledge representation with explicit path\ndiscovery, PropRAG achieves state-of-the-art zero-shot Recall@5 and F1 scores\non 2Wiki, HotpotQA, and MuSiQue, advancing non-parametric knowledge integration\nby improving evidence retrieval through richer representation and efficient\nreasoning path discovery."
                },
                "authors": [
                    {
                        "name": "Jingjin Wang"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main Conference). Camera-ready version. Code\n  and data: https://github.com/ReLink-Inc/PropRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12113v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12113v4",
                "updated": "2025-10-03T08:07:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    8,
                    7,
                    28,
                    4,
                    276,
                    0
                ],
                "published": "2025-06-13T13:39:00Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    39,
                    0,
                    4,
                    164,
                    0
                ],
                "title": "Semantic Preprocessing for LLM-based Malware Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Preprocessing for LLM-based Malware Analysis"
                },
                "summary": "In a context of malware analysis, numerous approaches rely on Artificial\nIntelligence to handle a large volume of data. However, these techniques focus\non data view (images, sequences) and not on an expert's view. Noticing this\nissue, we propose a preprocessing that focuses on expert knowledge to improve\nmalware semantic analysis and result interpretability. We propose a new\npreprocessing method which creates JSON reports for Portable Executable files.\nThese reports gather features from both static and behavioral analysis, and\nincorporate packer signature detection, MITRE ATT\\&CK and Malware Behavior\nCatalog (MBC) knowledge. The purpose of this preprocessing is to gather a\nsemantic representation of binary files, understandable by malware analysts,\nand that can enhance AI models' explainability for malicious files analysis.\nUsing this preprocessing to train a Large Language Model for Malware\nclassification, we achieve a weighted-average F1-score of 0.94 on a complex\ndataset, representative of market reality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a context of malware analysis, numerous approaches rely on Artificial\nIntelligence to handle a large volume of data. However, these techniques focus\non data view (images, sequences) and not on an expert's view. Noticing this\nissue, we propose a preprocessing that focuses on expert knowledge to improve\nmalware semantic analysis and result interpretability. We propose a new\npreprocessing method which creates JSON reports for Portable Executable files.\nThese reports gather features from both static and behavioral analysis, and\nincorporate packer signature detection, MITRE ATT\\&CK and Malware Behavior\nCatalog (MBC) knowledge. The purpose of this preprocessing is to gather a\nsemantic representation of binary files, understandable by malware analysts,\nand that can enhance AI models' explainability for malicious files analysis.\nUsing this preprocessing to train a Large Language Model for Malware\nclassification, we achieve a weighted-average F1-score of 0.94 on a complex\ndataset, representative of market reality."
                },
                "authors": [
                    {
                        "name": "Benjamin Marais"
                    },
                    {
                        "name": "Tony Quertier"
                    },
                    {
                        "name": "Grgoire Barrue"
                    }
                ],
                "author_detail": {
                    "name": "Grgoire Barrue"
                },
                "author": "Grgoire Barrue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12113v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12113v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02793v1",
                "updated": "2025-10-03T08:06:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    8,
                    6,
                    29,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T08:06:29Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    8,
                    6,
                    29,
                    4,
                    276,
                    0
                ],
                "title": "Pioneering Scalable Prototyping for Mid-Band XL-MIMO Systems: Design and\n  Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pioneering Scalable Prototyping for Mid-Band XL-MIMO Systems: Design and\n  Implementation"
                },
                "summary": "The mid-band frequency range, combined with extra large-scale multiple-input\nmultiple-output (XL-MIMO), is emerging as a key enabler for future\ncommunication systems. Thanks to the advent of new spectrum resources and\ndegrees of freedom brought by the near-field propagation, the mid-band XL-MIMO\nsystem is expected to significantly enhance throughput and inherently support\nadvanced functionalities such as integrated sensing and communication. Although\ntheoretical studies have highlighted the benefits of mid-band XL-MIMO systems,\nthe promised performance gains have yet to be validated in practical systems,\nposing a major challenge to the standardization. In this paper, preliminaries\nare first discussed, followed by an analysis of key challenges in constructing\na real-time prototype system. Subsequently, the design and implementation of a\nreal-time mid-band XL-MIMO prototype system are presented. Benefiting from the\nnovel architecture, the proposed prototype system supports metrics aligned with\nstandardization, including a bandwidth of 200 MHz, up to 1024 antenna elements,\nand up to 256 transceiver chains. Operating in time-division duplexing (TDD)\nmode, the prototype enables multiuser communication with support for up to 12\nusers, while retaining standard communication procedures. Built on\nsoftware-defined radio (SDR) platforms, the system is programmable and allows\nfor flexible deployment of advanced algorithms. Moreover, the modular\narchitecture ensures high scalability, making the system adaptable to various\nconfigurations, including distributed deployments and decentralized signal\nprocessing. Experimental results with the proposed prototype system demonstrate\nreal-time digital sample processing at 1167.85 Gbps, a peak data throughput of\n15.81 Gbps for 12 users, and a maximal spectral efficiency approaching 80\nbit/s/Hz.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mid-band frequency range, combined with extra large-scale multiple-input\nmultiple-output (XL-MIMO), is emerging as a key enabler for future\ncommunication systems. Thanks to the advent of new spectrum resources and\ndegrees of freedom brought by the near-field propagation, the mid-band XL-MIMO\nsystem is expected to significantly enhance throughput and inherently support\nadvanced functionalities such as integrated sensing and communication. Although\ntheoretical studies have highlighted the benefits of mid-band XL-MIMO systems,\nthe promised performance gains have yet to be validated in practical systems,\nposing a major challenge to the standardization. In this paper, preliminaries\nare first discussed, followed by an analysis of key challenges in constructing\na real-time prototype system. Subsequently, the design and implementation of a\nreal-time mid-band XL-MIMO prototype system are presented. Benefiting from the\nnovel architecture, the proposed prototype system supports metrics aligned with\nstandardization, including a bandwidth of 200 MHz, up to 1024 antenna elements,\nand up to 256 transceiver chains. Operating in time-division duplexing (TDD)\nmode, the prototype enables multiuser communication with support for up to 12\nusers, while retaining standard communication procedures. Built on\nsoftware-defined radio (SDR) platforms, the system is programmable and allows\nfor flexible deployment of advanced algorithms. Moreover, the modular\narchitecture ensures high scalability, making the system adaptable to various\nconfigurations, including distributed deployments and decentralized signal\nprocessing. Experimental results with the proposed prototype system demonstrate\nreal-time digital sample processing at 1167.85 Gbps, a peak data throughput of\n15.81 Gbps for 12 users, and a maximal spectral efficiency approaching 80\nbit/s/Hz."
                },
                "authors": [
                    {
                        "name": "Jiachen Tian"
                    },
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Zhengtao Jin"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Wankai Tang"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Wenjin Wang"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11191v3",
                "updated": "2025-10-03T07:53:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    7,
                    53,
                    53,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-16T16:34:49Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    34,
                    49,
                    6,
                    47,
                    0
                ],
                "title": "Primus: A Pioneering Collection of Open-Source Datasets for\n  Cybersecurity LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Primus: A Pioneering Collection of Open-Source Datasets for\n  Cybersecurity LLM Training"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable advancements in\nspecialized fields such as finance, law, and medicine. However, in\ncybersecurity, we have noticed a lack of open-source datasets, with a\nparticular lack of high-quality cybersecurity pretraining corpora, even though\nmuch research indicates that LLMs acquire their knowledge during pretraining.\nTo address this, we present a comprehensive suite of datasets covering all\nmajor training stages, including pretraining, instruction fine-tuning, and\nreasoning distillation with cybersecurity-specific self-reflection data.\nExtensive ablation studies demonstrate their effectiveness on public\ncybersecurity benchmarks. In particular, continual pre-training on our dataset\nyields a 15.9% improvement in the aggregate score, while reasoning distillation\nleads to a 15.8% gain in security certification (CISSP). We will release all\ndatasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to\nencourage further research in the community. For access to all datasets and\nmodel weights, please refer to\nhttps://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable advancements in\nspecialized fields such as finance, law, and medicine. However, in\ncybersecurity, we have noticed a lack of open-source datasets, with a\nparticular lack of high-quality cybersecurity pretraining corpora, even though\nmuch research indicates that LLMs acquire their knowledge during pretraining.\nTo address this, we present a comprehensive suite of datasets covering all\nmajor training stages, including pretraining, instruction fine-tuning, and\nreasoning distillation with cybersecurity-specific self-reflection data.\nExtensive ablation studies demonstrate their effectiveness on public\ncybersecurity benchmarks. In particular, continual pre-training on our dataset\nyields a 15.9% improvement in the aggregate score, while reasoning distillation\nleads to a 15.8% gain in security certification (CISSP). We will release all\ndatasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to\nencourage further research in the community. For access to all datasets and\nmodel weights, please refer to\nhttps://huggingface.co/collections/trendmicro-ailab/primus-67b1fd27052b802b4af9d243."
                },
                "authors": [
                    {
                        "name": "Yao-Ching Yu"
                    },
                    {
                        "name": "Tsun-Han Chiang"
                    },
                    {
                        "name": "Cheng-Wei Tsai"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    },
                    {
                        "name": "Wen-Kwang Tsao"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Kwang Tsao"
                },
                "author": "Wen-Kwang Tsao",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02780v1",
                "updated": "2025-10-03T07:27:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    7,
                    27,
                    47,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T07:27:47Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    7,
                    27,
                    47,
                    4,
                    276,
                    0
                ],
                "title": "Reasoning Riddles: How Explainability Reveals Cognitive Limits in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Riddles: How Explainability Reveals Cognitive Limits in\n  Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet their\ncognitive processes remain opaque on complex lateral thinking challenges like\nrebus puzzles. While recent work has demonstrated these models struggle\nsignificantly with rebus puzzle solving, the underlying reasoning processes and\nfailure patterns remain largely unexplored. We address this gap through a\ncomprehensive explainability analysis that moves beyond performance metrics to\nunderstand how VLMs approach these complex lateral thinking challenges. Our\nstudy contributes a systematically annotated dataset of 221 rebus puzzles\nacross six cognitive categories, paired with an evaluation framework that\nseparates reasoning quality from answer correctness. We investigate three\nprompting strategies designed to elicit different types of explanatory\nprocesses and reveal critical insights into VLM cognitive processes. Our\nfindings demonstrate that reasoning quality varies dramatically across puzzle\ncategories, with models showing systematic strengths in visual composition\nwhile exhibiting fundamental limitations in absence interpretation and cultural\nsymbolism. We also discover that prompting strategy substantially influences\nboth cognitive approach and problem-solving effectiveness, establishing\nexplainability as an integral component of model performance rather than a\npost-hoc consideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet their\ncognitive processes remain opaque on complex lateral thinking challenges like\nrebus puzzles. While recent work has demonstrated these models struggle\nsignificantly with rebus puzzle solving, the underlying reasoning processes and\nfailure patterns remain largely unexplored. We address this gap through a\ncomprehensive explainability analysis that moves beyond performance metrics to\nunderstand how VLMs approach these complex lateral thinking challenges. Our\nstudy contributes a systematically annotated dataset of 221 rebus puzzles\nacross six cognitive categories, paired with an evaluation framework that\nseparates reasoning quality from answer correctness. We investigate three\nprompting strategies designed to elicit different types of explanatory\nprocesses and reveal critical insights into VLM cognitive processes. Our\nfindings demonstrate that reasoning quality varies dramatically across puzzle\ncategories, with models showing systematic strengths in visual composition\nwhile exhibiting fundamental limitations in absence interpretation and cultural\nsymbolism. We also discover that prompting strategy substantially influences\nboth cognitive approach and problem-solving effectiveness, establishing\nexplainability as an integral component of model performance rather than a\npost-hoc consideration."
                },
                "authors": [
                    {
                        "name": "Prahitha Movva"
                    }
                ],
                "author_detail": {
                    "name": "Prahitha Movva"
                },
                "author": "Prahitha Movva",
                "arxiv_journal_ref": "COLM 2025: First Workshop on the Application of LLM Explainability\n  to Reasoning and Planning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02773v1",
                "updated": "2025-10-03T07:14:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    7,
                    14,
                    18,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T07:14:18Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    7,
                    14,
                    18,
                    4,
                    276,
                    0
                ],
                "title": "Automated Repair of OpenID Connect Programs (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Repair of OpenID Connect Programs (Extended Version)"
                },
                "summary": "OpenID Connect has revolutionized online authentication based on single\nsign-on (SSO) by providing a secure and convenient method for accessing\nmultiple services with a single set of credentials. Despite its widespread\nadoption, critical security bugs in OpenID Connect have resulted in significant\nfinancial losses and security breaches, highlighting the need for robust\nmitigation strategies. Automated program repair presents a promising solution\nfor generating candidate patches for OpenID implementations. However,\nchallenges such as domain-specific complexities and the necessity for precise\nfault localization and patch verification must be addressed. We propose\nAuthFix, a counterexample-guided repair engine leveraging LLMs for automated\nOpenID bug fixing. AuthFix integrates three key components: fault localization,\npatch synthesis, and patch verification. By employing a novel Petri-net-based\nmodel checker, AuthFix ensures the correctness of patches by effectively\nmodeling interactions. Our evaluation on a dataset of OpenID bugs demonstrates\nthat AuthFix successfully generated correct patches for 17 out of 23 bugs\n(74%), with a high proportion of patches semantically equivalent to\ndeveloper-written fixes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenID Connect has revolutionized online authentication based on single\nsign-on (SSO) by providing a secure and convenient method for accessing\nmultiple services with a single set of credentials. Despite its widespread\nadoption, critical security bugs in OpenID Connect have resulted in significant\nfinancial losses and security breaches, highlighting the need for robust\nmitigation strategies. Automated program repair presents a promising solution\nfor generating candidate patches for OpenID implementations. However,\nchallenges such as domain-specific complexities and the necessity for precise\nfault localization and patch verification must be addressed. We propose\nAuthFix, a counterexample-guided repair engine leveraging LLMs for automated\nOpenID bug fixing. AuthFix integrates three key components: fault localization,\npatch synthesis, and patch verification. By employing a novel Petri-net-based\nmodel checker, AuthFix ensures the correctness of patches by effectively\nmodeling interactions. Our evaluation on a dataset of OpenID bugs demonstrates\nthat AuthFix successfully generated correct patches for 17 out of 23 bugs\n(74%), with a high proportion of patches semantically equivalent to\ndeveloper-written fixes."
                },
                "authors": [
                    {
                        "name": "Tamjid Al Rahat"
                    },
                    {
                        "name": "Yanju Chen"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Yuan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Tian"
                },
                "author": "Yuan Tian",
                "arxiv_comment": "This is an extended version. The original paper is accepted to ASE\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02768v1",
                "updated": "2025-10-03T07:01:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    7,
                    1,
                    45,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T07:01:45Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    7,
                    1,
                    45,
                    4,
                    276,
                    0
                ],
                "title": "A Granular Study of Safety Pretraining under Model Abliteration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Granular Study of Safety Pretraining under Model Abliteration"
                },
                "summary": "Open-weight LLMs can be modified at inference time with simple activation\nedits, which raises a practical question for safety: do common safety\ninterventions like refusal training or metatag training survive such edits? We\nstudy model abliteration, a lightweight projection technique designed to remove\nrefusal-sensitive directions, and conduct a controlled evaluation across a\ngranular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside\nwidely used open baselines. For each of 20 systems, original and abliterated,\nwe issue 100 prompts with balanced harmful and harmless cases, classify\nresponses as **Refusal** or **Non-Refusal** using multiple judges, and validate\njudge fidelity on a small human-labeled subset. We also probe whether models\ncan identify refusal in their own outputs. Our study produces a\ncheckpoint-level characterization of which data-centric safety components\nremain robust under abliteration, quantifies how judge selection influences\nevaluation outcomes, and outlines a practical protocol for integrating\ninference-time edits into safety assessments. Code:\nhttps://github.com/shashankskagnihotri/safety_pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-weight LLMs can be modified at inference time with simple activation\nedits, which raises a practical question for safety: do common safety\ninterventions like refusal training or metatag training survive such edits? We\nstudy model abliteration, a lightweight projection technique designed to remove\nrefusal-sensitive directions, and conduct a controlled evaluation across a\ngranular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside\nwidely used open baselines. For each of 20 systems, original and abliterated,\nwe issue 100 prompts with balanced harmful and harmless cases, classify\nresponses as **Refusal** or **Non-Refusal** using multiple judges, and validate\njudge fidelity on a small human-labeled subset. We also probe whether models\ncan identify refusal in their own outputs. Our study produces a\ncheckpoint-level characterization of which data-centric safety components\nremain robust under abliteration, quantifies how judge selection influences\nevaluation outcomes, and outlines a practical protocol for integrating\ninference-time edits into safety assessments. Code:\nhttps://github.com/shashankskagnihotri/safety_pretraining."
                },
                "authors": [
                    {
                        "name": "Shashank Agnihotri"
                    },
                    {
                        "name": "Jonas Jakubassa"
                    },
                    {
                        "name": "Priyam Dey"
                    },
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Venkatesh Babu Radhakrishnan"
                    },
                    {
                        "name": "Margret Keuper"
                    }
                ],
                "author_detail": {
                    "name": "Margret Keuper"
                },
                "author": "Margret Keuper",
                "arxiv_comment": "Accepted at NeurIPS 2025 bWorkshop Lock-LLM. *Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10620v2",
                "updated": "2025-10-03T06:59:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    59,
                    4,
                    4,
                    276,
                    0
                ],
                "published": "2025-03-13T17:57:32Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    57,
                    32,
                    3,
                    72,
                    0
                ],
                "title": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM"
                },
                "summary": "We introduce Spire, a speech-augmented language model (LM) capable of both\ntranslating and transcribing speech input from English into 10 other languages\nas well as translating text input in both language directions. Spire integrates\nthe speech modality into an existing multilingual LM via speech discretization\nand continued pre-training using only 42.5K hours of speech. In particular, we\nadopt the pretraining framework of multilingual LMs and treat discretized\nspeech input as an additional translation language. This approach not only\nequips the model with speech capabilities, but also preserves its strong\ntext-based performance. We achieve this using significantly less data than\nexisting speech LMs, demonstrating that discretized speech input integration as\nan additional language is feasible during LM adaptation. We make our code and\nmodels available to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Spire, a speech-augmented language model (LM) capable of both\ntranslating and transcribing speech input from English into 10 other languages\nas well as translating text input in both language directions. Spire integrates\nthe speech modality into an existing multilingual LM via speech discretization\nand continued pre-training using only 42.5K hours of speech. In particular, we\nadopt the pretraining framework of multilingual LMs and treat discretized\nspeech input as an additional translation language. This approach not only\nequips the model with speech capabilities, but also preserves its strong\ntext-based performance. We achieve this using significantly less data than\nexisting speech LMs, demonstrating that discretized speech input integration as\nan additional language is feasible during LM adaptation. We make our code and\nmodels available to the community."
                },
                "authors": [
                    {
                        "name": "Kshitij Ambilduke"
                    },
                    {
                        "name": "Ben Peters"
                    },
                    {
                        "name": "Sonal Sannigrahi"
                    },
                    {
                        "name": "Anil Keshwani"
                    },
                    {
                        "name": "Tsz Kin Lam"
                    },
                    {
                        "name": "Bruno Martins"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    },
                    {
                        "name": "Marcely Zanon Boito"
                    }
                ],
                "author_detail": {
                    "name": "Marcely Zanon Boito"
                },
                "author": "Marcely Zanon Boito",
                "arxiv_comment": "EMNLP 2025 (Findings) camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02759v1",
                "updated": "2025-10-03T06:43:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    35,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:43:35Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    35,
                    4,
                    276,
                    0
                ],
                "title": "Prototyping Digital Social Spaces through Metaphor-Driven Design:\n  Translating Spatial Concepts into an Interactive Social Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prototyping Digital Social Spaces through Metaphor-Driven Design:\n  Translating Spatial Concepts into an Interactive Social Simulation"
                },
                "summary": "Social media platforms are central to communication, yet their designs remain\nnarrowly focused on engagement and scale. While researchers have proposed\nalternative visions for online spaces, these ideas are difficult to prototype\nwithin platform constraints. In this paper, we introduce a metaphor-driven\nsystem to help users imagine and explore new social media environments. The\nsystem translates users' metaphors into structured sets of platform features\nand generates interactive simulations populated with LLM-driven agents. To\nevaluate this approach, we conducted a study where participants created and\ninteracted with simulated social media spaces. Our findings show that metaphors\nallow users to express distinct social expectations, and that perceived\nauthenticity of the simulation depended on how well it captured dynamics like\nintimacy, participation, and temporal engagement. We conclude by discussing how\nmetaphor-driven simulation can be a powerful design tool for prototyping\nalternative social architectures and expanding the design space for future\nsocial platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media platforms are central to communication, yet their designs remain\nnarrowly focused on engagement and scale. While researchers have proposed\nalternative visions for online spaces, these ideas are difficult to prototype\nwithin platform constraints. In this paper, we introduce a metaphor-driven\nsystem to help users imagine and explore new social media environments. The\nsystem translates users' metaphors into structured sets of platform features\nand generates interactive simulations populated with LLM-driven agents. To\nevaluate this approach, we conducted a study where participants created and\ninteracted with simulated social media spaces. Our findings show that metaphors\nallow users to express distinct social expectations, and that perceived\nauthenticity of the simulation depended on how well it captured dynamics like\nintimacy, participation, and temporal engagement. We conclude by discussing how\nmetaphor-driven simulation can be a powerful design tool for prototyping\nalternative social architectures and expanding the design space for future\nsocial platforms."
                },
                "authors": [
                    {
                        "name": "Yoojin Hong"
                    },
                    {
                        "name": "Martina Di Paola"
                    },
                    {
                        "name": "Braahmi Padmakumar"
                    },
                    {
                        "name": "Hwi Joon Lee"
                    },
                    {
                        "name": "Mahnoor Shafiq"
                    },
                    {
                        "name": "Joseph Seering"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Seering"
                },
                "author": "Joseph Seering",
                "arxiv_comment": "25 pages, in submission to CHI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02758v1",
                "updated": "2025-10-03T06:43:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:43:24Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "title": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling"
                },
                "summary": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Chuheng Du"
                    },
                    {
                        "name": "Renyuan Liu"
                    },
                    {
                        "name": "Shuochao Yao"
                    },
                    {
                        "name": "Dingtian Yan"
                    },
                    {
                        "name": "Jiang Liao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Accepted by EuroSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02752v1",
                "updated": "2025-10-03T06:32:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    32,
                    10,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:32:10Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    32,
                    10,
                    4,
                    276,
                    0
                ],
                "title": "The Path of Self-Evolving Large Language Models: Achieving\n  Data-Efficient Learning via Intrinsic Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Path of Self-Evolving Large Language Models: Achieving\n  Data-Efficient Learning via Intrinsic Feedback"
                },
                "summary": "Reinforcement learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of large language models (LLMs), but such training\ntypically demands substantial efforts in creating and annotating data. In this\nwork, we explore improving LLMs through RL with minimal data. Our approach\nalternates between the LLM proposing a task and then attempting to solve it. To\nminimize data dependency, we introduce two novel mechanisms grounded in\nself-awareness: (1) self-aware difficulty prediction, where the model learns to\nassess task difficulty relative to its own abilities and prioritize challenging\nyet solvable tasks, and (2) self-aware limit breaking, where the model\nrecognizes when a task is beyond its capability boundary and proactively\nrequests external data to break through that limit. Extensive experiments on\nnine benchmarks showing a 53.8% relative improvement with less than 1.2% extra\ndata demonstrate the efficacy of self-aware RL and underscore the promise of\nself-evolving agent training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of large language models (LLMs), but such training\ntypically demands substantial efforts in creating and annotating data. In this\nwork, we explore improving LLMs through RL with minimal data. Our approach\nalternates between the LLM proposing a task and then attempting to solve it. To\nminimize data dependency, we introduce two novel mechanisms grounded in\nself-awareness: (1) self-aware difficulty prediction, where the model learns to\nassess task difficulty relative to its own abilities and prioritize challenging\nyet solvable tasks, and (2) self-aware limit breaking, where the model\nrecognizes when a task is beyond its capability boundary and proactively\nrequests external data to break through that limit. Extensive experiments on\nnine benchmarks showing a 53.8% relative improvement with less than 1.2% extra\ndata demonstrate the efficacy of self-aware RL and underscore the promise of\nself-evolving agent training."
                },
                "authors": [
                    {
                        "name": "Hangfan Zhang"
                    },
                    {
                        "name": "Siyuan Xu"
                    },
                    {
                        "name": "Zhimeng Guo"
                    },
                    {
                        "name": "Huaisheng Zhu"
                    },
                    {
                        "name": "Shicheng Liu"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Shuyue Hu"
                    }
                ],
                "author_detail": {
                    "name": "Shuyue Hu"
                },
                "author": "Shuyue Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02750v1",
                "updated": "2025-10-03T06:27:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:27:33Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "title": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models"
                },
                "summary": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks."
                },
                "authors": [
                    {
                        "name": "Lihua Zhou"
                    },
                    {
                        "name": "Mao Ye"
                    },
                    {
                        "name": "Shuaifeng Li"
                    },
                    {
                        "name": "Nianxin Li"
                    },
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Lei Deng"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00415v2",
                "updated": "2025-10-03T06:17:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    17,
                    38,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-01T12:33:23Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    12,
                    33,
                    23,
                    5,
                    32,
                    0
                ],
                "title": "MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents"
                },
                "summary": "MarketSenseAI is a novel framework for holistic stock analysis which\nleverages Large Language Models (LLMs) to process financial news, historical\nprices, company fundamentals and the macroeconomic environment to support\ndecision making in stock analysis and selection. In this paper, we present the\nlatest advancements on MarketSenseAI, driven by rapid technological expansion\nin LLMs. Through a novel architecture combining Retrieval-Augmented Generation\nand LLM agents, the framework processes SEC filings and earnings calls, while\nenriching macroeconomic analysis through systematic processing of diverse\ninstitutional reports. We demonstrate a significant improvement in fundamental\nanalysis accuracy over the previous version. Empirical evaluation on S\\&P 100\nstocks over two years (2023-2024) shows MarketSenseAI achieving cumulative\nreturns of 125.9% compared to the index return of 73.5%, while maintaining\ncomparable risk profiles. Further validation on S\\&P 500 stocks during 2024\ndemonstrates the framework's scalability, delivering a 33.8% higher Sortino\nratio than the market. This work marks a significant advancement in applying\nLLM technology to financial analysis, offering insights into the robustness of\nLLM-driven investment strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MarketSenseAI is a novel framework for holistic stock analysis which\nleverages Large Language Models (LLMs) to process financial news, historical\nprices, company fundamentals and the macroeconomic environment to support\ndecision making in stock analysis and selection. In this paper, we present the\nlatest advancements on MarketSenseAI, driven by rapid technological expansion\nin LLMs. Through a novel architecture combining Retrieval-Augmented Generation\nand LLM agents, the framework processes SEC filings and earnings calls, while\nenriching macroeconomic analysis through systematic processing of diverse\ninstitutional reports. We demonstrate a significant improvement in fundamental\nanalysis accuracy over the previous version. Empirical evaluation on S\\&P 100\nstocks over two years (2023-2024) shows MarketSenseAI achieving cumulative\nreturns of 125.9% compared to the index return of 73.5%, while maintaining\ncomparable risk profiles. Further validation on S\\&P 500 stocks during 2024\ndemonstrates the framework's scalability, delivering a 33.8% higher Sortino\nratio than the market. This work marks a significant advancement in applying\nLLM technology to financial analysis, offering insights into the robustness of\nLLM-driven investment strategies."
                },
                "authors": [
                    {
                        "name": "George Fatouros"
                    },
                    {
                        "name": "Kostas Metaxas"
                    },
                    {
                        "name": "John Soldatos"
                    },
                    {
                        "name": "Manos Karathanassis"
                    }
                ],
                "author_detail": {
                    "name": "Manos Karathanassis"
                },
                "author": "Manos Karathanassis",
                "arxiv_comment": "25 pages, 7 figures, Under review at Financial Innovation (FIN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50, 91G10, 91G15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]