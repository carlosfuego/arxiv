[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11273v1",
                "updated": "2025-07-15T12:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T12:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding"
                },
                "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v3",
                "updated": "2025-07-15T11:31:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    31,
                    14,
                    1,
                    196,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag) accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11121v1",
                "updated": "2025-07-15T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "title": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging"
                },
                "summary": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging."
                },
                "authors": [
                    {
                        "name": "Tatsunori Shibuya"
                    },
                    {
                        "name": "Eichi Terasawa"
                    },
                    {
                        "name": "Hiromi Kimura"
                    },
                    {
                        "name": "Takeshi Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Fujiwara"
                },
                "author": "Takeshi Fujiwara",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11067v1",
                "updated": "2025-07-15T08:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T08:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit"
                },
                "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Lin Gan"
                    },
                    {
                        "name": "Wubing Wan"
                    },
                    {
                        "name": "Zeyu Song"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Lanke He"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zekun Yin"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "arxiv_comment": "Yinuo Wang and Tianqi Mao contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v1",
                "updated": "2025-07-14T20:38:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v2",
                "updated": "2025-07-14T19:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    51,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10757v1",
                "updated": "2025-07-14T19:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:31:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block"
                },
                "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo."
                },
                "authors": [
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v4",
                "updated": "2025-07-14T18:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    18,
                    22,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v1",
                "updated": "2025-07-14T17:49:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v4",
                "updated": "2025-07-11T22:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    22,
                    14,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v1",
                "updated": "2025-07-11T21:19:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v3",
                "updated": "2025-07-11T19:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    19,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing"
                },
                "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Cristbal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "Jos Mara Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Muoz"
                    },
                    {
                        "name": "Manuel Gil Prez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10579v1",
                "updated": "2025-07-11T10:57:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:57:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors"
                },
                "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain."
                },
                "authors": [
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Anas Tack"
                    },
                    {
                        "name": "Justin Vasselli"
                    }
                ],
                "author_detail": {
                    "name": "Justin Vasselli"
                },
                "author": "Justin Vasselli",
                "arxiv_comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "Jos Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "Jos Emilio Labra Gayo"
                },
                "author": "Jos Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08278v1",
                "updated": "2025-07-11T02:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "title": "Observation of the electric Breit-Rabi Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of the electric Breit-Rabi Effect"
                },
                "summary": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions."
                },
                "authors": [
                    {
                        "name": "S. -Z. Wang"
                    },
                    {
                        "name": "S. -B. Wang"
                    },
                    {
                        "name": "Z. -J. Tao"
                    },
                    {
                        "name": "T. Xia"
                    },
                    {
                        "name": "Z. -T. Lu"
                    }
                ],
                "author_detail": {
                    "name": "Z. -T. Lu"
                },
                "author": "Z. -T. Lu",
                "arxiv_doi": "10.1073/pnas.2423902122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2423902122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_journal_ref": "122 (26)e2423902122 June 27 2025",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08232v1",
                "updated": "2025-07-11T00:36:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T00:36:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08143v1",
                "updated": "2025-07-10T20:03:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores"
                },
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07990v1",
                "updated": "2025-07-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs"
                },
                "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm."
                },
                "authors": [
                    {
                        "name": "Jeongseok Hyun"
                    },
                    {
                        "name": "Sukjun Hwang"
                    },
                    {
                        "name": "Su Ho Han"
                    },
                    {
                        "name": "Taeoh Kim"
                    },
                    {
                        "name": "Inwoong Lee"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Joon-Young Lee"
                    },
                    {
                        "name": "Seon Joo Kim"
                    },
                    {
                        "name": "Minho Shim"
                    }
                ],
                "author_detail": {
                    "name": "Minho Shim"
                },
                "author": "Minho Shim",
                "arxiv_comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v1",
                "updated": "2025-07-10T17:47:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code and models are available at https://github.com/NVlabs/Long-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v3",
                "updated": "2025-07-10T17:10:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    10,
                    49,
                    3,
                    191,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07400v1",
                "updated": "2025-07-10T03:39:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T03:39:23Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows"
                },
                "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows."
                },
                "authors": [
                    {
                        "name": "Zaifeng Pan"
                    },
                    {
                        "name": "Ajjkumar Patel"
                    },
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Yipeng Shen"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Wan-Lu Li"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Yufei Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ding"
                },
                "author": "Yufei Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v1",
                "updated": "2025-07-10T01:51:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07290v1",
                "updated": "2025-07-09T21:18:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T21:18:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics"
                },
                "summary": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults."
                },
                "authors": [
                    {
                        "name": "M. Karakaya"
                    },
                    {
                        "name": "I. Gurbuz"
                    },
                    {
                        "name": "L. Fulanovic"
                    },
                    {
                        "name": "U. Adem"
                    }
                ],
                "author_detail": {
                    "name": "U. Adem"
                },
                "author": "U. Adem",
                "arxiv_doi": "10.1039/D4TC01735H",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1039/D4TC01735H",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted version of the article published in J. Mater. Chem. C. 10\n  Pages, 7 Figures. Plus SI file as a single pdf",
                "arxiv_journal_ref": "J. Mater. Chem. C, 2024,12, 19612-19619",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06739v1",
                "updated": "2025-07-09T10:53:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:53:05Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold"
                },
                "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."
                },
                "authors": [
                    {
                        "name": "Zishen Huang"
                    },
                    {
                        "name": "Chunyu Yang"
                    },
                    {
                        "name": "Mengyuan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Ren"
                },
                "author": "Mengyuan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v2",
                "updated": "2025-07-09T07:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Safety Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Safety Inference Scaling"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for\n  LLM Safety Assurance\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06567v1",
                "updated": "2025-07-09T05:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v3",
                "updated": "2025-07-09T04:43:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    4,
                    43,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06517v1",
                "updated": "2025-07-09T03:33:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T03:33:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance."
                },
                "authors": [
                    {
                        "name": "Zicong Tang"
                    },
                    {
                        "name": "Shi Luohe"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "arxiv_comment": "Accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v2",
                "updated": "2025-07-09T02:35:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    35,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted By ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v2",
                "updated": "2025-07-08T21:23:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    21,
                    23,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "21 pages, 10 figures. Supplement 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06349v1",
                "updated": "2025-07-08T19:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T19:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design"
                },
                "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization."
                },
                "authors": [
                    {
                        "name": "Erin Ransom"
                    },
                    {
                        "name": "Andrew Lim"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v3",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07061v1",
                "updated": "2025-07-08T09:20:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:20:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems"
                },
                "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems."
                },
                "authors": [
                    {
                        "name": "Shervin Ghaffari"
                    },
                    {
                        "name": "Zohre Bahranifard"
                    },
                    {
                        "name": "Mohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Akbari"
                },
                "author": "Mohammad Akbari",
                "arxiv_comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v4",
                "updated": "2025-07-08T07:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03622v3",
                "updated": "2025-07-08T02:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    15,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2023-06-06T12:19:05Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    12,
                    19,
                    5,
                    1,
                    157,
                    0
                ],
                "title": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference"
                },
                "summary": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Xiaonan Luo"
                    },
                    {
                        "name": "Zhuohao Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Yu Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yu Ding"
                },
                "author": "Yu Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v2",
                "updated": "2025-07-08T00:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    51,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07120v1",
                "updated": "2025-07-07T19:47:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T19:47:24Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding"
                },
                "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical."
                },
                "authors": [
                    {
                        "name": "Nidhi Bhatia"
                    },
                    {
                        "name": "Ankit More"
                    },
                    {
                        "name": "Ritika Borkar"
                    },
                    {
                        "name": "Tiyasa Mitra"
                    },
                    {
                        "name": "Ramon Matas"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Maximilian Golub"
                    },
                    {
                        "name": "Dheevatsa Mudigere"
                    },
                    {
                        "name": "Brian Pharris"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    }
                ],
                "author_detail": {
                    "name": "Bita Darvish Rouhani"
                },
                "author": "Bita Darvish Rouhani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v2",
                "updated": "2025-07-07T09:25:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    25,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v1",
                "updated": "2025-07-06T15:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v2",
                "updated": "2025-07-05T15:51:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    51,
                    57,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas Khler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v2",
                "updated": "2025-07-05T15:40:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    40,
                    51,
                    5,
                    186,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v2",
                "updated": "2025-07-05T13:37:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    37,
                    48,
                    5,
                    186,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Gabriel Franco"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03980v1",
                "updated": "2025-07-05T10:11:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T10:11:37Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "title": "Combination generators with optimal cache utilization and communication\n  free parallel execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combination generators with optimal cache utilization and communication\n  free parallel execution"
                },
                "summary": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators."
                },
                "authors": [
                    {
                        "name": "Xi He"
                    },
                    {
                        "name": "Max. A. Little"
                    }
                ],
                "author_detail": {
                    "name": "Max. A. Little"
                },
                "author": "Max. A. Little",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03919v1",
                "updated": "2025-07-05T06:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T06:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "title": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery"
                },
                "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design"
                },
                "authors": [
                    {
                        "name": "Duy Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy Le"
                },
                "author": "Duy Le",
                "arxiv_comment": "6 pages, 3 figures, 3 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v3",
                "updated": "2025-07-05T01:08:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    1,
                    8,
                    40,
                    5,
                    186,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Sean Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "36 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v1",
                "updated": "2025-07-04T21:09:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. Khn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Khn"
                },
                "author": "Martin J. Khn",
                "arxiv_comment": "29 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03445v1",
                "updated": "2025-07-04T10:01:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "summary": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results."
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03396v1",
                "updated": "2025-07-04T09:03:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:03:18Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "title": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge"
                },
                "summary": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains."
                },
                "authors": [
                    {
                        "name": "Fariborz Momtazzadeh"
                    },
                    {
                        "name": "Farshad Sohbatzadeh"
                    },
                    {
                        "name": "Hamed Soltani Ahmadi"
                    },
                    {
                        "name": "Ramin Mehrabifard"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Mehrabifard"
                },
                "author": "Ramin Mehrabifard",
                "arxiv_doi": "10.1007/S40042-025-01392-9.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/S40042-025-01392-9.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.03396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v3",
                "updated": "2025-07-04T06:49:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    31,
                    4,
                    185,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v3",
                "updated": "2025-07-04T06:36:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    36,
                    38,
                    4,
                    185,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03231v1",
                "updated": "2025-07-04T00:16:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T00:16:15Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "title": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching"
                },
                "summary": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Brian Plancher"
                    }
                ],
                "author_detail": {
                    "name": "Brian Plancher"
                },
                "author": "Brian Plancher",
                "arxiv_comment": "Accepted to IROS 2025, 7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03153v1",
                "updated": "2025-07-03T20:20:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T20:20:33Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "title": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference"
                },
                "summary": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware."
                },
                "authors": [
                    {
                        "name": "Weishu Deng"
                    },
                    {
                        "name": "Yujie Yang"
                    },
                    {
                        "name": "Peiran Du"
                    },
                    {
                        "name": "Lingfeng Xiang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Jia Rao"
                    }
                ],
                "author_detail": {
                    "name": "Jia Rao"
                },
                "author": "Jia Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04789v2",
                "updated": "2025-07-03T17:11:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    11,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2023-12-08T02:03:55Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    2,
                    3,
                    55,
                    4,
                    342,
                    0
                ],
                "title": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System"
                },
                "summary": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jishen Zhao"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_doi": "10.1145/3676642.3736119",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736119",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appears in the Proceedings of the 30th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems,\n  Volume 3 (ASPLOS 25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v3",
                "updated": "2025-07-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    22,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02397v1",
                "updated": "2025-07-03T07:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics"
                },
                "summary": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li",
                "arxiv_comment": "16 pages, 5 figures, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02227v1",
                "updated": "2025-07-03T01:22:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations"
                },
                "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications."
                },
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02006v1",
                "updated": "2025-07-02T00:35:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T00:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design"
                },
                "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Shakya Jayakody"
                    },
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v1",
                "updated": "2025-07-01T22:27:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01154v1",
                "updated": "2025-07-01T19:28:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T19:28:37Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDP: Private Training Large Language Models with Efficient DP-SGD"
                },
                "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."
                },
                "authors": [
                    {
                        "name": "Liangyu Wang"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zihang Xiang"
                    },
                    {
                        "name": "David E. Keyes"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00797v1",
                "updated": "2025-07-01T14:30:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T14:30:31Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator"
                },
                "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."
                },
                "authors": [
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Haroon Waris"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Jianfei Jiang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Guanghui He"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui He"
                },
                "author": "Guanghui He",
                "arxiv_comment": "DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00727v1",
                "updated": "2025-07-01T13:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T13:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "title": "On Hierarchical Coded Caching with Offline Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Hierarchical Coded Caching with Offline Users"
                },
                "summary": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A short version of this is accepted for presentation in 2025 IEEE\n  Information Theory Workshop; 8 pages, one figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00716v1",
                "updated": "2025-07-01T12:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:51:09Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "title": "Accelerating Loading WebGraphs in ParaGrapher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Loading WebGraphs in ParaGrapher"
                },
                "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Koohi Esfahani"
                },
                "author": "Mohsen Koohi Esfahani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00715v1",
                "updated": "2025-07-01T12:42:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:42:06Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "title": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens"
                },
                "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Xianjing Han"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00614v1",
                "updated": "2025-07-01T09:47:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T09:47:38Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "title": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications"
                },
                "summary": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart."
                },
                "authors": [
                    {
                        "name": "B. Ramachandran"
                    },
                    {
                        "name": "N. Sudarshan"
                    },
                    {
                        "name": "G. Mangamma"
                    },
                    {
                        "name": "M. S. Ramachandra Rao"
                    }
                ],
                "author_detail": {
                    "name": "M. S. Ramachandra Rao"
                },
                "author": "M. S. Ramachandra Rao",
                "arxiv_doi": "10.1007/s10832-025-00423-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10832-025-00423-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 7 figures, 1 Table and Accepted for publication in Journal\n  of Electroceramics",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00462v1",
                "updated": "2025-07-01T06:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T06:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation"
                },
                "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training."
                },
                "authors": [
                    {
                        "name": "Jizhou Han"
                    },
                    {
                        "name": "Chenhao Ding"
                    },
                    {
                        "name": "SongLin Dong"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Xinyuan Gao"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.13353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13353v1",
                "updated": "2025-07-17T17:59:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    59,
                    59,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:59:59Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    59,
                    59,
                    3,
                    198,
                    0
                ],
                "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal\n  Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoITG: Multimodal Video Understanding with Instructed Temporal\n  Grounding"
                },
                "summary": "Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding."
                },
                "authors": [
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "De-an Huang"
                    },
                    {
                        "name": "Zhiqi Li"
                    },
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Guilin Li"
                    },
                    {
                        "name": "Jose M. Alvarez"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Zhiding Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiding Yu"
                },
                "author": "Zhiding Yu",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13348v1",
                "updated": "2025-07-17T17:59:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    59,
                    55,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:59:55Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    59,
                    55,
                    3,
                    198,
                    0
                ],
                "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning"
                },
                "summary": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink."
                },
                "authors": [
                    {
                        "name": "Senqiao Yang"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Xin Lai"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Code and models are available at\n  https://github.com/dvlab-research/VisionThink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13343v1",
                "updated": "2025-07-17T17:59:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    59,
                    10,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:59:10Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    59,
                    10,
                    3,
                    198,
                    0
                ],
                "title": "Taming Diffusion Transformer for Real-Time Mobile Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Diffusion Transformer for Real-Time Mobile Video Generation"
                },
                "summary": "Diffusion Transformers (DiT) have shown strong performance in video\ngeneration tasks, but their high computational cost makes them impractical for\nresource-constrained devices like smartphones, and real-time generation is even\nmore challenging. In this work, we propose a series of novel optimizations to\nsignificantly accelerate video generation and enable real-time performance on\nmobile platforms. First, we employ a highly compressed variational autoencoder\n(VAE) to reduce the dimensionality of the input data without sacrificing visual\nquality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning\nstrategy to shrink the model size to suit mobile platform while preserving\ncritical performance characteristics. Third, we develop an adversarial step\ndistillation technique tailored for DiT, which allows us to reduce the number\nof inference steps to four. Combined, these optimizations enable our model to\nachieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max,\ndemonstrating the feasibility of real-time, high-quality video generation on\nmobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have shown strong performance in video\ngeneration tasks, but their high computational cost makes them impractical for\nresource-constrained devices like smartphones, and real-time generation is even\nmore challenging. In this work, we propose a series of novel optimizations to\nsignificantly accelerate video generation and enable real-time performance on\nmobile platforms. First, we employ a highly compressed variational autoencoder\n(VAE) to reduce the dimensionality of the input data without sacrificing visual\nquality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning\nstrategy to shrink the model size to suit mobile platform while preserving\ncritical performance characteristics. Third, we develop an adversarial step\ndistillation technique tailored for DiT, which allows us to reduce the number\nof inference steps to four. Combined, these optimizations enable our model to\nachieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max,\ndemonstrating the feasibility of real-time, high-quality video generation on\nmobile devices."
                },
                "authors": [
                    {
                        "name": "Yushu Wu"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Anil Kag"
                    },
                    {
                        "name": "Ivan Skorokhodov"
                    },
                    {
                        "name": "Willi Menapace"
                    },
                    {
                        "name": "Ke Ma"
                    },
                    {
                        "name": "Arpit Sahni"
                    },
                    {
                        "name": "Ju Hu"
                    },
                    {
                        "name": "Aliaksandr Siarohin"
                    },
                    {
                        "name": "Dhritiman Sagar"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Sergey Tulyakov"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Tulyakov"
                },
                "author": "Sergey Tulyakov",
                "arxiv_comment": "9 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01772v2",
                "updated": "2025-07-17T17:58:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    58,
                    50,
                    3,
                    198,
                    0
                ],
                "published": "2024-10-02T17:29:34Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    29,
                    34,
                    2,
                    276,
                    0
                ],
                "title": "DeFine: Decision-Making with Analogical Reasoning over Factor Profiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFine: Decision-Making with Analogical Reasoning over Factor Profiles"
                },
                "summary": "LLMs are ideal for decision-making thanks to their ability to reason over\nlong contexts. However, challenges arise when processing speech transcripts\nthat describe complex scenarios, as they are verbose and include repetition,\nhedging, and vagueness. E.g., during a company's earnings call, an executive\nmight project a positive revenue outlook to reassure investors, despite\nuncertainty regarding future earnings. It is crucial for LLMs to incorporate\nthis uncertainty systematically when making decisions. In this paper, we\nintroduce \\textsc{DeFine}, a modular framework that constructs probabilistic\nfactor profiles from complex scenarios. It then integrates these profiles with\nanalogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in new situations. Our framework\nseparates the tasks of quantifying uncertainty and incorporating it into LLM\ndecision-making. This approach is particularly useful in areas such as\nconsulting and financial deliberation, where making decisions under uncertainty\nis vital.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are ideal for decision-making thanks to their ability to reason over\nlong contexts. However, challenges arise when processing speech transcripts\nthat describe complex scenarios, as they are verbose and include repetition,\nhedging, and vagueness. E.g., during a company's earnings call, an executive\nmight project a positive revenue outlook to reassure investors, despite\nuncertainty regarding future earnings. It is crucial for LLMs to incorporate\nthis uncertainty systematically when making decisions. In this paper, we\nintroduce \\textsc{DeFine}, a modular framework that constructs probabilistic\nfactor profiles from complex scenarios. It then integrates these profiles with\nanalogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in new situations. Our framework\nseparates the tasks of quantifying uncertainty and incorporating it into LLM\ndecision-making. This approach is particularly useful in areas such as\nconsulting and financial deliberation, where making decisions under uncertainty\nis vital."
                },
                "authors": [
                    {
                        "name": "Yebowen Hu"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Yiming Lu"
                    },
                    {
                        "name": "Daoan Zhang"
                    },
                    {
                        "name": "Hassan Foroosh"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Fei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Liu"
                },
                "author": "Fei Liu",
                "arxiv_comment": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025), Vienna, Austria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13339v1",
                "updated": "2025-07-17T17:57:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    57,
                    18,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:57:18Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    57,
                    18,
                    3,
                    198,
                    0
                ],
                "title": "SpectraLift: Physics-Guided Spectral-Inversion Network for\n  Self-Supervised Hyperspectral Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpectraLift: Physics-Guided Spectral-Inversion Network for\n  Self-Supervised Hyperspectral Image Super-Resolution"
                },
                "summary": "High-spatial-resolution hyperspectral images (HSI) are essential for\napplications such as remote sensing and medical imaging, yet HSI sensors\ninherently trade spatial detail for spectral richness. Fusing\nhigh-spatial-resolution multispectral images (HR-MSI) with\nlow-spatial-resolution hyperspectral images (LR-HSI) is a promising route to\nrecover fine spatial structures without sacrificing spectral fidelity. Most\nstate-of-the-art methods for HSI-MSI fusion demand point spread function (PSF)\ncalibration or ground truth high resolution HSI (HR-HSI), both of which are\nimpractical to obtain in real world settings. We present SpectraLift, a fully\nself-supervised framework that fuses LR-HSI and HR-MSI inputs using only the\nMSI's Spectral Response Function (SRF). SpectraLift trains a lightweight\nper-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic\nlow-spatial-resolution multispectral image (LR-MSI) obtained by applying the\nSRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an\n$\\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as\nthe optimization objective. At inference, SpectraLift uses the trained network\nto map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in\nminutes, is agnostic to spatial blur and resolution, and outperforms\nstate-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-spatial-resolution hyperspectral images (HSI) are essential for\napplications such as remote sensing and medical imaging, yet HSI sensors\ninherently trade spatial detail for spectral richness. Fusing\nhigh-spatial-resolution multispectral images (HR-MSI) with\nlow-spatial-resolution hyperspectral images (LR-HSI) is a promising route to\nrecover fine spatial structures without sacrificing spectral fidelity. Most\nstate-of-the-art methods for HSI-MSI fusion demand point spread function (PSF)\ncalibration or ground truth high resolution HSI (HR-HSI), both of which are\nimpractical to obtain in real world settings. We present SpectraLift, a fully\nself-supervised framework that fuses LR-HSI and HR-MSI inputs using only the\nMSI's Spectral Response Function (SRF). SpectraLift trains a lightweight\nper-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic\nlow-spatial-resolution multispectral image (LR-MSI) obtained by applying the\nSRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an\n$\\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as\nthe optimization objective. At inference, SpectraLift uses the trained network\nto map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in\nminutes, is agnostic to spatial blur and resolution, and outperforms\nstate-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks."
                },
                "authors": [
                    {
                        "name": "Ritik Shah"
                    },
                    {
                        "name": "Marco F. Duarte"
                    }
                ],
                "author_detail": {
                    "name": "Marco F. Duarte"
                },
                "author": "Marco F. Duarte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13335v1",
                "updated": "2025-07-17T17:51:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    51,
                    20,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:51:20Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    51,
                    20,
                    3,
                    198,
                    0
                ],
                "title": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour\n  Understanding from Traditional Puns to Topical Jokes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour\n  Understanding from Traditional Puns to Topical Jokes"
                },
                "summary": "Humour, as a complex language form, is derived from myriad aspects of life,\nwhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes. In this work, we investigate whether the ability of\nLarge Language Models (LLMs) to explain humour depends on the particular humour\nform. We compare models on simple puns and more complex topical humour that\nrequires knowledge of real-world entities and events. In doing so, we curate a\ndataset of 600 jokes split across 4 joke types and manually write high-quality\nexplanations. These jokes include heterographic and homographic puns,\ncontemporary internet humour, and topical jokes, where understanding relies on\nreasoning beyond \"common sense\", rooted instead in world knowledge regarding\nnews events and pop culture. Using this dataset, we compare the zero-shot\nabilities of a range of LLMs to accurately and comprehensively explain jokes of\ndifferent types, identifying key research gaps in the task of humour\nexplanation. We find that none of the tested models (inc. reasoning models) are\ncapable of reliably generating adequate explanations of all joke types, further\nhighlighting the narrow focus of most works in computational humour on overly\nsimple joke forms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humour, as a complex language form, is derived from myriad aspects of life,\nwhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes. In this work, we investigate whether the ability of\nLarge Language Models (LLMs) to explain humour depends on the particular humour\nform. We compare models on simple puns and more complex topical humour that\nrequires knowledge of real-world entities and events. In doing so, we curate a\ndataset of 600 jokes split across 4 joke types and manually write high-quality\nexplanations. These jokes include heterographic and homographic puns,\ncontemporary internet humour, and topical jokes, where understanding relies on\nreasoning beyond \"common sense\", rooted instead in world knowledge regarding\nnews events and pop culture. Using this dataset, we compare the zero-shot\nabilities of a range of LLMs to accurately and comprehensively explain jokes of\ndifferent types, identifying key research gaps in the task of humour\nexplanation. We find that none of the tested models (inc. reasoning models) are\ncapable of reliably generating adequate explanations of all joke types, further\nhighlighting the narrow focus of most works in computational humour on overly\nsimple joke forms."
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "William Thorne"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13334v1",
                "updated": "2025-07-17T17:50:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    50,
                    36,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:50:36Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    50,
                    36,
                    3,
                    198,
                    0
                ],
                "title": "A Survey of Context Engineering for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Context Engineering for Large Language Models"
                },
                "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI."
                },
                "authors": [
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Jiayu Yao"
                    },
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Jiazhi Liu"
                    },
                    {
                        "name": "Mingyu Li"
                    },
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Chenlin Zhou"
                    },
                    {
                        "name": "Jiayi Mao"
                    },
                    {
                        "name": "Tianze Xia"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Shenghua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shenghua Liu"
                },
                "author": "Shenghua Liu",
                "arxiv_comment": "ongoing work; 165 pages, 1401 citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13332v1",
                "updated": "2025-07-17T17:50:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    50,
                    7,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:50:07Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    50,
                    7,
                    3,
                    198,
                    0
                ],
                "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner"
                },
                "summary": "Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data."
                },
                "authors": [
                    {
                        "name": "Zhouqi Hua"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Chengqi Lyu"
                    },
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13326v1",
                "updated": "2025-07-17T17:45:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    45,
                    9,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:45:09Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    45,
                    9,
                    3,
                    198,
                    0
                ],
                "title": "A Real-Time System for Egocentric Hand-Object Interaction Detection in\n  Industrial Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Real-Time System for Egocentric Hand-Object Interaction Detection in\n  Industrial Domains"
                },
                "summary": "Hand-object interaction detection remains an open challenge in real-time\napplications, where intuitive user experiences depend on fast and accurate\ndetection of interactions with surrounding objects. We propose an efficient\napproach for detecting hand-objects interactions from streaming egocentric\nvision that operates in real time. Our approach consists of an action\nrecognition module and an object detection module for identifying active\nobjects upon confirmed interaction. Our Mamba model with EfficientNetV2 as\nbackbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark\nat 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object.\nWe implement our models in a cascaded architecture where the action recognition\nand object detection modules operate sequentially. When the action recognition\npredicts a contact state, it activates the object detection module, which in\nturn performs inference on the relevant frame to detect and classify the active\nobject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hand-object interaction detection remains an open challenge in real-time\napplications, where intuitive user experiences depend on fast and accurate\ndetection of interactions with surrounding objects. We propose an efficient\napproach for detecting hand-objects interactions from streaming egocentric\nvision that operates in real time. Our approach consists of an action\nrecognition module and an object detection module for identifying active\nobjects upon confirmed interaction. Our Mamba model with EfficientNetV2 as\nbackbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark\nat 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object.\nWe implement our models in a cascaded architecture where the action recognition\nand object detection modules operate sequentially. When the action recognition\npredicts a contact state, it activates the object detection module, which in\nturn performs inference on the relevant frame to detect and classify the active\nobject."
                },
                "authors": [
                    {
                        "name": "Antonio Finocchiaro"
                    },
                    {
                        "name": "Alessandro Sebastiano Catinello"
                    },
                    {
                        "name": "Michele Mazzamuto"
                    },
                    {
                        "name": "Rosario Leonardi"
                    },
                    {
                        "name": "Antonino Furnari"
                    },
                    {
                        "name": "Giovanni Maria Farinella"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Maria Farinella"
                },
                "author": "Giovanni Maria Farinella",
                "arxiv_comment": "12 pages, 4 figures, In International Conference on Image Analysis\n  and Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13323v1",
                "updated": "2025-07-17T17:42:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    42,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:42:29Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    42,
                    29,
                    3,
                    198,
                    0
                ],
                "title": "GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic\n  Estimation using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic\n  Estimation using LLM"
                },
                "summary": "Socio-economic indicators like regional GDP, population, and education\nlevels, are crucial to shaping policy decisions and fostering sustainable\ndevelopment. This research introduces GeoReg a regression model that integrates\ndiverse data sources, including satellite imagery and web-based geospatial\ninformation, to estimate these indicators even for data-scarce regions such as\ndeveloping countries. Our approach leverages the prior knowledge of large\nlanguage model (LLM) to address the scarcity of labeled data, with the LLM\nfunctioning as a data engineer by extracting informative features to enable\neffective estimation in few-shot settings. Specifically, our model obtains\ncontextual relationships between data features and the target indicator,\ncategorizing their correlations as positive, negative, mixed, or irrelevant.\nThese features are then fed into the linear estimator with tailored weight\nconstraints for each category. To capture nonlinear patterns, the model also\nidentifies meaningful feature interactions and integrates them, along with\nnonlinear transformations. Experiments across three countries at different\nstages of development demonstrate that our model outperforms baselines in\nestimating socio-economic indicators, even for low-income countries with\nlimited data availability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socio-economic indicators like regional GDP, population, and education\nlevels, are crucial to shaping policy decisions and fostering sustainable\ndevelopment. This research introduces GeoReg a regression model that integrates\ndiverse data sources, including satellite imagery and web-based geospatial\ninformation, to estimate these indicators even for data-scarce regions such as\ndeveloping countries. Our approach leverages the prior knowledge of large\nlanguage model (LLM) to address the scarcity of labeled data, with the LLM\nfunctioning as a data engineer by extracting informative features to enable\neffective estimation in few-shot settings. Specifically, our model obtains\ncontextual relationships between data features and the target indicator,\ncategorizing their correlations as positive, negative, mixed, or irrelevant.\nThese features are then fed into the linear estimator with tailored weight\nconstraints for each category. To capture nonlinear patterns, the model also\nidentifies meaningful feature interactions and integrates them, along with\nnonlinear transformations. Experiments across three countries at different\nstages of development demonstrate that our model outperforms baselines in\nestimating socio-economic indicators, even for low-income countries with\nlimited data availability."
                },
                "authors": [
                    {
                        "name": "Kyeongjin Ahn"
                    },
                    {
                        "name": "Sungwon Han"
                    },
                    {
                        "name": "Seungeon Lee"
                    },
                    {
                        "name": "Donghyun Ahn"
                    },
                    {
                        "name": "Hyoshin Kim"
                    },
                    {
                        "name": "Jungwon Kim"
                    },
                    {
                        "name": "Jihee Kim"
                    },
                    {
                        "name": "Sangyoon Park"
                    },
                    {
                        "name": "Meeyoung Cha"
                    }
                ],
                "author_detail": {
                    "name": "Meeyoung Cha"
                },
                "author": "Meeyoung Cha",
                "arxiv_comment": "15 pages, 13 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13317v1",
                "updated": "2025-07-17T17:36:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    36,
                    13,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:36:13Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    36,
                    13,
                    3,
                    198,
                    0
                ],
                "title": "Testing halo models for constraining astrophysical feedback with\n  multi-probe modeling: I. 3D Power spectra and mass fractions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing halo models for constraining astrophysical feedback with\n  multi-probe modeling: I. 3D Power spectra and mass fractions"
                },
                "summary": "Upcoming Stage-IV surveys will deliver measurements of distribution of matter\nwith unprecedented precision, demanding highly accurate theoretical models for\ncosmological parameter inference. A major source of modeling uncertainty lies\nin astrophysical processes associated with galaxy formation and evolution,\nwhich remain poorly understood. Probes such as the thermal and kinematic\nSunyaev-Zel'dovich effects, X-rays, and dispersion measure from fast radio\nbursts offer a promising avenue for mapping the distribution and thermal\nproperties of cosmic baryons. A unified analytical framework capable of jointly\nmodeling these observables is essential for fully harnessing the complementary\ninformation while mitigating probe-specific systematics. In this work, we\npresent a detailed assessment of existing analytical models, which differ in\ntheir assumptions and prescriptions for simultaneously describing the\ndistribution of matter and baryons in the universe. Using the Magneticum\nhydrodynamical simulation, we test these models by jointly analyzing the 3D\nauto- and cross-power spectra of the matter and baryonic fields that underpin\nthe above probes. We find that all models can reproduce the power spectra at\nsub-percent to few-percent accuracy, depending on the tracer combination and\nnumber of free parameters. Their ability to recover underlying halo properties,\nsuch as the evolution of gas abundance and thermodynamic profiles with halo\nmass, varies considerably. Our results suggest that these models require\nfurther refinement and testing for reliable interpretation of multi-wavelength\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upcoming Stage-IV surveys will deliver measurements of distribution of matter\nwith unprecedented precision, demanding highly accurate theoretical models for\ncosmological parameter inference. A major source of modeling uncertainty lies\nin astrophysical processes associated with galaxy formation and evolution,\nwhich remain poorly understood. Probes such as the thermal and kinematic\nSunyaev-Zel'dovich effects, X-rays, and dispersion measure from fast radio\nbursts offer a promising avenue for mapping the distribution and thermal\nproperties of cosmic baryons. A unified analytical framework capable of jointly\nmodeling these observables is essential for fully harnessing the complementary\ninformation while mitigating probe-specific systematics. In this work, we\npresent a detailed assessment of existing analytical models, which differ in\ntheir assumptions and prescriptions for simultaneously describing the\ndistribution of matter and baryons in the universe. Using the Magneticum\nhydrodynamical simulation, we test these models by jointly analyzing the 3D\nauto- and cross-power spectra of the matter and baryonic fields that underpin\nthe above probes. We find that all models can reproduce the power spectra at\nsub-percent to few-percent accuracy, depending on the tracer combination and\nnumber of free parameters. Their ability to recover underlying halo properties,\nsuch as the evolution of gas abundance and thermodynamic profiles with halo\nmass, varies considerably. Our results suggest that these models require\nfurther refinement and testing for reliable interpretation of multi-wavelength\ndatasets."
                },
                "authors": [
                    {
                        "name": "Pranjal R. S."
                    },
                    {
                        "name": "Shivam Pandey"
                    },
                    {
                        "name": "Dhayaa Anbajagane"
                    },
                    {
                        "name": "Elisabeth Krause"
                    },
                    {
                        "name": "Klaus Dolag"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Dolag"
                },
                "author": "Klaus Dolag",
                "arxiv_comment": "Comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03780v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03780v3",
                "updated": "2025-07-17T17:31:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    31,
                    44,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-30T12:57:21Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    57,
                    21,
                    2,
                    120,
                    0
                ],
                "title": "GPU Performance Portability needs Autotuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU Performance Portability needs Autotuning"
                },
                "summary": "As LLMs grow in complexity, achieving state-of-the-art performance requires\ntight co-design across algorithms, software, and hardware. Today's reliance on\na single dominant platform limits portability, creates vendor lock-in, and\nraises barriers for new AI hardware. In this work, we make the case for\ncombining just-in-time (JIT) compilation with comprehensive kernel parameter\nautotuning to enable portable LLM inference with state-of-the-art performance\nwithout code changes. Focusing on performance-critical LLM kernels, we\ndemonstrate that this approach explores up to 15x more kernel parameter\nconfigurations, produces significantly more diverse code across multiple\ndimensions, and even outperforms vendor-optimized implementations by up to\n230%, all while reducing kernel code size by 70x and eliminating manual code\noptimizations. Our results highlight autotuning as a promising path to\nunlocking model portability across GPU vendors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs grow in complexity, achieving state-of-the-art performance requires\ntight co-design across algorithms, software, and hardware. Today's reliance on\na single dominant platform limits portability, creates vendor lock-in, and\nraises barriers for new AI hardware. In this work, we make the case for\ncombining just-in-time (JIT) compilation with comprehensive kernel parameter\nautotuning to enable portable LLM inference with state-of-the-art performance\nwithout code changes. Focusing on performance-critical LLM kernels, we\ndemonstrate that this approach explores up to 15x more kernel parameter\nconfigurations, produces significantly more diverse code across multiple\ndimensions, and even outperforms vendor-optimized implementations by up to\n230%, all while reducing kernel code size by 70x and eliminating manual code\noptimizations. Our results highlight autotuning as a promising path to\nunlocking model portability across GPU vendors."
                },
                "authors": [
                    {
                        "name": "Burkhard Ringlein"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Radu Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Radu Stoica"
                },
                "author": "Radu Stoica",
                "arxiv_comment": "revision after reviewers feedback, broadening autotune study",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03780v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03780v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13305v1",
                "updated": "2025-07-17T17:22:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    22,
                    41,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:22:41Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    22,
                    41,
                    3,
                    198,
                    0
                ],
                "title": "Boosting Team Modeling through Tempo-Relational Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Team Modeling through Tempo-Relational Representation Learning"
                },
                "summary": "Team modeling remains a fundamental challenge at the intersection of\nArtificial Intelligence and the Social Sciences. Social Science research\nemphasizes the need to jointly model dynamics and relations, while practical\napplications demand unified models capable of inferring multiple team\nconstructs simultaneously, providing interpretable insights and actionable\nrecommendations to enhance team performance. However, existing works do not\nmeet these practical demands. To bridge this gap, we present TRENN, a novel\ntempo-relational architecture that integrates: (i) an automatic temporal graph\nextractor, (ii) a tempo-relational encoder, (iii) a decoder for team construct\nprediction, and (iv) two complementary explainability modules. TRENN jointly\ncaptures relational and temporal team dynamics, providing a solid foundation\nfor MT-TRENN, which extends TReNN by replacing the decoder with a multi-task\nhead, enabling the model to learn shared Social Embeddings and simultaneously\npredict multiple team constructs, including Emergent Leadership, Leadership\nStyle, and Teamwork components. Experimental results demonstrate that our\napproach significantly outperforms approaches that rely exclusively on temporal\nor relational information. Additionally, experimental evaluation has shown that\nthe explainability modules integrated in MT-TRENN yield interpretable insights\nand actionable suggestions to support team improvement. These capabilities make\nour approach particularly well-suited for Human-Centered AI applications, such\nas intelligent decision-support systems in high-stakes collaborative\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Team modeling remains a fundamental challenge at the intersection of\nArtificial Intelligence and the Social Sciences. Social Science research\nemphasizes the need to jointly model dynamics and relations, while practical\napplications demand unified models capable of inferring multiple team\nconstructs simultaneously, providing interpretable insights and actionable\nrecommendations to enhance team performance. However, existing works do not\nmeet these practical demands. To bridge this gap, we present TRENN, a novel\ntempo-relational architecture that integrates: (i) an automatic temporal graph\nextractor, (ii) a tempo-relational encoder, (iii) a decoder for team construct\nprediction, and (iv) two complementary explainability modules. TRENN jointly\ncaptures relational and temporal team dynamics, providing a solid foundation\nfor MT-TRENN, which extends TReNN by replacing the decoder with a multi-task\nhead, enabling the model to learn shared Social Embeddings and simultaneously\npredict multiple team constructs, including Emergent Leadership, Leadership\nStyle, and Teamwork components. Experimental results demonstrate that our\napproach significantly outperforms approaches that rely exclusively on temporal\nor relational information. Additionally, experimental evaluation has shown that\nthe explainability modules integrated in MT-TRENN yield interpretable insights\nand actionable suggestions to support team improvement. These capabilities make\nour approach particularly well-suited for Human-Centered AI applications, such\nas intelligent decision-support systems in high-stakes collaborative\nenvironments."
                },
                "authors": [
                    {
                        "name": "Vincenzo Marco De Luca"
                    },
                    {
                        "name": "Giovanna Varni"
                    },
                    {
                        "name": "Andrea Passerini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Passerini"
                },
                "author": "Andrea Passerini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08589v3",
                "updated": "2025-07-17T17:20:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    20,
                    34,
                    3,
                    198,
                    0
                ],
                "published": "2024-10-11T07:36:14Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    36,
                    14,
                    4,
                    285,
                    0
                ],
                "title": "Retraining-Free Merging of Sparse MoE via Hierarchical Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retraining-Free Merging of Sparse MoE via Hierarchical Clustering"
                },
                "summary": "Sparse Mixture-of-Experts (SMoE) models represent a significant advancement\nin large language model (LLM) development through their efficient parameter\nutilization. These models achieve substantial performance improvements at\nreduced inference costs. However, the deployment of SMoE models faces\nconstraints from extensive memory requirements of expert components in\nresource-limited environments. To address these limitations, this paper\nintroduces Hierarchical Clustering for Sparsely activated Mixture of Experts\n(HC-SMoE), a task-agnostic expert merging framework for parameter reduction\nwithout retraining. HC-SMoE introduces a novel hierarchical clustering approach\nbased on expert outputs to ensure merging robustness independent of routing\ndecisions. The proposed output-based clustering method enables effective\ncapture of functional relationships between experts for large-scale\narchitectures. We provide theoretical analysis and comprehensive evaluations\nacross multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness\nin state-of-the-art models including Qwen and Mixtral. The experimental results\nvalidate HC-SMoE's superior performance and practical applicability for\nreal-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture-of-Experts (SMoE) models represent a significant advancement\nin large language model (LLM) development through their efficient parameter\nutilization. These models achieve substantial performance improvements at\nreduced inference costs. However, the deployment of SMoE models faces\nconstraints from extensive memory requirements of expert components in\nresource-limited environments. To address these limitations, this paper\nintroduces Hierarchical Clustering for Sparsely activated Mixture of Experts\n(HC-SMoE), a task-agnostic expert merging framework for parameter reduction\nwithout retraining. HC-SMoE introduces a novel hierarchical clustering approach\nbased on expert outputs to ensure merging robustness independent of routing\ndecisions. The proposed output-based clustering method enables effective\ncapture of functional relationships between experts for large-scale\narchitectures. We provide theoretical analysis and comprehensive evaluations\nacross multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness\nin state-of-the-art models including Qwen and Mixtral. The experimental results\nvalidate HC-SMoE's superior performance and practical applicability for\nreal-world deployments."
                },
                "authors": [
                    {
                        "name": "I-Chun Chen"
                    },
                    {
                        "name": "Hsu-Shen Liu"
                    },
                    {
                        "name": "Wei-Fang Sun"
                    },
                    {
                        "name": "Chen-Hao Chao"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    },
                    {
                        "name": "Chun-Yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Yi Lee"
                },
                "author": "Chun-Yi Lee",
                "arxiv_comment": "Code: https://github.com/wazenmai/HC-SMoE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13302v1",
                "updated": "2025-07-17T17:11:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    11,
                    14,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:11:14Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    11,
                    14,
                    3,
                    198,
                    0
                ],
                "title": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in\n  Large Language Model (LLM) Human Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in\n  Large Language Model (LLM) Human Evaluations"
                },
                "summary": "The evaluation of large language models is a complex task, in which several\napproaches have been proposed. The most common is the use of automated\nbenchmarks in which LLMs have to answer multiple-choice questions of different\ntopics. However, this method has certain limitations, being the most\nconcerning, the poor correlation with the humans. An alternative approach, is\nto have humans evaluate the LLMs. This poses scalability issues as there is a\nlarge and growing number of models to evaluate making it impractical (and\ncostly) to run traditional studies based on recruiting a number of evaluators\nand having them rank the responses of the models. An alternative approach is\nthe use of public arenas, such as the popular LM arena, on which any user can\nfreely evaluate models on any question and rank the responses of two models.\nThe results are then elaborated into a model ranking. An increasingly important\naspect of LLMs is their energy consumption and, therefore, evaluating how\nenergy awareness influences the decisions of humans in selecting a model is of\ninterest. In this paper, we present GEA, the Generative Energy Arena, an arena\nthat incorporates information on the energy consumption of the model in the\nevaluation process. Preliminary results obtained with GEA are also presented,\nshowing that for most questions, when users are aware of the energy\nconsumption, they favor smaller and more energy efficient models. This suggests\nthat for most user interactions, the extra cost and energy incurred by the more\ncomplex and top-performing models do not provide an increase in the perceived\nquality of the responses that justifies their use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of large language models is a complex task, in which several\napproaches have been proposed. The most common is the use of automated\nbenchmarks in which LLMs have to answer multiple-choice questions of different\ntopics. However, this method has certain limitations, being the most\nconcerning, the poor correlation with the humans. An alternative approach, is\nto have humans evaluate the LLMs. This poses scalability issues as there is a\nlarge and growing number of models to evaluate making it impractical (and\ncostly) to run traditional studies based on recruiting a number of evaluators\nand having them rank the responses of the models. An alternative approach is\nthe use of public arenas, such as the popular LM arena, on which any user can\nfreely evaluate models on any question and rank the responses of two models.\nThe results are then elaborated into a model ranking. An increasingly important\naspect of LLMs is their energy consumption and, therefore, evaluating how\nenergy awareness influences the decisions of humans in selecting a model is of\ninterest. In this paper, we present GEA, the Generative Energy Arena, an arena\nthat incorporates information on the energy consumption of the model in the\nevaluation process. Preliminary results obtained with GEA are also presented,\nshowing that for most questions, when users are aware of the energy\nconsumption, they favor smaller and more energy efficient models. This suggests\nthat for most user interactions, the extra cost and energy incurred by the more\ncomplex and top-performing models do not provide an increase in the perceived\nquality of the responses that justifies their use."
                },
                "authors": [
                    {
                        "name": "Carlos Arriaga"
                    },
                    {
                        "name": "Gonzalo Martnez"
                    },
                    {
                        "name": "Eneko Sendin"
                    },
                    {
                        "name": "Javier Conde"
                    },
                    {
                        "name": "Pedro Reviriego"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Reviriego"
                },
                "author": "Pedro Reviriego",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13300v1",
                "updated": "2025-07-17T17:09:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    9,
                    22,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:09:22Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    9,
                    22,
                    3,
                    198,
                    0
                ],
                "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research"
                },
                "summary": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks."
                },
                "authors": [
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Weiyuan Chen"
                    },
                    {
                        "name": "Zhijian Xu"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Chengye Wang"
                    },
                    {
                        "name": "Lovekesh Vig"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13290v1",
                "updated": "2025-07-17T16:54:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    54,
                    42,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T16:54:42Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    54,
                    42,
                    3,
                    198,
                    0
                ],
                "title": "Towards Formal Verification of LLM-Generated Code from Natural Language\n  Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Formal Verification of LLM-Generated Code from Natural Language\n  Prompts"
                },
                "summary": "In the past few years LLMs have emerged as a tool that can aid programmers by\ntaking natural language descriptions and generating code based on it. However,\nLLMs often generate incorrect code that users need to fix and the literature\nsuggests users often struggle to detect these errors. In this work we seek to\noffer formal guarantees of correctness to LLM generated code; such guarantees\ncould improve the experience of using AI Code Assistants and potentially enable\nnatural language programming for users with little or no programming knowledge.\nTo address this challenge we propose to incorporate a formal query language\nthat can represent a user's intent in a formally defined but natural\nlanguage-like manner that a user can confirm matches their intent. Then, using\nsuch a query we propose to verify LLM generated code to ensure it matches the\nuser's intent. We implement these ideas in our system, Astrogator, for the\nAnsible programming language which includes such a formal query language, a\ncalculus for representing the behavior of Ansible programs, and a symbolic\ninterpreter which is used for the verification. On a benchmark suite of 21\ncode-generation tasks, our verifier is able to verify correct code in 83% of\ncases and identify incorrect code in 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past few years LLMs have emerged as a tool that can aid programmers by\ntaking natural language descriptions and generating code based on it. However,\nLLMs often generate incorrect code that users need to fix and the literature\nsuggests users often struggle to detect these errors. In this work we seek to\noffer formal guarantees of correctness to LLM generated code; such guarantees\ncould improve the experience of using AI Code Assistants and potentially enable\nnatural language programming for users with little or no programming knowledge.\nTo address this challenge we propose to incorporate a formal query language\nthat can represent a user's intent in a formally defined but natural\nlanguage-like manner that a user can confirm matches their intent. Then, using\nsuch a query we propose to verify LLM generated code to ensure it matches the\nuser's intent. We implement these ideas in our system, Astrogator, for the\nAnsible programming language which includes such a formal query language, a\ncalculus for representing the behavior of Ansible programs, and a symbolic\ninterpreter which is used for the verification. On a benchmark suite of 21\ncode-generation tasks, our verifier is able to verify correct code in 83% of\ncases and identify incorrect code in 92%."
                },
                "authors": [
                    {
                        "name": "Aaron Councilman"
                    },
                    {
                        "name": "David Fu"
                    },
                    {
                        "name": "Aryan Gupta"
                    },
                    {
                        "name": "Chengxiao Wang"
                    },
                    {
                        "name": "David Grove"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    },
                    {
                        "name": "Vikram Adve"
                    }
                ],
                "author_detail": {
                    "name": "Vikram Adve"
                },
                "author": "Vikram Adve",
                "arxiv_comment": "31 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01302v2",
                "updated": "2025-07-17T16:44:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    44,
                    11,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-02T02:22:56Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    2,
                    22,
                    56,
                    2,
                    92,
                    0
                ],
                "title": "GPU-Accelerated Gravitational Lensing & Dynamical (GLaD) Modeling for\n  Cosmology and Galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Gravitational Lensing & Dynamical (GLaD) Modeling for\n  Cosmology and Galaxies"
                },
                "summary": "Time-delay distance measurements from strongly lensed quasars provide a\nrobust, independent method for determining the Hubble constant ($H_0$). This\napproach cross-checks $H_0$ estimates from the distance ladder in the late\nuniverse and the cosmic microwave background in the early universe. However,\nthe mass-sheet degeneracy in lensing models introduces systematic uncertainty,\nlimiting precision. Dynamical modeling complements strong lensing by\nconstraining the mass distribution with independent observational data. We\ndevelop a methodology and software framework for joint modeling of stellar\nkinematics and lensing data. Using simulated data for the lensed quasar\nRXJ1131$-$1131, we demonstrate that high-quality kinematic data can achieve\n$\\sim$4% precision on $H_0$. Through extensive modeling, we examine the impact\nof the presence of a supermassive black hole in the lens galaxy and potential\nsystematic biases in kinematic data on $H_0$ measurements. Our results show\nthat imposing priors on black hole mass and orbital anisotropy, or excluding\ncentral kinematic bins, mitigates biases in $H_0$ estimates. By testing on mock\nkinematic data with systematic biases, we highlight the need for sub-percent\ncontrol of kinematic systematics, which is achievable with current technology.\nAdditionally, we leverage GPU parallelization to accelerate Bayesian inference,\nreducing a previously month-long process by an order of magnitude. This\npipeline offers significant potential for advancing cosmological and galaxy\nevolution studies with large datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-delay distance measurements from strongly lensed quasars provide a\nrobust, independent method for determining the Hubble constant ($H_0$). This\napproach cross-checks $H_0$ estimates from the distance ladder in the late\nuniverse and the cosmic microwave background in the early universe. However,\nthe mass-sheet degeneracy in lensing models introduces systematic uncertainty,\nlimiting precision. Dynamical modeling complements strong lensing by\nconstraining the mass distribution with independent observational data. We\ndevelop a methodology and software framework for joint modeling of stellar\nkinematics and lensing data. Using simulated data for the lensed quasar\nRXJ1131$-$1131, we demonstrate that high-quality kinematic data can achieve\n$\\sim$4% precision on $H_0$. Through extensive modeling, we examine the impact\nof the presence of a supermassive black hole in the lens galaxy and potential\nsystematic biases in kinematic data on $H_0$ measurements. Our results show\nthat imposing priors on black hole mass and orbital anisotropy, or excluding\ncentral kinematic bins, mitigates biases in $H_0$ estimates. By testing on mock\nkinematic data with systematic biases, we highlight the need for sub-percent\ncontrol of kinematic systematics, which is achievable with current technology.\nAdditionally, we leverage GPU parallelization to accelerate Bayesian inference,\nreducing a previously month-long process by an order of magnitude. This\npipeline offers significant potential for advancing cosmological and galaxy\nevolution studies with large datasets."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Sherry H. Suyu"
                    },
                    {
                        "name": "Aymeric Galan"
                    },
                    {
                        "name": "Aleksi Halkola"
                    },
                    {
                        "name": "Michele Cappellari"
                    },
                    {
                        "name": "Anowar J. Shajib"
                    },
                    {
                        "name": "Miha Cernetic"
                    }
                ],
                "author_detail": {
                    "name": "Miha Cernetic"
                },
                "author": "Miha Cernetic",
                "arxiv_comment": "20 pages, 11 Figures, 4 Tables, accepted for publication in Astronomy\n  & Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16658v2",
                "updated": "2025-07-17T16:35:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    35,
                    56,
                    3,
                    198,
                    0
                ],
                "published": "2024-04-25T14:49:46Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    14,
                    49,
                    46,
                    3,
                    116,
                    0
                ],
                "title": "A fast and accurate method for inferring solid-state diffusivity in\n  lithium-ion battery active materials: improving upon the classical GITT\n  approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast and accurate method for inferring solid-state diffusivity in\n  lithium-ion battery active materials: improving upon the classical GITT\n  approach"
                },
                "summary": "Data collected using the galvanostatic intermittent titration technique\n(GITT) and application of the Sand equation is a ubiquitous method for\ninferring the solid-state diffusivity in lithium-ion battery active materials.\nHowever, the experiment is notoriously time-consuming and the Sand equation\nrelies on assumptions whose applicability can be questionable. We propose a\nnovel methodology, termed Inference from a Consistent Model (ICM), which\nenables inference of solid-state diffusivity using the same physical model\nemployed for prediction, and is applicable to more general and quick-to-measure\ndata. We infer the diffusivity (as a function of inserted lithium\nconcentration) by minimising the residual sum of squares between data and\nsolutions to a spherically-symmetric nonlinear diffusion model in a single\nrepresentative active material particle. Using data harvested from the NMC\ncathode of a commercial LG M50 cell we demonstrate that the ICM is robust, and\nyields more accurate diffusivity estimates, while relying on data that are five\ntimes faster to collect than that required by the classical approach. Moreover,\nthere is good reason to believe that further speed ups could be achieved when\nother types of data are available. This work contributes towards developing\nfaster and more reliable techniques in parameter inference for lithium-ion\nbatteries, and the code required to deploy ICM is provided to facilitate its\nadoption in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data collected using the galvanostatic intermittent titration technique\n(GITT) and application of the Sand equation is a ubiquitous method for\ninferring the solid-state diffusivity in lithium-ion battery active materials.\nHowever, the experiment is notoriously time-consuming and the Sand equation\nrelies on assumptions whose applicability can be questionable. We propose a\nnovel methodology, termed Inference from a Consistent Model (ICM), which\nenables inference of solid-state diffusivity using the same physical model\nemployed for prediction, and is applicable to more general and quick-to-measure\ndata. We infer the diffusivity (as a function of inserted lithium\nconcentration) by minimising the residual sum of squares between data and\nsolutions to a spherically-symmetric nonlinear diffusion model in a single\nrepresentative active material particle. Using data harvested from the NMC\ncathode of a commercial LG M50 cell we demonstrate that the ICM is robust, and\nyields more accurate diffusivity estimates, while relying on data that are five\ntimes faster to collect than that required by the classical approach. Moreover,\nthere is good reason to believe that further speed ups could be achieved when\nother types of data are available. This work contributes towards developing\nfaster and more reliable techniques in parameter inference for lithium-ion\nbatteries, and the code required to deploy ICM is provided to facilitate its\nadoption in future research."
                },
                "authors": [
                    {
                        "name": "A. Emir Gumrukcuoglu"
                    },
                    {
                        "name": "James Burridge"
                    },
                    {
                        "name": "Kieran O'Regan"
                    },
                    {
                        "name": "Emma Kendrick"
                    },
                    {
                        "name": "Jamie M. Foster"
                    }
                ],
                "author_detail": {
                    "name": "Jamie M. Foster"
                },
                "author": "Jamie M. Foster",
                "arxiv_comment": "23 pages, 9 figures. Substantial revision with expanded analysis and\n  an additional co-author",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13425v2",
                "updated": "2025-07-17T16:32:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    32,
                    46,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-18T02:51:29Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    51,
                    29,
                    4,
                    108,
                    0
                ],
                "title": "Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with\n  Security Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with\n  Security Filtering"
                },
                "summary": "Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG."
                },
                "authors": [
                    {
                        "name": "Grace Byun"
                    },
                    {
                        "name": "Shinsun Lee"
                    },
                    {
                        "name": "Nayoung Choi"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13266v1",
                "updated": "2025-07-17T16:21:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    21,
                    47,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T16:21:47Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    21,
                    47,
                    3,
                    198,
                    0
                ],
                "title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation"
                },
                "summary": "Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL."
                },
                "authors": [
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Hong Lu"
                    },
                    {
                        "name": "Kaiyue Wen"
                    },
                    {
                        "name": "Zaiwen Yang"
                    },
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Hongzhou Lin"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jingzhao Zhang"
                },
                "author": "Jingzhao Zhang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13255v1",
                "updated": "2025-07-17T16:04:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    4,
                    55,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T16:04:55Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    4,
                    55,
                    3,
                    198,
                    0
                ],
                "title": "Automating Steering for Safe Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Steering for Safe Multimodal Large Language Models"
                },
                "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Lyucheng Wu"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Shumin Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shumin Deng"
                },
                "author": "Shumin Deng",
                "arxiv_comment": "Working in progress. 22 pages (8+ for main); 25 figures; 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13249v1",
                "updated": "2025-07-17T15:58:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    58,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T15:58:39Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    58,
                    39,
                    3,
                    198,
                    0
                ],
                "title": "Comparing astrophysical models to gravitational-wave data in the\n  observable space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing astrophysical models to gravitational-wave data in the\n  observable space"
                },
                "summary": "Comparing population-synthesis models to the results of hierarchical Bayesian\ninference in gravitational-wave astronomy requires a careful understanding of\nthe domain of validity of the models fitted to data. This comparison is usually\ndone using the inferred astrophysical distribution: from the data that were\ncollected, one deconvolves selection effects to reconstruct the generating\npopulation distribution. In this letter, we demonstrate the benefits of instead\ncomparing observable populations directly. In this approach, the domain of\nvalidity of the models is trivially respected, such that only the relevant\nparameter space regions as predicted by the astrophysical models of interest\ncontribute to the comparison. We clarify that unbiased inference of the\nobservable compact-binary population is indeed possible. Crucially, this\napproach still requires incorporating selection effects, but in a manner that\ndiffers from the standard implementation. We apply our observable-space\nreconstruction to LIGO-Virgo-KAGRA data from their third observing run and\nillustrate its potential by comparing the results to the predictions of a\nfiducial population-synthesis model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing population-synthesis models to the results of hierarchical Bayesian\ninference in gravitational-wave astronomy requires a careful understanding of\nthe domain of validity of the models fitted to data. This comparison is usually\ndone using the inferred astrophysical distribution: from the data that were\ncollected, one deconvolves selection effects to reconstruct the generating\npopulation distribution. In this letter, we demonstrate the benefits of instead\ncomparing observable populations directly. In this approach, the domain of\nvalidity of the models is trivially respected, such that only the relevant\nparameter space regions as predicted by the astrophysical models of interest\ncontribute to the comparison. We clarify that unbiased inference of the\nobservable compact-binary population is indeed possible. Crucially, this\napproach still requires incorporating selection effects, but in a manner that\ndiffers from the standard implementation. We apply our observable-space\nreconstruction to LIGO-Virgo-KAGRA data from their third observing run and\nillustrate its potential by comparing the results to the predictions of a\nfiducial population-synthesis model."
                },
                "authors": [
                    {
                        "name": "Alexandre Toubiana"
                    },
                    {
                        "name": "Davide Gerosa"
                    },
                    {
                        "name": "Matthew Mould"
                    },
                    {
                        "name": "Stefano Rinaldi"
                    },
                    {
                        "name": "Manuel Arca Sedda"
                    },
                    {
                        "name": "Tristan Bruel"
                    },
                    {
                        "name": "Riccardo Buscicchio"
                    },
                    {
                        "name": "Jonathan Gair"
                    },
                    {
                        "name": "Lavinia Paiella"
                    },
                    {
                        "name": "Filippo Santoliquido"
                    },
                    {
                        "name": "Rodrigo Tenorio"
                    },
                    {
                        "name": "Cristiano Ugolini"
                    }
                ],
                "author_detail": {
                    "name": "Cristiano Ugolini"
                },
                "author": "Cristiano Ugolini",
                "arxiv_comment": "5 pages, 11 with biblio and appendix, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16394v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16394v3",
                "updated": "2025-07-17T15:50:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    50,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-23T03:42:46Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    3,
                    42,
                    46,
                    2,
                    113,
                    0
                ],
                "title": "ConTextual: Improving Clinical Text Summarization in LLMs with\n  Context-preserving Token Filtering and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConTextual: Improving Clinical Text Summarization in LLMs with\n  Context-preserving Token Filtering and Knowledge Graphs"
                },
                "summary": "Unstructured clinical data can serve as a unique and rich source of\ninformation that can meaningfully inform clinical practice. Extracting the most\npertinent context from such data is critical for exploiting its true potential\ntoward optimal and timely decision-making in patient care. While prior research\nhas explored various methods for clinical text summarization, most prior\nstudies either process all input tokens uniformly or rely on heuristic-based\nfilters, which can overlook nuanced clinical cues and fail to prioritize\ninformation critical for decision-making. In this study, we propose Contextual,\na novel framework that integrates a Context-Preserving Token Filtering method\nwith a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By\npreserving context-specific important tokens and enriching them with structured\nknowledge, ConTextual improves both linguistic coherence and clinical fidelity.\nOur extensive empirical evaluations on two public benchmark datasets\ndemonstrate that ConTextual consistently outperforms other baselines. Our\nproposed approach highlights the complementary role of token-level filtering\nand structured retrieval in enhancing both linguistic and clinical integrity,\nas well as offering a scalable solution for improving precision in clinical\ntext generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unstructured clinical data can serve as a unique and rich source of\ninformation that can meaningfully inform clinical practice. Extracting the most\npertinent context from such data is critical for exploiting its true potential\ntoward optimal and timely decision-making in patient care. While prior research\nhas explored various methods for clinical text summarization, most prior\nstudies either process all input tokens uniformly or rely on heuristic-based\nfilters, which can overlook nuanced clinical cues and fail to prioritize\ninformation critical for decision-making. In this study, we propose Contextual,\na novel framework that integrates a Context-Preserving Token Filtering method\nwith a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By\npreserving context-specific important tokens and enriching them with structured\nknowledge, ConTextual improves both linguistic coherence and clinical fidelity.\nOur extensive empirical evaluations on two public benchmark datasets\ndemonstrate that ConTextual consistently outperforms other baselines. Our\nproposed approach highlights the complementary role of token-level filtering\nand structured retrieval in enhancing both linguistic and clinical integrity,\nas well as offering a scalable solution for improving precision in clinical\ntext generation."
                },
                "authors": [
                    {
                        "name": "Fahmida Liza Piya"
                    },
                    {
                        "name": "Rahmatollah Beheshti"
                    }
                ],
                "author_detail": {
                    "name": "Rahmatollah Beheshti"
                },
                "author": "Rahmatollah Beheshti",
                "arxiv_comment": "Accepted for MLHC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16394v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16394v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13238v1",
                "updated": "2025-07-17T15:47:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    47,
                    49,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T15:47:49Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    47,
                    49,
                    3,
                    198,
                    0
                ],
                "title": "HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language\n  Models"
                },
                "summary": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi."
                },
                "authors": [
                    {
                        "name": "Ashray Gupta"
                    },
                    {
                        "name": "Rohan Joseph"
                    },
                    {
                        "name": "Sunny Rai"
                    }
                ],
                "author_detail": {
                    "name": "Sunny Rai"
                },
                "author": "Sunny Rai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13236v1",
                "updated": "2025-07-17T15:47:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    47,
                    22,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T15:47:22Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    47,
                    22,
                    3,
                    198,
                    0
                ],
                "title": "Enhancing Cross-task Transfer of Large Language Models via Activation\n  Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cross-task Transfer of Large Language Models via Activation\n  Steering"
                },
                "summary": "Large language models (LLMs) have shown impressive abilities in leveraging\npretrained knowledge through prompting, but they often struggle with unseen\ntasks, particularly in data-scarce scenarios. While cross-task in-context\nlearning offers a direct solution for transferring knowledge across tasks, it\nstill faces critical challenges in terms of robustness, scalability, and\nefficiency. In this paper, we investigate whether cross-task transfer can be\nachieved via latent space steering without parameter updates or input\nexpansion. Through an analysis of activation patterns in the latent space of\nLLMs, we observe that the enhanced activations induced by in-context examples\nhave consistent patterns across different tasks. Inspired by these findings, we\npropose CAST, a novel Cross-task Activation Steering Transfer framework that\nenables effective transfer by manipulating the model's internal activation\nstates. Our approach first selects influential and diverse samples from\nhigh-resource tasks, then utilizes their contrastive representation-enhanced\nactivations to adapt LLMs to low-resource tasks. Extensive experiments across\nboth cross-domain and cross-lingual transfer settings show that our method\noutperforms competitive baselines and demonstrates superior scalability and\nlower computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive abilities in leveraging\npretrained knowledge through prompting, but they often struggle with unseen\ntasks, particularly in data-scarce scenarios. While cross-task in-context\nlearning offers a direct solution for transferring knowledge across tasks, it\nstill faces critical challenges in terms of robustness, scalability, and\nefficiency. In this paper, we investigate whether cross-task transfer can be\nachieved via latent space steering without parameter updates or input\nexpansion. Through an analysis of activation patterns in the latent space of\nLLMs, we observe that the enhanced activations induced by in-context examples\nhave consistent patterns across different tasks. Inspired by these findings, we\npropose CAST, a novel Cross-task Activation Steering Transfer framework that\nenables effective transfer by manipulating the model's internal activation\nstates. Our approach first selects influential and diverse samples from\nhigh-resource tasks, then utilizes their contrastive representation-enhanced\nactivations to adapt LLMs to low-resource tasks. Extensive experiments across\nboth cross-domain and cross-lingual transfer settings show that our method\noutperforms competitive baselines and demonstrates superior scalability and\nlower computational costs."
                },
                "authors": [
                    {
                        "name": "Xinyu Tang"
                    },
                    {
                        "name": "Zhihao Lv"
                    },
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zujie Wen"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13232v1",
                "updated": "2025-07-17T15:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    42,
                    28,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T15:42:28Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    42,
                    28,
                    3,
                    198,
                    0
                ],
                "title": "Unraveling the Feedback-Regulated Star Formation Activities around the\n  Expanding Galactic MIR Bubble [HKS2019] E71",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling the Feedback-Regulated Star Formation Activities around the\n  Expanding Galactic MIR Bubble [HKS2019] E71"
                },
                "summary": "We explore the physical environment of the Galactic mid-infrared (MIR) bubble\n[HKS2019] E71 (hereafter E71) through a multi-wavelength approach. E71 is\nlocated at the edge of a filamentary structure, as traced in Herschel images\n(250-500 $\\mu$m), Herschel column density map, and molecular maps in the\nvelocity range [-20,-14] km/s. It hosts a stellar cluster (radius~1.26 pc,\ndistance~1.81+/-0.15 kpc) associated with radio continuum emission, including a\ncentrally positioned B1.5-type massive star (hereafter 'm2'), along with an\nenhanced population of evolved low-mass stars and young stellar objects. MIR\nimages and molecular line maps reveal a PDR surrounding 'm2', exhibiting an\narc-like structure along the edges of E71. Regularly spaced molecular and dust\ncondensations are identified along this structure. The position-velocity map of\n12CO emission suggests an expansion of molecular gas concentrated at the\nperiphery of E71. Near-infrared spectroscopic observations with TANSPEC confirm\nthe presence of the accretion process in a massive young stellar object (MYSO)\nlocated near the edge of the bubble. High-resolution uGMRT radio continuum maps\nuncover substructures in the ionized emission, both toward the MYSO and the\ncenter of E71. These findings support that 'm2' has shaped an arc-like\nmorphology through its feedback processes. The pressure exerted by 'm2' and the\nvelocity structure of the 12/13CO(1-0) emission suggest that the stellar\nfeedback has likely driven out molecular material, leading to the formation of\nthe expanding E71 bubble. Our overall investigation infers that the \"collect\nand collapse\" process might be a possible mechanism that can describe the\nongoing star formation activities around the E71 bubble.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the physical environment of the Galactic mid-infrared (MIR) bubble\n[HKS2019] E71 (hereafter E71) through a multi-wavelength approach. E71 is\nlocated at the edge of a filamentary structure, as traced in Herschel images\n(250-500 $\\mu$m), Herschel column density map, and molecular maps in the\nvelocity range [-20,-14] km/s. It hosts a stellar cluster (radius~1.26 pc,\ndistance~1.81+/-0.15 kpc) associated with radio continuum emission, including a\ncentrally positioned B1.5-type massive star (hereafter 'm2'), along with an\nenhanced population of evolved low-mass stars and young stellar objects. MIR\nimages and molecular line maps reveal a PDR surrounding 'm2', exhibiting an\narc-like structure along the edges of E71. Regularly spaced molecular and dust\ncondensations are identified along this structure. The position-velocity map of\n12CO emission suggests an expansion of molecular gas concentrated at the\nperiphery of E71. Near-infrared spectroscopic observations with TANSPEC confirm\nthe presence of the accretion process in a massive young stellar object (MYSO)\nlocated near the edge of the bubble. High-resolution uGMRT radio continuum maps\nuncover substructures in the ionized emission, both toward the MYSO and the\ncenter of E71. These findings support that 'm2' has shaped an arc-like\nmorphology through its feedback processes. The pressure exerted by 'm2' and the\nvelocity structure of the 12/13CO(1-0) emission suggest that the stellar\nfeedback has likely driven out molecular material, leading to the formation of\nthe expanding E71 bubble. Our overall investigation infers that the \"collect\nand collapse\" process might be a possible mechanism that can describe the\nongoing star formation activities around the E71 bubble."
                },
                "authors": [
                    {
                        "name": "Aayushi Verma"
                    },
                    {
                        "name": "Saurabh Sharma"
                    },
                    {
                        "name": "Lokesh K. Dewangan"
                    },
                    {
                        "name": "Tarak Chand"
                    },
                    {
                        "name": "Ariful Hoque"
                    },
                    {
                        "name": "Devendra K. Ojha"
                    },
                    {
                        "name": "Harmeen Kaur"
                    },
                    {
                        "name": "Ram Kesh Yadav"
                    },
                    {
                        "name": "Mamta"
                    },
                    {
                        "name": "Manojit Chakraborty"
                    },
                    {
                        "name": "Archana Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Archana Gupta"
                },
                "author": "Archana Gupta",
                "arxiv_comment": "28 pages, 15 figures, and 5 tables; Accepted for publication in The\n  Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13231v1",
                "updated": "2025-07-17T15:41:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    41,
                    57,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T15:41:57Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    41,
                    57,
                    3,
                    198,
                    0
                ],
                "title": "VITA: Vision-to-Action Flow Matching Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA: Vision-to-Action Flow Matching Policy"
                },
                "summary": "We present VITA, a Vision-To-Action flow matching policy that evolves latent\nvisual representations into latent actions for visuomotor control. Traditional\nflow matching and diffusion policies sample from standard source distributions\n(e.g., Gaussian noise) and require additional conditioning mechanisms like\ncross-attention to condition action generation on visual information, creating\ntime and space overheads. VITA proposes a novel paradigm that treats latent\nimages as the flow source, learning an inherent mapping from vision to action\nwhile eliminating separate conditioning modules and preserving generative\nmodeling capabilities. Learning flows between fundamentally different\nmodalities like vision and action is challenging due to sparse action data\nlacking semantic structures and dimensional mismatches between high-dimensional\nvisual representations and raw actions. We address this by creating a\nstructured action latent space via an autoencoder as the flow matching target,\nup-sampling raw actions to match visual representation shapes. Crucially, we\nsupervise flow matching with both encoder targets and final action outputs\nthrough flow latent decoding, which backpropagates action reconstruction loss\nthrough sequential flow matching ODE solving steps for effective end-to-end\nlearning. Implemented as simple MLP layers, VITA is evaluated on challenging\nbi-manual manipulation tasks on the ALOHA platform, including 5 simulation and\n2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or\nmatches state-of-the-art generative policies while reducing inference latency\nby 50-130% compared to conventional flow matching policies requiring different\nconditioning mechanisms or complex architectures. To our knowledge, VITA is the\nfirst MLP-only flow matching policy capable of solving complex bi-manual\nmanipulation tasks like those in ALOHA benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present VITA, a Vision-To-Action flow matching policy that evolves latent\nvisual representations into latent actions for visuomotor control. Traditional\nflow matching and diffusion policies sample from standard source distributions\n(e.g., Gaussian noise) and require additional conditioning mechanisms like\ncross-attention to condition action generation on visual information, creating\ntime and space overheads. VITA proposes a novel paradigm that treats latent\nimages as the flow source, learning an inherent mapping from vision to action\nwhile eliminating separate conditioning modules and preserving generative\nmodeling capabilities. Learning flows between fundamentally different\nmodalities like vision and action is challenging due to sparse action data\nlacking semantic structures and dimensional mismatches between high-dimensional\nvisual representations and raw actions. We address this by creating a\nstructured action latent space via an autoencoder as the flow matching target,\nup-sampling raw actions to match visual representation shapes. Crucially, we\nsupervise flow matching with both encoder targets and final action outputs\nthrough flow latent decoding, which backpropagates action reconstruction loss\nthrough sequential flow matching ODE solving steps for effective end-to-end\nlearning. Implemented as simple MLP layers, VITA is evaluated on challenging\nbi-manual manipulation tasks on the ALOHA platform, including 5 simulation and\n2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or\nmatches state-of-the-art generative policies while reducing inference latency\nby 50-130% compared to conventional flow matching policies requiring different\nconditioning mechanisms or complex architectures. To our knowledge, VITA is the\nfirst MLP-only flow matching policy capable of solving complex bi-manual\nmanipulation tasks like those in ALOHA benchmarks."
                },
                "authors": [
                    {
                        "name": "Dechen Gao"
                    },
                    {
                        "name": "Boqi Zhao"
                    },
                    {
                        "name": "Andrew Lee"
                    },
                    {
                        "name": "Ian Chuang"
                    },
                    {
                        "name": "Hanchu Zhou"
                    },
                    {
                        "name": "Hang Wang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Junshan Zhang"
                    },
                    {
                        "name": "Iman Soltani"
                    }
                ],
                "author_detail": {
                    "name": "Iman Soltani"
                },
                "author": "Iman Soltani",
                "arxiv_comment": "Project page: https://ucd-dare.github.io/VITA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13733v2",
                "updated": "2025-07-17T15:31:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    31,
                    25,
                    3,
                    198,
                    0
                ],
                "published": "2025-03-17T21:41:37Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    41,
                    37,
                    0,
                    76,
                    0
                ],
                "title": "CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,\n  Multi-Generator and Multi-Domain Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,\n  Multi-Generator and Multi-Domain Settings"
                },
                "summary": "Large language models (LLMs) have revolutionized code generation, automating\nprogramming with remarkable efficiency. However, these advancements challenge\nprogramming skills, ethics, and assessment integrity, making the detection of\nLLM-generated code essential for maintaining accountability and standards.\nWhile, there has been some research on this problem, it generally lacks domain\ncoverage and robustness, and only covers a small number of programming\nlanguages. To this end, we propose a framework capable of distinguishing\nbetween human- and LLM-written code across multiple programming languages, code\ngenerators, and domains. We use a large-scale dataset from renowned platforms\nand LLM-based code generators, alongside applying rigorous data quality checks,\nfeature engineering, and comparative analysis using evaluation of traditional\nmachine learning models, pre-trained language models (PLMs), and LLMs for code\ndetection. We perform an evaluation on out-of-domain scenarios, such as\ndetecting the authorship and hybrid authorship of generated code and\ngeneralizing to unseen models, domains, and programming languages. Moreover,\nour extensive experiments show that our framework effectively distinguishes\nhuman- from LLM-written code and sets a new benchmark for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized code generation, automating\nprogramming with remarkable efficiency. However, these advancements challenge\nprogramming skills, ethics, and assessment integrity, making the detection of\nLLM-generated code essential for maintaining accountability and standards.\nWhile, there has been some research on this problem, it generally lacks domain\ncoverage and robustness, and only covers a small number of programming\nlanguages. To this end, we propose a framework capable of distinguishing\nbetween human- and LLM-written code across multiple programming languages, code\ngenerators, and domains. We use a large-scale dataset from renowned platforms\nand LLM-based code generators, alongside applying rigorous data quality checks,\nfeature engineering, and comparative analysis using evaluation of traditional\nmachine learning models, pre-trained language models (PLMs), and LLMs for code\ndetection. We perform an evaluation on out-of-domain scenarios, such as\ndetecting the authorship and hybrid authorship of generated code and\ngeneralizing to unseen models, domains, and programming languages. Moreover,\nour extensive experiments show that our framework effectively distinguishes\nhuman- from LLM-written code and sets a new benchmark for this task."
                },
                "authors": [
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Dilshod Azizov"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12039v2",
                "updated": "2025-07-17T15:27:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    27,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T08:56:19Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    8,
                    56,
                    19,
                    2,
                    197,
                    0
                ],
                "title": "A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans"
                },
                "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity."
                },
                "authors": [
                    {
                        "name": "Anca Dinu"
                    },
                    {
                        "name": "Andra-Maria Florescu"
                    },
                    {
                        "name": "Alina Resceanu"
                    }
                ],
                "author_detail": {
                    "name": "Alina Resceanu"
                },
                "author": "Alina Resceanu",
                "arxiv_comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13207v1",
                "updated": "2025-07-17T15:16:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    16,
                    30,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T15:16:30Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    16,
                    30,
                    3,
                    198,
                    0
                ],
                "title": "MoTM: Towards a Foundation Model for Time Series Imputation based on\n  Continuous Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoTM: Towards a Foundation Model for Time Series Imputation based on\n  Continuous Modeling"
                },
                "summary": "Recent years have witnessed a growing interest for time series foundation\nmodels, with a strong emphasis on the forecasting task. Yet, the crucial task\nof out-of-domain imputation of missing values remains largely underexplored. We\npropose a first step to fill this gap by leveraging implicit neural\nrepresentations (INRs). INRs model time series as continuous functions and\nnaturally handle various missing data scenarios and sampling rates. While they\nhave shown strong performance within specific distributions, they struggle\nunder distribution shifts. To address this, we introduce MoTM (Mixture of\nTimeflow Models), a step toward a foundation model for time series imputation.\nBuilding on the idea that a new time series is a mixture of previously seen\npatterns, MoTM combines a basis of INRs, each trained independently on a\ndistinct family of time series, with a ridge regressor that adapts to the\nobserved context at inference. We demonstrate robust in-domain and\nout-of-domain generalization across diverse imputation scenarios (e.g., block\nand pointwise missingness, variable sampling rates), paving the way for\nadaptable foundation imputation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed a growing interest for time series foundation\nmodels, with a strong emphasis on the forecasting task. Yet, the crucial task\nof out-of-domain imputation of missing values remains largely underexplored. We\npropose a first step to fill this gap by leveraging implicit neural\nrepresentations (INRs). INRs model time series as continuous functions and\nnaturally handle various missing data scenarios and sampling rates. While they\nhave shown strong performance within specific distributions, they struggle\nunder distribution shifts. To address this, we introduce MoTM (Mixture of\nTimeflow Models), a step toward a foundation model for time series imputation.\nBuilding on the idea that a new time series is a mixture of previously seen\npatterns, MoTM combines a basis of INRs, each trained independently on a\ndistinct family of time series, with a ridge regressor that adapts to the\nobserved context at inference. We demonstrate robust in-domain and\nout-of-domain generalization across diverse imputation scenarios (e.g., block\nand pointwise missingness, variable sampling rates), paving the way for\nadaptable foundation imputation models."
                },
                "authors": [
                    {
                        "name": "Etienne Le Naour"
                    },
                    {
                        "name": "Tahar Nabil"
                    },
                    {
                        "name": "Ghislain Agoua"
                    }
                ],
                "author_detail": {
                    "name": "Ghislain Agoua"
                },
                "author": "Ghislain Agoua",
                "arxiv_comment": "10th Workshop on Advanced Analytics and Learning on Temporal Data\n  (AALTD), ECML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13205v1",
                "updated": "2025-07-17T15:15:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    15,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T15:15:43Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    15,
                    43,
                    3,
                    198,
                    0
                ],
                "title": "Automatically assessing oral narratives of Afrikaans and isiXhosa\n  children",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically assessing oral narratives of Afrikaans and isiXhosa\n  children"
                },
                "summary": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning."
                },
                "authors": [
                    {
                        "name": "R. Louw"
                    },
                    {
                        "name": "E. Sharratt"
                    },
                    {
                        "name": "F. de Wet"
                    },
                    {
                        "name": "C. Jacobs"
                    },
                    {
                        "name": "A. Smith"
                    },
                    {
                        "name": "H. Kamper"
                    }
                ],
                "author_detail": {
                    "name": "H. Kamper"
                },
                "arxiv_affiliation": "Stellenbosch University",
                "author": "H. Kamper",
                "arxiv_comment": "Accepted to SLaTE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23033v2",
                "updated": "2025-07-17T15:06:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    6,
                    16,
                    3,
                    198,
                    0
                ],
                "published": "2025-03-29T10:36:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    10,
                    36,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "Imagine All The Relevance: Scenario-Profiled Indexing with Knowledge\n  Expansion for Dense Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine All The Relevance: Scenario-Profiled Indexing with Knowledge\n  Expansion for Dense Retrieval"
                },
                "summary": "Existing dense retrieval models struggle with reasoning-intensive retrieval\ntask as they fail to capture implicit relevance that requires reasoning beyond\nsurface-level semantic information. To address these challenges, we propose\nScenario-Profiled Indexing with Knowledge Expansion (SPIKE), a dense retrieval\nframework that explicitly indexes implicit relevance by decomposing documents\ninto scenario-based retrieval units. SPIKE organizes documents into scenario,\nwhich encapsulates the reasoning process necessary to uncover implicit\nrelationships between hypothetical information needs and document content.\nSPIKE constructs a scenario-augmented dataset using a powerful teacher large\nlanguage model (LLM), then distills these reasoning capabilities into a\nsmaller, efficient scenario generator. During inference, SPIKE incorporates\nscenario-level relevance alongside document-level relevance, enabling\nreasoning-aware retrieval. Extensive experiments demonstrate that SPIKE\nconsistently enhances retrieval performance across various query types and\ndense retrievers. It also enhances the retrieval experience for users through\nscenario and offers valuable contextual information for LLMs in\nretrieval-augmented generation (RAG).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing dense retrieval models struggle with reasoning-intensive retrieval\ntask as they fail to capture implicit relevance that requires reasoning beyond\nsurface-level semantic information. To address these challenges, we propose\nScenario-Profiled Indexing with Knowledge Expansion (SPIKE), a dense retrieval\nframework that explicitly indexes implicit relevance by decomposing documents\ninto scenario-based retrieval units. SPIKE organizes documents into scenario,\nwhich encapsulates the reasoning process necessary to uncover implicit\nrelationships between hypothetical information needs and document content.\nSPIKE constructs a scenario-augmented dataset using a powerful teacher large\nlanguage model (LLM), then distills these reasoning capabilities into a\nsmaller, efficient scenario generator. During inference, SPIKE incorporates\nscenario-level relevance alongside document-level relevance, enabling\nreasoning-aware retrieval. Extensive experiments demonstrate that SPIKE\nconsistently enhances retrieval performance across various query types and\ndense retrievers. It also enhances the retrieval experience for users through\nscenario and offers valuable contextual information for LLMs in\nretrieval-augmented generation (RAG)."
                },
                "authors": [
                    {
                        "name": "Sangam Lee"
                    },
                    {
                        "name": "Ryang Heo"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13916v2",
                "updated": "2025-07-17T15:05:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    5,
                    36,
                    3,
                    198,
                    0
                ],
                "published": "2025-06-16T18:51:44Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    18,
                    51,
                    44,
                    0,
                    167,
                    0
                ],
                "title": "Branching Stein Variational Gradient Descent for sampling multimodal\n  distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Branching Stein Variational Gradient Descent for sampling multimodal\n  distributions"
                },
                "summary": "We propose a novel particle-based variational inference method designed to\nwork with multimodal distributions. Our approach, referred to as Branched Stein\nVariational Gradient Descent (BSVGD), extends the classical Stein Variational\nGradient Descent (SVGD) algorithm by incorporating a random branching mechanism\nthat encourages the exploration of the state space. In this work, a theoretical\nguarantee for the convergence in distribution is presented, as well as\nnumerical experiments to validate the suitability of our algorithm. Performance\ncomparisons between the BSVGD and the SVGD are presented using the Wasserstein\ndistance between samples and the corresponding computational times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel particle-based variational inference method designed to\nwork with multimodal distributions. Our approach, referred to as Branched Stein\nVariational Gradient Descent (BSVGD), extends the classical Stein Variational\nGradient Descent (SVGD) algorithm by incorporating a random branching mechanism\nthat encourages the exploration of the state space. In this work, a theoretical\nguarantee for the convergence in distribution is presented, as well as\nnumerical experiments to validate the suitability of our algorithm. Performance\ncomparisons between the BSVGD and the SVGD are presented using the Wasserstein\ndistance between samples and the corresponding computational times."
                },
                "authors": [
                    {
                        "name": "Isaas Baales"
                    },
                    {
                        "name": "Arturo Jaramillo"
                    },
                    {
                        "name": "Joshu Hel Ricalde-Guerrero"
                    }
                ],
                "author_detail": {
                    "name": "Joshu Hel Ricalde-Guerrero"
                },
                "author": "Joshu Hel Ricalde-Guerrero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 65C05, 65C35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11628v2",
                "updated": "2025-07-17T15:01:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    1,
                    23,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-15T18:05:43Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    18,
                    5,
                    43,
                    1,
                    196,
                    0
                ],
                "title": "DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday\n  Storytelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday\n  Storytelling"
                },
                "summary": "An interactive vignette is a popular and immersive visual storytelling\napproach that invites viewers to role-play a character and influences the\nnarrative in an interactive environment. However, it has not been widely used\nby everyday storytellers yet due to authoring complexity, which conflicts with\nthe immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted\nauthoring system for interactive vignette creation in everyday storytelling. It\ntakes a natural language story as input and extracts the three core elements of\nan interactive vignette (environment, characters, and events), enabling authors\nto focus on refining these elements instead of constructing them from scratch.\nThen, it automatically transforms the single-branch story input into a\nbranch-and-bottleneck structure using an LLM-powered narrative planner, which\nenables flexible viewer interactions while freeing the author from\nmulti-branching. A technical evaluation (N=16) shows that DiaryPlay-generated\ncharacter activities are on par with human-authored ones regarding\nbelievability. A user study (N=16) shows that DiaryPlay effectively supports\nauthors in creating interactive vignette elements, maintains authorial intent\nwhile reacting to viewer interactions, and provides engaging viewing\nexperiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An interactive vignette is a popular and immersive visual storytelling\napproach that invites viewers to role-play a character and influences the\nnarrative in an interactive environment. However, it has not been widely used\nby everyday storytellers yet due to authoring complexity, which conflicts with\nthe immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted\nauthoring system for interactive vignette creation in everyday storytelling. It\ntakes a natural language story as input and extracts the three core elements of\nan interactive vignette (environment, characters, and events), enabling authors\nto focus on refining these elements instead of constructing them from scratch.\nThen, it automatically transforms the single-branch story input into a\nbranch-and-bottleneck structure using an LLM-powered narrative planner, which\nenables flexible viewer interactions while freeing the author from\nmulti-branching. A technical evaluation (N=16) shows that DiaryPlay-generated\ncharacter activities are on par with human-authored ones regarding\nbelievability. A user study (N=16) shows that DiaryPlay effectively supports\nauthors in creating interactive vignette elements, maintains authorial intent\nwhile reacting to viewer interactions, and provides engaging viewing\nexperiences."
                },
                "authors": [
                    {
                        "name": "Jiangnan Xu"
                    },
                    {
                        "name": "Haeseul Cha"
                    },
                    {
                        "name": "Gosu Choi"
                    },
                    {
                        "name": "Gyu-cheol Lee"
                    },
                    {
                        "name": "Yeo-Jin Yoon"
                    },
                    {
                        "name": "Zucheul Lee"
                    },
                    {
                        "name": "Konstantinos Papangelis"
                    },
                    {
                        "name": "Dae Hyun Kim"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10917v2",
                "updated": "2025-07-17T15:01:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    1,
                    8,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-15T02:13:54Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    2,
                    13,
                    54,
                    1,
                    196,
                    0
                ],
                "title": "LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation"
                },
                "summary": "Recently, much effort has been devoted to modeling users' multi-interests\nbased on their behaviors or auxiliary signals. However, existing methods often\nrely on heuristic assumptions, e.g., co-occurring items indicate the same\ninterest of users, failing to capture user multi-interests aligning with\nreal-world scenarios. While large language models (LLMs) show significant\npotential for multi-interest analysis due to their extensive knowledge and\npowerful reasoning capabilities, two key challenges remain. First, the\ngranularity of LLM-driven multi-interests is agnostic, possibly leading to\noverly fine or coarse interest grouping. Second, individual user analysis\nprovides limited insights due to the data sparsity issue. In this paper, we\npropose an LLM-driven dual-level multi-interest modeling framework for more\neffective recommendation. At the user-individual level, we exploit LLMs to\nflexibly allocate items engaged by users into different semantic clusters,\nindicating their diverse and distinct interests. To alleviate the agnostic\ngeneration of LLMs, we adaptively assign these semantic clusters to users'\ncollaborative multi-interests learned from global user-item interactions,\nallowing the granularity to be automatically adjusted according to the user's\nbehaviors using an alignment module. To alleviate the limited insights derived\nfrom individual users' behaviors, at the user-crowd level, we propose\naggregating user cliques into synthesized users with rich behaviors for more\ncomprehensive LLM-driven multi-interest analysis. We formulate a max covering\nproblem to ensure the compactness and representativeness of synthesized users'\nbehaviors, and then conduct contrastive learning based on their LLM-driven\nmulti-interests to disentangle item representations among different interests.\nExperiments on real-world datasets show the superiority of our approach against\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, much effort has been devoted to modeling users' multi-interests\nbased on their behaviors or auxiliary signals. However, existing methods often\nrely on heuristic assumptions, e.g., co-occurring items indicate the same\ninterest of users, failing to capture user multi-interests aligning with\nreal-world scenarios. While large language models (LLMs) show significant\npotential for multi-interest analysis due to their extensive knowledge and\npowerful reasoning capabilities, two key challenges remain. First, the\ngranularity of LLM-driven multi-interests is agnostic, possibly leading to\noverly fine or coarse interest grouping. Second, individual user analysis\nprovides limited insights due to the data sparsity issue. In this paper, we\npropose an LLM-driven dual-level multi-interest modeling framework for more\neffective recommendation. At the user-individual level, we exploit LLMs to\nflexibly allocate items engaged by users into different semantic clusters,\nindicating their diverse and distinct interests. To alleviate the agnostic\ngeneration of LLMs, we adaptively assign these semantic clusters to users'\ncollaborative multi-interests learned from global user-item interactions,\nallowing the granularity to be automatically adjusted according to the user's\nbehaviors using an alignment module. To alleviate the limited insights derived\nfrom individual users' behaviors, at the user-crowd level, we propose\naggregating user cliques into synthesized users with rich behaviors for more\ncomprehensive LLM-driven multi-interest analysis. We formulate a max covering\nproblem to ensure the compactness and representativeness of synthesized users'\nbehaviors, and then conduct contrastive learning based on their LLM-driven\nmulti-interests to disentangle item representations among different interests.\nExperiments on real-world datasets show the superiority of our approach against\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Ziyan Wang"
                    },
                    {
                        "name": "Yingpeng Du"
                    },
                    {
                        "name": "Zhu Sun"
                    },
                    {
                        "name": "Jieyi Bi"
                    },
                    {
                        "name": "Haoyan Chua"
                    },
                    {
                        "name": "Tianjun Wei"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11192v2",
                "updated": "2025-07-17T15:00:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    0,
                    34,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-15T10:52:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    10,
                    52,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Recent Advances in Simulation-based Inference for Gravitational Wave\n  Data Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Simulation-based Inference for Gravitational Wave\n  Data Analysis"
                },
                "summary": "The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration\nhas ushered in a new era of observational astronomy, emphasizing the need for\nrapid and detailed parameter estimation and population-level analyses.\nTraditional Bayesian inference methods, particularly Markov chain Monte Carlo,\nface significant computational challenges when dealing with the\nhigh-dimensional parameter spaces and complex noise characteristics inherent in\ngravitational wave data. This review examines the emerging role of\nsimulation-based inference methods in gravitational wave astronomy, with a\nfocus on approaches that leverage machine-learning techniques such as\nnormalizing flows and neural posterior estimation. We provide a comprehensive\noverview of the theoretical foundations underlying various simulation-based\ninference methods, including neural posterior estimation, neural ratio\nestimation, neural likelihood estimation, flow matching, and consistency\nmodels. We explore the applications of these methods across diverse\ngravitational wave data processing scenarios, from single-source parameter\nestimation and overlapping signal analysis to testing general relativity and\nconducting population studies. Although these techniques demonstrate speed\nimprovements over traditional methods in controlled studies, their\nmodel-dependent nature and sensitivity to prior assumptions are barriers to\ntheir widespread adoption. Their accuracy, which is similar to that of\nconventional methods, requires further validation across broader parameter\nspaces and noise conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration\nhas ushered in a new era of observational astronomy, emphasizing the need for\nrapid and detailed parameter estimation and population-level analyses.\nTraditional Bayesian inference methods, particularly Markov chain Monte Carlo,\nface significant computational challenges when dealing with the\nhigh-dimensional parameter spaces and complex noise characteristics inherent in\ngravitational wave data. This review examines the emerging role of\nsimulation-based inference methods in gravitational wave astronomy, with a\nfocus on approaches that leverage machine-learning techniques such as\nnormalizing flows and neural posterior estimation. We provide a comprehensive\noverview of the theoretical foundations underlying various simulation-based\ninference methods, including neural posterior estimation, neural ratio\nestimation, neural likelihood estimation, flow matching, and consistency\nmodels. We explore the applications of these methods across diverse\ngravitational wave data processing scenarios, from single-source parameter\nestimation and overlapping signal analysis to testing general relativity and\nconducting population studies. Although these techniques demonstrate speed\nimprovements over traditional methods in controlled studies, their\nmodel-dependent nature and sensitivity to prior assumptions are barriers to\ntheir widespread adoption. Their accuracy, which is similar to that of\nconventional methods, requires further validation across broader parameter\nspaces and noise conditions."
                },
                "authors": [
                    {
                        "name": "Bo Liang"
                    },
                    {
                        "name": "He Wang"
                    }
                ],
                "author_detail": {
                    "name": "He Wang"
                },
                "author": "He Wang",
                "arxiv_doi": "10.61977/ati2025020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.61977/ati2025020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.11192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "30 pages, 6 figures, 1 table. Minor clarifications added on page 3.\n  Literature covered up to early 2025",
                "arxiv_journal_ref": "Astronomical Techniques and Instruments, Vol. 2, No. 6, November\n  2025",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12273v2",
                "updated": "2025-07-17T14:54:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    54,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T14:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    22,
                    0,
                    2,
                    197,
                    0
                ],
                "title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot"
                },
                "summary": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments."
                },
                "authors": [
                    {
                        "name": "Luca Garello"
                    },
                    {
                        "name": "Francesca Cocchella"
                    },
                    {
                        "name": "Alessandra Sciutti"
                    },
                    {
                        "name": "Manuel Catalano"
                    },
                    {
                        "name": "Francesco Rea"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Rea"
                },
                "author": "Francesco Rea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10234v2",
                "updated": "2025-07-17T14:45:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    45,
                    40,
                    3,
                    198,
                    0
                ],
                "published": "2023-12-15T22:04:53Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    22,
                    4,
                    53,
                    4,
                    349,
                    0
                ],
                "title": "Flexible Nonparametric Inference for Causal Effects under the Front-Door\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Nonparametric Inference for Causal Effects under the Front-Door\n  Model"
                },
                "summary": "Evaluating causal treatment effects in observational studies requires\naddressing confounding. While the back-door criterion enables identification\nthrough adjustment for observed covariates, it fails in the presence of\nunmeasured confounding. The front-door criterion offers an alternative by\nleveraging variables that fully mediate the treatment effect and are unaffected\nby unmeasured confounders of the treatment-outcome pair. We develop novel\none-step and targeted minimum loss-based estimators for both the average\ntreatment effect and the average treatment effect on the treated under\nfront-door assumptions. Our estimators are built on multiple parameterizations\nof the observed data distribution, including approaches that avoid modeling the\nmediator density entirely, and are compatible with flexible, machine\nlearning-based nuisance estimation. We establish conditions for root-$n$\nconsistency and asymptotic linearity by deriving second-order remainder bounds.\nWe also develop flexible tests for assessing identification assumptions,\nincluding a doubly robust testing procedure, within a semiparametric extension\nof the front-door model that encodes generalized (Verma) independence\nconstraints. We further show how these constraints can be leveraged to improve\nthe efficiency of causal effect estimators. Simulation studies confirm\nfavorable finite-sample performance, and real-data applications in education\nand emergency medicine illustrate the practical utility of our methods. An\naccompanying R package, fdcausal, implements all proposed procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating causal treatment effects in observational studies requires\naddressing confounding. While the back-door criterion enables identification\nthrough adjustment for observed covariates, it fails in the presence of\nunmeasured confounding. The front-door criterion offers an alternative by\nleveraging variables that fully mediate the treatment effect and are unaffected\nby unmeasured confounders of the treatment-outcome pair. We develop novel\none-step and targeted minimum loss-based estimators for both the average\ntreatment effect and the average treatment effect on the treated under\nfront-door assumptions. Our estimators are built on multiple parameterizations\nof the observed data distribution, including approaches that avoid modeling the\nmediator density entirely, and are compatible with flexible, machine\nlearning-based nuisance estimation. We establish conditions for root-$n$\nconsistency and asymptotic linearity by deriving second-order remainder bounds.\nWe also develop flexible tests for assessing identification assumptions,\nincluding a doubly robust testing procedure, within a semiparametric extension\nof the front-door model that encodes generalized (Verma) independence\nconstraints. We further show how these constraints can be leveraged to improve\nthe efficiency of causal effect estimators. Simulation studies confirm\nfavorable finite-sample performance, and real-data applications in education\nand emergency medicine illustrate the practical utility of our methods. An\naccompanying R package, fdcausal, implements all proposed procedures."
                },
                "authors": [
                    {
                        "name": "Anna Guo"
                    },
                    {
                        "name": "David Benkeser"
                    },
                    {
                        "name": "Razieh Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Razieh Nabi"
                },
                "author": "Razieh Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13175v1",
                "updated": "2025-07-17T14:39:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    39,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:39:29Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    39,
                    29,
                    3,
                    198,
                    0
                ],
                "title": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in\n  the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in\n  the LLM Era"
                },
                "summary": "The advancement of powerful yet opaque large language models (LLMs)\nnecessitates a fundamental revision of the philosophical criteria used to\nevaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the\nassumption of transparent architectures, which LLMs defy due to their\nstochastic outputs and opaque internal states. This paper argues that\ntraditional ethical criteria are pragmatically obsolete for LLMs due to this\nmismatch. Engaging with core themes in the philosophy of technology, this paper\nproffers a revised set of ten functional criteria to evaluate LLM-based\nartificial moral agents: moral concordance, context sensitivity, normative\nintegrity, metaethical awareness, system resilience, trustworthiness,\ncorrigibility, partial transparency, functional autonomy, and moral\nimagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating\nMoral Agency through Large Language Systems), aim to steer AMAs toward greater\nalignment and beneficial societal integration in the coming years. We\nillustrate these criteria using hypothetical scenarios involving an autonomous\npublic bus (APB) to demonstrate their practical applicability in morally\nsalient contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of powerful yet opaque large language models (LLMs)\nnecessitates a fundamental revision of the philosophical criteria used to\nevaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the\nassumption of transparent architectures, which LLMs defy due to their\nstochastic outputs and opaque internal states. This paper argues that\ntraditional ethical criteria are pragmatically obsolete for LLMs due to this\nmismatch. Engaging with core themes in the philosophy of technology, this paper\nproffers a revised set of ten functional criteria to evaluate LLM-based\nartificial moral agents: moral concordance, context sensitivity, normative\nintegrity, metaethical awareness, system resilience, trustworthiness,\ncorrigibility, partial transparency, functional autonomy, and moral\nimagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating\nMoral Agency through Large Language Systems), aim to steer AMAs toward greater\nalignment and beneficial societal integration in the coming years. We\nillustrate these criteria using hypothetical scenarios involving an autonomous\npublic bus (APB) to demonstrate their practical applicability in morally\nsalient contexts."
                },
                "authors": [
                    {
                        "name": "Matthew E. Brophy"
                    }
                ],
                "author_detail": {
                    "name": "Matthew E. Brophy"
                },
                "author": "Matthew E. Brophy",
                "arxiv_comment": "42 pages. Supplementary material included at end of article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T27, 03B42 68T27, 03B4268T27, 03B42 68T27, 03B42 68T27, 03B42\n  68T27, 03B42 68T27, 03B42 68T27, 03B4268T27, 03B42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.9; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13169v1",
                "updated": "2025-07-17T14:33:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    33,
                    36,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:33:36Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    33,
                    36,
                    3,
                    198,
                    0
                ],
                "title": "Prompt Injection 2.0: Hybrid AI Threats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Injection 2.0: Hybrid AI Threats"
                },
                "summary": "Prompt injection attacks, where malicious input is designed to manipulate AI\nsystems into ignoring their original instructions and following unauthorized\ncommands instead, were first discovered by Preamble, Inc. in May 2022 and\nresponsibly disclosed to OpenAI. Over the last three years, these attacks have\ncontinued to pose a critical security threat to LLM-integrated systems. The\nemergence of agentic AI systems, where LLMs autonomously perform multistep\ntasks through tools and coordination with other agents, has fundamentally\ntransformed the threat landscape. Modern prompt injection attacks can now\ncombine with traditional cybersecurity exploits to create hybrid threats that\nsystematically evade traditional security controls. This paper presents a\ncomprehensive analysis of Prompt Injection 2.0, examining how prompt injections\nintegrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF),\nand other web security vulnerabilities to bypass traditional security measures.\nWe build upon Preamble's foundational research and mitigation technologies,\nevaluating them against contemporary threats, including AI worms, multi-agent\ninfections, and hybrid cyber-AI attacks. Our analysis incorporates recent\nbenchmarks that demonstrate how traditional web application firewalls, XSS\nfilters, and CSRF tokens fail against AI-enhanced attacks. We also present\narchitectural solutions that combine prompt isolation, runtime security, and\nprivilege separation with novel threat detection capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection attacks, where malicious input is designed to manipulate AI\nsystems into ignoring their original instructions and following unauthorized\ncommands instead, were first discovered by Preamble, Inc. in May 2022 and\nresponsibly disclosed to OpenAI. Over the last three years, these attacks have\ncontinued to pose a critical security threat to LLM-integrated systems. The\nemergence of agentic AI systems, where LLMs autonomously perform multistep\ntasks through tools and coordination with other agents, has fundamentally\ntransformed the threat landscape. Modern prompt injection attacks can now\ncombine with traditional cybersecurity exploits to create hybrid threats that\nsystematically evade traditional security controls. This paper presents a\ncomprehensive analysis of Prompt Injection 2.0, examining how prompt injections\nintegrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF),\nand other web security vulnerabilities to bypass traditional security measures.\nWe build upon Preamble's foundational research and mitigation technologies,\nevaluating them against contemporary threats, including AI worms, multi-agent\ninfections, and hybrid cyber-AI attacks. Our analysis incorporates recent\nbenchmarks that demonstrate how traditional web application firewalls, XSS\nfilters, and CSRF tokens fail against AI-enhanced attacks. We also present\narchitectural solutions that combine prompt isolation, runtime security, and\nprivilege separation with novel threat detection capabilities."
                },
                "authors": [
                    {
                        "name": "Jeremy McHugh"
                    },
                    {
                        "name": "Kristina ekrst"
                    },
                    {
                        "name": "Jon Cefalu"
                    }
                ],
                "author_detail": {
                    "name": "Jon Cefalu"
                },
                "author": "Jon Cefalu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13158v1",
                "updated": "2025-07-17T14:22:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    22,
                    24,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:22:24Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    22,
                    24,
                    3,
                    198,
                    0
                ],
                "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training:\n  Basics, Advances, and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Reinforcement Learning Meets Large Language Model Post-Training:\n  Basics, Advances, and Opportunities"
                },
                "summary": "In the era of Large Language Models (LLMs), alignment has emerged as a\nfundamental yet challenging problem in the pursuit of more reliable,\ncontrollable, and capable machine intelligence. The recent success of reasoning\nmodels and conversational AI systems has underscored the critical role of\nreinforcement learning (RL) in enhancing these systems, driving increased\nresearch interest at the intersection of RL and LLM alignment. This paper\nprovides a comprehensive review of recent advances in LLM alignment through the\nlens of inverse reinforcement learning (IRL), emphasizing the distinctions\nbetween RL techniques employed in LLM alignment and those in conventional RL\ntasks. In particular, we highlight the necessity of constructing neural reward\nmodels from human data and discuss the formal and practical implications of\nthis paradigm shift. We begin by introducing fundamental concepts in RL to\nprovide a foundation for readers unfamiliar with the field. We then examine\nrecent advances in this research agenda, discussing key challenges and\nopportunities in conducting IRL for LLM alignment. Beyond methodological\nconsiderations, we explore practical aspects, including datasets, benchmarks,\nevaluation metrics, infrastructure, and computationally efficient training and\ninference techniques. Finally, we draw insights from the literature on\nsparse-reward RL to identify open questions and potential research directions.\nBy synthesizing findings from diverse studies, we aim to provide a structured\nand critical overview of the field, highlight unresolved challenges, and\noutline promising future directions for improving LLM alignment through RL and\nIRL techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of Large Language Models (LLMs), alignment has emerged as a\nfundamental yet challenging problem in the pursuit of more reliable,\ncontrollable, and capable machine intelligence. The recent success of reasoning\nmodels and conversational AI systems has underscored the critical role of\nreinforcement learning (RL) in enhancing these systems, driving increased\nresearch interest at the intersection of RL and LLM alignment. This paper\nprovides a comprehensive review of recent advances in LLM alignment through the\nlens of inverse reinforcement learning (IRL), emphasizing the distinctions\nbetween RL techniques employed in LLM alignment and those in conventional RL\ntasks. In particular, we highlight the necessity of constructing neural reward\nmodels from human data and discuss the formal and practical implications of\nthis paradigm shift. We begin by introducing fundamental concepts in RL to\nprovide a foundation for readers unfamiliar with the field. We then examine\nrecent advances in this research agenda, discussing key challenges and\nopportunities in conducting IRL for LLM alignment. Beyond methodological\nconsiderations, we explore practical aspects, including datasets, benchmarks,\nevaluation metrics, infrastructure, and computationally efficient training and\ninference techniques. Finally, we draw insights from the literature on\nsparse-reward RL to identify open questions and potential research directions.\nBy synthesizing findings from diverse studies, we aim to provide a structured\nand critical overview of the field, highlight unresolved challenges, and\noutline promising future directions for improving LLM alignment through RL and\nIRL techniques."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13152v1",
                "updated": "2025-07-17T14:13:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    13,
                    50,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:13:50Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    13,
                    50,
                    3,
                    198,
                    0
                ],
                "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models"
                },
                "summary": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN."
                },
                "authors": [
                    {
                        "name": "Xiangyu Dong"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Jiang Gao"
                    },
                    {
                        "name": "Haozhou Li"
                    },
                    {
                        "name": "Xiaoguang Ma"
                    },
                    {
                        "name": "Yaoming Zhou"
                    },
                    {
                        "name": "Fuhai Chen"
                    },
                    {
                        "name": "Juan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Juan Liu"
                },
                "author": "Juan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19263v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19263v3",
                "updated": "2025-07-17T14:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    10,
                    6,
                    3,
                    198,
                    0
                ],
                "published": "2025-03-25T01:57:59Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    1,
                    57,
                    59,
                    1,
                    84,
                    0
                ],
                "title": "DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow\n  Generation & Instruct-Masking Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow\n  Generation & Instruct-Masking Tuning"
                },
                "summary": "Visual reasoning (VR), which is crucial in many fields for enabling\nhuman-like visual understanding, remains highly challenging. Recently,\ncompositional visual reasoning approaches, which leverage the reasoning\nabilities of large language models (LLMs) with integrated tools to solve\nproblems, have shown promise as more effective strategies than end-to-end VR\nmethods. However, these approaches face limitations, as frozen LLMs lack tool\nawareness in VR, leading to performance bottlenecks. While leveraging LLMs for\nreasoning is widely used in other domains, they are not directly applicable to\nVR due to limited training data, imperfect tools that introduce errors and\nreduce data collection efficiency in VR, and challenging in fine-tuning on\nnoisy workflows. To address these challenges, we propose DWIM: i)\nDiscrepancy-aware training Workflow generation, which assesses tool usage and\nextracts more viable workflows for training; and ii) Instruct-Masking\nfine-tuning, which guides the model to only clone effective actions, enabling\nthe generation of more practical solutions. Our experiments demonstrate that\nDWIM achieves state-of-the-art performance across various VR tasks, exhibiting\nstrong generalization on multiple widely-used datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning (VR), which is crucial in many fields for enabling\nhuman-like visual understanding, remains highly challenging. Recently,\ncompositional visual reasoning approaches, which leverage the reasoning\nabilities of large language models (LLMs) with integrated tools to solve\nproblems, have shown promise as more effective strategies than end-to-end VR\nmethods. However, these approaches face limitations, as frozen LLMs lack tool\nawareness in VR, leading to performance bottlenecks. While leveraging LLMs for\nreasoning is widely used in other domains, they are not directly applicable to\nVR due to limited training data, imperfect tools that introduce errors and\nreduce data collection efficiency in VR, and challenging in fine-tuning on\nnoisy workflows. To address these challenges, we propose DWIM: i)\nDiscrepancy-aware training Workflow generation, which assesses tool usage and\nextracts more viable workflows for training; and ii) Instruct-Masking\nfine-tuning, which guides the model to only clone effective actions, enabling\nthe generation of more practical solutions. Our experiments demonstrate that\nDWIM achieves state-of-the-art performance across various VR tasks, exhibiting\nstrong generalization on multiple widely-used datasets."
                },
                "authors": [
                    {
                        "name": "Fucai Ke"
                    },
                    {
                        "name": "Vijay Kumar B G"
                    },
                    {
                        "name": "Xingjian Leng"
                    },
                    {
                        "name": "Zhixi Cai"
                    },
                    {
                        "name": "Zaid Khan"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Pari Delir Haghighi"
                    },
                    {
                        "name": "Hamid Rezatofighi"
                    },
                    {
                        "name": "Manmohan Chandraker"
                    }
                ],
                "author_detail": {
                    "name": "Manmohan Chandraker"
                },
                "author": "Manmohan Chandraker",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19263v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19263v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11059v2",
                "updated": "2025-07-17T14:04:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    4,
                    7,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-15T07:52:33Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    7,
                    52,
                    33,
                    1,
                    196,
                    0
                ],
                "title": "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language\n  Models on Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language\n  Models on Software Engineering Tasks"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025."
                },
                "authors": [
                    {
                        "name": "Pavel Adamenko"
                    },
                    {
                        "name": "Mikhail Ivanov"
                    },
                    {
                        "name": "Aidar Valeev"
                    },
                    {
                        "name": "Rodion Levichev"
                    },
                    {
                        "name": "Pavel Zadorozhny"
                    },
                    {
                        "name": "Ivan Lopatin"
                    },
                    {
                        "name": "Dmitry Babayev"
                    },
                    {
                        "name": "Alena Fenogenova"
                    },
                    {
                        "name": "Valentin Malykh"
                    }
                ],
                "author_detail": {
                    "name": "Valentin Malykh"
                },
                "author": "Valentin Malykh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13140v1",
                "updated": "2025-07-17T14:02:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    2,
                    40,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:02:40Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    2,
                    40,
                    3,
                    198,
                    0
                ],
                "title": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and\n  Intention-Driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and\n  Intention-Driven Agents"
                },
                "summary": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments."
                },
                "authors": [
                    {
                        "name": "Kuiyuan Ding"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Jianzhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jianzhang Guo"
                },
                "author": "Jianzhang Guo",
                "arxiv_comment": "6 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13138v1",
                "updated": "2025-07-17T14:00:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    0,
                    13,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:00:13Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    0,
                    13,
                    3,
                    198,
                    0
                ],
                "title": "Assessing the Reliability of LLMs Annotations in the Context of\n  Demographic Bias and Model Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Reliability of LLMs Annotations in the Context of\n  Demographic Bias and Model Explanation"
                },
                "summary": "Understanding the sources of variability in annotations is crucial for\ndeveloping fair NLP systems, especially for tasks like sexism detection where\ndemographic bias is a concern. This study investigates the extent to which\nannotator demographic features influence labeling decisions compared to text\ncontent. Using a Generalized Linear Mixed Model, we quantify this inf luence,\nfinding that while statistically present, demographic factors account for a\nminor fraction ( 8%) of the observed variance, with tweet content being the\ndominant factor. We then assess the reliability of Generative AI (GenAI) models\nas annotators, specifically evaluating if guiding them with demographic\npersonas improves alignment with human judgments. Our results indicate that\nsimplistic persona prompting often fails to enhance, and sometimes degrades,\nperformance compared to baseline models. Furthermore, explainable AI (XAI)\ntechniques reveal that model predictions rely heavily on content-specific\ntokens related to sexism, rather than correlates of demographic\ncharacteristics. We argue that focusing on content-driven explanations and\nrobust annotation protocols offers a more reliable path towards fairness than\npotentially persona simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the sources of variability in annotations is crucial for\ndeveloping fair NLP systems, especially for tasks like sexism detection where\ndemographic bias is a concern. This study investigates the extent to which\nannotator demographic features influence labeling decisions compared to text\ncontent. Using a Generalized Linear Mixed Model, we quantify this inf luence,\nfinding that while statistically present, demographic factors account for a\nminor fraction ( 8%) of the observed variance, with tweet content being the\ndominant factor. We then assess the reliability of Generative AI (GenAI) models\nas annotators, specifically evaluating if guiding them with demographic\npersonas improves alignment with human judgments. Our results indicate that\nsimplistic persona prompting often fails to enhance, and sometimes degrades,\nperformance compared to baseline models. Furthermore, explainable AI (XAI)\ntechniques reveal that model predictions rely heavily on content-specific\ntokens related to sexism, rather than correlates of demographic\ncharacteristics. We argue that focusing on content-driven explanations and\nrobust annotation protocols offers a more reliable path towards fairness than\npotentially persona simulation."
                },
                "authors": [
                    {
                        "name": "Hadi Mohammadi"
                    },
                    {
                        "name": "Tina Shahedi"
                    },
                    {
                        "name": "Pablo Mosteiro"
                    },
                    {
                        "name": "Massimo Poesio"
                    },
                    {
                        "name": "Ayoub Bagheri"
                    },
                    {
                        "name": "Anastasia Giachanou"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Giachanou"
                },
                "author": "Anastasia Giachanou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13123v1",
                "updated": "2025-07-17T13:38:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    38,
                    16,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T13:38:16Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    38,
                    16,
                    3,
                    198,
                    0
                ],
                "title": "Detecting LLM-generated Code with Subtle Modification by Adversarial\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting LLM-generated Code with Subtle Modification by Adversarial\n  Training"
                },
                "summary": "With the rapid development of Large Language Models (LLMs), their powerful\ncode-generation capabilities have been widely applied in tasks like code\ncompletion and automated development, demonstrating the value of improving\ncoding efficiency. However, the extensive use of LLM-generated code also raises\nseveral new challenges. On the one hand, issues such as the regulation of code\nprovenance, copyright disputes, and code quality have become increasingly\nconcerning. How to effectively detect LLM-generated code and ensure its\ncompliant and responsible use has become a critical and urgent issue. On the\nother hand, in practical applications, LLM-generated code is often subject to\nmanual modifications, such as variable renaming or structural adjustments.\nAlthough some recent studies have proposed training-based and zero-shot methods\nfor detecting LLM-generated code, these approaches show insufficient robustness\nwhen facing modified LLM-generated code, and there is a lack of an effective\nsolution. To address the real-world scenario where LLM-generated code may\nundergo minor modifications, we propose CodeGPTSensor+, an enhanced version of\nCodeGPTSensor, which employs adversarial training to improve robustness against\ninput perturbations. CodeGPTSensor+ integrates an adversarial sample generation\nmodule, Multi-objective Identifier and Structure Transformation (MIST), which\nsystematically generates both high-quality and representative adversarial\nsamples. This module effectively enhances the model's resistance against\ndiverse adversarial attacks. Experimental results on the HMCorp dataset\ndemonstrate that CodeGPTSensor+ significantly improves detection accuracy on\nthe adversarial test set while maintaining high accuracy on the original test\nset, showcasing superior robustness compared to CodeGPTSensor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of Large Language Models (LLMs), their powerful\ncode-generation capabilities have been widely applied in tasks like code\ncompletion and automated development, demonstrating the value of improving\ncoding efficiency. However, the extensive use of LLM-generated code also raises\nseveral new challenges. On the one hand, issues such as the regulation of code\nprovenance, copyright disputes, and code quality have become increasingly\nconcerning. How to effectively detect LLM-generated code and ensure its\ncompliant and responsible use has become a critical and urgent issue. On the\nother hand, in practical applications, LLM-generated code is often subject to\nmanual modifications, such as variable renaming or structural adjustments.\nAlthough some recent studies have proposed training-based and zero-shot methods\nfor detecting LLM-generated code, these approaches show insufficient robustness\nwhen facing modified LLM-generated code, and there is a lack of an effective\nsolution. To address the real-world scenario where LLM-generated code may\nundergo minor modifications, we propose CodeGPTSensor+, an enhanced version of\nCodeGPTSensor, which employs adversarial training to improve robustness against\ninput perturbations. CodeGPTSensor+ integrates an adversarial sample generation\nmodule, Multi-objective Identifier and Structure Transformation (MIST), which\nsystematically generates both high-quality and representative adversarial\nsamples. This module effectively enhances the model's resistance against\ndiverse adversarial attacks. Experimental results on the HMCorp dataset\ndemonstrate that CodeGPTSensor+ significantly improves detection accuracy on\nthe adversarial test set while maintaining high accuracy on the original test\nset, showcasing superior robustness compared to CodeGPTSensor."
                },
                "authors": [
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13117v1",
                "updated": "2025-07-17T13:32:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    32,
                    59,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T13:32:59Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    32,
                    59,
                    3,
                    198,
                    0
                ],
                "title": "Inferring Attributed Grammars from Parser Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Attributed Grammars from Parser Implementations"
                },
                "summary": "Software systems that process structured inputs often lack complete and\nup-to-date specifications, which specify the input syntax and the semantics of\ninput processing. While grammar mining techniques have focused on recovering\nsyntactic structures, the semantics of input processing remains largely\nunexplored. In this work, we introduce a novel approach for inferring\nattributed grammars from parser implementations. Given an input grammar, our\ntechnique dynamically analyzes the implementation of recursive descent parsers\nto reconstruct the semantic aspects of input handling, resulting in\nspecifications in the form of attributed grammars. By observing program\nexecutions and mapping the program's runtime behavior to the grammar, we\nsystematically extract and embed semantic actions into the grammar rules. This\nenables comprehensive specification recovery. We demonstrate the feasibility of\nour approach using an initial set of programs, showing that it can accurately\nreproduce program behavior through the generated attributed grammars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software systems that process structured inputs often lack complete and\nup-to-date specifications, which specify the input syntax and the semantics of\ninput processing. While grammar mining techniques have focused on recovering\nsyntactic structures, the semantics of input processing remains largely\nunexplored. In this work, we introduce a novel approach for inferring\nattributed grammars from parser implementations. Given an input grammar, our\ntechnique dynamically analyzes the implementation of recursive descent parsers\nto reconstruct the semantic aspects of input handling, resulting in\nspecifications in the form of attributed grammars. By observing program\nexecutions and mapping the program's runtime behavior to the grammar, we\nsystematically extract and embed semantic actions into the grammar rules. This\nenables comprehensive specification recovery. We demonstrate the feasibility of\nour approach using an initial set of programs, showing that it can accurately\nreproduce program behavior through the generated attributed grammars."
                },
                "authors": [
                    {
                        "name": "Andreas Pointner"
                    },
                    {
                        "name": "Josef Pichler"
                    },
                    {
                        "name": "Herbert Prhofer"
                    }
                ],
                "author_detail": {
                    "name": "Herbert Prhofer"
                },
                "author": "Herbert Prhofer",
                "arxiv_comment": "Accepted to ICSME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07389v2",
                "updated": "2025-07-17T13:24:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    24,
                    18,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-10T02:19:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    2,
                    19,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Task-Circuit Quantization: Leveraging Knowledge Localization and\n  Interpretability for Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Circuit Quantization: Leveraging Knowledge Localization and\n  Interpretability for Compression"
                },
                "summary": "Post-training quantization (PTQ) reduces a model's memory footprint by\nmapping full precision weights into low bit weights without costly retraining,\nbut can degrade its downstream performance especially in low 2- to 3-bit\nsettings. We develop a new mixed-precision PTQ approach, Task-Circuit\nQuantization (TaCQ), that draws parallels to automated circuit discovery,\ndirectly conditioning the quantization process on specific weight circuits --\nwhich we define as sets of weights associated with downstream task performance.\nThese weights are kept as 16-bit weights, while others are quantized,\nmaintaining performance while only adding a marginal memory cost. Specifically,\nTaCQ contrasts unquantized model weights with a uniformly-quantized model to\nestimate the expected change in weights due to quantization and uses gradient\ninformation to predict the resulting impact on task performance, allowing us to\npreserve task-specific weights. We compare TaCQ-based quantization to existing\nmixed-precision quantization methods when conditioning both on general-purpose\nand task-specific data. Across QA, math reasoning, and text-to-SQL tasks for\nboth Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the\nsame calibration data and a lower weight budget, achieving major improvements\nin the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of\nLlama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25%\nabsolute improvement over SPQR. We also observe consistently large gains over\nexisting methods in the 2-bit regime, with an average gain of 14.74% over the\nstrongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without\nconditioning on specific tasks, showing TaCQ's ability to identify important\nweights is not limited to task-conditioned settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) reduces a model's memory footprint by\nmapping full precision weights into low bit weights without costly retraining,\nbut can degrade its downstream performance especially in low 2- to 3-bit\nsettings. We develop a new mixed-precision PTQ approach, Task-Circuit\nQuantization (TaCQ), that draws parallels to automated circuit discovery,\ndirectly conditioning the quantization process on specific weight circuits --\nwhich we define as sets of weights associated with downstream task performance.\nThese weights are kept as 16-bit weights, while others are quantized,\nmaintaining performance while only adding a marginal memory cost. Specifically,\nTaCQ contrasts unquantized model weights with a uniformly-quantized model to\nestimate the expected change in weights due to quantization and uses gradient\ninformation to predict the resulting impact on task performance, allowing us to\npreserve task-specific weights. We compare TaCQ-based quantization to existing\nmixed-precision quantization methods when conditioning both on general-purpose\nand task-specific data. Across QA, math reasoning, and text-to-SQL tasks for\nboth Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the\nsame calibration data and a lower weight budget, achieving major improvements\nin the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of\nLlama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25%\nabsolute improvement over SPQR. We also observe consistently large gains over\nexisting methods in the 2-bit regime, with an average gain of 14.74% over the\nstrongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without\nconditioning on specific tasks, showing TaCQ's ability to identify important\nweights is not limited to task-conditioned settings."
                },
                "authors": [
                    {
                        "name": "Hanqi Xiao"
                    },
                    {
                        "name": "Yi-Lin Sung"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "COLM 2025 Camera Ready. Code:\n  https://github.com/The-Inscrutable-X/TACQ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13107v1",
                "updated": "2025-07-17T13:22:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    22,
                    40,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T13:22:40Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    22,
                    40,
                    3,
                    198,
                    0
                ],
                "title": "R^2MoE: Redundancy-Removal Mixture of Experts for Lifelong Concept\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R^2MoE: Redundancy-Removal Mixture of Experts for Lifelong Concept\n  Learning"
                },
                "summary": "Enabling large-scale generative models to continuously learn new visual\nconcepts is essential for personalizing pre-trained models to meet individual\nuser preferences. Existing approaches for continual visual concept learning are\nconstrained by two fundamental challenges: catastrophic forgetting and\nparameter expansion. In this paper, we propose Redundancy-Removal Mixture of\nExperts (R^2MoE), a parameter-efficient framework for lifelong visual concept\nlearning that effectively learns new concepts while incurring minimal parameter\noverhead. Our framework includes three key innovative contributions: First, we\npropose a mixture-of-experts framework with a routing distillation mechanism\nthat enables experts to acquire concept-specific knowledge while preserving the\ngating network's routing capability, thereby effectively mitigating\ncatastrophic forgetting. Second, we propose a strategy for eliminating\nredundant layer-wise experts that reduces the number of expert parameters by\nfully utilizing previously learned experts. Third, we employ a hierarchical\nlocal attention-guided inference approach to mitigate interference between\ngenerated visual concepts. Extensive experiments have demonstrated that our\nmethod generates images with superior conceptual fidelity compared to the\nstate-of-the-art (SOTA) method, achieving an impressive 87.8\\% reduction in\nforgetting rates and 63.3\\% fewer parameters on the CustomConcept 101 dataset.\nOur code is available at {https://github.com/learninginvision/R2MoE}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling large-scale generative models to continuously learn new visual\nconcepts is essential for personalizing pre-trained models to meet individual\nuser preferences. Existing approaches for continual visual concept learning are\nconstrained by two fundamental challenges: catastrophic forgetting and\nparameter expansion. In this paper, we propose Redundancy-Removal Mixture of\nExperts (R^2MoE), a parameter-efficient framework for lifelong visual concept\nlearning that effectively learns new concepts while incurring minimal parameter\noverhead. Our framework includes three key innovative contributions: First, we\npropose a mixture-of-experts framework with a routing distillation mechanism\nthat enables experts to acquire concept-specific knowledge while preserving the\ngating network's routing capability, thereby effectively mitigating\ncatastrophic forgetting. Second, we propose a strategy for eliminating\nredundant layer-wise experts that reduces the number of expert parameters by\nfully utilizing previously learned experts. Third, we employ a hierarchical\nlocal attention-guided inference approach to mitigate interference between\ngenerated visual concepts. Extensive experiments have demonstrated that our\nmethod generates images with superior conceptual fidelity compared to the\nstate-of-the-art (SOTA) method, achieving an impressive 87.8\\% reduction in\nforgetting rates and 63.3\\% fewer parameters on the CustomConcept 101 dataset.\nOur code is available at {https://github.com/learninginvision/R2MoE}"
                },
                "authors": [
                    {
                        "name": "Xiaohan Guo"
                    },
                    {
                        "name": "Yusong Cai"
                    },
                    {
                        "name": "Zejia Liu"
                    },
                    {
                        "name": "Zhengning Wang"
                    },
                    {
                        "name": "Lili Pan"
                    },
                    {
                        "name": "Hongliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongliang Li"
                },
                "author": "Hongliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13105v1",
                "updated": "2025-07-17T13:19:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    19,
                    50,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T13:19:50Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    19,
                    50,
                    3,
                    198,
                    0
                ],
                "title": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated\n  Summaries For Scientific Abstracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated\n  Summaries For Scientific Abstracts"
                },
                "summary": "We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach."
                },
                "authors": [
                    {
                        "name": "Marc Brinner"
                    },
                    {
                        "name": "Sina Zarriess"
                    }
                ],
                "author_detail": {
                    "name": "Sina Zarriess"
                },
                "author": "Sina Zarriess",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04348v2",
                "updated": "2025-07-17T13:07:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    7,
                    41,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-06T11:21:47Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    11,
                    21,
                    47,
                    6,
                    187,
                    0
                ],
                "title": "SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level\n  Length Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level\n  Length Control"
                },
                "summary": "Large reasoning models (LRMs) have exhibited remarkable reasoning\ncapabilities through inference-time scaling, but this progress has also\nintroduced considerable redundancy and inefficiency into their reasoning\nprocesses, resulting in substantial computational waste. Previous work has\nattempted to mitigate this issue by penalizing the overall length of generated\nsamples during reinforcement learning (RL), with the goal of encouraging a more\nconcise chains of thought. However, we observe that such global length penalty\noften lead to excessive compression of critical reasoning steps while\npreserving unnecessary details in simpler ones, yielding a suboptimal trade-off\nbetween accuracy and efficiency. To address this issue, we propose\nSmartThinker, a two-stage learnable framework designed to enable fine-grained\ncontrol over the length of reasoning chains based on the importance of each\nindividual step. In the first stage, SmartThinker adapts a reasoning model to a\nshort-form reasoning mode through rejection sampling combined with supervised\nfine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length\nControl Policy Optimization (SCPO) to refine the model output distribution,\nwhich increases the proportion of length allocated to critical steps while\nreducing redundancy in less important ones. SCPO consists of four core\ncomponents: an online importance estimator, a step-level length control reward\nfunction, a step-level generalized advantage estimation (S-GAE) and a\ndifficulty-adaptive clipping strategy. Working in concert, these components\nenable SCPO to implement differentiated length control across reasoning steps.\nEmpirical results across multiple reasoning benchmarks and various backbone\nmodels demonstrate that SmartThinker significantly reduces redundant reasoning\nwhile achieving comparable or even superior performance to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have exhibited remarkable reasoning\ncapabilities through inference-time scaling, but this progress has also\nintroduced considerable redundancy and inefficiency into their reasoning\nprocesses, resulting in substantial computational waste. Previous work has\nattempted to mitigate this issue by penalizing the overall length of generated\nsamples during reinforcement learning (RL), with the goal of encouraging a more\nconcise chains of thought. However, we observe that such global length penalty\noften lead to excessive compression of critical reasoning steps while\npreserving unnecessary details in simpler ones, yielding a suboptimal trade-off\nbetween accuracy and efficiency. To address this issue, we propose\nSmartThinker, a two-stage learnable framework designed to enable fine-grained\ncontrol over the length of reasoning chains based on the importance of each\nindividual step. In the first stage, SmartThinker adapts a reasoning model to a\nshort-form reasoning mode through rejection sampling combined with supervised\nfine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length\nControl Policy Optimization (SCPO) to refine the model output distribution,\nwhich increases the proportion of length allocated to critical steps while\nreducing redundancy in less important ones. SCPO consists of four core\ncomponents: an online importance estimator, a step-level length control reward\nfunction, a step-level generalized advantage estimation (S-GAE) and a\ndifficulty-adaptive clipping strategy. Working in concert, these components\nenable SCPO to implement differentiated length control across reasoning steps.\nEmpirical results across multiple reasoning benchmarks and various backbone\nmodels demonstrate that SmartThinker significantly reduces redundant reasoning\nwhile achieving comparable or even superior performance to existing methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Xiao Ling"
                    },
                    {
                        "name": "Jie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Liu"
                },
                "author": "Jie Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04037v2",
                "updated": "2025-07-17T13:02:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    2,
                    5,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-05T13:31:21Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    31,
                    21,
                    5,
                    186,
                    0
                ],
                "title": "Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in\n  Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in\n  Dynamic Environments"
                },
                "summary": "The gap between static benchmarks and the dynamic nature of real-world legal\npractice poses a key barrier to advancing legal intelligence. To this end, we\nintroduce J1-ENVS, the first interactive and dynamic legal environment tailored\nfor LLM-based agents. Guided by legal experts, it comprises six representative\nscenarios from Chinese legal practices across three levels of environmental\ncomplexity. We further introduce J1-EVAL, a fine-grained evaluation framework,\ndesigned to assess both task performance and procedural compliance across\nvarying levels of legal proficiency. Extensive experiments on 17 LLM agents\nreveal that, while many models demonstrate solid legal knowledge, they struggle\nwith procedural execution in dynamic settings. Even the SOTA model, GPT-4o,\nfalls short of 60% overall performance. These findings highlight persistent\nchallenges in achieving dynamic legal intelligence and offer valuable insights\nto guide future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gap between static benchmarks and the dynamic nature of real-world legal\npractice poses a key barrier to advancing legal intelligence. To this end, we\nintroduce J1-ENVS, the first interactive and dynamic legal environment tailored\nfor LLM-based agents. Guided by legal experts, it comprises six representative\nscenarios from Chinese legal practices across three levels of environmental\ncomplexity. We further introduce J1-EVAL, a fine-grained evaluation framework,\ndesigned to assess both task performance and procedural compliance across\nvarying levels of legal proficiency. Extensive experiments on 17 LLM agents\nreveal that, while many models demonstrate solid legal knowledge, they struggle\nwith procedural execution in dynamic settings. Even the SOTA model, GPT-4o,\nfalls short of 60% overall performance. These findings highlight persistent\nchallenges in achieving dynamic legal intelligence and offer valuable insights\nto guide future research."
                },
                "authors": [
                    {
                        "name": "Zheng Jia"
                    },
                    {
                        "name": "Shengbin Yue"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Yidong Liu"
                    },
                    {
                        "name": "Yun Song"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12284v2",
                "updated": "2025-07-17T12:55:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    55,
                    32,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T14:31:33Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    31,
                    33,
                    2,
                    197,
                    0
                ],
                "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks"
                },
                "summary": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures."
                },
                "authors": [
                    {
                        "name": "Artem Chervyakov"
                    },
                    {
                        "name": "Alexander Kharitonov"
                    },
                    {
                        "name": "Pavel Zadorozhny"
                    },
                    {
                        "name": "Adamenko Pavel"
                    },
                    {
                        "name": "Rodion Levichev"
                    },
                    {
                        "name": "Dmitrii Vorobev"
                    },
                    {
                        "name": "Dmitrii Salikhov"
                    },
                    {
                        "name": "Aidar Valeev"
                    },
                    {
                        "name": "Alena Pestova"
                    },
                    {
                        "name": "Maria Dziuba"
                    },
                    {
                        "name": "Ilseyar Alimova"
                    },
                    {
                        "name": "Artem Zavgorodnev"
                    },
                    {
                        "name": "Aleksandr Medvedev"
                    },
                    {
                        "name": "Stanislav Moiseev"
                    },
                    {
                        "name": "Elena Bruches"
                    },
                    {
                        "name": "Daniil Grebenkin"
                    },
                    {
                        "name": "Roman Derunets"
                    },
                    {
                        "name": "Vikulov Vladimir"
                    },
                    {
                        "name": "Anton Emelyanov"
                    },
                    {
                        "name": "Dmitrii Babaev"
                    },
                    {
                        "name": "Vladimir V. Ivanov"
                    },
                    {
                        "name": "Valentin Malykh"
                    },
                    {
                        "name": "Alena Fenogenova"
                    }
                ],
                "author_detail": {
                    "name": "Alena Fenogenova"
                },
                "author": "Alena Fenogenova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02419v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02419v3",
                "updated": "2025-07-17T12:34:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    34,
                    14,
                    3,
                    198,
                    0
                ],
                "published": "2023-12-05T01:35:39Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    1,
                    35,
                    39,
                    1,
                    339,
                    0
                ],
                "title": "Human Demonstrations are Generalizable Knowledge for Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Demonstrations are Generalizable Knowledge for Robots"
                },
                "summary": "Learning from human demonstrations is an emerging trend for designing\nintelligent robotic systems. However, previous methods typically regard videos\nas instructions, simply dividing them into action sequences for robotic\nrepetition, which poses obstacles to generalization to diverse tasks or object\ninstances. In this paper, we propose a different perspective, considering human\ndemonstration videos not as mere instructions, but as a source of knowledge for\nrobots. Motivated by this perspective and the remarkable comprehension and\ngeneralization capabilities exhibited by large language models (LLMs), we\npropose DigKnow, a method that DIstills Generalizable KNOWledge with a\nhierarchical structure. Specifically, DigKnow begins by converting human\ndemonstration video frames into observation knowledge. This knowledge is then\nsubjected to analysis to extract human action knowledge and further distilled\ninto pattern knowledge compassing task and object instances, resulting in the\nacquisition of generalizable knowledge with a hierarchical structure. In\nsettings with different tasks or object instances, DigKnow retrieves relevant\nknowledge for the current task and object instances. Subsequently, the\nLLM-based planner conducts planning based on the retrieved knowledge, and the\npolicy executes actions in line with the plan to achieve the designated task.\nUtilizing the retrieved knowledge, we validate and rectify planning and\nexecution outcomes, resulting in a substantial enhancement of the success rate.\nExperimental results across a range of tasks and scenes demonstrate the\neffectiveness of this approach in facilitating real-world robots to accomplish\ntasks with the knowledge derived from human demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human demonstrations is an emerging trend for designing\nintelligent robotic systems. However, previous methods typically regard videos\nas instructions, simply dividing them into action sequences for robotic\nrepetition, which poses obstacles to generalization to diverse tasks or object\ninstances. In this paper, we propose a different perspective, considering human\ndemonstration videos not as mere instructions, but as a source of knowledge for\nrobots. Motivated by this perspective and the remarkable comprehension and\ngeneralization capabilities exhibited by large language models (LLMs), we\npropose DigKnow, a method that DIstills Generalizable KNOWledge with a\nhierarchical structure. Specifically, DigKnow begins by converting human\ndemonstration video frames into observation knowledge. This knowledge is then\nsubjected to analysis to extract human action knowledge and further distilled\ninto pattern knowledge compassing task and object instances, resulting in the\nacquisition of generalizable knowledge with a hierarchical structure. In\nsettings with different tasks or object instances, DigKnow retrieves relevant\nknowledge for the current task and object instances. Subsequently, the\nLLM-based planner conducts planning based on the retrieved knowledge, and the\npolicy executes actions in line with the plan to achieve the designated task.\nUtilizing the retrieved knowledge, we validate and rectify planning and\nexecution outcomes, resulting in a substantial enhancement of the success rate.\nExperimental results across a range of tasks and scenes demonstrate the\neffectiveness of this approach in facilitating real-world robots to accomplish\ntasks with the knowledge derived from human demonstrations."
                },
                "authors": [
                    {
                        "name": "Te Cui"
                    },
                    {
                        "name": "Tianxing Zhou"
                    },
                    {
                        "name": "Zicai Peng"
                    },
                    {
                        "name": "Mengxiao Hu"
                    },
                    {
                        "name": "Haoyang Lu"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Guangyan Chen"
                    },
                    {
                        "name": "Meiling Wang"
                    },
                    {
                        "name": "Yufeng Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yufeng Yue"
                },
                "author": "Yufeng Yue",
                "arxiv_comment": "accepted for publication in lEEE/RSJ international Conference on\n  Intelligent Robots and Systems (lROS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02419v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02419v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06796v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06796v3",
                "updated": "2025-07-17T12:33:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    33,
                    32,
                    3,
                    198,
                    0
                ],
                "published": "2024-11-11T08:50:24Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    50,
                    24,
                    0,
                    316,
                    0
                ],
                "title": "Write Your Own CodeChecker: An Automated Test-Driven Checker Development\n  Approach with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write Your Own CodeChecker: An Automated Test-Driven Checker Development\n  Approach with LLMs"
                },
                "summary": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic and the complex API usage of large-scale\nchecker frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we propose AutoChecker, an innovative LLM-powered approach that can\nwrite code checkers automatically based on only a rule description and a test\nsuite. To achieve comprehensive checking logic, AutoChecker incrementally\nupdates the checker's logic by focusing on solving one selected case each time.\nTo obtain precise API knowledge, during each iteration, it leverages\nfine-grained logic-guided API-context retrieval, where it first decomposes the\nchecking logic into a series of sub-operations and then retrieves\nchecker-related API-contexts for each sub-operation. For evaluation, we apply\nAutoChecker, five baselines, and three ablation methods using multiple LLMs to\ngenerate checkers for 20 randomly selected PMD rules. Experimental results show\nthat AutoChecker significantly outperforms others across all effectiveness\nmetrics, with an average test pass rate of 82.28%. Additionally, the checkers\ngenerated by AutoChecker can be successfully applied to real-world projects,\nmatching the performance of official checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic and the complex API usage of large-scale\nchecker frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we propose AutoChecker, an innovative LLM-powered approach that can\nwrite code checkers automatically based on only a rule description and a test\nsuite. To achieve comprehensive checking logic, AutoChecker incrementally\nupdates the checker's logic by focusing on solving one selected case each time.\nTo obtain precise API knowledge, during each iteration, it leverages\nfine-grained logic-guided API-context retrieval, where it first decomposes the\nchecking logic into a series of sub-operations and then retrieves\nchecker-related API-contexts for each sub-operation. For evaluation, we apply\nAutoChecker, five baselines, and three ablation methods using multiple LLMs to\ngenerate checkers for 20 randomly selected PMD rules. Experimental results show\nthat AutoChecker significantly outperforms others across all effectiveness\nmetrics, with an average test pass rate of 82.28%. Additionally, the checkers\ngenerated by AutoChecker can be successfully applied to real-world projects,\nmatching the performance of official checkers."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Yuanyuan Xie"
                    },
                    {
                        "name": "Jiwei Yan"
                    },
                    {
                        "name": "Jinhao Huang"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "arxiv_comment": "update metadata and artifact url",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06796v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06796v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13052v1",
                "updated": "2025-07-17T12:25:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    25,
                    1,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T12:25:01Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    25,
                    1,
                    3,
                    198,
                    0
                ],
                "title": "Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient\n  Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient\n  Communication"
                },
                "summary": "The advancement and maturity of large language models (LLMs) and robotics\nhave unlocked vast potential for human-computer interaction, particularly in\nthe field of robotic ultrasound. While existing research primarily focuses on\neither patient-robot or physician-robot interaction, the role of an intelligent\nvirtual sonographer (IVS) bridging physician-robot-patient communication\nremains underexplored. This work introduces a conversational virtual agent in\nExtended Reality (XR) that facilitates real-time interaction between\nphysicians, a robotic ultrasound system(RUS), and patients. The IVS agent\ncommunicates with physicians in a professional manner while offering empathetic\nexplanations and reassurance to patients. Furthermore, it actively controls the\nRUS by executing physician commands and transparently relays these actions to\nthe patient. By integrating LLM-powered dialogue with speech-to-text,\ntext-to-speech, and robotic control, our system enhances the efficiency,\nclarity, and accessibility of robotic ultrasound acquisition. This work\nconstitutes a first step toward understanding how IVS can bridge communication\ngaps in physician-robot-patient interaction, providing more control and\ntherefore trust into physician-robot interaction while improving patient\nexperience and acceptance of robotic ultrasound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement and maturity of large language models (LLMs) and robotics\nhave unlocked vast potential for human-computer interaction, particularly in\nthe field of robotic ultrasound. While existing research primarily focuses on\neither patient-robot or physician-robot interaction, the role of an intelligent\nvirtual sonographer (IVS) bridging physician-robot-patient communication\nremains underexplored. This work introduces a conversational virtual agent in\nExtended Reality (XR) that facilitates real-time interaction between\nphysicians, a robotic ultrasound system(RUS), and patients. The IVS agent\ncommunicates with physicians in a professional manner while offering empathetic\nexplanations and reassurance to patients. Furthermore, it actively controls the\nRUS by executing physician commands and transparently relays these actions to\nthe patient. By integrating LLM-powered dialogue with speech-to-text,\ntext-to-speech, and robotic control, our system enhances the efficiency,\nclarity, and accessibility of robotic ultrasound acquisition. This work\nconstitutes a first step toward understanding how IVS can bridge communication\ngaps in physician-robot-patient interaction, providing more control and\ntherefore trust into physician-robot interaction while improving patient\nexperience and acceptance of robotic ultrasound."
                },
                "authors": [
                    {
                        "name": "Tianyu Song"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Yuan Bi"
                    },
                    {
                        "name": "Angelos Karlas"
                    },
                    {
                        "name": "Amir Yousefi"
                    },
                    {
                        "name": "Daniela Branzan"
                    },
                    {
                        "name": "Zhongliang Jiang"
                    },
                    {
                        "name": "Ulrich Eck"
                    },
                    {
                        "name": "Nassir Navab"
                    }
                ],
                "author_detail": {
                    "name": "Nassir Navab"
                },
                "author": "Nassir Navab",
                "arxiv_comment": "Accepted at MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13048v1",
                "updated": "2025-07-17T12:21:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    21,
                    4,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T12:21:04Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    21,
                    4,
                    3,
                    198,
                    0
                ],
                "title": "Gravitational Lensing by Black Holes in Einstein-nonlinear\n  Electrodynamic Theories with Multiple Photon Spheres",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational Lensing by Black Holes in Einstein-nonlinear\n  Electrodynamic Theories with Multiple Photon Spheres"
                },
                "summary": "In this paper, we study the gravitational lensing effects of non-linear\nelectrodynamic black holes. Non-linear electrodynamic black holes serve as\ntypical models for multi-event horizon black holes. Depending on the choice of\nmetric parameters, these black holes can possess more than five event horizons.\nConsequently, within certain parameter ranges, black holes can have more than\nthree photon spheres of varying sizes outside the event horizon. Specifically,\nwe focus on the strong gravitational lensing effects near the triple photon\nspheres, particularly the formation of higher-order images of point sources and\ncelestial spheres. The presence of one, two, or three or more photon spheres\nsignificantly increases the number of higher-order images of a point source.\nWhen a black hole is illuminated by a celestial sphere, the three photon\nspheres generate three critical curves in the black hole image, with the\nsmallest critical curve coinciding with the shadow's edge. Additionally, since\nnon-linear electrodynamic black holes are models of multi-event horizon black\nholes, we can infer the gravitational lensing effects and the changes in\ncelestial images for black holes with more than three photon spheres by\nanalyzing the distinctions and patterns between the gravitational lensing\neffects of one, two, and three photon spheres.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the gravitational lensing effects of non-linear\nelectrodynamic black holes. Non-linear electrodynamic black holes serve as\ntypical models for multi-event horizon black holes. Depending on the choice of\nmetric parameters, these black holes can possess more than five event horizons.\nConsequently, within certain parameter ranges, black holes can have more than\nthree photon spheres of varying sizes outside the event horizon. Specifically,\nwe focus on the strong gravitational lensing effects near the triple photon\nspheres, particularly the formation of higher-order images of point sources and\ncelestial spheres. The presence of one, two, or three or more photon spheres\nsignificantly increases the number of higher-order images of a point source.\nWhen a black hole is illuminated by a celestial sphere, the three photon\nspheres generate three critical curves in the black hole image, with the\nsmallest critical curve coinciding with the shadow's edge. Additionally, since\nnon-linear electrodynamic black holes are models of multi-event horizon black\nholes, we can infer the gravitational lensing effects and the changes in\ncelestial images for black holes with more than three photon spheres by\nanalyzing the distinctions and patterns between the gravitational lensing\neffects of one, two, and three photon spheres."
                },
                "authors": [
                    {
                        "name": "Siyuan Hui"
                    },
                    {
                        "name": "Benrong Mu"
                    },
                    {
                        "name": "Peng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Wang"
                },
                "author": "Peng Wang",
                "arxiv_comment": "24 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13038v1",
                "updated": "2025-07-17T12:09:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    9,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T12:09:39Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    9,
                    39,
                    3,
                    198,
                    0
                ],
                "title": "MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent\n  Debate Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent\n  Debate Systems"
                },
                "summary": "Multi-agent debate (MAD) systems leverage collaborative interactions among\nlarge language models (LLMs) agents to improve reasoning capabilities. While\nrecent studies have focused on increasing the accuracy and scalability of MAD\nsystems, their security vulnerabilities have received limited attention. In\nthis work, we introduce MAD-Spear, a targeted prompt injection attack that\ncompromises a small subset of agents but significantly disrupts the overall MAD\nprocess. Manipulated agents produce multiple plausible yet incorrect responses,\nexploiting LLMs' conformity tendencies to propagate misinformation and degrade\nconsensus quality. Furthermore, the attack can be composed with other\nstrategies, such as communication attacks, to further amplify its impact by\nincreasing the exposure of agents to incorrect responses. To assess MAD's\nresilience under attack, we propose a formal definition of MAD fault-tolerance\nand develop a comprehensive evaluation framework that jointly considers\naccuracy, consensus efficiency, and scalability. Extensive experiments on five\nbenchmark datasets with varying difficulty levels demonstrate that MAD-Spear\nconsistently outperforms the baseline attack in degrading system performance.\nAdditionally, we observe that agent diversity substantially improves MAD\nperformance in mathematical reasoning tasks, which challenges prior work\nsuggesting that agent diversity has minimal impact on performance. These\nfindings highlight the urgent need to improve the security in MAD design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent debate (MAD) systems leverage collaborative interactions among\nlarge language models (LLMs) agents to improve reasoning capabilities. While\nrecent studies have focused on increasing the accuracy and scalability of MAD\nsystems, their security vulnerabilities have received limited attention. In\nthis work, we introduce MAD-Spear, a targeted prompt injection attack that\ncompromises a small subset of agents but significantly disrupts the overall MAD\nprocess. Manipulated agents produce multiple plausible yet incorrect responses,\nexploiting LLMs' conformity tendencies to propagate misinformation and degrade\nconsensus quality. Furthermore, the attack can be composed with other\nstrategies, such as communication attacks, to further amplify its impact by\nincreasing the exposure of agents to incorrect responses. To assess MAD's\nresilience under attack, we propose a formal definition of MAD fault-tolerance\nand develop a comprehensive evaluation framework that jointly considers\naccuracy, consensus efficiency, and scalability. Extensive experiments on five\nbenchmark datasets with varying difficulty levels demonstrate that MAD-Spear\nconsistently outperforms the baseline attack in degrading system performance.\nAdditionally, we observe that agent diversity substantially improves MAD\nperformance in mathematical reasoning tasks, which challenges prior work\nsuggesting that agent diversity has minimal impact on performance. These\nfindings highlight the urgent need to improve the security in MAD design."
                },
                "authors": [
                    {
                        "name": "Yu Cui"
                    },
                    {
                        "name": "Hongyang Du"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Du"
                },
                "author": "Hongyang Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13032v1",
                "updated": "2025-07-17T12:02:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    2,
                    38,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T12:02:38Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    2,
                    38,
                    3,
                    198,
                    0
                ],
                "title": "Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image\n  Generation"
                },
                "summary": "AutoRegressive (AR) models have made notable progress in image generation,\nwith Masked AutoRegressive (MAR) models gaining attention for their efficient\nparallel decoding. However, MAR models have traditionally underperformed when\ncompared to standard AR models. This study refines the MAR architecture to\nimprove image generation quality. We begin by evaluating various image\ntokenizers to identify the most effective one. Subsequently, we introduce an\nimproved Bidirectional LLaMA architecture by replacing causal attention with\nbidirectional attention and incorporating 2D RoPE, which together form our\nadvanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves\na FID score of 3.71, matching state-of-the-art AR models in the ImageNet\n256x256 benchmark, while requiring only 8 inference steps compared to the 256\nsteps of AR models. Furthermore, we develop a text-driven MaskGIL model with\n775M parameters for generating images from text at various resolutions. Beyond\nimage generation, MaskGIL extends to accelerate AR-based generation and enable\nreal-time speech-to-image conversion. Our codes and models are available at\nhttps://github.com/synbol/MaskGIL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRegressive (AR) models have made notable progress in image generation,\nwith Masked AutoRegressive (MAR) models gaining attention for their efficient\nparallel decoding. However, MAR models have traditionally underperformed when\ncompared to standard AR models. This study refines the MAR architecture to\nimprove image generation quality. We begin by evaluating various image\ntokenizers to identify the most effective one. Subsequently, we introduce an\nimproved Bidirectional LLaMA architecture by replacing causal attention with\nbidirectional attention and incorporating 2D RoPE, which together form our\nadvanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves\na FID score of 3.71, matching state-of-the-art AR models in the ImageNet\n256x256 benchmark, while requiring only 8 inference steps compared to the 256\nsteps of AR models. Furthermore, we develop a text-driven MaskGIL model with\n775M parameters for generating images from text at various resolutions. Beyond\nimage generation, MaskGIL extends to accelerate AR-based generation and enable\nreal-time speech-to-image conversion. Our codes and models are available at\nhttps://github.com/synbol/MaskGIL."
                },
                "authors": [
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Yuewen Cao"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Guangtao Zhai"
                    },
                    {
                        "name": "Xiaohong Liu"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "arxiv_comment": "24 pages, 10 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13024v1",
                "updated": "2025-07-17T11:52:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    52,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T11:52:27Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    52,
                    27,
                    3,
                    198,
                    0
                ],
                "title": "When Pattern-by-Pattern Works: Theoretical and Empirical Insights for\n  Logistic Models with Missing Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Pattern-by-Pattern Works: Theoretical and Empirical Insights for\n  Logistic Models with Missing Values"
                },
                "summary": "Predicting a response with partially missing inputs remains a challenging\ntask even in parametric models, since parameter estimation in itself is not\nsufficient to predict on partially observed inputs. Several works study\nprediction in linear models. In this paper, we focus on logistic models, which\npresent their own difficulties. From a theoretical perspective, we prove that a\nPattern-by-Pattern strategy (PbP), which learns one logistic model per\nmissingness pattern, accurately approximates Bayes probabilities in various\nmissing data scenarios (MCAR, MAR and MNAR). Empirically, we thoroughly compare\nvarious methods (constant and iterative imputations, complete case analysis,\nPbP, and an EM algorithm) across classification, probability estimation,\ncalibration, and parameter inference. Our analysis provides a comprehensive\nview on the logistic regression with missing values. It reveals that mean\nimputation can be used as baseline for low sample sizes, and improved\nperformance is obtained via nonlinear multiple iterative imputation techniques\nwith the labels (MICE.RF.Y). For large sample sizes, PbP is the best method for\nGaussian mixtures, and we recommend MICE.RF.Y in presence of nonlinear\nfeatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting a response with partially missing inputs remains a challenging\ntask even in parametric models, since parameter estimation in itself is not\nsufficient to predict on partially observed inputs. Several works study\nprediction in linear models. In this paper, we focus on logistic models, which\npresent their own difficulties. From a theoretical perspective, we prove that a\nPattern-by-Pattern strategy (PbP), which learns one logistic model per\nmissingness pattern, accurately approximates Bayes probabilities in various\nmissing data scenarios (MCAR, MAR and MNAR). Empirically, we thoroughly compare\nvarious methods (constant and iterative imputations, complete case analysis,\nPbP, and an EM algorithm) across classification, probability estimation,\ncalibration, and parameter inference. Our analysis provides a comprehensive\nview on the logistic regression with missing values. It reveals that mean\nimputation can be used as baseline for low sample sizes, and improved\nperformance is obtained via nonlinear multiple iterative imputation techniques\nwith the labels (MICE.RF.Y). For large sample sizes, PbP is the best method for\nGaussian mixtures, and we recommend MICE.RF.Y in presence of nonlinear\nfeatures."
                },
                "authors": [
                    {
                        "name": "Christophe Muller"
                    },
                    {
                        "name": "Erwan Scornet"
                    },
                    {
                        "name": "Julie Josse"
                    }
                ],
                "author_detail": {
                    "name": "Julie Josse"
                },
                "arxiv_affiliation": "PREMEDICAL",
                "author": "Julie Josse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13019v1",
                "updated": "2025-07-17T11:46:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    46,
                    0,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T11:46:00Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    46,
                    0,
                    3,
                    198,
                    0
                ],
                "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A\n  Holistic Study of Physical and Visual Disparities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A\n  Holistic Study of Physical and Visual Disparities"
                },
                "summary": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but\ntheir idealized assumptions about robot movement and control fail to reflect\nphysically embodied deployment challenges. To bridge this gap, we introduce\nVLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and\nwheeled robots. For the first time, we systematically evaluate several\nego-centric VLN methods in physical robotic settings across different technical\npipelines, including classification models for single-step discrete action\nprediction, a diffusion model for dense waypoint prediction, and a train-free,\nmap-based large language model (LLM) integrated with path planning. Our results\nreveal significant performance degradation due to limited robot observation\nspace, environmental lighting variations, and physical challenges like\ncollisions and falls. This also exposes locomotion constraints for legged\nrobots in complex environments. VLN-PE is highly extensible, allowing seamless\nintegration of new scenes beyond MP3D, thereby enabling more comprehensive VLN\nevaluation. Despite the weak generalization of current models in physical\ndeployment, VLN-PE provides a new pathway for improving cross-embodiment's\noverall adaptability. We hope our findings and tools inspire the community to\nrethink VLN limitations and advance robust, practical VLN models. The code is\navailable at https://crystalsixone.github.io/vln_pe.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but\ntheir idealized assumptions about robot movement and control fail to reflect\nphysically embodied deployment challenges. To bridge this gap, we introduce\nVLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and\nwheeled robots. For the first time, we systematically evaluate several\nego-centric VLN methods in physical robotic settings across different technical\npipelines, including classification models for single-step discrete action\nprediction, a diffusion model for dense waypoint prediction, and a train-free,\nmap-based large language model (LLM) integrated with path planning. Our results\nreveal significant performance degradation due to limited robot observation\nspace, environmental lighting variations, and physical challenges like\ncollisions and falls. This also exposes locomotion constraints for legged\nrobots in complex environments. VLN-PE is highly extensible, allowing seamless\nintegration of new scenes beyond MP3D, thereby enabling more comprehensive VLN\nevaluation. Despite the weak generalization of current models in physical\ndeployment, VLN-PE provides a new pathway for improving cross-embodiment's\noverall adaptability. We hope our findings and tools inspire the community to\nrethink VLN limitations and advance robust, practical VLN models. The code is\navailable at https://crystalsixone.github.io/vln_pe.github.io/."
                },
                "authors": [
                    {
                        "name": "Liuyi Wang"
                    },
                    {
                        "name": "Xinyuan Xia"
                    },
                    {
                        "name": "Hui Zhao"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Chengju Liu"
                    },
                    {
                        "name": "Qijun Chen"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03390v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03390v3",
                "updated": "2025-07-17T11:36:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    36,
                    41,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-04T12:03:11Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    12,
                    3,
                    11,
                    4,
                    94,
                    0
                ],
                "title": "Eigen-inference by Marchenko-Pastur inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen-inference by Marchenko-Pastur inversion"
                },
                "summary": "A new method of estimating population linear spectral statistics from\nhigh-dimensional data is introduced. When the dimension $d$ grows with the\nsample size $n$ such that $\\frac{d}{n} \\rightarrow c>0$, the introduced method\nis the first to provably achieve eigen-inference with fast convergence rates of\n$\\mathcal{O}(n^{\\varepsilon-1})$ for any $\\varepsilon > 0$ in the general\nnon-parametric setting. This is achieved though a novel Marchenko-Pastur\ninversion formula, which may also be formulated as a semi-explicit solution to\nthe Marchenko-Pastur equation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new method of estimating population linear spectral statistics from\nhigh-dimensional data is introduced. When the dimension $d$ grows with the\nsample size $n$ such that $\\frac{d}{n} \\rightarrow c>0$, the introduced method\nis the first to provably achieve eigen-inference with fast convergence rates of\n$\\mathcal{O}(n^{\\varepsilon-1})$ for any $\\varepsilon > 0$ in the general\nnon-parametric setting. This is achieved though a novel Marchenko-Pastur\ninversion formula, which may also be formulated as a semi-explicit solution to\nthe Marchenko-Pastur equation."
                },
                "authors": [
                    {
                        "name": "Ben Deitmar"
                    }
                ],
                "author_detail": {
                    "name": "Ben Deitmar"
                },
                "author": "Ben Deitmar",
                "arxiv_comment": "44 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03390v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03390v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05438v2",
                "updated": "2025-07-17T11:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    22,
                    53,
                    3,
                    198,
                    0
                ],
                "published": "2025-05-08T17:27:44Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    27,
                    44,
                    3,
                    128,
                    0
                ],
                "title": "Scalable Bernoulli factories for Bayesian inference with intractable\n  likelihoods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Bernoulli factories for Bayesian inference with intractable\n  likelihoods"
                },
                "summary": "Bernoulli factory MCMC algorithms implement accept-reject Markov chains\nwithout explicit computation of acceptance probabilities, and are used to\ntarget posterior distributions associated with intractable likelihood models.\nIntractable likelihoods naturally arise in continuous-time models and mixture\ndistributions, or from the marginalisation of a tractable augmented model.\nBernoulli factory MCMC algorithms often mix better than alternatives that\ntarget a tractable augmented posterior. However, for a likelihood that\nfactorizes over observations, we show that their computational performance\ntypically deteriorates exponentially with data size. To address this, we\npropose a simple divide-and-conquer Bernoulli factory MCMC algorithm and prove\nthat it has polynomial complexity of degree between 1 and 2, with the exact\ndegree depending on the existence of efficient unbiased estimators of the\nintractable likelihood ratio. We demonstrate the effectiveness of our approach\nwith applications to Bayesian inference in two intractable likelihood models,\nand observe respective polynomial cost of degree 1.2 and 1 in the data size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bernoulli factory MCMC algorithms implement accept-reject Markov chains\nwithout explicit computation of acceptance probabilities, and are used to\ntarget posterior distributions associated with intractable likelihood models.\nIntractable likelihoods naturally arise in continuous-time models and mixture\ndistributions, or from the marginalisation of a tractable augmented model.\nBernoulli factory MCMC algorithms often mix better than alternatives that\ntarget a tractable augmented posterior. However, for a likelihood that\nfactorizes over observations, we show that their computational performance\ntypically deteriorates exponentially with data size. To address this, we\npropose a simple divide-and-conquer Bernoulli factory MCMC algorithm and prove\nthat it has polynomial complexity of degree between 1 and 2, with the exact\ndegree depending on the existence of efficient unbiased estimators of the\nintractable likelihood ratio. We demonstrate the effectiveness of our approach\nwith applications to Bayesian inference in two intractable likelihood models,\nand observe respective polynomial cost of degree 1.2 and 1 in the data size."
                },
                "authors": [
                    {
                        "name": "Timothe Stumpf-Ftizon"
                    },
                    {
                        "name": "Flvio B. Gonalves"
                    }
                ],
                "author_detail": {
                    "name": "Flvio B. Gonalves"
                },
                "author": "Flvio B. Gonalves",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15192v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15192v3",
                "updated": "2025-07-17T11:07:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    7,
                    59,
                    3,
                    198,
                    0
                ],
                "published": "2024-08-27T16:51:51Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    16,
                    51,
                    51,
                    1,
                    240,
                    0
                ],
                "title": "Simultaneously Constraining the Neutron Star Equation of State and Mass\n  Distribution through Multimessenger Observations and Nuclear Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Constraining the Neutron Star Equation of State and Mass\n  Distribution through Multimessenger Observations and Nuclear Benchmarks"
                },
                "summary": "With ongoing advancements in nuclear theory and experimentation, together\nwith a growing body of neutron star (NS) observations, a wealth of information\non the equation of state (EOS) for matter at extreme densities has become\naccessible. Here, we utilize a hybrid EOS formulation that combines an\nempirical parameterization centered around the nuclear saturation density with\na generic three-segment piecewise polytrope model at higher densities. We\nincorporate data derived from chiral effective field theory ($\\chi$EFT),\nperturbative quantum chromodynamics (pQCD), and from experiments such as\nPREX-II and CREX. Furthermore, we examine the influence of a total of 129 NS\nmass measurements up to April 2023, as well as simultaneous mass and radius\nmeasurements derived from the X-ray emission from surface hot spots on NSs.\nAdditionally, we consider constraints on tidal properties inferred from the\ngravitational waves emitted by coalescing NS binaries. To integrate this\nextensive and varied array of constraints, we utilize a hierarchical Bayesian\nstatistical framework to simultaneously deduce the EOS and the distribution of\nNS masses. We find that incorporating data from $\\chi$EFT significantly\ntightens the constraints on the EOS of NSs near or below the nuclear saturation\ndensity. However, constraints derived from pQCD computations and nuclear\nexperiments such as PREX-II and CREX have minimal impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With ongoing advancements in nuclear theory and experimentation, together\nwith a growing body of neutron star (NS) observations, a wealth of information\non the equation of state (EOS) for matter at extreme densities has become\naccessible. Here, we utilize a hybrid EOS formulation that combines an\nempirical parameterization centered around the nuclear saturation density with\na generic three-segment piecewise polytrope model at higher densities. We\nincorporate data derived from chiral effective field theory ($\\chi$EFT),\nperturbative quantum chromodynamics (pQCD), and from experiments such as\nPREX-II and CREX. Furthermore, we examine the influence of a total of 129 NS\nmass measurements up to April 2023, as well as simultaneous mass and radius\nmeasurements derived from the X-ray emission from surface hot spots on NSs.\nAdditionally, we consider constraints on tidal properties inferred from the\ngravitational waves emitted by coalescing NS binaries. To integrate this\nextensive and varied array of constraints, we utilize a hierarchical Bayesian\nstatistical framework to simultaneously deduce the EOS and the distribution of\nNS masses. We find that incorporating data from $\\chi$EFT significantly\ntightens the constraints on the EOS of NSs near or below the nuclear saturation\ndensity. However, constraints derived from pQCD computations and nuclear\nexperiments such as PREX-II and CREX have minimal impact."
                },
                "authors": [
                    {
                        "name": "Bhaskar Biswas"
                    },
                    {
                        "name": "Stephan Rosswog"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Rosswog"
                },
                "author": "Stephan Rosswog",
                "arxiv_comment": "Accepted in PRD. arXiv admin note: text overlap with arXiv:2309.02345\n  by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15192v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15192v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12990v1",
                "updated": "2025-07-17T10:57:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    57,
                    49,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T10:57:49Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    57,
                    49,
                    3,
                    198,
                    0
                ],
                "title": "Teach Old SAEs New Domain Tricks with Boosting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teach Old SAEs New Domain Tricks with Boosting"
                },
                "summary": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs."
                },
                "authors": [
                    {
                        "name": "Nikita Koriagin"
                    },
                    {
                        "name": "Yaroslav Aksenov"
                    },
                    {
                        "name": "Daniil Laptev"
                    },
                    {
                        "name": "Gleb Gerasimov"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12986v1",
                "updated": "2025-07-17T10:46:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    46,
                    0,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T10:46:00Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    46,
                    0,
                    3,
                    198,
                    0
                ],
                "title": "Robustness Requirement Coverage using a Situation Coverage Approach for\n  Vision-based AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness Requirement Coverage using a Situation Coverage Approach for\n  Vision-based AI Systems"
                },
                "summary": "AI-based robots and vehicles are expected to operate safely in complex and\ndynamic environments, even in the presence of component degradation. In such\nsystems, perception relies on sensors such as cameras to capture environmental\ndata, which is then processed by AI models to support decision-making. However,\ndegradation in sensor performance directly impacts input data quality and can\nimpair AI inference. Specifying safety requirements for all possible sensor\ndegradation scenarios leads to unmanageable complexity and inevitable gaps. In\nthis position paper, we present a novel framework that integrates camera noise\nfactor identification with situation coverage analysis to systematically elicit\nrobustness-related safety requirements for AI-based perception systems. We\nfocus specifically on camera degradation in the automotive domain. Building on\nan existing framework for identifying degradation modes, we propose involving\ndomain, sensor, and safety experts, and incorporating Operational Design Domain\nspecifications to extend the degradation model by incorporating noise factors\nrelevant to AI performance. Situation coverage analysis is then applied to\nidentify representative operational contexts. This work marks an initial step\ntoward integrating noise factor analysis and situational coverage to support\nprincipled formulation and completeness assessment of robustness requirements\nfor camera-based AI perception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-based robots and vehicles are expected to operate safely in complex and\ndynamic environments, even in the presence of component degradation. In such\nsystems, perception relies on sensors such as cameras to capture environmental\ndata, which is then processed by AI models to support decision-making. However,\ndegradation in sensor performance directly impacts input data quality and can\nimpair AI inference. Specifying safety requirements for all possible sensor\ndegradation scenarios leads to unmanageable complexity and inevitable gaps. In\nthis position paper, we present a novel framework that integrates camera noise\nfactor identification with situation coverage analysis to systematically elicit\nrobustness-related safety requirements for AI-based perception systems. We\nfocus specifically on camera degradation in the automotive domain. Building on\nan existing framework for identifying degradation modes, we propose involving\ndomain, sensor, and safety experts, and incorporating Operational Design Domain\nspecifications to extend the degradation model by incorporating noise factors\nrelevant to AI performance. Situation coverage analysis is then applied to\nidentify representative operational contexts. This work marks an initial step\ntoward integrating noise factor analysis and situational coverage to support\nprincipled formulation and completeness assessment of robustness requirements\nfor camera-based AI perception."
                },
                "authors": [
                    {
                        "name": "Sepeedeh Shahbeigi"
                    },
                    {
                        "name": "Nawshin Mannan Proma"
                    },
                    {
                        "name": "Victoria Hodge"
                    },
                    {
                        "name": "Richard Hawkins"
                    },
                    {
                        "name": "Boda Li"
                    },
                    {
                        "name": "Valentina Donzella"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Donzella"
                },
                "author": "Valentina Donzella",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12981v1",
                "updated": "2025-07-17T10:33:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    33,
                    36,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T10:33:36Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    33,
                    36,
                    3,
                    198,
                    0
                ],
                "title": "MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with\n  Multiple Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with\n  Multiple Steps"
                },
                "summary": "This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas\ny Respuestas sobre Tablas en Espa\\~nol (Questions and Answers about Tables in\nSpanish). Our solution obtains answers to the questions by implementing Python\ncode generation with LLMs that is used to filter and process the table. This\nsolution evolves from the MRT implementation for the Semeval 2025 related task.\nThe process consists of multiple steps: analyzing and understanding the content\nof the table, selecting the useful columns, generating instructions in natural\nlanguage, translating these instructions to code, running it, and handling\npotential errors or exceptions. These steps use open-source LLMs and\nfine-grained optimized prompts for each step. With this approach, we achieved\nan accuracy score of 85\\% in the task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas\ny Respuestas sobre Tablas en Espa\\~nol (Questions and Answers about Tables in\nSpanish). Our solution obtains answers to the questions by implementing Python\ncode generation with LLMs that is used to filter and process the table. This\nsolution evolves from the MRT implementation for the Semeval 2025 related task.\nThe process consists of multiple steps: analyzing and understanding the content\nof the table, selecting the useful columns, generating instructions in natural\nlanguage, translating these instructions to code, running it, and handling\npotential errors or exceptions. These steps use open-source LLMs and\nfine-grained optimized prompts for each step. With this approach, we achieved\nan accuracy score of 85\\% in the task."
                },
                "authors": [
                    {
                        "name": "Maximiliano Hormazbal Lagos"
                    },
                    {
                        "name": "lvaro Bueno Sez"
                    },
                    {
                        "name": "Hctor Cerezo-Costas"
                    },
                    {
                        "name": "Pedro Alonso Doval"
                    },
                    {
                        "name": "Jorge Alcalde Vesteiro"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Alcalde Vesteiro"
                },
                "author": "Jorge Alcalde Vesteiro",
                "arxiv_comment": "Accepted as an official challenge paper in the PRESTA: Questions and\n  Answers over Tabular Data shared task at IberLEF 2025, colocated with the\n  41st SEPLN Conference in Zaragoza, Spain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20770v2",
                "updated": "2025-07-17T10:10:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    10,
                    45,
                    3,
                    198,
                    0
                ],
                "published": "2025-05-27T06:21:56Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    21,
                    56,
                    1,
                    147,
                    0
                ],
                "title": "Can Large Language Models Predict Audio Effects Parameters from Natural\n  Language?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Predict Audio Effects Parameters from Natural\n  Language?"
                },
                "summary": "In music production, manipulating audio effects (Fx) parameters through\nnatural language has the potential to reduce technical barriers for\nnon-experts. We present LLM2Fx, a framework leveraging Large Language Models\n(LLMs) to predict Fx parameters directly from textual descriptions without\nrequiring task-specific training or fine-tuning. Our approach address the\ntext-to-effect parameter prediction (Text2Fx) task by mapping natural language\ndescriptions to the corresponding Fx parameters for equalization and\nreverberation. We demonstrate that LLMs can generate Fx parameters in a\nzero-shot manner that elucidates the relationship between timbre semantics and\naudio effects in music production. To enhance performance, we introduce three\ntypes of in-context examples: audio Digital Signal Processing (DSP) features,\nDSP function code, and few-shot examples. Our results demonstrate that\nLLM-based Fx parameter generation outperforms previous optimization approaches,\noffering competitive performance in translating natural language descriptions\nto appropriate Fx settings. Furthermore, LLMs can serve as text-driven\ninterfaces for audio production, paving the way for more intuitive and\naccessible music production tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In music production, manipulating audio effects (Fx) parameters through\nnatural language has the potential to reduce technical barriers for\nnon-experts. We present LLM2Fx, a framework leveraging Large Language Models\n(LLMs) to predict Fx parameters directly from textual descriptions without\nrequiring task-specific training or fine-tuning. Our approach address the\ntext-to-effect parameter prediction (Text2Fx) task by mapping natural language\ndescriptions to the corresponding Fx parameters for equalization and\nreverberation. We demonstrate that LLMs can generate Fx parameters in a\nzero-shot manner that elucidates the relationship between timbre semantics and\naudio effects in music production. To enhance performance, we introduce three\ntypes of in-context examples: audio Digital Signal Processing (DSP) features,\nDSP function code, and few-shot examples. Our results demonstrate that\nLLM-based Fx parameter generation outperforms previous optimization approaches,\noffering competitive performance in translating natural language descriptions\nto appropriate Fx settings. Furthermore, LLMs can serve as text-driven\ninterfaces for audio production, paving the way for more intuitive and\naccessible music production tools."
                },
                "authors": [
                    {
                        "name": "Seungheon Doh"
                    },
                    {
                        "name": "Junghyun Koo"
                    },
                    {
                        "name": "Marco A. Martnez-Ramrez"
                    },
                    {
                        "name": "Wei-Hsiang Liao"
                    },
                    {
                        "name": "Juhan Nam"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "Accepted for publication at The IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21725v2",
                "updated": "2025-07-17T09:56:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    56,
                    45,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-30T15:11:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    15,
                    11,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "New {\\em ab initio} constrained extended Skyrme equations of state for\n  simulations of neutron stars, supernovae and binary mergers: I. Subsaturation\n  density domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New {\\em ab initio} constrained extended Skyrme equations of state for\n  simulations of neutron stars, supernovae and binary mergers: I. Subsaturation\n  density domain"
                },
                "summary": "In numerical simulations of core-collapse supernova and binary neutron stars\nmergers, information about the energetics and composition of matter is\nimplemented via external tables covering the huge ranges of thermodynamic\nconditions explored during the astrophysical evolution. More than 120\ngeneral-purpose equation of state (EOS) tables have been contributed so far.\nUnfortunately, not all of them comply with current constraints from theoretical\nand experimental nuclear physics and astrophysical observations of neutron\nstars. Systematic investigations of the role that dense matter properties play\nin the evolution of these astrophysical phenomena require that more EOS tables\nare provided. We build a set of general-purpose EOS tables. At zero\ntemperature, they comply with all currently accepted constraints, including\n{\\em ab initio} chiral effective field theory calculations of pure neutron\nmatter. This set is designed to explore a wide variety of the behaviors of the\neffective masses as functions of density, which is reflected into a wide range\nof thermal behaviors. We employ Brussels extended Skyrme interactions generated\nby means of Bayesian inference techniques. An extended nuclear statistical\nequilibrium model is developed for modeling sub-saturated inhomogeneous nuclear\nmatter (NM). Here, we study the properties of sub-saturated inhomogeneous NM\nover wide ranges of density, temperature, and proton fraction. We analyze in\ndetail the mechanisms of transition to homogeneous matter and estimate the\ntransition density. Our key results include a thick layer of neutron rich\nisotopes of He or H in the inner crusts of neo-neutron stars, significant\nabundance of exotic isotopes of H and He in warm and neutron-rich matter and a\ndetailed study of the thermodynamic stability of cold stellar matter. The EOS\ntables are publicly available in the \\textsc{CompOSE} online database.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerical simulations of core-collapse supernova and binary neutron stars\nmergers, information about the energetics and composition of matter is\nimplemented via external tables covering the huge ranges of thermodynamic\nconditions explored during the astrophysical evolution. More than 120\ngeneral-purpose equation of state (EOS) tables have been contributed so far.\nUnfortunately, not all of them comply with current constraints from theoretical\nand experimental nuclear physics and astrophysical observations of neutron\nstars. Systematic investigations of the role that dense matter properties play\nin the evolution of these astrophysical phenomena require that more EOS tables\nare provided. We build a set of general-purpose EOS tables. At zero\ntemperature, they comply with all currently accepted constraints, including\n{\\em ab initio} chiral effective field theory calculations of pure neutron\nmatter. This set is designed to explore a wide variety of the behaviors of the\neffective masses as functions of density, which is reflected into a wide range\nof thermal behaviors. We employ Brussels extended Skyrme interactions generated\nby means of Bayesian inference techniques. An extended nuclear statistical\nequilibrium model is developed for modeling sub-saturated inhomogeneous nuclear\nmatter (NM). Here, we study the properties of sub-saturated inhomogeneous NM\nover wide ranges of density, temperature, and proton fraction. We analyze in\ndetail the mechanisms of transition to homogeneous matter and estimate the\ntransition density. Our key results include a thick layer of neutron rich\nisotopes of He or H in the inner crusts of neo-neutron stars, significant\nabundance of exotic isotopes of H and He in warm and neutron-rich matter and a\ndetailed study of the thermodynamic stability of cold stellar matter. The EOS\ntables are publicly available in the \\textsc{CompOSE} online database."
                },
                "authors": [
                    {
                        "name": "Adriana R. Raduta"
                    },
                    {
                        "name": "Mikhail V. Beznogov"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail V. Beznogov"
                },
                "author": "Mikhail V. Beznogov",
                "arxiv_comment": "21 pages, 12 figures, 2 tables; submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12948v1",
                "updated": "2025-07-17T09:40:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    40,
                    56,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T09:40:56Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    40,
                    56,
                    3,
                    198,
                    0
                ],
                "title": "Probabilistic Soundness Guarantees in LLM Reasoning Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Soundness Guarantees in LLM Reasoning Chains"
                },
                "summary": "In reasoning chains generated by large language models (LLMs), initial errors\noften propagate and undermine the reliability of the final conclusion. Current\nLLM-based error detection methods often fail to detect propagated errors\nbecause they do not properly account for how earlier errors might corrupt\njudgments of downstream reasoning. To better detect such propagated errors, we\nintroduce Autoregressive Reasoning Entailment Stability (ARES), a novel\nprobabilistic framework that prevents error propagation by judging each claim\nbased only on previously-assessed sound premises. This inductive method yields\na nuanced score for each step and provides certified statistical guarantees of\nits soundness, rather than a brittle binary label. ARES achieves\nstate-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2\npoints) and demonstrates superior robustness on very long synthetic reasoning\nchains, where it excels at detecting propagated errors (90.3% F1, +27.6\npoints).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In reasoning chains generated by large language models (LLMs), initial errors\noften propagate and undermine the reliability of the final conclusion. Current\nLLM-based error detection methods often fail to detect propagated errors\nbecause they do not properly account for how earlier errors might corrupt\njudgments of downstream reasoning. To better detect such propagated errors, we\nintroduce Autoregressive Reasoning Entailment Stability (ARES), a novel\nprobabilistic framework that prevents error propagation by judging each claim\nbased only on previously-assessed sound premises. This inductive method yields\na nuanced score for each step and provides certified statistical guarantees of\nits soundness, rather than a brittle binary label. ARES achieves\nstate-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2\npoints) and demonstrates superior robustness on very long synthetic reasoning\nchains, where it excels at detecting propagated errors (90.3% F1, +27.6\npoints)."
                },
                "authors": [
                    {
                        "name": "Weiqiu You"
                    },
                    {
                        "name": "Anton Xue"
                    },
                    {
                        "name": "Shreya Havaldar"
                    },
                    {
                        "name": "Delip Rao"
                    },
                    {
                        "name": "Helen Jin"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08161v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08161v4",
                "updated": "2025-07-17T09:34:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    34,
                    49,
                    3,
                    198,
                    0
                ],
                "published": "2025-03-11T08:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    26,
                    37,
                    1,
                    70,
                    0
                ],
                "title": "OASIS: Order-Augmented Strategy for Improved Code Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Order-Augmented Strategy for Improved Code Search"
                },
                "summary": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training."
                },
                "authors": [
                    {
                        "name": "Zuchen Gao"
                    },
                    {
                        "name": "Zizheng Zhan"
                    },
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Erxin Yu"
                    },
                    {
                        "name": "Ziqi Zhan"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08161v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08161v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12943v1",
                "updated": "2025-07-17T09:32:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    32,
                    33,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T09:32:33Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    32,
                    33,
                    3,
                    198,
                    0
                ],
                "title": "Disorder-induced spin excitation continuum and spin-glass ground state\n  in the inverse spinel CuGa$_2$O$_4$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disorder-induced spin excitation continuum and spin-glass ground state\n  in the inverse spinel CuGa$_2$O$_4$"
                },
                "summary": "Spinel-structured compounds serve as prototypical examples of highly\nfrustrated systems, and are promising candidates for realizing the long-sought\nquantum spin liquid (QSL) state. However, structural disorder is inevitable in\nmany real QSL candidates and its impact remains a topic of intense debate. In\nthis work, we conduct comprehensive investigations on CuGa$_2$O$_4$, a spinel\ncompound with significant structural disorder, focusing on its thermodynamic\nproperties and spectroscopic behaviors. No long-range magnetic order is\nobserved down to $\\sim$80 mK, as evidenced by magnetic susceptibility, specific\nheat and elastic neutron scattering measurements. More intriguingly, inelastic\nneutron scattering experiments reveal a broad gapless continuum of magnetic\nexcitations around the Brillouin zone boundary, resembling the magnetic\nexcitation spectra expected for a QSL. Nevertheless, a spin-freezing transition\nat $T_{\\rm{f}} \\approx $ 0.88 K is identified from the cusp in the dc\nsusceptibility curves, where a bifurcation between zero-field-cooling and\nfield-cooling curves occurs. Furthermore, ac susceptibility measurements show a\npeak close to $T_{\\rm{f}}$ at low frequency, which shifts to higher temperature\nwith increasing frequency. These results are evident that CuGa$_2$O$_4$ has\nspin-glass ground state, consistent with the establishment of short-range order\ninferred from the specific heat measurements. Collectively, these results\nillustrate the crucial role of disorder in defining the excitation spectrum out\nof the disordered ground state. Our findings shed light onto the broader class\nof AB$_2$O$_4$ spinels and advance our understanding of the spin dynamics in\nmagnetically disordered systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spinel-structured compounds serve as prototypical examples of highly\nfrustrated systems, and are promising candidates for realizing the long-sought\nquantum spin liquid (QSL) state. However, structural disorder is inevitable in\nmany real QSL candidates and its impact remains a topic of intense debate. In\nthis work, we conduct comprehensive investigations on CuGa$_2$O$_4$, a spinel\ncompound with significant structural disorder, focusing on its thermodynamic\nproperties and spectroscopic behaviors. No long-range magnetic order is\nobserved down to $\\sim$80 mK, as evidenced by magnetic susceptibility, specific\nheat and elastic neutron scattering measurements. More intriguingly, inelastic\nneutron scattering experiments reveal a broad gapless continuum of magnetic\nexcitations around the Brillouin zone boundary, resembling the magnetic\nexcitation spectra expected for a QSL. Nevertheless, a spin-freezing transition\nat $T_{\\rm{f}} \\approx $ 0.88 K is identified from the cusp in the dc\nsusceptibility curves, where a bifurcation between zero-field-cooling and\nfield-cooling curves occurs. Furthermore, ac susceptibility measurements show a\npeak close to $T_{\\rm{f}}$ at low frequency, which shifts to higher temperature\nwith increasing frequency. These results are evident that CuGa$_2$O$_4$ has\nspin-glass ground state, consistent with the establishment of short-range order\ninferred from the specific heat measurements. Collectively, these results\nillustrate the crucial role of disorder in defining the excitation spectrum out\nof the disordered ground state. Our findings shed light onto the broader class\nof AB$_2$O$_4$ spinels and advance our understanding of the spin dynamics in\nmagnetically disordered systems."
                },
                "authors": [
                    {
                        "name": "Zhentao Huang"
                    },
                    {
                        "name": "Zhijun Xu"
                    },
                    {
                        "name": "Shuaiwei Li"
                    },
                    {
                        "name": "Qingchen Duan"
                    },
                    {
                        "name": "Junbo Liao"
                    },
                    {
                        "name": "Song Bao"
                    },
                    {
                        "name": "Yanyan Shangguan"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Shufan Cheng"
                    },
                    {
                        "name": "Zihang Song"
                    },
                    {
                        "name": "Shuai Dong"
                    },
                    {
                        "name": "Maofeng Wu"
                    },
                    {
                        "name": "M. B. Stone"
                    },
                    {
                        "name": "Yiming Qiu"
                    },
                    {
                        "name": "Ruidan Zhong"
                    },
                    {
                        "name": "Guangyong Xu"
                    },
                    {
                        "name": "Zhen Ma"
                    },
                    {
                        "name": "G. D. Gu"
                    },
                    {
                        "name": "J. M. Tranquada"
                    },
                    {
                        "name": "Jinsheng Wen"
                    }
                ],
                "author_detail": {
                    "name": "Jinsheng Wen"
                },
                "author": "Jinsheng Wen",
                "arxiv_doi": "10.1103/1xtf-wg3q",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/1xtf-wg3q",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.12943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in PRB, 11 pages, 5 figures",
                "arxiv_journal_ref": "Phys. Rev. B 112, 035128 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05338v2",
                "updated": "2025-07-17T09:22:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    22,
                    51,
                    3,
                    198,
                    0
                ],
                "published": "2024-12-06T02:12:22Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    2,
                    12,
                    22,
                    4,
                    341,
                    0
                ],
                "title": "Theoretical Radio Signals from Radio-Band Gravitational Waves Converted\n  from the Neutron Star Magnetic Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Radio Signals from Radio-Band Gravitational Waves Converted\n  from the Neutron Star Magnetic Field"
                },
                "summary": "Gravitational waves (GWs) can convert into electromagnetic waves in the\npresence of a magnetic field via the Gertsenshtein-Zeldovich (GZ) effect. The\ncharacteristics of the magnetic field substantially affect this conversion\nprobability. This paper confirms that strong magnetic fields in neutron stars\nsignificantly enhance the conversion probability, facilitating detectable radio\nsignatures of very high-frequency (VHF,\n$\\left(10^6-10^{11}\\mathrm{~Hz}\\right)$) gravitational waves. We theoretically\nidentify two distinct signatures using single-dish telescopes (FAST, TMRT, QTT,\nGBT) and interferometers (SKA1/2-MID): transient signals from burst-like\ngravitational wave sources and persistent signals from cosmological background\ngravitational wave sources. These signatures are mapped to graviton spectral\nlines derived from quantum field theory by incorporating spin-2 and mass\nconstraints, resulting in smooth, featureless profiles that are critical for\ndistinguishing gravitational wave signals from astrophysical foregrounds. FAST\nattains a characteristic strain bound of $h_c<10^{-23}$, approaching $10^{-24}$\nin the frequency range of $1-3\\mathrm{~GHz}$ with a 6-hour observation period.\nThis performance exceeds the $5 \\sigma$ detection thresholds for GWs\noriginating from primordial black holes (PBHs) and nears the limits set by Big\nBang nucleosynthesis. Additionally, projections for SKA2-MID indicate even\ngreater sensitivity. Detecting such gravitational waves would improve our\ncomprehension of cosmological models, refine the parameter spaces for\nprimordial black holes, and function as a test for quantum field theory. This\napproach addresses significant deficiencies in VHF GW research, improving\ndetection sensitivity and facilitating the advancement of next-generation radio\ntelescopes such as FASTA and SKA, which feature larger fields of view and\nenhanced gain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves (GWs) can convert into electromagnetic waves in the\npresence of a magnetic field via the Gertsenshtein-Zeldovich (GZ) effect. The\ncharacteristics of the magnetic field substantially affect this conversion\nprobability. This paper confirms that strong magnetic fields in neutron stars\nsignificantly enhance the conversion probability, facilitating detectable radio\nsignatures of very high-frequency (VHF,\n$\\left(10^6-10^{11}\\mathrm{~Hz}\\right)$) gravitational waves. We theoretically\nidentify two distinct signatures using single-dish telescopes (FAST, TMRT, QTT,\nGBT) and interferometers (SKA1/2-MID): transient signals from burst-like\ngravitational wave sources and persistent signals from cosmological background\ngravitational wave sources. These signatures are mapped to graviton spectral\nlines derived from quantum field theory by incorporating spin-2 and mass\nconstraints, resulting in smooth, featureless profiles that are critical for\ndistinguishing gravitational wave signals from astrophysical foregrounds. FAST\nattains a characteristic strain bound of $h_c<10^{-23}$, approaching $10^{-24}$\nin the frequency range of $1-3\\mathrm{~GHz}$ with a 6-hour observation period.\nThis performance exceeds the $5 \\sigma$ detection thresholds for GWs\noriginating from primordial black holes (PBHs) and nears the limits set by Big\nBang nucleosynthesis. Additionally, projections for SKA2-MID indicate even\ngreater sensitivity. Detecting such gravitational waves would improve our\ncomprehension of cosmological models, refine the parameter spaces for\nprimordial black holes, and function as a test for quantum field theory. This\napproach addresses significant deficiencies in VHF GW research, improving\ndetection sensitivity and facilitating the advancement of next-generation radio\ntelescopes such as FASTA and SKA, which feature larger fields of view and\nenhanced gain."
                },
                "authors": [
                    {
                        "name": "Wei Hong"
                    },
                    {
                        "name": "Zhen-Zhao Tao"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Tong-Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong-Jie Zhang"
                },
                "author": "Tong-Jie Zhang",
                "arxiv_comment": "40 pages, 19 figures, 4 tables. Accepted for publication in ApJ. We\n  infer two novel types of the converted radio signals: transient and\n  persistent signals. Considering the mass and spin of the graviton, the\n  expected spectral line shape of the graviton is derived. FAST is the most\n  sensitive telescope to detect VHFGWs in a single-dish telescope. In addition,\n  SKA2-MID has greater detection potential",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12935v1",
                "updated": "2025-07-17T09:20:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    20,
                    51,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T09:20:51Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    20,
                    51,
                    3,
                    198,
                    0
                ],
                "title": "MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov\n  Chain Monte Carlo Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov\n  Chain Monte Carlo Acceleration"
                },
                "summary": "An increasing number of applications are exploiting sampling-based algorithms\nfor planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)\nalgorithms form the computational backbone of this emerging branch of machine\nlearning. Unfortunately, the high computational cost limits their feasibility\nfor large-scale problems and real-world applications, and the existing MCMC\nacceleration solutions are either limited in hardware flexibility or fail to\nmaintain efficiency at the system level across a variety of end-to-end\napplications. This paper introduces \\textbf{MC$^2$A}, an algorithm-hardware\nco-design framework, enabling efficient and flexible optimization for MCMC\nacceleration. Firstly, \\textbf{MC$^2$A} analyzes the MCMC workload diversity\nthrough an extension of the processor performance roofline model with a 3rd\ndimension to derive the optimal balance between the compute, sampling and\nmemory parameters. Secondly, \\textbf{MC$^2$A} proposes a parametrized hardware\naccelerator architecture with flexible and efficient support of MCMC kernels\nwith a pipeline of ISA-programmable tree-structured processing units,\nreconfigurable samplers and a crossbar interconnect to support irregular\naccess. Thirdly, the core of \\textbf{MC$^2$A} is powered by a novel Gumbel\nsampler that eliminates exponential and normalization operations. In the\nend-to-end case study, \\textbf{MC$^2$A} achieves an overall {$307.6\\times$,\n$1.4\\times$, $2.0\\times$, $84.2\\times$} speedup compared to the CPU, GPU, TPU\nand state-of-the-art MCMC accelerator. Evaluated on various representative MCMC\nworkloads, this work demonstrates and exploits the feasibility of general\nhardware acceleration to popularize MCMC-based solutions in diverse application\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An increasing number of applications are exploiting sampling-based algorithms\nfor planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)\nalgorithms form the computational backbone of this emerging branch of machine\nlearning. Unfortunately, the high computational cost limits their feasibility\nfor large-scale problems and real-world applications, and the existing MCMC\nacceleration solutions are either limited in hardware flexibility or fail to\nmaintain efficiency at the system level across a variety of end-to-end\napplications. This paper introduces \\textbf{MC$^2$A}, an algorithm-hardware\nco-design framework, enabling efficient and flexible optimization for MCMC\nacceleration. Firstly, \\textbf{MC$^2$A} analyzes the MCMC workload diversity\nthrough an extension of the processor performance roofline model with a 3rd\ndimension to derive the optimal balance between the compute, sampling and\nmemory parameters. Secondly, \\textbf{MC$^2$A} proposes a parametrized hardware\naccelerator architecture with flexible and efficient support of MCMC kernels\nwith a pipeline of ISA-programmable tree-structured processing units,\nreconfigurable samplers and a crossbar interconnect to support irregular\naccess. Thirdly, the core of \\textbf{MC$^2$A} is powered by a novel Gumbel\nsampler that eliminates exponential and normalization operations. In the\nend-to-end case study, \\textbf{MC$^2$A} achieves an overall {$307.6\\times$,\n$1.4\\times$, $2.0\\times$, $84.2\\times$} speedup compared to the CPU, GPU, TPU\nand state-of-the-art MCMC accelerator. Evaluated on various representative MCMC\nworkloads, this work demonstrates and exploits the feasibility of general\nhardware acceleration to popularize MCMC-based solutions in diverse application\ndomains."
                },
                "authors": [
                    {
                        "name": "Shirui Zhao"
                    },
                    {
                        "name": "Jun Yin"
                    },
                    {
                        "name": "Lingyun Yao"
                    },
                    {
                        "name": "Martin Andraud"
                    },
                    {
                        "name": "Wannes Meert"
                    },
                    {
                        "name": "Marian Verhelst"
                    }
                ],
                "author_detail": {
                    "name": "Marian Verhelst"
                },
                "author": "Marian Verhelst",
                "arxiv_comment": "14 pages, 15 figures, IEEE journal paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12916v1",
                "updated": "2025-07-17T09:02:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    2,
                    4,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T09:02:04Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    2,
                    4,
                    3,
                    198,
                    0
                ],
                "title": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding\n  With Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding\n  With Large Language Models"
                },
                "summary": "Advancements in foundation models have made it possible to conduct\napplications in various downstream tasks. Especially, the new era has witnessed\na remarkable capability to extend Large Language Models (LLMs) for tackling\ntasks of 3D scene understanding. Current methods rely heavily on 3D point\nclouds, but the 3D point cloud reconstruction of an indoor scene often results\nin information loss. Some textureless planes or repetitive patterns are prone\nto omission and manifest as voids within the reconstructed 3D point clouds.\nBesides, objects with complex structures tend to introduce distortion of\ndetails caused by misalignments between the captured images and the dense\nreconstructed point clouds. 2D multi-view images present visual consistency\nwith 3D point clouds and provide more detailed representations of scene\ncomponents, which can naturally compensate for these deficiencies. Based on\nthese insights, we propose Argus, a novel 3D multimodal framework that\nleverages multi-view images for enhanced 3D scene understanding with LLMs. In\ngeneral, Argus can be treated as a 3D Large Multimodal Foundation Model\n(3D-LMM) since it takes various modalities as input(text instructions, 2D\nmulti-view images, and 3D point clouds) and expands the capability of LLMs to\ntackle 3D tasks. Argus involves fusing and integrating multi-view images and\ncamera poses into view-as-scene features, which interact with the 3D features\nto create comprehensive and detailed 3D-aware scene embeddings. Our approach\ncompensates for the information loss while reconstructing 3D point clouds and\nhelps LLMs better understand the 3D world. Extensive experiments demonstrate\nthat our method outperforms existing 3D-LMMs in various downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in foundation models have made it possible to conduct\napplications in various downstream tasks. Especially, the new era has witnessed\na remarkable capability to extend Large Language Models (LLMs) for tackling\ntasks of 3D scene understanding. Current methods rely heavily on 3D point\nclouds, but the 3D point cloud reconstruction of an indoor scene often results\nin information loss. Some textureless planes or repetitive patterns are prone\nto omission and manifest as voids within the reconstructed 3D point clouds.\nBesides, objects with complex structures tend to introduce distortion of\ndetails caused by misalignments between the captured images and the dense\nreconstructed point clouds. 2D multi-view images present visual consistency\nwith 3D point clouds and provide more detailed representations of scene\ncomponents, which can naturally compensate for these deficiencies. Based on\nthese insights, we propose Argus, a novel 3D multimodal framework that\nleverages multi-view images for enhanced 3D scene understanding with LLMs. In\ngeneral, Argus can be treated as a 3D Large Multimodal Foundation Model\n(3D-LMM) since it takes various modalities as input(text instructions, 2D\nmulti-view images, and 3D point clouds) and expands the capability of LLMs to\ntackle 3D tasks. Argus involves fusing and integrating multi-view images and\ncamera poses into view-as-scene features, which interact with the 3D features\nto create comprehensive and detailed 3D-aware scene embeddings. Our approach\ncompensates for the information loss while reconstructing 3D point clouds and\nhelps LLMs better understand the 3D world. Extensive experiments demonstrate\nthat our method outperforms existing 3D-LMMs in various downstream tasks."
                },
                "authors": [
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Xiaoyan Wang"
                    },
                    {
                        "name": "Ruifei Ma"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Zeju Li"
                    },
                    {
                        "name": "Xiangde Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangde Liu"
                },
                "author": "Xiangde Liu",
                "arxiv_doi": "10.1109/TNNLS.2025.3581411",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNNLS.2025.3581411",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.12916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by TNNLS2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15841v2",
                "updated": "2025-07-17T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    53,
                    48,
                    3,
                    198,
                    0
                ],
                "published": "2025-06-18T19:44:46Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    19,
                    44,
                    46,
                    2,
                    169,
                    0
                ],
                "title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient\n  Long-Horizon Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEM1: Learning to Synergize Memory and Reasoning for Efficient\n  Long-Horizon Agents"
                },
                "summary": "Modern language agents must operate over long-horizon, multi-turn\ninteractions, where they retrieve external information, adapt to observations,\nand answer interdependent queries. Yet, most LLM systems rely on full-context\nprompting, appending all past turns regardless of their relevance. This leads\nto unbounded memory growth, increased computational costs, and degraded\nreasoning performance on out-of-distribution input lengths. We introduce MEM1,\nan end-to-end reinforcement learning framework that enables agents to operate\nwith constant memory across long multi-turn tasks. At each turn, MEM1 updates a\ncompact shared internal state that jointly supports memory consolidation and\nreasoning. This state integrates prior memory with new observations from the\nenvironment while strategically discarding irrelevant or redundant information.\nTo support training in more realistic and compositional settings, we propose a\nsimple yet effective and scalable approach to constructing multi-turn\nenvironments by composing existing datasets into arbitrarily complex task\nsequences. Experiments across three domains, including internal retrieval QA,\nopen-domain web QA, and multi-turn web shopping, show that MEM1-7B improves\nperformance by 3.5x while reducing memory usage by 3.7x compared to\nQwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes\nbeyond the training horizon. Our results demonstrate the promise of\nreasoning-driven memory consolidation as a scalable alternative to existing\nsolutions for training long-horizon interactive agents, where both efficiency\nand performance are optimized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern language agents must operate over long-horizon, multi-turn\ninteractions, where they retrieve external information, adapt to observations,\nand answer interdependent queries. Yet, most LLM systems rely on full-context\nprompting, appending all past turns regardless of their relevance. This leads\nto unbounded memory growth, increased computational costs, and degraded\nreasoning performance on out-of-distribution input lengths. We introduce MEM1,\nan end-to-end reinforcement learning framework that enables agents to operate\nwith constant memory across long multi-turn tasks. At each turn, MEM1 updates a\ncompact shared internal state that jointly supports memory consolidation and\nreasoning. This state integrates prior memory with new observations from the\nenvironment while strategically discarding irrelevant or redundant information.\nTo support training in more realistic and compositional settings, we propose a\nsimple yet effective and scalable approach to constructing multi-turn\nenvironments by composing existing datasets into arbitrarily complex task\nsequences. Experiments across three domains, including internal retrieval QA,\nopen-domain web QA, and multi-turn web shopping, show that MEM1-7B improves\nperformance by 3.5x while reducing memory usage by 3.7x compared to\nQwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes\nbeyond the training horizon. Our results demonstrate the promise of\nreasoning-driven memory consolidation as a scalable alternative to existing\nsolutions for training long-horizon interactive agents, where both efficiency\nand performance are optimized."
                },
                "authors": [
                    {
                        "name": "Zijian Zhou"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Zhaoxuan Wu"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Alok Prakash"
                    },
                    {
                        "name": "Daniela Rus"
                    },
                    {
                        "name": "Jinhua Zhao"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Paul Pu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Paul Pu Liang"
                },
                "author": "Paul Pu Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13886v3",
                "updated": "2025-07-17T08:46:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    46,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-05-20T03:47:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    47,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General\n  Reasoning"
                },
                "summary": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic."
                },
                "authors": [
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Jixin Tang"
                    },
                    {
                        "name": "Hangcheng Li"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Yanbo Wen"
                    },
                    {
                        "name": "Fan Song"
                    },
                    {
                        "name": "Jiahao Zhan"
                    },
                    {
                        "name": "Yuyang Lu"
                    },
                    {
                        "name": "Chaoran Tao"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Jizhou Yu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "63 pages, 23 figures, submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12901v1",
                "updated": "2025-07-17T08:40:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    40,
                    45,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T08:40:45Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    40,
                    45,
                    3,
                    198,
                    0
                ],
                "title": "Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic\n  Chain-of-Thought Synthesis Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic\n  Chain-of-Thought Synthesis Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable general reasoning capabilities, holding significant potential for\napplications in the financial domain, a field that requires robust and reliable\nreasoning. It has been demonstrated that distilling high-quality\nchain-of-thought (CoT) rationales from advanced general reasoning models offers\na promising and efficient path to the financial reasoning model. However,\nexisting CoT synthesis methods suffer from shallow CoT sampling, leaving the\nquestion of how to construct a well-designed knowledge space for finance\nreasoning unexplored. In this paper, we present\n\\textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset\ncharacterized by its systematic CoT synthesis optimization. We first introduce\na comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge\nExtraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and\ndeep financial reasoning trajectories. Furthermore, a systematic investigation,\ntermed CoT Cube, is conducted to analyze critical factors that influence CoT\neffectiveness, such as necessity, length and synthesizer, yielding valuable\ninsights for high-quality financial CoT construction. Experiments demonstrate\nthat models trained on our Agentar-DeepFinance-300K achieve significant\nimprovements on financial benchmarks. We publicly release\nAgentar-DeepFinance-300K , hoping to advance the research in financial\nreasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable general reasoning capabilities, holding significant potential for\napplications in the financial domain, a field that requires robust and reliable\nreasoning. It has been demonstrated that distilling high-quality\nchain-of-thought (CoT) rationales from advanced general reasoning models offers\na promising and efficient path to the financial reasoning model. However,\nexisting CoT synthesis methods suffer from shallow CoT sampling, leaving the\nquestion of how to construct a well-designed knowledge space for finance\nreasoning unexplored. In this paper, we present\n\\textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset\ncharacterized by its systematic CoT synthesis optimization. We first introduce\na comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge\nExtraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and\ndeep financial reasoning trajectories. Furthermore, a systematic investigation,\ntermed CoT Cube, is conducted to analyze critical factors that influence CoT\neffectiveness, such as necessity, length and synthesizer, yielding valuable\ninsights for high-quality financial CoT construction. Experiments demonstrate\nthat models trained on our Agentar-DeepFinance-300K achieve significant\nimprovements on financial benchmarks. We publicly release\nAgentar-DeepFinance-300K , hoping to advance the research in financial\nreasoning models."
                },
                "authors": [
                    {
                        "name": "Xiaoke Zhao"
                    },
                    {
                        "name": "Zhaowen Zhou"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Lihong Wang"
                    },
                    {
                        "name": "Zhiyi Huang"
                    },
                    {
                        "name": "Kaiyuan Zheng"
                    },
                    {
                        "name": "Yanjun Zheng"
                    },
                    {
                        "name": "Xiyang Du"
                    },
                    {
                        "name": "Longfei Liao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Xiang Qi"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06208v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06208v3",
                "updated": "2025-07-17T08:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    39,
                    11,
                    3,
                    198,
                    0
                ],
                "published": "2024-11-09T15:12:43Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    15,
                    12,
                    43,
                    5,
                    314,
                    0
                ],
                "title": "IOPO: Empowering LLMs with Complex Instruction Following via\n  Input-Output Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IOPO: Empowering LLMs with Complex Instruction Following via\n  Input-Output Preference Optimization"
                },
                "summary": "In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively."
                },
                "authors": [
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Cheng Fu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06208v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06208v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04631v2",
                "updated": "2025-07-17T08:20:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    20,
                    16,
                    3,
                    198,
                    0
                ],
                "published": "2024-04-06T13:38:15Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    13,
                    38,
                    15,
                    5,
                    97,
                    0
                ],
                "title": "On the Limitations of Large Language Models (LLMs): False Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limitations of Large Language Models (LLMs): False Attribution"
                },
                "summary": "In this work, we introduce a new hallucination metric - Simple Hallucination\nIndex (SHI) and provide insight into one important limitation of the parametric\nknowledge of large language models (LLMs), i.e. false attribution. The task of\nautomatic author attribution for relatively small chunks of text is an\nimportant NLP task but can be challenging. We empirically evaluate the power of\n3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and\nLLaMA-2-13B). We acquired the top 10 most popular books of a month, according\nto Project Gutenberg, divided each one into equal chunks of 400 words, and\nprompted each LLM to predict the author. We then randomly sampled 162 chunks\nper book for human evaluation, based on the error margin of 7% and a confidence\nlevel of 95%. The average results show that Mixtral 8x7B has the highest\nprediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724,\n0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B.\nHowever, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as\nhigh as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong\nnegative correlation of accuracy and SHI, given by r, demonstrates the fidelity\nof the new hallucination metric, which may generalize to other tasks. We also\nshow that prediction accuracies correlate positively with the frequencies of\nWikipedia instances of the book titles instead of the downloads and we perform\nerror analyses of predictions. We publicly release the annotated chunks of data\nand our codes to aid the reproducibility and evaluation of other models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce a new hallucination metric - Simple Hallucination\nIndex (SHI) and provide insight into one important limitation of the parametric\nknowledge of large language models (LLMs), i.e. false attribution. The task of\nautomatic author attribution for relatively small chunks of text is an\nimportant NLP task but can be challenging. We empirically evaluate the power of\n3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and\nLLaMA-2-13B). We acquired the top 10 most popular books of a month, according\nto Project Gutenberg, divided each one into equal chunks of 400 words, and\nprompted each LLM to predict the author. We then randomly sampled 162 chunks\nper book for human evaluation, based on the error margin of 7% and a confidence\nlevel of 95%. The average results show that Mixtral 8x7B has the highest\nprediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724,\n0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B.\nHowever, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as\nhigh as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong\nnegative correlation of accuracy and SHI, given by r, demonstrates the fidelity\nof the new hallucination metric, which may generalize to other tasks. We also\nshow that prediction accuracies correlate positively with the frequencies of\nWikipedia instances of the book titles instead of the downloads and we perform\nerror analyses of predictions. We publicly release the annotated chunks of data\nand our codes to aid the reproducibility and evaluation of other models."
                },
                "authors": [
                    {
                        "name": "Tosin Adewumi"
                    },
                    {
                        "name": "Nudrat Habib"
                    },
                    {
                        "name": "Lama Alkhaled"
                    },
                    {
                        "name": "Elisa Barney"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Barney"
                },
                "author": "Elisa Barney",
                "arxiv_comment": "This paper was accepted for presentation by Recent Advances in NLP\n  (RANLP) 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12889v1",
                "updated": "2025-07-17T08:17:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    17,
                    35,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T08:17:35Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    17,
                    35,
                    3,
                    198,
                    0
                ],
                "title": "Camera-based implicit mind reading by capturing higher-order semantic\n  dynamics of human gaze within environmental context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera-based implicit mind reading by capturing higher-order semantic\n  dynamics of human gaze within environmental context"
                },
                "summary": "Emotion recognition,as a step toward mind reading,seeks to infer internal\nstates from external cues.Most existing methods rely on explicit signals-such\nas facial expressions,speech,or gestures-that reflect only bodily responses and\noverlook the influence of environmental context.These cues are often\nvoluntary,easy to mask,and insufficient for capturing deeper,implicit emotions.\nPhysiological signal-based approaches offer more direct access to internal\nstates but require complex sensors that compromise natural behavior and limit\nscalability.Gaze-based methods typically rely on static fixation analysis and\nfail to capture the rich,dynamic interactions between gaze and the\nenvironment,and thus cannot uncover the deep connection between emotion and\nimplicit behavior.To address these limitations,we propose a novel\ncamera-based,user-unaware emotion recognition approach that integrates gaze\nfixation patterns with environmental semantics and temporal dynamics.Leveraging\nstandard HD cameras,our method unobtrusively captures users'eye appearance and\nhead movements in natural settings-without the need for specialized hardware or\nactive user participation.From these visual cues,the system estimates gaze\ntrajectories over time and space, providing the basis for modeling the spatial,\nsemantic,and temporal dimensions of gaze behavior. This allows us to capture\nthe dynamic interplay between visual attention and the surrounding\nenvironment,revealing that emotions are not merely physiological responses but\ncomplex outcomes of human-environment interactions.The proposed approach\nenables user-unaware,real-time,and continuous emotion recognition,offering high\ngeneralizability and low deployment cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion recognition,as a step toward mind reading,seeks to infer internal\nstates from external cues.Most existing methods rely on explicit signals-such\nas facial expressions,speech,or gestures-that reflect only bodily responses and\noverlook the influence of environmental context.These cues are often\nvoluntary,easy to mask,and insufficient for capturing deeper,implicit emotions.\nPhysiological signal-based approaches offer more direct access to internal\nstates but require complex sensors that compromise natural behavior and limit\nscalability.Gaze-based methods typically rely on static fixation analysis and\nfail to capture the rich,dynamic interactions between gaze and the\nenvironment,and thus cannot uncover the deep connection between emotion and\nimplicit behavior.To address these limitations,we propose a novel\ncamera-based,user-unaware emotion recognition approach that integrates gaze\nfixation patterns with environmental semantics and temporal dynamics.Leveraging\nstandard HD cameras,our method unobtrusively captures users'eye appearance and\nhead movements in natural settings-without the need for specialized hardware or\nactive user participation.From these visual cues,the system estimates gaze\ntrajectories over time and space, providing the basis for modeling the spatial,\nsemantic,and temporal dimensions of gaze behavior. This allows us to capture\nthe dynamic interplay between visual attention and the surrounding\nenvironment,revealing that emotions are not merely physiological responses but\ncomplex outcomes of human-environment interactions.The proposed approach\nenables user-unaware,real-time,and continuous emotion recognition,offering high\ngeneralizability and low deployment cost."
                },
                "authors": [
                    {
                        "name": "Mengke Song"
                    },
                    {
                        "name": "Yuge Xie"
                    },
                    {
                        "name": "Qi Cui"
                    },
                    {
                        "name": "Luming Li"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Guotao Wang"
                    },
                    {
                        "name": "Chenglizhao Chen"
                    },
                    {
                        "name": "Shanchen Pang"
                    }
                ],
                "author_detail": {
                    "name": "Shanchen Pang"
                },
                "author": "Shanchen Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16054v2",
                "updated": "2025-07-17T08:13:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    13,
                    7,
                    3,
                    198,
                    0
                ],
                "published": "2023-12-26T13:54:00Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    13,
                    54,
                    0,
                    1,
                    360,
                    0
                ],
                "title": "A Logically Consistent Chain-of-Thought Approach for Stance Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Logically Consistent Chain-of-Thought Approach for Stance Detection"
                },
                "summary": "Zero-shot stance detection (ZSSD) aims to detect stances toward unseen\ntargets. Incorporating background knowledge to enhance transferability between\nseen and unseen targets constitutes the primary approach of ZSSD. However,\nthese methods often struggle with a knowledge-task disconnect and lack logical\nconsistency in their predictions. To address these issues, we introduce a novel\napproach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which\nimproves stance detection by ensuring relevant and logically sound knowledge\nextraction. LC-CoT employs a three-step process. Initially, it assesses whether\nsupplementary external knowledge is necessary. Subsequently, it uses API calls\nto retrieve this knowledge, which can be processed by a separate LLM. Finally,\na manual exemplar guides the LLM to infer stance categories, using an if-then\nlogical structure to maintain relevance and logical coherence. This structured\napproach to eliciting background knowledge enhances the model's capability,\noutperforming traditional supervised methods without relying on labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot stance detection (ZSSD) aims to detect stances toward unseen\ntargets. Incorporating background knowledge to enhance transferability between\nseen and unseen targets constitutes the primary approach of ZSSD. However,\nthese methods often struggle with a knowledge-task disconnect and lack logical\nconsistency in their predictions. To address these issues, we introduce a novel\napproach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which\nimproves stance detection by ensuring relevant and logically sound knowledge\nextraction. LC-CoT employs a three-step process. Initially, it assesses whether\nsupplementary external knowledge is necessary. Subsequently, it uses API calls\nto retrieve this knowledge, which can be processed by a separate LLM. Finally,\na manual exemplar guides the LLM to infer stance categories, using an if-then\nlogical structure to maintain relevance and logical coherence. This structured\napproach to eliciting background knowledge enhances the model's capability,\noutperforming traditional supervised methods without relying on labeled data."
                },
                "authors": [
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Daijun Ding"
                    },
                    {
                        "name": "Liwen Jing"
                    },
                    {
                        "name": "Hu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hu Huang"
                },
                "author": "Hu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.16054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12885v1",
                "updated": "2025-07-17T08:10:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    10,
                    55,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T08:10:55Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    10,
                    55,
                    3,
                    198,
                    0
                ],
                "title": "VAR-MATH: Probing True Mathematical Reasoning in Large Language Models\n  via Symbolic Multi-Instance Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAR-MATH: Probing True Mathematical Reasoning in Large Language Models\n  via Symbolic Multi-Instance Benchmarks"
                },
                "summary": "Recent advances in reinforcement learning (RL) have led to substantial\nimprovements in the mathematical reasoning abilities of large language models\n(LLMs), as measured by standard benchmarks. However, these gains often persist\neven when models are trained with flawed signals, such as random or inverted\nrewards, raising a fundamental question: do such improvements reflect true\nreasoning, or are they merely artifacts of overfitting to benchmark-specific\npatterns? To address this question, we take an evaluation-centric perspective\nand identify two critical shortcomings in existing protocols. First,\n\\emph{benchmark contamination} arises from the public availability of test\nproblems, increasing the risk of data leakage. Second, \\emph{evaluation\nfragility} stems from the reliance on single-instance assessments, which are\nhighly sensitive to stochastic outputs and fail to capture reasoning\nconsistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic\nevaluation framework designed to probe genuine reasoning ability. By converting\nfixed numerical problems into symbolic templates and requiring models to solve\nmultiple instantiations of each, VAR-MATH enforces consistent reasoning across\nstructurally equivalent variants, thereby mitigating contamination and\nimproving evaluation robustness. We apply VAR-MATH to transform two popular\nbenchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and\nVAR-AIME24. Experimental results reveal substantial performance drops for\nRL-trained models on the variabilized versions, especially for smaller models,\nwith average declines of 48.0\\% on AMC23 and 58.3\\% on AIME24. These findings\nsuggest that many existing RL methods rely on superficial heuristics and fail\nto generalize beyond specific numerical forms. Overall, VAR-MATH offers a\nprincipled, contamination-resistant evaluation paradigm for mathematical\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have led to substantial\nimprovements in the mathematical reasoning abilities of large language models\n(LLMs), as measured by standard benchmarks. However, these gains often persist\neven when models are trained with flawed signals, such as random or inverted\nrewards, raising a fundamental question: do such improvements reflect true\nreasoning, or are they merely artifacts of overfitting to benchmark-specific\npatterns? To address this question, we take an evaluation-centric perspective\nand identify two critical shortcomings in existing protocols. First,\n\\emph{benchmark contamination} arises from the public availability of test\nproblems, increasing the risk of data leakage. Second, \\emph{evaluation\nfragility} stems from the reliance on single-instance assessments, which are\nhighly sensitive to stochastic outputs and fail to capture reasoning\nconsistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic\nevaluation framework designed to probe genuine reasoning ability. By converting\nfixed numerical problems into symbolic templates and requiring models to solve\nmultiple instantiations of each, VAR-MATH enforces consistent reasoning across\nstructurally equivalent variants, thereby mitigating contamination and\nimproving evaluation robustness. We apply VAR-MATH to transform two popular\nbenchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and\nVAR-AIME24. Experimental results reveal substantial performance drops for\nRL-trained models on the variabilized versions, especially for smaller models,\nwith average declines of 48.0\\% on AMC23 and 58.3\\% on AIME24. These findings\nsuggest that many existing RL methods rely on superficial heuristics and fail\nto generalize beyond specific numerical forms. Overall, VAR-MATH offers a\nprincipled, contamination-resistant evaluation paradigm for mathematical\nreasoning."
                },
                "authors": [
                    {
                        "name": "Jian Yao"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21773v2",
                "updated": "2025-07-17T08:03:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    3,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-30T16:17:53Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    17,
                    53,
                    2,
                    120,
                    0
                ],
                "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness"
                },
                "summary": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision."
                },
                "authors": [
                    {
                        "name": "Junsheng Huang"
                    },
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Yucheng Huang"
                    },
                    {
                        "name": "Sandeep Polisetty"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "May Fung"
                    }
                ],
                "author_detail": {
                    "name": "May Fung"
                },
                "author": "May Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08898v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08898v3",
                "updated": "2025-07-17T08:01:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    1,
                    44,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-11T05:15:35Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    5,
                    15,
                    35,
                    4,
                    192,
                    0
                ],
                "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast\n  Asian Languages for LLM Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast\n  Asian Languages for LLM Software Systems"
                },
                "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. We release our pre-trained model and\nbenchmark at https://github.com/awsm-research/SEALGuard to support further\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. We release our pre-trained model and\nbenchmark at https://github.com/awsm-research/SEALGuard to support further\nresearch."
                },
                "authors": [
                    {
                        "name": "Wenliang Shan"
                    },
                    {
                        "name": "Michael Fu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    }
                ],
                "author_detail": {
                    "name": "Chakkrit Tantithamthavorn"
                },
                "author": "Chakkrit Tantithamthavorn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08898v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08898v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12878v1",
                "updated": "2025-07-17T07:55:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    55,
                    34,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T07:55:34Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    55,
                    34,
                    3,
                    198,
                    0
                ],
                "title": "Bayesian Modeling and Estimation of Linear Time-Variant Systems using\n  Neural Networks and Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Modeling and Estimation of Linear Time-Variant Systems using\n  Neural Networks and Gaussian Processes"
                },
                "summary": "The identification of Linear Time-Variant (LTV) systems from input-output\ndata is a fundamental yet challenging ill-posed inverse problem. This work\nintroduces a unified Bayesian framework that models the system's impulse\nresponse, $h(t, \\tau)$, as a stochastic process. We decompose the response into\na posterior mean and a random fluctuation term, a formulation that provides a\nprincipled approach for quantifying uncertainty and naturally defines a new,\nuseful system class we term Linear Time-Invariant in Expectation (LTIE). To\nperform inference, we leverage modern machine learning techniques, including\nBayesian neural networks and Gaussian Processes, using scalable variational\ninference. We demonstrate through a series of experiments that our framework\ncan robustly infer the properties of an LTI system from a single noisy\nobservation, show superior data efficiency compared to classical methods in a\nsimulated ambient noise tomography problem, and successfully track a\ncontinuously varying LTV impulse response by using a structured Gaussian\nProcess prior. This work provides a flexible and robust methodology for\nuncertainty-aware system identification in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The identification of Linear Time-Variant (LTV) systems from input-output\ndata is a fundamental yet challenging ill-posed inverse problem. This work\nintroduces a unified Bayesian framework that models the system's impulse\nresponse, $h(t, \\tau)$, as a stochastic process. We decompose the response into\na posterior mean and a random fluctuation term, a formulation that provides a\nprincipled approach for quantifying uncertainty and naturally defines a new,\nuseful system class we term Linear Time-Invariant in Expectation (LTIE). To\nperform inference, we leverage modern machine learning techniques, including\nBayesian neural networks and Gaussian Processes, using scalable variational\ninference. We demonstrate through a series of experiments that our framework\ncan robustly infer the properties of an LTI system from a single noisy\nobservation, show superior data efficiency compared to classical methods in a\nsimulated ambient noise tomography problem, and successfully track a\ncontinuously varying LTV impulse response by using a structured Gaussian\nProcess prior. This work provides a flexible and robust methodology for\nuncertainty-aware system identification in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Yaniv Shulman"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Shulman"
                },
                "author": "Yaniv Shulman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09617v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09617v2",
                "updated": "2025-07-17T07:51:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    51,
                    28,
                    3,
                    198,
                    0
                ],
                "published": "2024-02-14T23:12:09Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    23,
                    12,
                    9,
                    2,
                    45,
                    0
                ],
                "title": "LLM-Enhanced User-Item Interactions: Leveraging Edge Information for\n  Optimized Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced User-Item Interactions: Leveraging Edge Information for\n  Optimized Recommendations"
                },
                "summary": "Graph recommendation methods, representing a connected interaction\nperspective, reformulate user-item interactions as graphs to leverage graph\nstructure and topology to recommend and have proved practical effectiveness at\nscale. Large language models, representing a textual generative perspective,\nexcel at modeling user languages, understanding behavioral contexts, capturing\nuser-item semantic relationships, analyzing textual sentiments, and generating\ncoherent and contextually relevant texts as recommendations. However, there is\na gap between the connected graph perspective and the text generation\nperspective as the task formulations are different. A research question arises:\nhow can we effectively integrate the two perspectives for more personalized\nrecsys? To fill this gap, we propose to incorporate graph-edge information into\nLLMs via prompt and attention innovations. We reformulate recommendations as a\nprobabilistic generative problem using prompts. We develop a framework to\nincorporate graph edge information from the prompt and attention mechanisms for\ngraph-structured LLM recommendations. We develop a new prompt design that\nbrings in both first-order and second-order graph relationships; we devise an\nimproved LLM attention mechanism to embed direct the spatial and connectivity\ninformation of edges. Our evaluation of real-world datasets demonstrates the\nframework's ability to understand connectivity information in graph data and to\nimprove the relevance and quality of recommendation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph recommendation methods, representing a connected interaction\nperspective, reformulate user-item interactions as graphs to leverage graph\nstructure and topology to recommend and have proved practical effectiveness at\nscale. Large language models, representing a textual generative perspective,\nexcel at modeling user languages, understanding behavioral contexts, capturing\nuser-item semantic relationships, analyzing textual sentiments, and generating\ncoherent and contextually relevant texts as recommendations. However, there is\na gap between the connected graph perspective and the text generation\nperspective as the task formulations are different. A research question arises:\nhow can we effectively integrate the two perspectives for more personalized\nrecsys? To fill this gap, we propose to incorporate graph-edge information into\nLLMs via prompt and attention innovations. We reformulate recommendations as a\nprobabilistic generative problem using prompts. We develop a framework to\nincorporate graph edge information from the prompt and attention mechanisms for\ngraph-structured LLM recommendations. We develop a new prompt design that\nbrings in both first-order and second-order graph relationships; we devise an\nimproved LLM attention mechanism to embed direct the spatial and connectivity\ninformation of edges. Our evaluation of real-world datasets demonstrates the\nframework's ability to understand connectivity information in graph data and to\nimprove the relevance and quality of recommendation results."
                },
                "authors": [
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Liang Wu"
                    },
                    {
                        "name": "Liangjie Hong"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09617v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12855v1",
                "updated": "2025-07-17T07:26:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    26,
                    22,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T07:26:22Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    26,
                    22,
                    3,
                    198,
                    0
                ],
                "title": "DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task\n  Demonstration Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task\n  Demonstration Learning"
                },
                "summary": "The integration of large language models (LLMs) with control systems has\ndemonstrated significant potential in various settings, such as task completion\nwith a robotic manipulator. A main reason for this success is the ability of\nLLMs to perform in-context learning, which, however, strongly relies on the\ndesign of task examples, closely related to the target tasks. Consequently,\nemploying LLMs to formulate optimal control problems often requires task\nexamples that contain explicit mathematical expressions, designed by trained\nengineers. Furthermore, there is often no principled way to evaluate for\nhallucination before task execution. To address these challenges, we propose\nDEMONSTRATE, a novel methodology that avoids the use of LLMs for complex\noptimization problem generations, and instead only relies on the embedding\nrepresentations of task descriptions. To do this, we leverage tools from\ninverse optimal control to replace in-context prompt examples with task\ndemonstrations, as well as the concept of multitask learning, which ensures\ntarget and example task similarity by construction. Given the fact that\nhardware demonstrations can easily be collected using teleoperation or guidance\nof the robot, our approach significantly reduces the reliance on engineering\nexpertise for designing in-context examples. Furthermore, the enforced\nmultitask structure enables learning from few demonstrations and assessment of\nhallucinations prior to task execution. We demonstrate the effectiveness of our\nmethod through simulation and hardware experiments involving a robotic arm\ntasked with tabletop manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) with control systems has\ndemonstrated significant potential in various settings, such as task completion\nwith a robotic manipulator. A main reason for this success is the ability of\nLLMs to perform in-context learning, which, however, strongly relies on the\ndesign of task examples, closely related to the target tasks. Consequently,\nemploying LLMs to formulate optimal control problems often requires task\nexamples that contain explicit mathematical expressions, designed by trained\nengineers. Furthermore, there is often no principled way to evaluate for\nhallucination before task execution. To address these challenges, we propose\nDEMONSTRATE, a novel methodology that avoids the use of LLMs for complex\noptimization problem generations, and instead only relies on the embedding\nrepresentations of task descriptions. To do this, we leverage tools from\ninverse optimal control to replace in-context prompt examples with task\ndemonstrations, as well as the concept of multitask learning, which ensures\ntarget and example task similarity by construction. Given the fact that\nhardware demonstrations can easily be collected using teleoperation or guidance\nof the robot, our approach significantly reduces the reliance on engineering\nexpertise for designing in-context examples. Furthermore, the enforced\nmultitask structure enables learning from few demonstrations and assessment of\nhallucinations prior to task execution. We demonstrate the effectiveness of our\nmethod through simulation and hardware experiments involving a robotic arm\ntasked with tabletop manipulation."
                },
                "authors": [
                    {
                        "name": "Rahel Rickenbach"
                    },
                    {
                        "name": "Bruce Lee"
                    },
                    {
                        "name": "Ren Zurbrgg"
                    },
                    {
                        "name": "Carmen Amo Alonso"
                    },
                    {
                        "name": "Melanie N. Zeilinger"
                    }
                ],
                "author_detail": {
                    "name": "Melanie N. Zeilinger"
                },
                "author": "Melanie N. Zeilinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17385v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17385v3",
                "updated": "2025-07-17T07:10:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    10,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2024-07-24T16:07:57Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    16,
                    7,
                    57,
                    2,
                    206,
                    0
                ],
                "title": "Formalising causal inference as prediction on a target population",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising causal inference as prediction on a target population"
                },
                "summary": "The standard approach to causal modelling especially in social and health\nsciences is the potential outcomes framework due to Neyman and Rubin. In this\nframework, observations are thought to be drawn from a distribution over\nvariables of interest, and the goal is to identify parameters of this\ndistribution. Even though the stated goal is often to inform decision making on\nsome target population, there is no straightforward way to include these target\npopulations in the framework. Instead of modelling the relationship between the\nobserved sample and the target population, the inductive assumptions in this\nframework take the form of abstract sampling and independence assumptions. In\nthis paper, we develop a version of this framework that construes causal\ninference as treatment-wise predictions for finite populations where all\nassumptions are testable in retrospect; this means that one can not only test\npredictions themselves (without any fundamental problem) but also investigate\nsources of error when they fail. Due to close connections to the original\nframework, established methods can still be be analysed under the new\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The standard approach to causal modelling especially in social and health\nsciences is the potential outcomes framework due to Neyman and Rubin. In this\nframework, observations are thought to be drawn from a distribution over\nvariables of interest, and the goal is to identify parameters of this\ndistribution. Even though the stated goal is often to inform decision making on\nsome target population, there is no straightforward way to include these target\npopulations in the framework. Instead of modelling the relationship between the\nobserved sample and the target population, the inductive assumptions in this\nframework take the form of abstract sampling and independence assumptions. In\nthis paper, we develop a version of this framework that construes causal\ninference as treatment-wise predictions for finite populations where all\nassumptions are testable in retrospect; this means that one can not only test\npredictions themselves (without any fundamental problem) but also investigate\nsources of error when they fail. Due to close connections to the original\nframework, established methods can still be be analysed under the new\nframework."
                },
                "authors": [
                    {
                        "name": "Benedikt Hltgen"
                    },
                    {
                        "name": "Robert C. Williamson"
                    }
                ],
                "author_detail": {
                    "name": "Robert C. Williamson"
                },
                "author": "Robert C. Williamson",
                "arxiv_comment": "Presented at the Humans, Algorithmic Decision-Making and Society\n  Workshop at ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17385v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17385v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12840v1",
                "updated": "2025-07-17T06:59:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    59,
                    52,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T06:59:52Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    59,
                    52,
                    3,
                    198,
                    0
                ],
                "title": "Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better\n  Understand Public Concerns about Vaccines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better\n  Understand Public Concerns about Vaccines"
                },
                "summary": "Vaccine hesitancy threatens public health, leading to delayed or rejected\nvaccines. Social media is a vital source for understanding public concerns, and\ntraditional methods like topic modelling often struggle to capture nuanced\nopinions. Though trained for query answering, large Language Models (LLMs)\noften miss current events and community concerns. Additionally, hallucinations\nin LLMs can compromise public health communication. To address these\nlimitations, we developed a tool (VaxPulse Query Corner) using the Retrieval\nAugmented Generation technique. It addresses complex queries about public\nvaccine concerns on various online platforms, aiding public health\nadministrators and stakeholders in understanding public concerns and\nimplementing targeted interventions to boost vaccine confidence. Analysing\n35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and\nrelevance (0.94).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vaccine hesitancy threatens public health, leading to delayed or rejected\nvaccines. Social media is a vital source for understanding public concerns, and\ntraditional methods like topic modelling often struggle to capture nuanced\nopinions. Though trained for query answering, large Language Models (LLMs)\noften miss current events and community concerns. Additionally, hallucinations\nin LLMs can compromise public health communication. To address these\nlimitations, we developed a tool (VaxPulse Query Corner) using the Retrieval\nAugmented Generation technique. It addresses complex queries about public\nvaccine concerns on various online platforms, aiding public health\nadministrators and stakeholders in understanding public concerns and\nimplementing targeted interventions to boost vaccine confidence. Analysing\n35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and\nrelevance (0.94)."
                },
                "authors": [
                    {
                        "name": "Muhammad Javed"
                    },
                    {
                        "name": "Sedigh Khademi Habibabadi"
                    },
                    {
                        "name": "Christopher Palmer"
                    },
                    {
                        "name": "Hazel Clothier"
                    },
                    {
                        "name": "Jim Buttery"
                    },
                    {
                        "name": "Gerardo Luis Dimaguila"
                    }
                ],
                "author_detail": {
                    "name": "Gerardo Luis Dimaguila"
                },
                "author": "Gerardo Luis Dimaguila",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12457v2",
                "updated": "2025-07-17T06:38:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    38,
                    8,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T17:56:44Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    56,
                    44,
                    2,
                    197,
                    0
                ],
                "title": "Does $K$-fold CV based penalty perform variable selection or does it\n  lead to $n^{1/2}$-consistency in Lasso?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does $K$-fold CV based penalty perform variable selection or does it\n  lead to $n^{1/2}$-consistency in Lasso?"
                },
                "summary": "Least absolute shrinkage and selection operator or Lasso, introduced by\nTibshirani (1996), is one of the widely used regularization methods in\nregression. It is observed that the properties of Lasso vary wildly depending\non the choice of the penalty parameter. The recent results of Lahiri (2021)\nsuggest that, depending on the nature of the penalty parameter, Lasso can\neither be variable selection consistent or be $n^{1/2}-$consistent. However,\npractitioners generally implement Lasso by choosing the penalty parameter in a\ndata-dependent way, the most popular being the $K$-fold cross-validation. In\nthis paper, we explore the variable selection consistency and\n$n^{1/2}-$consistency of Lasso when the penalty is chosen based on $K$-fold\ncross-validation with $K$ being fixed. We consider the fixed-dimensional\nheteroscedastic linear regression model and show that Lasso with $K$-fold\ncross-validation based penalty is $n^{1/2}-$consistent, but not variable\nselection consistent. We also establish the $n^{1/2}-$consistency of the\n$K$-fold cross-validation based penalty as an intermediate result.\nAdditionally, as a consequence of $n^{1/2}-$consistency, we establish the\nvalidity of Bootstrap to approximate the distribution of the Lasso estimator\nbased on $K-$fold cross-validation. We validate the Bootstrap approximation in\nfinite samples based on a moderate simulation study. Thus, our results\nessentially justify the use of $K$-fold cross-validation in practice to draw\ninferences based on $n^{1/2}-$scaled pivotal quantities in Lasso regression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Least absolute shrinkage and selection operator or Lasso, introduced by\nTibshirani (1996), is one of the widely used regularization methods in\nregression. It is observed that the properties of Lasso vary wildly depending\non the choice of the penalty parameter. The recent results of Lahiri (2021)\nsuggest that, depending on the nature of the penalty parameter, Lasso can\neither be variable selection consistent or be $n^{1/2}-$consistent. However,\npractitioners generally implement Lasso by choosing the penalty parameter in a\ndata-dependent way, the most popular being the $K$-fold cross-validation. In\nthis paper, we explore the variable selection consistency and\n$n^{1/2}-$consistency of Lasso when the penalty is chosen based on $K$-fold\ncross-validation with $K$ being fixed. We consider the fixed-dimensional\nheteroscedastic linear regression model and show that Lasso with $K$-fold\ncross-validation based penalty is $n^{1/2}-$consistent, but not variable\nselection consistent. We also establish the $n^{1/2}-$consistency of the\n$K$-fold cross-validation based penalty as an intermediate result.\nAdditionally, as a consequence of $n^{1/2}-$consistency, we establish the\nvalidity of Bootstrap to approximate the distribution of the Lasso estimator\nbased on $K-$fold cross-validation. We validate the Bootstrap approximation in\nfinite samples based on a moderate simulation study. Thus, our results\nessentially justify the use of $K$-fold cross-validation in practice to draw\ninferences based on $n^{1/2}-$scaled pivotal quantities in Lasso regression."
                },
                "authors": [
                    {
                        "name": "Mayukh Choudhury"
                    },
                    {
                        "name": "Debraj Das"
                    }
                ],
                "author_detail": {
                    "name": "Debraj Das"
                },
                "author": "Debraj Das",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.19515",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12820v1",
                "updated": "2025-07-17T06:24:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    24,
                    20,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T06:24:20Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    24,
                    20,
                    3,
                    198,
                    0
                ],
                "title": "Emotional Support with LLM-based Empathetic Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional Support with LLM-based Empathetic Dialogue Generation"
                },
                "summary": "Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems."
                },
                "authors": [
                    {
                        "name": "Shiquan Wang"
                    },
                    {
                        "name": "Ruiyu Fang"
                    },
                    {
                        "name": "Zhongjiang He"
                    },
                    {
                        "name": "Shuangyong Song"
                    },
                    {
                        "name": "Yongxiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongxiang Li"
                },
                "author": "Yongxiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12819v1",
                "updated": "2025-07-17T06:22:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    22,
                    49,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T06:22:49Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    22,
                    49,
                    3,
                    198,
                    0
                ],
                "title": "MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free\n  Zero-Shot Composed Image Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free\n  Zero-Shot Composed Image Retrieval"
                },
                "summary": "Composed Image Retrieval (CIR) is the task of retrieving a target image from\na gallery using a composed query consisting of a reference image and a\nmodification text. Among various CIR approaches, training-free zero-shot\nmethods based on pre-trained models are cost-effective but still face notable\nlimitations. For example, sequential VLM-LLM pipelines process each modality\nindependently, which often results in information loss and limits cross-modal\ninteraction. In contrast, methods based on multimodal large language models\n(MLLMs) often focus exclusively on applying changes indicated by the text,\nwithout fully utilizing the contextual visual information from the reference\nimage. To address these issues, we propose multi-faceted Chain-of-Thought with\nre-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes\nmulti-faceted Chain-of-Thought to guide the MLLM to balance explicit\nmodifications and contextual visual cues, generating two distinct captions: one\nfocused on modification and the other integrating comprehensive visual-textual\ncontext. The first caption is used to filter candidate images. Subsequently, we\ncombine these two captions and the reference image to perform multi-grained\nre-ranking. This two-stage approach facilitates precise retrieval by aligning\nwith the textual modification instructions while preserving the visual context\nof the reference image. Through extensive experiments, MCoT-RE achieves\nstate-of-the-art results among training-free methods, yielding improvements of\nup to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composed Image Retrieval (CIR) is the task of retrieving a target image from\na gallery using a composed query consisting of a reference image and a\nmodification text. Among various CIR approaches, training-free zero-shot\nmethods based on pre-trained models are cost-effective but still face notable\nlimitations. For example, sequential VLM-LLM pipelines process each modality\nindependently, which often results in information loss and limits cross-modal\ninteraction. In contrast, methods based on multimodal large language models\n(MLLMs) often focus exclusively on applying changes indicated by the text,\nwithout fully utilizing the contextual visual information from the reference\nimage. To address these issues, we propose multi-faceted Chain-of-Thought with\nre-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes\nmulti-faceted Chain-of-Thought to guide the MLLM to balance explicit\nmodifications and contextual visual cues, generating two distinct captions: one\nfocused on modification and the other integrating comprehensive visual-textual\ncontext. The first caption is used to filter candidate images. Subsequently, we\ncombine these two captions and the reference image to perform multi-grained\nre-ranking. This two-stage approach facilitates precise retrieval by aligning\nwith the textual modification instructions while preserving the visual context\nof the reference image. Through extensive experiments, MCoT-RE achieves\nstate-of-the-art results among training-free methods, yielding improvements of\nup to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR."
                },
                "authors": [
                    {
                        "name": "Jeong-Woo Park"
                    },
                    {
                        "name": "Seong-Whan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Whan Lee"
                },
                "author": "Seong-Whan Lee",
                "arxiv_comment": "6 pages, 4 figures, 2025 IEEE International Conference on Systems,\n  Man, and Cybernetics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12808v1",
                "updated": "2025-07-17T05:48:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    48,
                    45,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T05:48:45Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    48,
                    45,
                    3,
                    198,
                    0
                ],
                "title": "Large Language Models' Internal Perception of Symbolic Music",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models' Internal Perception of Symbolic Music"
                },
                "summary": "Large language models (LLMs) excel at modeling relationships between strings\nin natural language and have shown promise in extending to other symbolic\ndomains like coding or mathematics. However, the extent to which they\nimplicitly model symbolic music remains underexplored. This paper investigates\nhow LLMs represent musical concepts by generating symbolic music data from\ntextual prompts describing combinations of genres and styles, and evaluating\ntheir utility through recognition and generation tasks. We produce a dataset of\nLLM-generated MIDI files without relying on explicit musical training. We then\ntrain neural networks entirely on this LLM-generated MIDI dataset and perform\ngenre and style classification as well as melody completion, benchmarking their\nperformance against established models. Our results demonstrate that LLMs can\ninfer rudimentary musical structures and temporal relationships from text,\nhighlighting both their potential to implicitly encode musical patterns and\ntheir limitations due to a lack of explicit musical context, shedding light on\ntheir generative capabilities for symbolic music.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at modeling relationships between strings\nin natural language and have shown promise in extending to other symbolic\ndomains like coding or mathematics. However, the extent to which they\nimplicitly model symbolic music remains underexplored. This paper investigates\nhow LLMs represent musical concepts by generating symbolic music data from\ntextual prompts describing combinations of genres and styles, and evaluating\ntheir utility through recognition and generation tasks. We produce a dataset of\nLLM-generated MIDI files without relying on explicit musical training. We then\ntrain neural networks entirely on this LLM-generated MIDI dataset and perform\ngenre and style classification as well as melody completion, benchmarking their\nperformance against established models. Our results demonstrate that LLMs can\ninfer rudimentary musical structures and temporal relationships from text,\nhighlighting both their potential to implicitly encode musical patterns and\ntheir limitations due to a lack of explicit musical context, shedding light on\ntheir generative capabilities for symbolic music."
                },
                "authors": [
                    {
                        "name": "Andrew Shin"
                    },
                    {
                        "name": "Kunitake Kaneko"
                    }
                ],
                "author_detail": {
                    "name": "Kunitake Kaneko"
                },
                "author": "Kunitake Kaneko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09592v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09592v3",
                "updated": "2025-07-17T05:47:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    47,
                    22,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-13T11:48:24Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    11,
                    48,
                    24,
                    6,
                    194,
                    0
                ],
                "title": "THOR: Transformer Heuristics for On-Demand Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THOR: Transformer Heuristics for On-Demand Retrieval"
                },
                "summary": "We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)\nModule, designed and implemented by eSapiens, a secure, scalable engine that\ntransforms natural-language questions into verified, read-only SQL analytics\nfor enterprise databases. The Text-to-SQL module follows a decoupled\norchestration/execution architecture: a Supervisor Agent routes queries, Schema\nRetrieval dynamically injects table and column metadata, and a SQL Generation\nAgent emits single-statement SELECT queries protected by a read-only guardrail.\nAn integrated Self-Correction & Rating loop captures empty results, execution\nerrors, or low-quality outputs and triggers up to five LLM-driven regeneration\nattempts. Finally, a Result Interpretation Agent produces concise,\nhuman-readable insights and hands raw rows to the Insight & Intelligence engine\nfor visualization or forecasting.\n  Smoke tests across finance, sales, and operations scenarios demonstrate\nreliable ad-hoc querying and automated periodic reporting. By embedding schema\nawareness, fault-tolerant execution, and compliance guardrails, the THOR Module\nempowers non-technical users to access live data with zero-SQL simplicity and\nenterprise-grade safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)\nModule, designed and implemented by eSapiens, a secure, scalable engine that\ntransforms natural-language questions into verified, read-only SQL analytics\nfor enterprise databases. The Text-to-SQL module follows a decoupled\norchestration/execution architecture: a Supervisor Agent routes queries, Schema\nRetrieval dynamically injects table and column metadata, and a SQL Generation\nAgent emits single-statement SELECT queries protected by a read-only guardrail.\nAn integrated Self-Correction & Rating loop captures empty results, execution\nerrors, or low-quality outputs and triggers up to five LLM-driven regeneration\nattempts. Finally, a Result Interpretation Agent produces concise,\nhuman-readable insights and hands raw rows to the Insight & Intelligence engine\nfor visualization or forecasting.\n  Smoke tests across finance, sales, and operations scenarios demonstrate\nreliable ad-hoc querying and automated periodic reporting. By embedding schema\nawareness, fault-tolerant execution, and compliance guardrails, the THOR Module\nempowers non-technical users to access live data with zero-SQL simplicity and\nenterprise-grade safety."
                },
                "authors": [
                    {
                        "name": "Isaac Shi"
                    },
                    {
                        "name": "Zeyuan Li"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Wenli Wang"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09592v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09592v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12806v1",
                "updated": "2025-07-17T05:46:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    46,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T05:46:27Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    46,
                    27,
                    3,
                    198,
                    0
                ],
                "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models"
                },
                "summary": "The rapid rise of Large Language Models (LLMs)-based intelligent agents\nunderscores the need for robust, scalable evaluation frameworks. Existing\nmethods rely on static benchmarks and labor-intensive data collection, limiting\npractical assessment. We introduce \\oursystemname, an open-source Model Context\nProtocol (MCP)-based framework that automates end-to-end task generation and\ndeep evaluation of LLM agents across diverse domains. MCPEval standardizes\nmetrics, seamlessly integrates with native agent tools, and eliminates manual\neffort in building evaluation pipelines. Empirical results across five\nreal-world domains show its effectiveness in revealing nuanced, domain-specific\nperformance. We publicly release MCPEval\nhttps://github.com/SalesforceAIResearch/MCPEval to promote reproducible and\nstandardized LLM agent evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Large Language Models (LLMs)-based intelligent agents\nunderscores the need for robust, scalable evaluation frameworks. Existing\nmethods rely on static benchmarks and labor-intensive data collection, limiting\npractical assessment. We introduce \\oursystemname, an open-source Model Context\nProtocol (MCP)-based framework that automates end-to-end task generation and\ndeep evaluation of LLM agents across diverse domains. MCPEval standardizes\nmetrics, seamlessly integrates with native agent tools, and eliminates manual\neffort in building evaluation pipelines. Empirical results across five\nreal-world domains show its effectiveness in revealing nuanced, domain-specific\nperformance. We publicly release MCPEval\nhttps://github.com/SalesforceAIResearch/MCPEval to promote reproducible and\nstandardized LLM agent evaluation."
                },
                "authors": [
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Jielin Qiu"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Roshan Ram"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "https://github.com/SalesforceAIResearch/MCPEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18248v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18248v3",
                "updated": "2025-07-17T05:35:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    35,
                    13,
                    3,
                    198,
                    0
                ],
                "published": "2025-06-23T02:35:09Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    2,
                    35,
                    9,
                    0,
                    174,
                    0
                ],
                "title": "Semantic Structure-Aware Generative Attacks for Enhanced Adversarial\n  Transferability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Structure-Aware Generative Attacks for Enhanced Adversarial\n  Transferability"
                },
                "summary": "Generative adversarial attacks train a perturbation generator on a white-box\nsurrogate model and subsequently apply the crafted perturbations to unseen\nblack-box victim models. In contrast to iterative attacks, these methods\ndeliver superior inference-time efficiency, scalability, and transferability;\nhowever, up until now, existing studies have not fully exploited the\nrepresentational capacity of generative models to preserve and harness semantic\ninformation. Specifically, the intermediate activations of the generator encode\nrich semantic features--object boundaries and coarse shapes--that remain\nunder-exploited, thereby limiting the alignment of perturbations with\nobject-salient regions which are critical for adversarial transferability. To\nremedy this, we introduce a semantic structure-aware attack framework based on\nthe Mean Teacher, which serves as a temporally smoothed feature reference. With\nthis smoothed reference, we further direct semantic consistency between the\nearly-layer activations in the student and those of the semantically rich\nteacher by feature distillation. By anchoring perturbation synthesis to the\nsemantically salient early intermediate blocks within the generator based on\nempirical findings, our method guides progressive adversarial perturbation on\nregions that substantially enhance adversarial transferability. We conduct\nextensive experiments over diverse models, domains and tasks to demonstrate\nconsistent improvements relative to state-of-the-art generative attacks,\ncomprehensively evaluated using conventional metrics and our newly proposed\nAccidental Correction Rate (ACR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative adversarial attacks train a perturbation generator on a white-box\nsurrogate model and subsequently apply the crafted perturbations to unseen\nblack-box victim models. In contrast to iterative attacks, these methods\ndeliver superior inference-time efficiency, scalability, and transferability;\nhowever, up until now, existing studies have not fully exploited the\nrepresentational capacity of generative models to preserve and harness semantic\ninformation. Specifically, the intermediate activations of the generator encode\nrich semantic features--object boundaries and coarse shapes--that remain\nunder-exploited, thereby limiting the alignment of perturbations with\nobject-salient regions which are critical for adversarial transferability. To\nremedy this, we introduce a semantic structure-aware attack framework based on\nthe Mean Teacher, which serves as a temporally smoothed feature reference. With\nthis smoothed reference, we further direct semantic consistency between the\nearly-layer activations in the student and those of the semantically rich\nteacher by feature distillation. By anchoring perturbation synthesis to the\nsemantically salient early intermediate blocks within the generator based on\nempirical findings, our method guides progressive adversarial perturbation on\nregions that substantially enhance adversarial transferability. We conduct\nextensive experiments over diverse models, domains and tasks to demonstrate\nconsistent improvements relative to state-of-the-art generative attacks,\ncomprehensively evaluated using conventional metrics and our newly proposed\nAccidental Correction Rate (ACR)."
                },
                "authors": [
                    {
                        "name": "Jongoh Jeong"
                    },
                    {
                        "name": "Hunmin Yang"
                    },
                    {
                        "name": "Jaeseok Jeong"
                    },
                    {
                        "name": "Kuk-Jin Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Kuk-Jin Yoon"
                },
                "author": "Kuk-Jin Yoon",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18248v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18248v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.13353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13353v1",
                "updated": "2025-07-17T17:59:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    59,
                    59,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:59:59Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    59,
                    59,
                    3,
                    198,
                    0
                ],
                "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal\n  Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoITG: Multimodal Video Understanding with Instructed Temporal\n  Grounding"
                },
                "summary": "Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding."
                },
                "authors": [
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "De-an Huang"
                    },
                    {
                        "name": "Zhiqi Li"
                    },
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Guilin Li"
                    },
                    {
                        "name": "Jose M. Alvarez"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Zhiding Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiding Yu"
                },
                "author": "Zhiding Yu",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13348v1",
                "updated": "2025-07-17T17:59:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    59,
                    55,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:59:55Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    59,
                    55,
                    3,
                    198,
                    0
                ],
                "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning"
                },
                "summary": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink."
                },
                "authors": [
                    {
                        "name": "Senqiao Yang"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Xin Lai"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Code and models are available at\n  https://github.com/dvlab-research/VisionThink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01772v2",
                "updated": "2025-07-17T17:58:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    58,
                    50,
                    3,
                    198,
                    0
                ],
                "published": "2024-10-02T17:29:34Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    29,
                    34,
                    2,
                    276,
                    0
                ],
                "title": "DeFine: Decision-Making with Analogical Reasoning over Factor Profiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFine: Decision-Making with Analogical Reasoning over Factor Profiles"
                },
                "summary": "LLMs are ideal for decision-making thanks to their ability to reason over\nlong contexts. However, challenges arise when processing speech transcripts\nthat describe complex scenarios, as they are verbose and include repetition,\nhedging, and vagueness. E.g., during a company's earnings call, an executive\nmight project a positive revenue outlook to reassure investors, despite\nuncertainty regarding future earnings. It is crucial for LLMs to incorporate\nthis uncertainty systematically when making decisions. In this paper, we\nintroduce \\textsc{DeFine}, a modular framework that constructs probabilistic\nfactor profiles from complex scenarios. It then integrates these profiles with\nanalogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in new situations. Our framework\nseparates the tasks of quantifying uncertainty and incorporating it into LLM\ndecision-making. This approach is particularly useful in areas such as\nconsulting and financial deliberation, where making decisions under uncertainty\nis vital.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are ideal for decision-making thanks to their ability to reason over\nlong contexts. However, challenges arise when processing speech transcripts\nthat describe complex scenarios, as they are verbose and include repetition,\nhedging, and vagueness. E.g., during a company's earnings call, an executive\nmight project a positive revenue outlook to reassure investors, despite\nuncertainty regarding future earnings. It is crucial for LLMs to incorporate\nthis uncertainty systematically when making decisions. In this paper, we\nintroduce \\textsc{DeFine}, a modular framework that constructs probabilistic\nfactor profiles from complex scenarios. It then integrates these profiles with\nanalogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in new situations. Our framework\nseparates the tasks of quantifying uncertainty and incorporating it into LLM\ndecision-making. This approach is particularly useful in areas such as\nconsulting and financial deliberation, where making decisions under uncertainty\nis vital."
                },
                "authors": [
                    {
                        "name": "Yebowen Hu"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Yiming Lu"
                    },
                    {
                        "name": "Daoan Zhang"
                    },
                    {
                        "name": "Hassan Foroosh"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Fei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Liu"
                },
                "author": "Fei Liu",
                "arxiv_comment": "Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025), Vienna, Austria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13335v1",
                "updated": "2025-07-17T17:51:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    51,
                    20,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:51:20Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    51,
                    20,
                    3,
                    198,
                    0
                ],
                "title": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour\n  Understanding from Traditional Puns to Topical Jokes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour\n  Understanding from Traditional Puns to Topical Jokes"
                },
                "summary": "Humour, as a complex language form, is derived from myriad aspects of life,\nwhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes. In this work, we investigate whether the ability of\nLarge Language Models (LLMs) to explain humour depends on the particular humour\nform. We compare models on simple puns and more complex topical humour that\nrequires knowledge of real-world entities and events. In doing so, we curate a\ndataset of 600 jokes split across 4 joke types and manually write high-quality\nexplanations. These jokes include heterographic and homographic puns,\ncontemporary internet humour, and topical jokes, where understanding relies on\nreasoning beyond \"common sense\", rooted instead in world knowledge regarding\nnews events and pop culture. Using this dataset, we compare the zero-shot\nabilities of a range of LLMs to accurately and comprehensively explain jokes of\ndifferent types, identifying key research gaps in the task of humour\nexplanation. We find that none of the tested models (inc. reasoning models) are\ncapable of reliably generating adequate explanations of all joke types, further\nhighlighting the narrow focus of most works in computational humour on overly\nsimple joke forms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humour, as a complex language form, is derived from myriad aspects of life,\nwhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes. In this work, we investigate whether the ability of\nLarge Language Models (LLMs) to explain humour depends on the particular humour\nform. We compare models on simple puns and more complex topical humour that\nrequires knowledge of real-world entities and events. In doing so, we curate a\ndataset of 600 jokes split across 4 joke types and manually write high-quality\nexplanations. These jokes include heterographic and homographic puns,\ncontemporary internet humour, and topical jokes, where understanding relies on\nreasoning beyond \"common sense\", rooted instead in world knowledge regarding\nnews events and pop culture. Using this dataset, we compare the zero-shot\nabilities of a range of LLMs to accurately and comprehensively explain jokes of\ndifferent types, identifying key research gaps in the task of humour\nexplanation. We find that none of the tested models (inc. reasoning models) are\ncapable of reliably generating adequate explanations of all joke types, further\nhighlighting the narrow focus of most works in computational humour on overly\nsimple joke forms."
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "William Thorne"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13334v1",
                "updated": "2025-07-17T17:50:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    50,
                    36,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:50:36Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    50,
                    36,
                    3,
                    198,
                    0
                ],
                "title": "A Survey of Context Engineering for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Context Engineering for Large Language Models"
                },
                "summary": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI."
                },
                "authors": [
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Jiayu Yao"
                    },
                    {
                        "name": "Yuyao Ge"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Jiazhi Liu"
                    },
                    {
                        "name": "Mingyu Li"
                    },
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Chenlin Zhou"
                    },
                    {
                        "name": "Jiayi Mao"
                    },
                    {
                        "name": "Tianze Xia"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Shenghua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shenghua Liu"
                },
                "author": "Shenghua Liu",
                "arxiv_comment": "ongoing work; 165 pages, 1401 citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13332v1",
                "updated": "2025-07-17T17:50:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    50,
                    7,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:50:07Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    50,
                    7,
                    3,
                    198,
                    0
                ],
                "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner"
                },
                "summary": "Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data."
                },
                "authors": [
                    {
                        "name": "Zhouqi Hua"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Chengqi Lyu"
                    },
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13328v1",
                "updated": "2025-07-17T17:47:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    47,
                    47,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:47:47Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    47,
                    47,
                    3,
                    198,
                    0
                ],
                "title": "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does\n  Not Fundamentally Alter It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does\n  Not Fundamentally Alter It"
                },
                "summary": "Does vision-and-language (VL) training change the linguistic representations\nof language models in meaningful ways? Most results in the literature have\nshown inconsistent or marginal differences, both behaviorally and\nrepresentationally. In this work, we start from the hypothesis that the domain\nin which VL training could have a significant effect is lexical-conceptual\nknowledge, in particular its taxonomic organization. Through comparing minimal\npairs of text-only LMs and their VL-trained counterparts, we first show that\nthe VL models often outperform their text-only counterparts on a text-only\nquestion-answering task that requires taxonomic understanding of concepts\nmentioned in the questions. Using an array of targeted behavioral and\nrepresentational analyses, we show that the LMs and VLMs do not differ\nsignificantly in terms of their taxonomic knowledge itself, but they differ in\nhow they represent questions that contain concepts in a taxonomic relation vs.\na non-taxonomic relation. This implies that the taxonomic knowledge itself does\nnot change substantially through additional VL training, but VL training does\nimprove the deployment of this knowledge in the context of a specific task,\neven when the presentation of the task is purely linguistic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does vision-and-language (VL) training change the linguistic representations\nof language models in meaningful ways? Most results in the literature have\nshown inconsistent or marginal differences, both behaviorally and\nrepresentationally. In this work, we start from the hypothesis that the domain\nin which VL training could have a significant effect is lexical-conceptual\nknowledge, in particular its taxonomic organization. Through comparing minimal\npairs of text-only LMs and their VL-trained counterparts, we first show that\nthe VL models often outperform their text-only counterparts on a text-only\nquestion-answering task that requires taxonomic understanding of concepts\nmentioned in the questions. Using an array of targeted behavioral and\nrepresentational analyses, we show that the LMs and VLMs do not differ\nsignificantly in terms of their taxonomic knowledge itself, but they differ in\nhow they represent questions that contain concepts in a taxonomic relation vs.\na non-taxonomic relation. This implies that the taxonomic knowledge itself does\nnot change substantially through additional VL training, but VL training does\nimprove the deployment of this knowledge in the context of a specific task,\neven when the presentation of the task is purely linguistic."
                },
                "authors": [
                    {
                        "name": "Yulu Qin"
                    },
                    {
                        "name": "Dheeraj Varghese"
                    },
                    {
                        "name": "Adam Dahlgren Lindstrm"
                    },
                    {
                        "name": "Lucia Donatelli"
                    },
                    {
                        "name": "Kanishka Misra"
                    },
                    {
                        "name": "Najoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Najoung Kim"
                },
                "author": "Najoung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13323v1",
                "updated": "2025-07-17T17:42:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    42,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:42:29Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    42,
                    29,
                    3,
                    198,
                    0
                ],
                "title": "GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic\n  Estimation using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic\n  Estimation using LLM"
                },
                "summary": "Socio-economic indicators like regional GDP, population, and education\nlevels, are crucial to shaping policy decisions and fostering sustainable\ndevelopment. This research introduces GeoReg a regression model that integrates\ndiverse data sources, including satellite imagery and web-based geospatial\ninformation, to estimate these indicators even for data-scarce regions such as\ndeveloping countries. Our approach leverages the prior knowledge of large\nlanguage model (LLM) to address the scarcity of labeled data, with the LLM\nfunctioning as a data engineer by extracting informative features to enable\neffective estimation in few-shot settings. Specifically, our model obtains\ncontextual relationships between data features and the target indicator,\ncategorizing their correlations as positive, negative, mixed, or irrelevant.\nThese features are then fed into the linear estimator with tailored weight\nconstraints for each category. To capture nonlinear patterns, the model also\nidentifies meaningful feature interactions and integrates them, along with\nnonlinear transformations. Experiments across three countries at different\nstages of development demonstrate that our model outperforms baselines in\nestimating socio-economic indicators, even for low-income countries with\nlimited data availability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socio-economic indicators like regional GDP, population, and education\nlevels, are crucial to shaping policy decisions and fostering sustainable\ndevelopment. This research introduces GeoReg a regression model that integrates\ndiverse data sources, including satellite imagery and web-based geospatial\ninformation, to estimate these indicators even for data-scarce regions such as\ndeveloping countries. Our approach leverages the prior knowledge of large\nlanguage model (LLM) to address the scarcity of labeled data, with the LLM\nfunctioning as a data engineer by extracting informative features to enable\neffective estimation in few-shot settings. Specifically, our model obtains\ncontextual relationships between data features and the target indicator,\ncategorizing their correlations as positive, negative, mixed, or irrelevant.\nThese features are then fed into the linear estimator with tailored weight\nconstraints for each category. To capture nonlinear patterns, the model also\nidentifies meaningful feature interactions and integrates them, along with\nnonlinear transformations. Experiments across three countries at different\nstages of development demonstrate that our model outperforms baselines in\nestimating socio-economic indicators, even for low-income countries with\nlimited data availability."
                },
                "authors": [
                    {
                        "name": "Kyeongjin Ahn"
                    },
                    {
                        "name": "Sungwon Han"
                    },
                    {
                        "name": "Seungeon Lee"
                    },
                    {
                        "name": "Donghyun Ahn"
                    },
                    {
                        "name": "Hyoshin Kim"
                    },
                    {
                        "name": "Jungwon Kim"
                    },
                    {
                        "name": "Jihee Kim"
                    },
                    {
                        "name": "Sangyoon Park"
                    },
                    {
                        "name": "Meeyoung Cha"
                    }
                ],
                "author_detail": {
                    "name": "Meeyoung Cha"
                },
                "author": "Meeyoung Cha",
                "arxiv_comment": "15 pages, 13 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03780v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03780v3",
                "updated": "2025-07-17T17:31:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    31,
                    44,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-30T12:57:21Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    57,
                    21,
                    2,
                    120,
                    0
                ],
                "title": "GPU Performance Portability needs Autotuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU Performance Portability needs Autotuning"
                },
                "summary": "As LLMs grow in complexity, achieving state-of-the-art performance requires\ntight co-design across algorithms, software, and hardware. Today's reliance on\na single dominant platform limits portability, creates vendor lock-in, and\nraises barriers for new AI hardware. In this work, we make the case for\ncombining just-in-time (JIT) compilation with comprehensive kernel parameter\nautotuning to enable portable LLM inference with state-of-the-art performance\nwithout code changes. Focusing on performance-critical LLM kernels, we\ndemonstrate that this approach explores up to 15x more kernel parameter\nconfigurations, produces significantly more diverse code across multiple\ndimensions, and even outperforms vendor-optimized implementations by up to\n230%, all while reducing kernel code size by 70x and eliminating manual code\noptimizations. Our results highlight autotuning as a promising path to\nunlocking model portability across GPU vendors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs grow in complexity, achieving state-of-the-art performance requires\ntight co-design across algorithms, software, and hardware. Today's reliance on\na single dominant platform limits portability, creates vendor lock-in, and\nraises barriers for new AI hardware. In this work, we make the case for\ncombining just-in-time (JIT) compilation with comprehensive kernel parameter\nautotuning to enable portable LLM inference with state-of-the-art performance\nwithout code changes. Focusing on performance-critical LLM kernels, we\ndemonstrate that this approach explores up to 15x more kernel parameter\nconfigurations, produces significantly more diverse code across multiple\ndimensions, and even outperforms vendor-optimized implementations by up to\n230%, all while reducing kernel code size by 70x and eliminating manual code\noptimizations. Our results highlight autotuning as a promising path to\nunlocking model portability across GPU vendors."
                },
                "authors": [
                    {
                        "name": "Burkhard Ringlein"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Radu Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Radu Stoica"
                },
                "author": "Radu Stoica",
                "arxiv_comment": "revision after reviewers feedback, broadening autotune study",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03780v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03780v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08589v3",
                "updated": "2025-07-17T17:20:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    20,
                    34,
                    3,
                    198,
                    0
                ],
                "published": "2024-10-11T07:36:14Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    36,
                    14,
                    4,
                    285,
                    0
                ],
                "title": "Retraining-Free Merging of Sparse MoE via Hierarchical Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retraining-Free Merging of Sparse MoE via Hierarchical Clustering"
                },
                "summary": "Sparse Mixture-of-Experts (SMoE) models represent a significant advancement\nin large language model (LLM) development through their efficient parameter\nutilization. These models achieve substantial performance improvements at\nreduced inference costs. However, the deployment of SMoE models faces\nconstraints from extensive memory requirements of expert components in\nresource-limited environments. To address these limitations, this paper\nintroduces Hierarchical Clustering for Sparsely activated Mixture of Experts\n(HC-SMoE), a task-agnostic expert merging framework for parameter reduction\nwithout retraining. HC-SMoE introduces a novel hierarchical clustering approach\nbased on expert outputs to ensure merging robustness independent of routing\ndecisions. The proposed output-based clustering method enables effective\ncapture of functional relationships between experts for large-scale\narchitectures. We provide theoretical analysis and comprehensive evaluations\nacross multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness\nin state-of-the-art models including Qwen and Mixtral. The experimental results\nvalidate HC-SMoE's superior performance and practical applicability for\nreal-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture-of-Experts (SMoE) models represent a significant advancement\nin large language model (LLM) development through their efficient parameter\nutilization. These models achieve substantial performance improvements at\nreduced inference costs. However, the deployment of SMoE models faces\nconstraints from extensive memory requirements of expert components in\nresource-limited environments. To address these limitations, this paper\nintroduces Hierarchical Clustering for Sparsely activated Mixture of Experts\n(HC-SMoE), a task-agnostic expert merging framework for parameter reduction\nwithout retraining. HC-SMoE introduces a novel hierarchical clustering approach\nbased on expert outputs to ensure merging robustness independent of routing\ndecisions. The proposed output-based clustering method enables effective\ncapture of functional relationships between experts for large-scale\narchitectures. We provide theoretical analysis and comprehensive evaluations\nacross multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness\nin state-of-the-art models including Qwen and Mixtral. The experimental results\nvalidate HC-SMoE's superior performance and practical applicability for\nreal-world deployments."
                },
                "authors": [
                    {
                        "name": "I-Chun Chen"
                    },
                    {
                        "name": "Hsu-Shen Liu"
                    },
                    {
                        "name": "Wei-Fang Sun"
                    },
                    {
                        "name": "Chen-Hao Chao"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    },
                    {
                        "name": "Chun-Yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Yi Lee"
                },
                "author": "Chun-Yi Lee",
                "arxiv_comment": "Code: https://github.com/wazenmai/HC-SMoE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13302v1",
                "updated": "2025-07-17T17:11:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    11,
                    14,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:11:14Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    11,
                    14,
                    3,
                    198,
                    0
                ],
                "title": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in\n  Large Language Model (LLM) Human Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in\n  Large Language Model (LLM) Human Evaluations"
                },
                "summary": "The evaluation of large language models is a complex task, in which several\napproaches have been proposed. The most common is the use of automated\nbenchmarks in which LLMs have to answer multiple-choice questions of different\ntopics. However, this method has certain limitations, being the most\nconcerning, the poor correlation with the humans. An alternative approach, is\nto have humans evaluate the LLMs. This poses scalability issues as there is a\nlarge and growing number of models to evaluate making it impractical (and\ncostly) to run traditional studies based on recruiting a number of evaluators\nand having them rank the responses of the models. An alternative approach is\nthe use of public arenas, such as the popular LM arena, on which any user can\nfreely evaluate models on any question and rank the responses of two models.\nThe results are then elaborated into a model ranking. An increasingly important\naspect of LLMs is their energy consumption and, therefore, evaluating how\nenergy awareness influences the decisions of humans in selecting a model is of\ninterest. In this paper, we present GEA, the Generative Energy Arena, an arena\nthat incorporates information on the energy consumption of the model in the\nevaluation process. Preliminary results obtained with GEA are also presented,\nshowing that for most questions, when users are aware of the energy\nconsumption, they favor smaller and more energy efficient models. This suggests\nthat for most user interactions, the extra cost and energy incurred by the more\ncomplex and top-performing models do not provide an increase in the perceived\nquality of the responses that justifies their use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of large language models is a complex task, in which several\napproaches have been proposed. The most common is the use of automated\nbenchmarks in which LLMs have to answer multiple-choice questions of different\ntopics. However, this method has certain limitations, being the most\nconcerning, the poor correlation with the humans. An alternative approach, is\nto have humans evaluate the LLMs. This poses scalability issues as there is a\nlarge and growing number of models to evaluate making it impractical (and\ncostly) to run traditional studies based on recruiting a number of evaluators\nand having them rank the responses of the models. An alternative approach is\nthe use of public arenas, such as the popular LM arena, on which any user can\nfreely evaluate models on any question and rank the responses of two models.\nThe results are then elaborated into a model ranking. An increasingly important\naspect of LLMs is their energy consumption and, therefore, evaluating how\nenergy awareness influences the decisions of humans in selecting a model is of\ninterest. In this paper, we present GEA, the Generative Energy Arena, an arena\nthat incorporates information on the energy consumption of the model in the\nevaluation process. Preliminary results obtained with GEA are also presented,\nshowing that for most questions, when users are aware of the energy\nconsumption, they favor smaller and more energy efficient models. This suggests\nthat for most user interactions, the extra cost and energy incurred by the more\ncomplex and top-performing models do not provide an increase in the perceived\nquality of the responses that justifies their use."
                },
                "authors": [
                    {
                        "name": "Carlos Arriaga"
                    },
                    {
                        "name": "Gonzalo Martnez"
                    },
                    {
                        "name": "Eneko Sendin"
                    },
                    {
                        "name": "Javier Conde"
                    },
                    {
                        "name": "Pedro Reviriego"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Reviriego"
                },
                "author": "Pedro Reviriego",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13300v1",
                "updated": "2025-07-17T17:09:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    9,
                    22,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T17:09:22Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    17,
                    9,
                    22,
                    3,
                    198,
                    0
                ],
                "title": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research"
                },
                "summary": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks."
                },
                "authors": [
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Weiyuan Chen"
                    },
                    {
                        "name": "Zhijian Xu"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Chengye Wang"
                    },
                    {
                        "name": "Lovekesh Vig"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13290v1",
                "updated": "2025-07-17T16:54:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    54,
                    42,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T16:54:42Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    54,
                    42,
                    3,
                    198,
                    0
                ],
                "title": "Towards Formal Verification of LLM-Generated Code from Natural Language\n  Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Formal Verification of LLM-Generated Code from Natural Language\n  Prompts"
                },
                "summary": "In the past few years LLMs have emerged as a tool that can aid programmers by\ntaking natural language descriptions and generating code based on it. However,\nLLMs often generate incorrect code that users need to fix and the literature\nsuggests users often struggle to detect these errors. In this work we seek to\noffer formal guarantees of correctness to LLM generated code; such guarantees\ncould improve the experience of using AI Code Assistants and potentially enable\nnatural language programming for users with little or no programming knowledge.\nTo address this challenge we propose to incorporate a formal query language\nthat can represent a user's intent in a formally defined but natural\nlanguage-like manner that a user can confirm matches their intent. Then, using\nsuch a query we propose to verify LLM generated code to ensure it matches the\nuser's intent. We implement these ideas in our system, Astrogator, for the\nAnsible programming language which includes such a formal query language, a\ncalculus for representing the behavior of Ansible programs, and a symbolic\ninterpreter which is used for the verification. On a benchmark suite of 21\ncode-generation tasks, our verifier is able to verify correct code in 83% of\ncases and identify incorrect code in 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past few years LLMs have emerged as a tool that can aid programmers by\ntaking natural language descriptions and generating code based on it. However,\nLLMs often generate incorrect code that users need to fix and the literature\nsuggests users often struggle to detect these errors. In this work we seek to\noffer formal guarantees of correctness to LLM generated code; such guarantees\ncould improve the experience of using AI Code Assistants and potentially enable\nnatural language programming for users with little or no programming knowledge.\nTo address this challenge we propose to incorporate a formal query language\nthat can represent a user's intent in a formally defined but natural\nlanguage-like manner that a user can confirm matches their intent. Then, using\nsuch a query we propose to verify LLM generated code to ensure it matches the\nuser's intent. We implement these ideas in our system, Astrogator, for the\nAnsible programming language which includes such a formal query language, a\ncalculus for representing the behavior of Ansible programs, and a symbolic\ninterpreter which is used for the verification. On a benchmark suite of 21\ncode-generation tasks, our verifier is able to verify correct code in 83% of\ncases and identify incorrect code in 92%."
                },
                "authors": [
                    {
                        "name": "Aaron Councilman"
                    },
                    {
                        "name": "David Fu"
                    },
                    {
                        "name": "Aryan Gupta"
                    },
                    {
                        "name": "Chengxiao Wang"
                    },
                    {
                        "name": "David Grove"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    },
                    {
                        "name": "Vikram Adve"
                    }
                ],
                "author_detail": {
                    "name": "Vikram Adve"
                },
                "author": "Vikram Adve",
                "arxiv_comment": "31 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13425v2",
                "updated": "2025-07-17T16:32:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    32,
                    46,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-18T02:51:29Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    2,
                    51,
                    29,
                    4,
                    108,
                    0
                ],
                "title": "Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with\n  Security Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with\n  Security Filtering"
                },
                "summary": "Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG."
                },
                "authors": [
                    {
                        "name": "Grace Byun"
                    },
                    {
                        "name": "Shinsun Lee"
                    },
                    {
                        "name": "Nayoung Choi"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13266v1",
                "updated": "2025-07-17T16:21:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    21,
                    47,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T16:21:47Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    21,
                    47,
                    3,
                    198,
                    0
                ],
                "title": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation"
                },
                "summary": "Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL."
                },
                "authors": [
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Hong Lu"
                    },
                    {
                        "name": "Kaiyue Wen"
                    },
                    {
                        "name": "Zaiwen Yang"
                    },
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Hongzhou Lin"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jingzhao Zhang"
                },
                "author": "Jingzhao Zhang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13255v1",
                "updated": "2025-07-17T16:04:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    4,
                    55,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T16:04:55Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    16,
                    4,
                    55,
                    3,
                    198,
                    0
                ],
                "title": "Automating Steering for Safe Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Steering for Safe Multimodal Large Language Models"
                },
                "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Lyucheng Wu"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Shumin Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shumin Deng"
                },
                "author": "Shumin Deng",
                "arxiv_comment": "Working in progress. 22 pages (8+ for main); 25 figures; 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16394v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16394v3",
                "updated": "2025-07-17T15:50:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    50,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-23T03:42:46Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    3,
                    42,
                    46,
                    2,
                    113,
                    0
                ],
                "title": "ConTextual: Improving Clinical Text Summarization in LLMs with\n  Context-preserving Token Filtering and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConTextual: Improving Clinical Text Summarization in LLMs with\n  Context-preserving Token Filtering and Knowledge Graphs"
                },
                "summary": "Unstructured clinical data can serve as a unique and rich source of\ninformation that can meaningfully inform clinical practice. Extracting the most\npertinent context from such data is critical for exploiting its true potential\ntoward optimal and timely decision-making in patient care. While prior research\nhas explored various methods for clinical text summarization, most prior\nstudies either process all input tokens uniformly or rely on heuristic-based\nfilters, which can overlook nuanced clinical cues and fail to prioritize\ninformation critical for decision-making. In this study, we propose Contextual,\na novel framework that integrates a Context-Preserving Token Filtering method\nwith a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By\npreserving context-specific important tokens and enriching them with structured\nknowledge, ConTextual improves both linguistic coherence and clinical fidelity.\nOur extensive empirical evaluations on two public benchmark datasets\ndemonstrate that ConTextual consistently outperforms other baselines. Our\nproposed approach highlights the complementary role of token-level filtering\nand structured retrieval in enhancing both linguistic and clinical integrity,\nas well as offering a scalable solution for improving precision in clinical\ntext generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unstructured clinical data can serve as a unique and rich source of\ninformation that can meaningfully inform clinical practice. Extracting the most\npertinent context from such data is critical for exploiting its true potential\ntoward optimal and timely decision-making in patient care. While prior research\nhas explored various methods for clinical text summarization, most prior\nstudies either process all input tokens uniformly or rely on heuristic-based\nfilters, which can overlook nuanced clinical cues and fail to prioritize\ninformation critical for decision-making. In this study, we propose Contextual,\na novel framework that integrates a Context-Preserving Token Filtering method\nwith a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By\npreserving context-specific important tokens and enriching them with structured\nknowledge, ConTextual improves both linguistic coherence and clinical fidelity.\nOur extensive empirical evaluations on two public benchmark datasets\ndemonstrate that ConTextual consistently outperforms other baselines. Our\nproposed approach highlights the complementary role of token-level filtering\nand structured retrieval in enhancing both linguistic and clinical integrity,\nas well as offering a scalable solution for improving precision in clinical\ntext generation."
                },
                "authors": [
                    {
                        "name": "Fahmida Liza Piya"
                    },
                    {
                        "name": "Rahmatollah Beheshti"
                    }
                ],
                "author_detail": {
                    "name": "Rahmatollah Beheshti"
                },
                "author": "Rahmatollah Beheshti",
                "arxiv_comment": "Accepted for MLHC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16394v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16394v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13238v1",
                "updated": "2025-07-17T15:47:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    47,
                    49,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T15:47:49Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    47,
                    49,
                    3,
                    198,
                    0
                ],
                "title": "HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language\n  Models"
                },
                "summary": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi."
                },
                "authors": [
                    {
                        "name": "Ashray Gupta"
                    },
                    {
                        "name": "Rohan Joseph"
                    },
                    {
                        "name": "Sunny Rai"
                    }
                ],
                "author_detail": {
                    "name": "Sunny Rai"
                },
                "author": "Sunny Rai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13236v1",
                "updated": "2025-07-17T15:47:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    47,
                    22,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T15:47:22Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    47,
                    22,
                    3,
                    198,
                    0
                ],
                "title": "Enhancing Cross-task Transfer of Large Language Models via Activation\n  Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cross-task Transfer of Large Language Models via Activation\n  Steering"
                },
                "summary": "Large language models (LLMs) have shown impressive abilities in leveraging\npretrained knowledge through prompting, but they often struggle with unseen\ntasks, particularly in data-scarce scenarios. While cross-task in-context\nlearning offers a direct solution for transferring knowledge across tasks, it\nstill faces critical challenges in terms of robustness, scalability, and\nefficiency. In this paper, we investigate whether cross-task transfer can be\nachieved via latent space steering without parameter updates or input\nexpansion. Through an analysis of activation patterns in the latent space of\nLLMs, we observe that the enhanced activations induced by in-context examples\nhave consistent patterns across different tasks. Inspired by these findings, we\npropose CAST, a novel Cross-task Activation Steering Transfer framework that\nenables effective transfer by manipulating the model's internal activation\nstates. Our approach first selects influential and diverse samples from\nhigh-resource tasks, then utilizes their contrastive representation-enhanced\nactivations to adapt LLMs to low-resource tasks. Extensive experiments across\nboth cross-domain and cross-lingual transfer settings show that our method\noutperforms competitive baselines and demonstrates superior scalability and\nlower computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive abilities in leveraging\npretrained knowledge through prompting, but they often struggle with unseen\ntasks, particularly in data-scarce scenarios. While cross-task in-context\nlearning offers a direct solution for transferring knowledge across tasks, it\nstill faces critical challenges in terms of robustness, scalability, and\nefficiency. In this paper, we investigate whether cross-task transfer can be\nachieved via latent space steering without parameter updates or input\nexpansion. Through an analysis of activation patterns in the latent space of\nLLMs, we observe that the enhanced activations induced by in-context examples\nhave consistent patterns across different tasks. Inspired by these findings, we\npropose CAST, a novel Cross-task Activation Steering Transfer framework that\nenables effective transfer by manipulating the model's internal activation\nstates. Our approach first selects influential and diverse samples from\nhigh-resource tasks, then utilizes their contrastive representation-enhanced\nactivations to adapt LLMs to low-resource tasks. Extensive experiments across\nboth cross-domain and cross-lingual transfer settings show that our method\noutperforms competitive baselines and demonstrates superior scalability and\nlower computational costs."
                },
                "authors": [
                    {
                        "name": "Xinyu Tang"
                    },
                    {
                        "name": "Zhihao Lv"
                    },
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zujie Wen"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13733v2",
                "updated": "2025-07-17T15:31:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    31,
                    25,
                    3,
                    198,
                    0
                ],
                "published": "2025-03-17T21:41:37Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    41,
                    37,
                    0,
                    76,
                    0
                ],
                "title": "CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,\n  Multi-Generator and Multi-Domain Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,\n  Multi-Generator and Multi-Domain Settings"
                },
                "summary": "Large language models (LLMs) have revolutionized code generation, automating\nprogramming with remarkable efficiency. However, these advancements challenge\nprogramming skills, ethics, and assessment integrity, making the detection of\nLLM-generated code essential for maintaining accountability and standards.\nWhile, there has been some research on this problem, it generally lacks domain\ncoverage and robustness, and only covers a small number of programming\nlanguages. To this end, we propose a framework capable of distinguishing\nbetween human- and LLM-written code across multiple programming languages, code\ngenerators, and domains. We use a large-scale dataset from renowned platforms\nand LLM-based code generators, alongside applying rigorous data quality checks,\nfeature engineering, and comparative analysis using evaluation of traditional\nmachine learning models, pre-trained language models (PLMs), and LLMs for code\ndetection. We perform an evaluation on out-of-domain scenarios, such as\ndetecting the authorship and hybrid authorship of generated code and\ngeneralizing to unseen models, domains, and programming languages. Moreover,\nour extensive experiments show that our framework effectively distinguishes\nhuman- from LLM-written code and sets a new benchmark for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized code generation, automating\nprogramming with remarkable efficiency. However, these advancements challenge\nprogramming skills, ethics, and assessment integrity, making the detection of\nLLM-generated code essential for maintaining accountability and standards.\nWhile, there has been some research on this problem, it generally lacks domain\ncoverage and robustness, and only covers a small number of programming\nlanguages. To this end, we propose a framework capable of distinguishing\nbetween human- and LLM-written code across multiple programming languages, code\ngenerators, and domains. We use a large-scale dataset from renowned platforms\nand LLM-based code generators, alongside applying rigorous data quality checks,\nfeature engineering, and comparative analysis using evaluation of traditional\nmachine learning models, pre-trained language models (PLMs), and LLMs for code\ndetection. We perform an evaluation on out-of-domain scenarios, such as\ndetecting the authorship and hybrid authorship of generated code and\ngeneralizing to unseen models, domains, and programming languages. Moreover,\nour extensive experiments show that our framework effectively distinguishes\nhuman- from LLM-written code and sets a new benchmark for this task."
                },
                "authors": [
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Dilshod Azizov"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12039v2",
                "updated": "2025-07-17T15:27:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    27,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T08:56:19Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    8,
                    56,
                    19,
                    2,
                    197,
                    0
                ],
                "title": "A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans"
                },
                "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity."
                },
                "authors": [
                    {
                        "name": "Anca Dinu"
                    },
                    {
                        "name": "Andra-Maria Florescu"
                    },
                    {
                        "name": "Alina Resceanu"
                    }
                ],
                "author_detail": {
                    "name": "Alina Resceanu"
                },
                "author": "Alina Resceanu",
                "arxiv_comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13205v1",
                "updated": "2025-07-17T15:15:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    15,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T15:15:43Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    15,
                    43,
                    3,
                    198,
                    0
                ],
                "title": "Automatically assessing oral narratives of Afrikaans and isiXhosa\n  children",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically assessing oral narratives of Afrikaans and isiXhosa\n  children"
                },
                "summary": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning."
                },
                "authors": [
                    {
                        "name": "R. Louw"
                    },
                    {
                        "name": "E. Sharratt"
                    },
                    {
                        "name": "F. de Wet"
                    },
                    {
                        "name": "C. Jacobs"
                    },
                    {
                        "name": "A. Smith"
                    },
                    {
                        "name": "H. Kamper"
                    }
                ],
                "author_detail": {
                    "name": "H. Kamper"
                },
                "arxiv_affiliation": "Stellenbosch University",
                "author": "H. Kamper",
                "arxiv_comment": "Accepted to SLaTE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23033v2",
                "updated": "2025-07-17T15:06:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    6,
                    16,
                    3,
                    198,
                    0
                ],
                "published": "2025-03-29T10:36:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    10,
                    36,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "Imagine All The Relevance: Scenario-Profiled Indexing with Knowledge\n  Expansion for Dense Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine All The Relevance: Scenario-Profiled Indexing with Knowledge\n  Expansion for Dense Retrieval"
                },
                "summary": "Existing dense retrieval models struggle with reasoning-intensive retrieval\ntask as they fail to capture implicit relevance that requires reasoning beyond\nsurface-level semantic information. To address these challenges, we propose\nScenario-Profiled Indexing with Knowledge Expansion (SPIKE), a dense retrieval\nframework that explicitly indexes implicit relevance by decomposing documents\ninto scenario-based retrieval units. SPIKE organizes documents into scenario,\nwhich encapsulates the reasoning process necessary to uncover implicit\nrelationships between hypothetical information needs and document content.\nSPIKE constructs a scenario-augmented dataset using a powerful teacher large\nlanguage model (LLM), then distills these reasoning capabilities into a\nsmaller, efficient scenario generator. During inference, SPIKE incorporates\nscenario-level relevance alongside document-level relevance, enabling\nreasoning-aware retrieval. Extensive experiments demonstrate that SPIKE\nconsistently enhances retrieval performance across various query types and\ndense retrievers. It also enhances the retrieval experience for users through\nscenario and offers valuable contextual information for LLMs in\nretrieval-augmented generation (RAG).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing dense retrieval models struggle with reasoning-intensive retrieval\ntask as they fail to capture implicit relevance that requires reasoning beyond\nsurface-level semantic information. To address these challenges, we propose\nScenario-Profiled Indexing with Knowledge Expansion (SPIKE), a dense retrieval\nframework that explicitly indexes implicit relevance by decomposing documents\ninto scenario-based retrieval units. SPIKE organizes documents into scenario,\nwhich encapsulates the reasoning process necessary to uncover implicit\nrelationships between hypothetical information needs and document content.\nSPIKE constructs a scenario-augmented dataset using a powerful teacher large\nlanguage model (LLM), then distills these reasoning capabilities into a\nsmaller, efficient scenario generator. During inference, SPIKE incorporates\nscenario-level relevance alongside document-level relevance, enabling\nreasoning-aware retrieval. Extensive experiments demonstrate that SPIKE\nconsistently enhances retrieval performance across various query types and\ndense retrievers. It also enhances the retrieval experience for users through\nscenario and offers valuable contextual information for LLMs in\nretrieval-augmented generation (RAG)."
                },
                "authors": [
                    {
                        "name": "Sangam Lee"
                    },
                    {
                        "name": "Ryang Heo"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11628v2",
                "updated": "2025-07-17T15:01:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    1,
                    23,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-15T18:05:43Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    18,
                    5,
                    43,
                    1,
                    196,
                    0
                ],
                "title": "DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday\n  Storytelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday\n  Storytelling"
                },
                "summary": "An interactive vignette is a popular and immersive visual storytelling\napproach that invites viewers to role-play a character and influences the\nnarrative in an interactive environment. However, it has not been widely used\nby everyday storytellers yet due to authoring complexity, which conflicts with\nthe immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted\nauthoring system for interactive vignette creation in everyday storytelling. It\ntakes a natural language story as input and extracts the three core elements of\nan interactive vignette (environment, characters, and events), enabling authors\nto focus on refining these elements instead of constructing them from scratch.\nThen, it automatically transforms the single-branch story input into a\nbranch-and-bottleneck structure using an LLM-powered narrative planner, which\nenables flexible viewer interactions while freeing the author from\nmulti-branching. A technical evaluation (N=16) shows that DiaryPlay-generated\ncharacter activities are on par with human-authored ones regarding\nbelievability. A user study (N=16) shows that DiaryPlay effectively supports\nauthors in creating interactive vignette elements, maintains authorial intent\nwhile reacting to viewer interactions, and provides engaging viewing\nexperiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An interactive vignette is a popular and immersive visual storytelling\napproach that invites viewers to role-play a character and influences the\nnarrative in an interactive environment. However, it has not been widely used\nby everyday storytellers yet due to authoring complexity, which conflicts with\nthe immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted\nauthoring system for interactive vignette creation in everyday storytelling. It\ntakes a natural language story as input and extracts the three core elements of\nan interactive vignette (environment, characters, and events), enabling authors\nto focus on refining these elements instead of constructing them from scratch.\nThen, it automatically transforms the single-branch story input into a\nbranch-and-bottleneck structure using an LLM-powered narrative planner, which\nenables flexible viewer interactions while freeing the author from\nmulti-branching. A technical evaluation (N=16) shows that DiaryPlay-generated\ncharacter activities are on par with human-authored ones regarding\nbelievability. A user study (N=16) shows that DiaryPlay effectively supports\nauthors in creating interactive vignette elements, maintains authorial intent\nwhile reacting to viewer interactions, and provides engaging viewing\nexperiences."
                },
                "authors": [
                    {
                        "name": "Jiangnan Xu"
                    },
                    {
                        "name": "Haeseul Cha"
                    },
                    {
                        "name": "Gosu Choi"
                    },
                    {
                        "name": "Gyu-cheol Lee"
                    },
                    {
                        "name": "Yeo-Jin Yoon"
                    },
                    {
                        "name": "Zucheul Lee"
                    },
                    {
                        "name": "Konstantinos Papangelis"
                    },
                    {
                        "name": "Dae Hyun Kim"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10917v2",
                "updated": "2025-07-17T15:01:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    1,
                    8,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-15T02:13:54Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    2,
                    13,
                    54,
                    1,
                    196,
                    0
                ],
                "title": "LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation"
                },
                "summary": "Recently, much effort has been devoted to modeling users' multi-interests\nbased on their behaviors or auxiliary signals. However, existing methods often\nrely on heuristic assumptions, e.g., co-occurring items indicate the same\ninterest of users, failing to capture user multi-interests aligning with\nreal-world scenarios. While large language models (LLMs) show significant\npotential for multi-interest analysis due to their extensive knowledge and\npowerful reasoning capabilities, two key challenges remain. First, the\ngranularity of LLM-driven multi-interests is agnostic, possibly leading to\noverly fine or coarse interest grouping. Second, individual user analysis\nprovides limited insights due to the data sparsity issue. In this paper, we\npropose an LLM-driven dual-level multi-interest modeling framework for more\neffective recommendation. At the user-individual level, we exploit LLMs to\nflexibly allocate items engaged by users into different semantic clusters,\nindicating their diverse and distinct interests. To alleviate the agnostic\ngeneration of LLMs, we adaptively assign these semantic clusters to users'\ncollaborative multi-interests learned from global user-item interactions,\nallowing the granularity to be automatically adjusted according to the user's\nbehaviors using an alignment module. To alleviate the limited insights derived\nfrom individual users' behaviors, at the user-crowd level, we propose\naggregating user cliques into synthesized users with rich behaviors for more\ncomprehensive LLM-driven multi-interest analysis. We formulate a max covering\nproblem to ensure the compactness and representativeness of synthesized users'\nbehaviors, and then conduct contrastive learning based on their LLM-driven\nmulti-interests to disentangle item representations among different interests.\nExperiments on real-world datasets show the superiority of our approach against\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, much effort has been devoted to modeling users' multi-interests\nbased on their behaviors or auxiliary signals. However, existing methods often\nrely on heuristic assumptions, e.g., co-occurring items indicate the same\ninterest of users, failing to capture user multi-interests aligning with\nreal-world scenarios. While large language models (LLMs) show significant\npotential for multi-interest analysis due to their extensive knowledge and\npowerful reasoning capabilities, two key challenges remain. First, the\ngranularity of LLM-driven multi-interests is agnostic, possibly leading to\noverly fine or coarse interest grouping. Second, individual user analysis\nprovides limited insights due to the data sparsity issue. In this paper, we\npropose an LLM-driven dual-level multi-interest modeling framework for more\neffective recommendation. At the user-individual level, we exploit LLMs to\nflexibly allocate items engaged by users into different semantic clusters,\nindicating their diverse and distinct interests. To alleviate the agnostic\ngeneration of LLMs, we adaptively assign these semantic clusters to users'\ncollaborative multi-interests learned from global user-item interactions,\nallowing the granularity to be automatically adjusted according to the user's\nbehaviors using an alignment module. To alleviate the limited insights derived\nfrom individual users' behaviors, at the user-crowd level, we propose\naggregating user cliques into synthesized users with rich behaviors for more\ncomprehensive LLM-driven multi-interest analysis. We formulate a max covering\nproblem to ensure the compactness and representativeness of synthesized users'\nbehaviors, and then conduct contrastive learning based on their LLM-driven\nmulti-interests to disentangle item representations among different interests.\nExperiments on real-world datasets show the superiority of our approach against\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Ziyan Wang"
                    },
                    {
                        "name": "Yingpeng Du"
                    },
                    {
                        "name": "Zhu Sun"
                    },
                    {
                        "name": "Jieyi Bi"
                    },
                    {
                        "name": "Haoyan Chua"
                    },
                    {
                        "name": "Tianjun Wei"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12273v2",
                "updated": "2025-07-17T14:54:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    54,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T14:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    22,
                    0,
                    2,
                    197,
                    0
                ],
                "title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot"
                },
                "summary": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments."
                },
                "authors": [
                    {
                        "name": "Luca Garello"
                    },
                    {
                        "name": "Francesca Cocchella"
                    },
                    {
                        "name": "Alessandra Sciutti"
                    },
                    {
                        "name": "Manuel Catalano"
                    },
                    {
                        "name": "Francesco Rea"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Rea"
                },
                "author": "Francesco Rea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13175v1",
                "updated": "2025-07-17T14:39:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    39,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:39:29Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    39,
                    29,
                    3,
                    198,
                    0
                ],
                "title": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in\n  the LLM Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in\n  the LLM Era"
                },
                "summary": "The advancement of powerful yet opaque large language models (LLMs)\nnecessitates a fundamental revision of the philosophical criteria used to\nevaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the\nassumption of transparent architectures, which LLMs defy due to their\nstochastic outputs and opaque internal states. This paper argues that\ntraditional ethical criteria are pragmatically obsolete for LLMs due to this\nmismatch. Engaging with core themes in the philosophy of technology, this paper\nproffers a revised set of ten functional criteria to evaluate LLM-based\nartificial moral agents: moral concordance, context sensitivity, normative\nintegrity, metaethical awareness, system resilience, trustworthiness,\ncorrigibility, partial transparency, functional autonomy, and moral\nimagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating\nMoral Agency through Large Language Systems), aim to steer AMAs toward greater\nalignment and beneficial societal integration in the coming years. We\nillustrate these criteria using hypothetical scenarios involving an autonomous\npublic bus (APB) to demonstrate their practical applicability in morally\nsalient contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of powerful yet opaque large language models (LLMs)\nnecessitates a fundamental revision of the philosophical criteria used to\nevaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the\nassumption of transparent architectures, which LLMs defy due to their\nstochastic outputs and opaque internal states. This paper argues that\ntraditional ethical criteria are pragmatically obsolete for LLMs due to this\nmismatch. Engaging with core themes in the philosophy of technology, this paper\nproffers a revised set of ten functional criteria to evaluate LLM-based\nartificial moral agents: moral concordance, context sensitivity, normative\nintegrity, metaethical awareness, system resilience, trustworthiness,\ncorrigibility, partial transparency, functional autonomy, and moral\nimagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating\nMoral Agency through Large Language Systems), aim to steer AMAs toward greater\nalignment and beneficial societal integration in the coming years. We\nillustrate these criteria using hypothetical scenarios involving an autonomous\npublic bus (APB) to demonstrate their practical applicability in morally\nsalient contexts."
                },
                "authors": [
                    {
                        "name": "Matthew E. Brophy"
                    }
                ],
                "author_detail": {
                    "name": "Matthew E. Brophy"
                },
                "author": "Matthew E. Brophy",
                "arxiv_comment": "42 pages. Supplementary material included at end of article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T27, 03B42 68T27, 03B4268T27, 03B42 68T27, 03B42 68T27, 03B42\n  68T27, 03B42 68T27, 03B42 68T27, 03B4268T27, 03B42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.9; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13169v1",
                "updated": "2025-07-17T14:33:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    33,
                    36,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:33:36Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    33,
                    36,
                    3,
                    198,
                    0
                ],
                "title": "Prompt Injection 2.0: Hybrid AI Threats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Injection 2.0: Hybrid AI Threats"
                },
                "summary": "Prompt injection attacks, where malicious input is designed to manipulate AI\nsystems into ignoring their original instructions and following unauthorized\ncommands instead, were first discovered by Preamble, Inc. in May 2022 and\nresponsibly disclosed to OpenAI. Over the last three years, these attacks have\ncontinued to pose a critical security threat to LLM-integrated systems. The\nemergence of agentic AI systems, where LLMs autonomously perform multistep\ntasks through tools and coordination with other agents, has fundamentally\ntransformed the threat landscape. Modern prompt injection attacks can now\ncombine with traditional cybersecurity exploits to create hybrid threats that\nsystematically evade traditional security controls. This paper presents a\ncomprehensive analysis of Prompt Injection 2.0, examining how prompt injections\nintegrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF),\nand other web security vulnerabilities to bypass traditional security measures.\nWe build upon Preamble's foundational research and mitigation technologies,\nevaluating them against contemporary threats, including AI worms, multi-agent\ninfections, and hybrid cyber-AI attacks. Our analysis incorporates recent\nbenchmarks that demonstrate how traditional web application firewalls, XSS\nfilters, and CSRF tokens fail against AI-enhanced attacks. We also present\narchitectural solutions that combine prompt isolation, runtime security, and\nprivilege separation with novel threat detection capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt injection attacks, where malicious input is designed to manipulate AI\nsystems into ignoring their original instructions and following unauthorized\ncommands instead, were first discovered by Preamble, Inc. in May 2022 and\nresponsibly disclosed to OpenAI. Over the last three years, these attacks have\ncontinued to pose a critical security threat to LLM-integrated systems. The\nemergence of agentic AI systems, where LLMs autonomously perform multistep\ntasks through tools and coordination with other agents, has fundamentally\ntransformed the threat landscape. Modern prompt injection attacks can now\ncombine with traditional cybersecurity exploits to create hybrid threats that\nsystematically evade traditional security controls. This paper presents a\ncomprehensive analysis of Prompt Injection 2.0, examining how prompt injections\nintegrate with Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF),\nand other web security vulnerabilities to bypass traditional security measures.\nWe build upon Preamble's foundational research and mitigation technologies,\nevaluating them against contemporary threats, including AI worms, multi-agent\ninfections, and hybrid cyber-AI attacks. Our analysis incorporates recent\nbenchmarks that demonstrate how traditional web application firewalls, XSS\nfilters, and CSRF tokens fail against AI-enhanced attacks. We also present\narchitectural solutions that combine prompt isolation, runtime security, and\nprivilege separation with novel threat detection capabilities."
                },
                "authors": [
                    {
                        "name": "Jeremy McHugh"
                    },
                    {
                        "name": "Kristina ekrst"
                    },
                    {
                        "name": "Jon Cefalu"
                    }
                ],
                "author_detail": {
                    "name": "Jon Cefalu"
                },
                "author": "Jon Cefalu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13158v1",
                "updated": "2025-07-17T14:22:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    22,
                    24,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:22:24Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    22,
                    24,
                    3,
                    198,
                    0
                ],
                "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training:\n  Basics, Advances, and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Reinforcement Learning Meets Large Language Model Post-Training:\n  Basics, Advances, and Opportunities"
                },
                "summary": "In the era of Large Language Models (LLMs), alignment has emerged as a\nfundamental yet challenging problem in the pursuit of more reliable,\ncontrollable, and capable machine intelligence. The recent success of reasoning\nmodels and conversational AI systems has underscored the critical role of\nreinforcement learning (RL) in enhancing these systems, driving increased\nresearch interest at the intersection of RL and LLM alignment. This paper\nprovides a comprehensive review of recent advances in LLM alignment through the\nlens of inverse reinforcement learning (IRL), emphasizing the distinctions\nbetween RL techniques employed in LLM alignment and those in conventional RL\ntasks. In particular, we highlight the necessity of constructing neural reward\nmodels from human data and discuss the formal and practical implications of\nthis paradigm shift. We begin by introducing fundamental concepts in RL to\nprovide a foundation for readers unfamiliar with the field. We then examine\nrecent advances in this research agenda, discussing key challenges and\nopportunities in conducting IRL for LLM alignment. Beyond methodological\nconsiderations, we explore practical aspects, including datasets, benchmarks,\nevaluation metrics, infrastructure, and computationally efficient training and\ninference techniques. Finally, we draw insights from the literature on\nsparse-reward RL to identify open questions and potential research directions.\nBy synthesizing findings from diverse studies, we aim to provide a structured\nand critical overview of the field, highlight unresolved challenges, and\noutline promising future directions for improving LLM alignment through RL and\nIRL techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of Large Language Models (LLMs), alignment has emerged as a\nfundamental yet challenging problem in the pursuit of more reliable,\ncontrollable, and capable machine intelligence. The recent success of reasoning\nmodels and conversational AI systems has underscored the critical role of\nreinforcement learning (RL) in enhancing these systems, driving increased\nresearch interest at the intersection of RL and LLM alignment. This paper\nprovides a comprehensive review of recent advances in LLM alignment through the\nlens of inverse reinforcement learning (IRL), emphasizing the distinctions\nbetween RL techniques employed in LLM alignment and those in conventional RL\ntasks. In particular, we highlight the necessity of constructing neural reward\nmodels from human data and discuss the formal and practical implications of\nthis paradigm shift. We begin by introducing fundamental concepts in RL to\nprovide a foundation for readers unfamiliar with the field. We then examine\nrecent advances in this research agenda, discussing key challenges and\nopportunities in conducting IRL for LLM alignment. Beyond methodological\nconsiderations, we explore practical aspects, including datasets, benchmarks,\nevaluation metrics, infrastructure, and computationally efficient training and\ninference techniques. Finally, we draw insights from the literature on\nsparse-reward RL to identify open questions and potential research directions.\nBy synthesizing findings from diverse studies, we aim to provide a structured\nand critical overview of the field, highlight unresolved challenges, and\noutline promising future directions for improving LLM alignment through RL and\nIRL techniques."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13152v1",
                "updated": "2025-07-17T14:13:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    13,
                    50,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:13:50Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    13,
                    50,
                    3,
                    198,
                    0
                ],
                "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models"
                },
                "summary": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN."
                },
                "authors": [
                    {
                        "name": "Xiangyu Dong"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Jiang Gao"
                    },
                    {
                        "name": "Haozhou Li"
                    },
                    {
                        "name": "Xiaoguang Ma"
                    },
                    {
                        "name": "Yaoming Zhou"
                    },
                    {
                        "name": "Fuhai Chen"
                    },
                    {
                        "name": "Juan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Juan Liu"
                },
                "author": "Juan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19263v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19263v3",
                "updated": "2025-07-17T14:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    10,
                    6,
                    3,
                    198,
                    0
                ],
                "published": "2025-03-25T01:57:59Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    1,
                    57,
                    59,
                    1,
                    84,
                    0
                ],
                "title": "DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow\n  Generation & Instruct-Masking Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DWIM: Towards Tool-aware Visual Reasoning via Discrepancy-aware Workflow\n  Generation & Instruct-Masking Tuning"
                },
                "summary": "Visual reasoning (VR), which is crucial in many fields for enabling\nhuman-like visual understanding, remains highly challenging. Recently,\ncompositional visual reasoning approaches, which leverage the reasoning\nabilities of large language models (LLMs) with integrated tools to solve\nproblems, have shown promise as more effective strategies than end-to-end VR\nmethods. However, these approaches face limitations, as frozen LLMs lack tool\nawareness in VR, leading to performance bottlenecks. While leveraging LLMs for\nreasoning is widely used in other domains, they are not directly applicable to\nVR due to limited training data, imperfect tools that introduce errors and\nreduce data collection efficiency in VR, and challenging in fine-tuning on\nnoisy workflows. To address these challenges, we propose DWIM: i)\nDiscrepancy-aware training Workflow generation, which assesses tool usage and\nextracts more viable workflows for training; and ii) Instruct-Masking\nfine-tuning, which guides the model to only clone effective actions, enabling\nthe generation of more practical solutions. Our experiments demonstrate that\nDWIM achieves state-of-the-art performance across various VR tasks, exhibiting\nstrong generalization on multiple widely-used datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual reasoning (VR), which is crucial in many fields for enabling\nhuman-like visual understanding, remains highly challenging. Recently,\ncompositional visual reasoning approaches, which leverage the reasoning\nabilities of large language models (LLMs) with integrated tools to solve\nproblems, have shown promise as more effective strategies than end-to-end VR\nmethods. However, these approaches face limitations, as frozen LLMs lack tool\nawareness in VR, leading to performance bottlenecks. While leveraging LLMs for\nreasoning is widely used in other domains, they are not directly applicable to\nVR due to limited training data, imperfect tools that introduce errors and\nreduce data collection efficiency in VR, and challenging in fine-tuning on\nnoisy workflows. To address these challenges, we propose DWIM: i)\nDiscrepancy-aware training Workflow generation, which assesses tool usage and\nextracts more viable workflows for training; and ii) Instruct-Masking\nfine-tuning, which guides the model to only clone effective actions, enabling\nthe generation of more practical solutions. Our experiments demonstrate that\nDWIM achieves state-of-the-art performance across various VR tasks, exhibiting\nstrong generalization on multiple widely-used datasets."
                },
                "authors": [
                    {
                        "name": "Fucai Ke"
                    },
                    {
                        "name": "Vijay Kumar B G"
                    },
                    {
                        "name": "Xingjian Leng"
                    },
                    {
                        "name": "Zhixi Cai"
                    },
                    {
                        "name": "Zaid Khan"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Pari Delir Haghighi"
                    },
                    {
                        "name": "Hamid Rezatofighi"
                    },
                    {
                        "name": "Manmohan Chandraker"
                    }
                ],
                "author_detail": {
                    "name": "Manmohan Chandraker"
                },
                "author": "Manmohan Chandraker",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19263v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19263v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11059v2",
                "updated": "2025-07-17T14:04:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    4,
                    7,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-15T07:52:33Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    7,
                    52,
                    33,
                    1,
                    196,
                    0
                ],
                "title": "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language\n  Models on Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language\n  Models on Software Engineering Tasks"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025."
                },
                "authors": [
                    {
                        "name": "Pavel Adamenko"
                    },
                    {
                        "name": "Mikhail Ivanov"
                    },
                    {
                        "name": "Aidar Valeev"
                    },
                    {
                        "name": "Rodion Levichev"
                    },
                    {
                        "name": "Pavel Zadorozhny"
                    },
                    {
                        "name": "Ivan Lopatin"
                    },
                    {
                        "name": "Dmitry Babayev"
                    },
                    {
                        "name": "Alena Fenogenova"
                    },
                    {
                        "name": "Valentin Malykh"
                    }
                ],
                "author_detail": {
                    "name": "Valentin Malykh"
                },
                "author": "Valentin Malykh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13140v1",
                "updated": "2025-07-17T14:02:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    2,
                    40,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:02:40Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    2,
                    40,
                    3,
                    198,
                    0
                ],
                "title": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and\n  Intention-Driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and\n  Intention-Driven Agents"
                },
                "summary": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments."
                },
                "authors": [
                    {
                        "name": "Kuiyuan Ding"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Jianzhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jianzhang Guo"
                },
                "author": "Jianzhang Guo",
                "arxiv_comment": "6 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13138v1",
                "updated": "2025-07-17T14:00:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    0,
                    13,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T14:00:13Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    0,
                    13,
                    3,
                    198,
                    0
                ],
                "title": "Assessing the Reliability of LLMs Annotations in the Context of\n  Demographic Bias and Model Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Reliability of LLMs Annotations in the Context of\n  Demographic Bias and Model Explanation"
                },
                "summary": "Understanding the sources of variability in annotations is crucial for\ndeveloping fair NLP systems, especially for tasks like sexism detection where\ndemographic bias is a concern. This study investigates the extent to which\nannotator demographic features influence labeling decisions compared to text\ncontent. Using a Generalized Linear Mixed Model, we quantify this inf luence,\nfinding that while statistically present, demographic factors account for a\nminor fraction ( 8%) of the observed variance, with tweet content being the\ndominant factor. We then assess the reliability of Generative AI (GenAI) models\nas annotators, specifically evaluating if guiding them with demographic\npersonas improves alignment with human judgments. Our results indicate that\nsimplistic persona prompting often fails to enhance, and sometimes degrades,\nperformance compared to baseline models. Furthermore, explainable AI (XAI)\ntechniques reveal that model predictions rely heavily on content-specific\ntokens related to sexism, rather than correlates of demographic\ncharacteristics. We argue that focusing on content-driven explanations and\nrobust annotation protocols offers a more reliable path towards fairness than\npotentially persona simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the sources of variability in annotations is crucial for\ndeveloping fair NLP systems, especially for tasks like sexism detection where\ndemographic bias is a concern. This study investigates the extent to which\nannotator demographic features influence labeling decisions compared to text\ncontent. Using a Generalized Linear Mixed Model, we quantify this inf luence,\nfinding that while statistically present, demographic factors account for a\nminor fraction ( 8%) of the observed variance, with tweet content being the\ndominant factor. We then assess the reliability of Generative AI (GenAI) models\nas annotators, specifically evaluating if guiding them with demographic\npersonas improves alignment with human judgments. Our results indicate that\nsimplistic persona prompting often fails to enhance, and sometimes degrades,\nperformance compared to baseline models. Furthermore, explainable AI (XAI)\ntechniques reveal that model predictions rely heavily on content-specific\ntokens related to sexism, rather than correlates of demographic\ncharacteristics. We argue that focusing on content-driven explanations and\nrobust annotation protocols offers a more reliable path towards fairness than\npotentially persona simulation."
                },
                "authors": [
                    {
                        "name": "Hadi Mohammadi"
                    },
                    {
                        "name": "Tina Shahedi"
                    },
                    {
                        "name": "Pablo Mosteiro"
                    },
                    {
                        "name": "Massimo Poesio"
                    },
                    {
                        "name": "Ayoub Bagheri"
                    },
                    {
                        "name": "Anastasia Giachanou"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Giachanou"
                },
                "author": "Anastasia Giachanou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13123v1",
                "updated": "2025-07-17T13:38:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    38,
                    16,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T13:38:16Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    38,
                    16,
                    3,
                    198,
                    0
                ],
                "title": "Detecting LLM-generated Code with Subtle Modification by Adversarial\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting LLM-generated Code with Subtle Modification by Adversarial\n  Training"
                },
                "summary": "With the rapid development of Large Language Models (LLMs), their powerful\ncode-generation capabilities have been widely applied in tasks like code\ncompletion and automated development, demonstrating the value of improving\ncoding efficiency. However, the extensive use of LLM-generated code also raises\nseveral new challenges. On the one hand, issues such as the regulation of code\nprovenance, copyright disputes, and code quality have become increasingly\nconcerning. How to effectively detect LLM-generated code and ensure its\ncompliant and responsible use has become a critical and urgent issue. On the\nother hand, in practical applications, LLM-generated code is often subject to\nmanual modifications, such as variable renaming or structural adjustments.\nAlthough some recent studies have proposed training-based and zero-shot methods\nfor detecting LLM-generated code, these approaches show insufficient robustness\nwhen facing modified LLM-generated code, and there is a lack of an effective\nsolution. To address the real-world scenario where LLM-generated code may\nundergo minor modifications, we propose CodeGPTSensor+, an enhanced version of\nCodeGPTSensor, which employs adversarial training to improve robustness against\ninput perturbations. CodeGPTSensor+ integrates an adversarial sample generation\nmodule, Multi-objective Identifier and Structure Transformation (MIST), which\nsystematically generates both high-quality and representative adversarial\nsamples. This module effectively enhances the model's resistance against\ndiverse adversarial attacks. Experimental results on the HMCorp dataset\ndemonstrate that CodeGPTSensor+ significantly improves detection accuracy on\nthe adversarial test set while maintaining high accuracy on the original test\nset, showcasing superior robustness compared to CodeGPTSensor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of Large Language Models (LLMs), their powerful\ncode-generation capabilities have been widely applied in tasks like code\ncompletion and automated development, demonstrating the value of improving\ncoding efficiency. However, the extensive use of LLM-generated code also raises\nseveral new challenges. On the one hand, issues such as the regulation of code\nprovenance, copyright disputes, and code quality have become increasingly\nconcerning. How to effectively detect LLM-generated code and ensure its\ncompliant and responsible use has become a critical and urgent issue. On the\nother hand, in practical applications, LLM-generated code is often subject to\nmanual modifications, such as variable renaming or structural adjustments.\nAlthough some recent studies have proposed training-based and zero-shot methods\nfor detecting LLM-generated code, these approaches show insufficient robustness\nwhen facing modified LLM-generated code, and there is a lack of an effective\nsolution. To address the real-world scenario where LLM-generated code may\nundergo minor modifications, we propose CodeGPTSensor+, an enhanced version of\nCodeGPTSensor, which employs adversarial training to improve robustness against\ninput perturbations. CodeGPTSensor+ integrates an adversarial sample generation\nmodule, Multi-objective Identifier and Structure Transformation (MIST), which\nsystematically generates both high-quality and representative adversarial\nsamples. This module effectively enhances the model's resistance against\ndiverse adversarial attacks. Experimental results on the HMCorp dataset\ndemonstrate that CodeGPTSensor+ significantly improves detection accuracy on\nthe adversarial test set while maintaining high accuracy on the original test\nset, showcasing superior robustness compared to CodeGPTSensor."
                },
                "authors": [
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07389v2",
                "updated": "2025-07-17T13:24:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    24,
                    18,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-10T02:19:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    2,
                    19,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Task-Circuit Quantization: Leveraging Knowledge Localization and\n  Interpretability for Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Circuit Quantization: Leveraging Knowledge Localization and\n  Interpretability for Compression"
                },
                "summary": "Post-training quantization (PTQ) reduces a model's memory footprint by\nmapping full precision weights into low bit weights without costly retraining,\nbut can degrade its downstream performance especially in low 2- to 3-bit\nsettings. We develop a new mixed-precision PTQ approach, Task-Circuit\nQuantization (TaCQ), that draws parallels to automated circuit discovery,\ndirectly conditioning the quantization process on specific weight circuits --\nwhich we define as sets of weights associated with downstream task performance.\nThese weights are kept as 16-bit weights, while others are quantized,\nmaintaining performance while only adding a marginal memory cost. Specifically,\nTaCQ contrasts unquantized model weights with a uniformly-quantized model to\nestimate the expected change in weights due to quantization and uses gradient\ninformation to predict the resulting impact on task performance, allowing us to\npreserve task-specific weights. We compare TaCQ-based quantization to existing\nmixed-precision quantization methods when conditioning both on general-purpose\nand task-specific data. Across QA, math reasoning, and text-to-SQL tasks for\nboth Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the\nsame calibration data and a lower weight budget, achieving major improvements\nin the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of\nLlama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25%\nabsolute improvement over SPQR. We also observe consistently large gains over\nexisting methods in the 2-bit regime, with an average gain of 14.74% over the\nstrongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without\nconditioning on specific tasks, showing TaCQ's ability to identify important\nweights is not limited to task-conditioned settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) reduces a model's memory footprint by\nmapping full precision weights into low bit weights without costly retraining,\nbut can degrade its downstream performance especially in low 2- to 3-bit\nsettings. We develop a new mixed-precision PTQ approach, Task-Circuit\nQuantization (TaCQ), that draws parallels to automated circuit discovery,\ndirectly conditioning the quantization process on specific weight circuits --\nwhich we define as sets of weights associated with downstream task performance.\nThese weights are kept as 16-bit weights, while others are quantized,\nmaintaining performance while only adding a marginal memory cost. Specifically,\nTaCQ contrasts unquantized model weights with a uniformly-quantized model to\nestimate the expected change in weights due to quantization and uses gradient\ninformation to predict the resulting impact on task performance, allowing us to\npreserve task-specific weights. We compare TaCQ-based quantization to existing\nmixed-precision quantization methods when conditioning both on general-purpose\nand task-specific data. Across QA, math reasoning, and text-to-SQL tasks for\nboth Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the\nsame calibration data and a lower weight budget, achieving major improvements\nin the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of\nLlama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25%\nabsolute improvement over SPQR. We also observe consistently large gains over\nexisting methods in the 2-bit regime, with an average gain of 14.74% over the\nstrongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without\nconditioning on specific tasks, showing TaCQ's ability to identify important\nweights is not limited to task-conditioned settings."
                },
                "authors": [
                    {
                        "name": "Hanqi Xiao"
                    },
                    {
                        "name": "Yi-Lin Sung"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "COLM 2025 Camera Ready. Code:\n  https://github.com/The-Inscrutable-X/TACQ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13105v1",
                "updated": "2025-07-17T13:19:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    19,
                    50,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T13:19:50Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    19,
                    50,
                    3,
                    198,
                    0
                ],
                "title": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated\n  Summaries For Scientific Abstracts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated\n  Summaries For Scientific Abstracts"
                },
                "summary": "We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach."
                },
                "authors": [
                    {
                        "name": "Marc Brinner"
                    },
                    {
                        "name": "Sina Zarriess"
                    }
                ],
                "author_detail": {
                    "name": "Sina Zarriess"
                },
                "author": "Sina Zarriess",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04037v2",
                "updated": "2025-07-17T13:02:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    2,
                    5,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-05T13:31:21Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    31,
                    21,
                    5,
                    186,
                    0
                ],
                "title": "Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in\n  Dynamic Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in\n  Dynamic Environments"
                },
                "summary": "The gap between static benchmarks and the dynamic nature of real-world legal\npractice poses a key barrier to advancing legal intelligence. To this end, we\nintroduce J1-ENVS, the first interactive and dynamic legal environment tailored\nfor LLM-based agents. Guided by legal experts, it comprises six representative\nscenarios from Chinese legal practices across three levels of environmental\ncomplexity. We further introduce J1-EVAL, a fine-grained evaluation framework,\ndesigned to assess both task performance and procedural compliance across\nvarying levels of legal proficiency. Extensive experiments on 17 LLM agents\nreveal that, while many models demonstrate solid legal knowledge, they struggle\nwith procedural execution in dynamic settings. Even the SOTA model, GPT-4o,\nfalls short of 60% overall performance. These findings highlight persistent\nchallenges in achieving dynamic legal intelligence and offer valuable insights\nto guide future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gap between static benchmarks and the dynamic nature of real-world legal\npractice poses a key barrier to advancing legal intelligence. To this end, we\nintroduce J1-ENVS, the first interactive and dynamic legal environment tailored\nfor LLM-based agents. Guided by legal experts, it comprises six representative\nscenarios from Chinese legal practices across three levels of environmental\ncomplexity. We further introduce J1-EVAL, a fine-grained evaluation framework,\ndesigned to assess both task performance and procedural compliance across\nvarying levels of legal proficiency. Extensive experiments on 17 LLM agents\nreveal that, while many models demonstrate solid legal knowledge, they struggle\nwith procedural execution in dynamic settings. Even the SOTA model, GPT-4o,\nfalls short of 60% overall performance. These findings highlight persistent\nchallenges in achieving dynamic legal intelligence and offer valuable insights\nto guide future research."
                },
                "authors": [
                    {
                        "name": "Zheng Jia"
                    },
                    {
                        "name": "Shengbin Yue"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Yidong Liu"
                    },
                    {
                        "name": "Yun Song"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12284v2",
                "updated": "2025-07-17T12:55:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    55,
                    32,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T14:31:33Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    31,
                    33,
                    2,
                    197,
                    0
                ],
                "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks"
                },
                "summary": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures."
                },
                "authors": [
                    {
                        "name": "Artem Chervyakov"
                    },
                    {
                        "name": "Alexander Kharitonov"
                    },
                    {
                        "name": "Pavel Zadorozhny"
                    },
                    {
                        "name": "Adamenko Pavel"
                    },
                    {
                        "name": "Rodion Levichev"
                    },
                    {
                        "name": "Dmitrii Vorobev"
                    },
                    {
                        "name": "Dmitrii Salikhov"
                    },
                    {
                        "name": "Aidar Valeev"
                    },
                    {
                        "name": "Alena Pestova"
                    },
                    {
                        "name": "Maria Dziuba"
                    },
                    {
                        "name": "Ilseyar Alimova"
                    },
                    {
                        "name": "Artem Zavgorodnev"
                    },
                    {
                        "name": "Aleksandr Medvedev"
                    },
                    {
                        "name": "Stanislav Moiseev"
                    },
                    {
                        "name": "Elena Bruches"
                    },
                    {
                        "name": "Daniil Grebenkin"
                    },
                    {
                        "name": "Roman Derunets"
                    },
                    {
                        "name": "Vikulov Vladimir"
                    },
                    {
                        "name": "Anton Emelyanov"
                    },
                    {
                        "name": "Dmitrii Babaev"
                    },
                    {
                        "name": "Vladimir V. Ivanov"
                    },
                    {
                        "name": "Valentin Malykh"
                    },
                    {
                        "name": "Alena Fenogenova"
                    }
                ],
                "author_detail": {
                    "name": "Alena Fenogenova"
                },
                "author": "Alena Fenogenova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13080v1",
                "updated": "2025-07-17T12:50:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    50,
                    45,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T12:50:45Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    50,
                    45,
                    3,
                    198,
                    0
                ],
                "title": "Unmodulated Visible Light Positioning: A Deep Dive into Techniques,\n  Studies, and Future Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmodulated Visible Light Positioning: A Deep Dive into Techniques,\n  Studies, and Future Prospects"
                },
                "summary": "Visible Light Positioning (VLP) has emerged as a promising technology for\nnext-generation indoor positioning systems (IPS), particularly within the scope\nof sixth-generation (6G) wireless networks. Its attractiveness stems from\nleveraging existing lighting infrastructures equipped with light-emitting\ndiodes (LEDs), enabling cost-efficient deployments and achieving high-precision\npositioning accuracy in the centimeter-todecimeter range. However, widespread\nadoption of traditional VLP solutions faces significant barriers due to the\nincreased costs and operational complexity associated with modulating LEDs,\nwhich consequently reduces illumination efficiency by lowering their radiant\nflux. To address these limitations, recent research has introduced the concept\nof unmodulated Visible Light Positioning (uVLP), which exploits Light Signals\nof Opportunity (LSOOP) emitted by unmodulated illumination sources such as\nconventional LEDs. This paradigm offers a cost-effective, lowinfrastructure\nalternative for indoor positioning by eliminating the need for modulation\nhardware and maintaining lighting efficiency. This paper delineates the\nfundamental principles of uVLP, provides a comparative analysis of uVLP versus\nconventional VLP methods, and classifies existing uVLP techniques according to\nreceiver technologies into intensity-based methods (e.g., photodiodes, solar\ncells, etc.) and imaging-based methods. Additionally, we propose a\ncomprehensive taxonomy categorizing techniques into demultiplexed and\nundemultiplexed approaches. Within this structured framework, we critically\nreview current advancements in uVLP, discuss prevailing challenges, and outline\npromising research directions essential for developing robust, scalable, and\nwidely deployable uVLP solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visible Light Positioning (VLP) has emerged as a promising technology for\nnext-generation indoor positioning systems (IPS), particularly within the scope\nof sixth-generation (6G) wireless networks. Its attractiveness stems from\nleveraging existing lighting infrastructures equipped with light-emitting\ndiodes (LEDs), enabling cost-efficient deployments and achieving high-precision\npositioning accuracy in the centimeter-todecimeter range. However, widespread\nadoption of traditional VLP solutions faces significant barriers due to the\nincreased costs and operational complexity associated with modulating LEDs,\nwhich consequently reduces illumination efficiency by lowering their radiant\nflux. To address these limitations, recent research has introduced the concept\nof unmodulated Visible Light Positioning (uVLP), which exploits Light Signals\nof Opportunity (LSOOP) emitted by unmodulated illumination sources such as\nconventional LEDs. This paradigm offers a cost-effective, lowinfrastructure\nalternative for indoor positioning by eliminating the need for modulation\nhardware and maintaining lighting efficiency. This paper delineates the\nfundamental principles of uVLP, provides a comparative analysis of uVLP versus\nconventional VLP methods, and classifies existing uVLP techniques according to\nreceiver technologies into intensity-based methods (e.g., photodiodes, solar\ncells, etc.) and imaging-based methods. Additionally, we propose a\ncomprehensive taxonomy categorizing techniques into demultiplexed and\nundemultiplexed approaches. Within this structured framework, we critically\nreview current advancements in uVLP, discuss prevailing challenges, and outline\npromising research directions essential for developing robust, scalable, and\nwidely deployable uVLP solutions."
                },
                "authors": [
                    {
                        "name": "Morteza Alijani"
                    },
                    {
                        "name": "Wout Joseph"
                    },
                    {
                        "name": "David Plets"
                    }
                ],
                "author_detail": {
                    "name": "David Plets"
                },
                "author": "David Plets",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13073v1",
                "updated": "2025-07-17T12:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    42,
                    28,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T12:42:28Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    42,
                    28,
                    3,
                    198,
                    0
                ],
                "title": "Dual LiDAR-Based Traffic Movement Count Estimation at a Signalized\n  Intersection: Deployment, Data Collection, and Preliminary Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual LiDAR-Based Traffic Movement Count Estimation at a Signalized\n  Intersection: Deployment, Data Collection, and Preliminary Analysis"
                },
                "summary": "Traffic Movement Count (TMC) at intersections is crucial for optimizing\nsignal timings, assessing the performance of existing traffic control measures,\nand proposing efficient lane configurations to minimize delays, reduce\ncongestion, and promote safety. Traditionally, methods such as manual counting,\nloop detectors, pneumatic road tubes, and camera-based recognition have been\nused for TMC estimation. Although generally reliable, camera-based TMC\nestimation is prone to inaccuracies under poor lighting conditions during harsh\nweather and nighttime. In contrast, Light Detection and Ranging (LiDAR)\ntechnology is gaining popularity in recent times due to reduced costs and its\nexpanding use in 3D object detection, tracking, and related applications. This\npaper presents the authors' endeavor to develop, deploy and evaluate a\ndual-LiDAR system at an intersection in the city of Rialto, California, for TMC\nestimation. The 3D bounding box detections from the two LiDARs are used to\nclassify vehicle counts based on traffic directions, vehicle movements, and\nvehicle classes. This work discusses the estimated TMC results and provides\ninsights into the observed trends and irregularities. Potential improvements\nare also discussed that could enhance not only TMC estimation, but also\ntrajectory forecasting and intent prediction at intersections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic Movement Count (TMC) at intersections is crucial for optimizing\nsignal timings, assessing the performance of existing traffic control measures,\nand proposing efficient lane configurations to minimize delays, reduce\ncongestion, and promote safety. Traditionally, methods such as manual counting,\nloop detectors, pneumatic road tubes, and camera-based recognition have been\nused for TMC estimation. Although generally reliable, camera-based TMC\nestimation is prone to inaccuracies under poor lighting conditions during harsh\nweather and nighttime. In contrast, Light Detection and Ranging (LiDAR)\ntechnology is gaining popularity in recent times due to reduced costs and its\nexpanding use in 3D object detection, tracking, and related applications. This\npaper presents the authors' endeavor to develop, deploy and evaluate a\ndual-LiDAR system at an intersection in the city of Rialto, California, for TMC\nestimation. The 3D bounding box detections from the two LiDARs are used to\nclassify vehicle counts based on traffic directions, vehicle movements, and\nvehicle classes. This work discusses the estimated TMC results and provides\ninsights into the observed trends and irregularities. Potential improvements\nare also discussed that could enhance not only TMC estimation, but also\ntrajectory forecasting and intent prediction at intersections."
                },
                "authors": [
                    {
                        "name": "Saswat Priyadarshi Nayak"
                    },
                    {
                        "name": "Guoyuan Wu"
                    },
                    {
                        "name": "Kanok Boriboonsomsin"
                    },
                    {
                        "name": "Matthew Barth"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Barth"
                },
                "author": "Matthew Barth",
                "arxiv_comment": "7 Pages, 8 Figures. This paper has been accepted for publication at\n  the 2025 IEEE ITSC. Copyright IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02419v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02419v3",
                "updated": "2025-07-17T12:34:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    34,
                    14,
                    3,
                    198,
                    0
                ],
                "published": "2023-12-05T01:35:39Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    1,
                    35,
                    39,
                    1,
                    339,
                    0
                ],
                "title": "Human Demonstrations are Generalizable Knowledge for Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Demonstrations are Generalizable Knowledge for Robots"
                },
                "summary": "Learning from human demonstrations is an emerging trend for designing\nintelligent robotic systems. However, previous methods typically regard videos\nas instructions, simply dividing them into action sequences for robotic\nrepetition, which poses obstacles to generalization to diverse tasks or object\ninstances. In this paper, we propose a different perspective, considering human\ndemonstration videos not as mere instructions, but as a source of knowledge for\nrobots. Motivated by this perspective and the remarkable comprehension and\ngeneralization capabilities exhibited by large language models (LLMs), we\npropose DigKnow, a method that DIstills Generalizable KNOWledge with a\nhierarchical structure. Specifically, DigKnow begins by converting human\ndemonstration video frames into observation knowledge. This knowledge is then\nsubjected to analysis to extract human action knowledge and further distilled\ninto pattern knowledge compassing task and object instances, resulting in the\nacquisition of generalizable knowledge with a hierarchical structure. In\nsettings with different tasks or object instances, DigKnow retrieves relevant\nknowledge for the current task and object instances. Subsequently, the\nLLM-based planner conducts planning based on the retrieved knowledge, and the\npolicy executes actions in line with the plan to achieve the designated task.\nUtilizing the retrieved knowledge, we validate and rectify planning and\nexecution outcomes, resulting in a substantial enhancement of the success rate.\nExperimental results across a range of tasks and scenes demonstrate the\neffectiveness of this approach in facilitating real-world robots to accomplish\ntasks with the knowledge derived from human demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from human demonstrations is an emerging trend for designing\nintelligent robotic systems. However, previous methods typically regard videos\nas instructions, simply dividing them into action sequences for robotic\nrepetition, which poses obstacles to generalization to diverse tasks or object\ninstances. In this paper, we propose a different perspective, considering human\ndemonstration videos not as mere instructions, but as a source of knowledge for\nrobots. Motivated by this perspective and the remarkable comprehension and\ngeneralization capabilities exhibited by large language models (LLMs), we\npropose DigKnow, a method that DIstills Generalizable KNOWledge with a\nhierarchical structure. Specifically, DigKnow begins by converting human\ndemonstration video frames into observation knowledge. This knowledge is then\nsubjected to analysis to extract human action knowledge and further distilled\ninto pattern knowledge compassing task and object instances, resulting in the\nacquisition of generalizable knowledge with a hierarchical structure. In\nsettings with different tasks or object instances, DigKnow retrieves relevant\nknowledge for the current task and object instances. Subsequently, the\nLLM-based planner conducts planning based on the retrieved knowledge, and the\npolicy executes actions in line with the plan to achieve the designated task.\nUtilizing the retrieved knowledge, we validate and rectify planning and\nexecution outcomes, resulting in a substantial enhancement of the success rate.\nExperimental results across a range of tasks and scenes demonstrate the\neffectiveness of this approach in facilitating real-world robots to accomplish\ntasks with the knowledge derived from human demonstrations."
                },
                "authors": [
                    {
                        "name": "Te Cui"
                    },
                    {
                        "name": "Tianxing Zhou"
                    },
                    {
                        "name": "Zicai Peng"
                    },
                    {
                        "name": "Mengxiao Hu"
                    },
                    {
                        "name": "Haoyang Lu"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Guangyan Chen"
                    },
                    {
                        "name": "Meiling Wang"
                    },
                    {
                        "name": "Yufeng Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yufeng Yue"
                },
                "author": "Yufeng Yue",
                "arxiv_comment": "accepted for publication in lEEE/RSJ international Conference on\n  Intelligent Robots and Systems (lROS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02419v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02419v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06796v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06796v3",
                "updated": "2025-07-17T12:33:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    33,
                    32,
                    3,
                    198,
                    0
                ],
                "published": "2024-11-11T08:50:24Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    50,
                    24,
                    0,
                    316,
                    0
                ],
                "title": "Write Your Own CodeChecker: An Automated Test-Driven Checker Development\n  Approach with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write Your Own CodeChecker: An Automated Test-Driven Checker Development\n  Approach with LLMs"
                },
                "summary": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic and the complex API usage of large-scale\nchecker frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we propose AutoChecker, an innovative LLM-powered approach that can\nwrite code checkers automatically based on only a rule description and a test\nsuite. To achieve comprehensive checking logic, AutoChecker incrementally\nupdates the checker's logic by focusing on solving one selected case each time.\nTo obtain precise API knowledge, during each iteration, it leverages\nfine-grained logic-guided API-context retrieval, where it first decomposes the\nchecking logic into a series of sub-operations and then retrieves\nchecker-related API-contexts for each sub-operation. For evaluation, we apply\nAutoChecker, five baselines, and three ablation methods using multiple LLMs to\ngenerate checkers for 20 randomly selected PMD rules. Experimental results show\nthat AutoChecker significantly outperforms others across all effectiveness\nmetrics, with an average test pass rate of 82.28%. Additionally, the checkers\ngenerated by AutoChecker can be successfully applied to real-world projects,\nmatching the performance of official checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic and the complex API usage of large-scale\nchecker frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we propose AutoChecker, an innovative LLM-powered approach that can\nwrite code checkers automatically based on only a rule description and a test\nsuite. To achieve comprehensive checking logic, AutoChecker incrementally\nupdates the checker's logic by focusing on solving one selected case each time.\nTo obtain precise API knowledge, during each iteration, it leverages\nfine-grained logic-guided API-context retrieval, where it first decomposes the\nchecking logic into a series of sub-operations and then retrieves\nchecker-related API-contexts for each sub-operation. For evaluation, we apply\nAutoChecker, five baselines, and three ablation methods using multiple LLMs to\ngenerate checkers for 20 randomly selected PMD rules. Experimental results show\nthat AutoChecker significantly outperforms others across all effectiveness\nmetrics, with an average test pass rate of 82.28%. Additionally, the checkers\ngenerated by AutoChecker can be successfully applied to real-world projects,\nmatching the performance of official checkers."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Yuanyuan Xie"
                    },
                    {
                        "name": "Jiwei Yan"
                    },
                    {
                        "name": "Jinhao Huang"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "arxiv_comment": "update metadata and artifact url",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06796v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06796v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13052v1",
                "updated": "2025-07-17T12:25:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    25,
                    1,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T12:25:01Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    25,
                    1,
                    3,
                    198,
                    0
                ],
                "title": "Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient\n  Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient\n  Communication"
                },
                "summary": "The advancement and maturity of large language models (LLMs) and robotics\nhave unlocked vast potential for human-computer interaction, particularly in\nthe field of robotic ultrasound. While existing research primarily focuses on\neither patient-robot or physician-robot interaction, the role of an intelligent\nvirtual sonographer (IVS) bridging physician-robot-patient communication\nremains underexplored. This work introduces a conversational virtual agent in\nExtended Reality (XR) that facilitates real-time interaction between\nphysicians, a robotic ultrasound system(RUS), and patients. The IVS agent\ncommunicates with physicians in a professional manner while offering empathetic\nexplanations and reassurance to patients. Furthermore, it actively controls the\nRUS by executing physician commands and transparently relays these actions to\nthe patient. By integrating LLM-powered dialogue with speech-to-text,\ntext-to-speech, and robotic control, our system enhances the efficiency,\nclarity, and accessibility of robotic ultrasound acquisition. This work\nconstitutes a first step toward understanding how IVS can bridge communication\ngaps in physician-robot-patient interaction, providing more control and\ntherefore trust into physician-robot interaction while improving patient\nexperience and acceptance of robotic ultrasound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement and maturity of large language models (LLMs) and robotics\nhave unlocked vast potential for human-computer interaction, particularly in\nthe field of robotic ultrasound. While existing research primarily focuses on\neither patient-robot or physician-robot interaction, the role of an intelligent\nvirtual sonographer (IVS) bridging physician-robot-patient communication\nremains underexplored. This work introduces a conversational virtual agent in\nExtended Reality (XR) that facilitates real-time interaction between\nphysicians, a robotic ultrasound system(RUS), and patients. The IVS agent\ncommunicates with physicians in a professional manner while offering empathetic\nexplanations and reassurance to patients. Furthermore, it actively controls the\nRUS by executing physician commands and transparently relays these actions to\nthe patient. By integrating LLM-powered dialogue with speech-to-text,\ntext-to-speech, and robotic control, our system enhances the efficiency,\nclarity, and accessibility of robotic ultrasound acquisition. This work\nconstitutes a first step toward understanding how IVS can bridge communication\ngaps in physician-robot-patient interaction, providing more control and\ntherefore trust into physician-robot interaction while improving patient\nexperience and acceptance of robotic ultrasound."
                },
                "authors": [
                    {
                        "name": "Tianyu Song"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Yuan Bi"
                    },
                    {
                        "name": "Angelos Karlas"
                    },
                    {
                        "name": "Amir Yousefi"
                    },
                    {
                        "name": "Daniela Branzan"
                    },
                    {
                        "name": "Zhongliang Jiang"
                    },
                    {
                        "name": "Ulrich Eck"
                    },
                    {
                        "name": "Nassir Navab"
                    }
                ],
                "author_detail": {
                    "name": "Nassir Navab"
                },
                "author": "Nassir Navab",
                "arxiv_comment": "Accepted at MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13042v1",
                "updated": "2025-07-17T12:15:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    15,
                    9,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T12:15:09Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    15,
                    9,
                    3,
                    198,
                    0
                ],
                "title": "Backscattering-Based Security in Wireless Power Transfer Applied to\n  Battery-Free BLE Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backscattering-Based Security in Wireless Power Transfer Applied to\n  Battery-Free BLE Sensors"
                },
                "summary": "The integration of security and energy efficiency in Internet of Things\nsystems remains a critical challenge, particularly for battery-free and\nresource-constrained devices. This paper explores the scalability and\nprotocol-agnostic nature of a backscattering-based security mechanism by\nintegrating it into Bluetooth Low Energy battery-free Wireless Sensor Network.\nThe proposed approach leverages the Wireless Power Transfer link, traditionally\nused for energy harvesting, to generate additional identification signals\nwithout increasing energy consumption or computational demands. Experimental\nvalidation demonstrates the solution's functionality using compact, low-gain\nantenna, ensuring compatibility with size-constrained applications such as\nStructural Health Monitoring and smart transport. Furthermore, this work\naddresses the challenges associated with backscattering dynamic range and\nmulti-node Wireless Sensor Network scenarios, discussing potential collisions\nbetween identification signals and proposing future improvements to enhance\ngeneralizability and scalability. The findings underscore the potential of the\nbackscattering-based security mechanism for creating secure, sustainable, and\nscalable IoT deployments across diverse protocols and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of security and energy efficiency in Internet of Things\nsystems remains a critical challenge, particularly for battery-free and\nresource-constrained devices. This paper explores the scalability and\nprotocol-agnostic nature of a backscattering-based security mechanism by\nintegrating it into Bluetooth Low Energy battery-free Wireless Sensor Network.\nThe proposed approach leverages the Wireless Power Transfer link, traditionally\nused for energy harvesting, to generate additional identification signals\nwithout increasing energy consumption or computational demands. Experimental\nvalidation demonstrates the solution's functionality using compact, low-gain\nantenna, ensuring compatibility with size-constrained applications such as\nStructural Health Monitoring and smart transport. Furthermore, this work\naddresses the challenges associated with backscattering dynamic range and\nmulti-node Wireless Sensor Network scenarios, discussing potential collisions\nbetween identification signals and proposing future improvements to enhance\ngeneralizability and scalability. The findings underscore the potential of the\nbackscattering-based security mechanism for creating secure, sustainable, and\nscalable IoT deployments across diverse protocols and applications."
                },
                "authors": [
                    {
                        "name": "Taki Eddine Djidjekh"
                    },
                    {
                        "name": "Gal Loubet"
                    },
                    {
                        "name": "Alexandru Takacs"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Takacs"
                },
                "arxiv_affiliation": "LAAS-MINC, UT",
                "author": "Alexandru Takacs",
                "arxiv_journal_ref": "2025 IEEE Wireless Power Technology Conference and Expo (WPTCE),\n  IEEE, Jun 2025, Rome, Italy. pp.1-4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13038v1",
                "updated": "2025-07-17T12:09:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    9,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T12:09:39Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    9,
                    39,
                    3,
                    198,
                    0
                ],
                "title": "MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent\n  Debate Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAD-Spear: A Conformity-Driven Prompt Injection Attack on Multi-Agent\n  Debate Systems"
                },
                "summary": "Multi-agent debate (MAD) systems leverage collaborative interactions among\nlarge language models (LLMs) agents to improve reasoning capabilities. While\nrecent studies have focused on increasing the accuracy and scalability of MAD\nsystems, their security vulnerabilities have received limited attention. In\nthis work, we introduce MAD-Spear, a targeted prompt injection attack that\ncompromises a small subset of agents but significantly disrupts the overall MAD\nprocess. Manipulated agents produce multiple plausible yet incorrect responses,\nexploiting LLMs' conformity tendencies to propagate misinformation and degrade\nconsensus quality. Furthermore, the attack can be composed with other\nstrategies, such as communication attacks, to further amplify its impact by\nincreasing the exposure of agents to incorrect responses. To assess MAD's\nresilience under attack, we propose a formal definition of MAD fault-tolerance\nand develop a comprehensive evaluation framework that jointly considers\naccuracy, consensus efficiency, and scalability. Extensive experiments on five\nbenchmark datasets with varying difficulty levels demonstrate that MAD-Spear\nconsistently outperforms the baseline attack in degrading system performance.\nAdditionally, we observe that agent diversity substantially improves MAD\nperformance in mathematical reasoning tasks, which challenges prior work\nsuggesting that agent diversity has minimal impact on performance. These\nfindings highlight the urgent need to improve the security in MAD design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent debate (MAD) systems leverage collaborative interactions among\nlarge language models (LLMs) agents to improve reasoning capabilities. While\nrecent studies have focused on increasing the accuracy and scalability of MAD\nsystems, their security vulnerabilities have received limited attention. In\nthis work, we introduce MAD-Spear, a targeted prompt injection attack that\ncompromises a small subset of agents but significantly disrupts the overall MAD\nprocess. Manipulated agents produce multiple plausible yet incorrect responses,\nexploiting LLMs' conformity tendencies to propagate misinformation and degrade\nconsensus quality. Furthermore, the attack can be composed with other\nstrategies, such as communication attacks, to further amplify its impact by\nincreasing the exposure of agents to incorrect responses. To assess MAD's\nresilience under attack, we propose a formal definition of MAD fault-tolerance\nand develop a comprehensive evaluation framework that jointly considers\naccuracy, consensus efficiency, and scalability. Extensive experiments on five\nbenchmark datasets with varying difficulty levels demonstrate that MAD-Spear\nconsistently outperforms the baseline attack in degrading system performance.\nAdditionally, we observe that agent diversity substantially improves MAD\nperformance in mathematical reasoning tasks, which challenges prior work\nsuggesting that agent diversity has minimal impact on performance. These\nfindings highlight the urgent need to improve the security in MAD design."
                },
                "authors": [
                    {
                        "name": "Yu Cui"
                    },
                    {
                        "name": "Hongyang Du"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Du"
                },
                "author": "Hongyang Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13028v1",
                "updated": "2025-07-17T11:57:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    57,
                    11,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T11:57:11Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    57,
                    11,
                    3,
                    198,
                    0
                ],
                "title": "From Paranoia to Compliance: The Bumpy Road of System Hardening\n  Practices on Stack Exchange",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Paranoia to Compliance: The Bumpy Road of System Hardening\n  Practices on Stack Exchange"
                },
                "summary": "Hardening computer systems against cyberattacks is crucial for security.\nHowever, past incidents illustrated, that many system operators struggle with\neffective system hardening. Hence, many computer systems and applications\nremain insecure. So far, the research community lacks an in-depth understanding\nof system operators motivation, practices, and challenges around system\nhardening. With a focus on practices and challenges, we qualitatively analyzed\n316 Stack Exchange (SE) posts related to system hardening. We find that access\ncontrol and deployment-related issues are the most challenging, and system\noperators suffer from misconceptions and unrealistic expectations. Most\nfrequently, posts focused on operating systems and server applications. System\noperators were driven by the fear of their systems getting attacked or by\ncompliance reasons. Finally, we discuss our research questions, make\nrecommendations for future system hardening, and illustrate the implications of\nour work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardening computer systems against cyberattacks is crucial for security.\nHowever, past incidents illustrated, that many system operators struggle with\neffective system hardening. Hence, many computer systems and applications\nremain insecure. So far, the research community lacks an in-depth understanding\nof system operators motivation, practices, and challenges around system\nhardening. With a focus on practices and challenges, we qualitatively analyzed\n316 Stack Exchange (SE) posts related to system hardening. We find that access\ncontrol and deployment-related issues are the most challenging, and system\noperators suffer from misconceptions and unrealistic expectations. Most\nfrequently, posts focused on operating systems and server applications. System\noperators were driven by the fear of their systems getting attacked or by\ncompliance reasons. Finally, we discuss our research questions, make\nrecommendations for future system hardening, and illustrate the implications of\nour work."
                },
                "authors": [
                    {
                        "name": "Niklas Busch"
                    },
                    {
                        "name": "Philip Klostermeyer"
                    },
                    {
                        "name": "Jan H. Klemmer"
                    },
                    {
                        "name": "Yasemin Acar"
                    },
                    {
                        "name": "Sascha Fahl"
                    }
                ],
                "author_detail": {
                    "name": "Sascha Fahl"
                },
                "arxiv_affiliation": "CISPA Helmholtz Center for Information Security",
                "author": "Sascha Fahl",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13019v1",
                "updated": "2025-07-17T11:46:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    46,
                    0,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T11:46:00Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    11,
                    46,
                    0,
                    3,
                    198,
                    0
                ],
                "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A\n  Holistic Study of Physical and Visual Disparities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A\n  Holistic Study of Physical and Visual Disparities"
                },
                "summary": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but\ntheir idealized assumptions about robot movement and control fail to reflect\nphysically embodied deployment challenges. To bridge this gap, we introduce\nVLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and\nwheeled robots. For the first time, we systematically evaluate several\nego-centric VLN methods in physical robotic settings across different technical\npipelines, including classification models for single-step discrete action\nprediction, a diffusion model for dense waypoint prediction, and a train-free,\nmap-based large language model (LLM) integrated with path planning. Our results\nreveal significant performance degradation due to limited robot observation\nspace, environmental lighting variations, and physical challenges like\ncollisions and falls. This also exposes locomotion constraints for legged\nrobots in complex environments. VLN-PE is highly extensible, allowing seamless\nintegration of new scenes beyond MP3D, thereby enabling more comprehensive VLN\nevaluation. Despite the weak generalization of current models in physical\ndeployment, VLN-PE provides a new pathway for improving cross-embodiment's\noverall adaptability. We hope our findings and tools inspire the community to\nrethink VLN limitations and advance robust, practical VLN models. The code is\navailable at https://crystalsixone.github.io/vln_pe.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but\ntheir idealized assumptions about robot movement and control fail to reflect\nphysically embodied deployment challenges. To bridge this gap, we introduce\nVLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and\nwheeled robots. For the first time, we systematically evaluate several\nego-centric VLN methods in physical robotic settings across different technical\npipelines, including classification models for single-step discrete action\nprediction, a diffusion model for dense waypoint prediction, and a train-free,\nmap-based large language model (LLM) integrated with path planning. Our results\nreveal significant performance degradation due to limited robot observation\nspace, environmental lighting variations, and physical challenges like\ncollisions and falls. This also exposes locomotion constraints for legged\nrobots in complex environments. VLN-PE is highly extensible, allowing seamless\nintegration of new scenes beyond MP3D, thereby enabling more comprehensive VLN\nevaluation. Despite the weak generalization of current models in physical\ndeployment, VLN-PE provides a new pathway for improving cross-embodiment's\noverall adaptability. We hope our findings and tools inspire the community to\nrethink VLN limitations and advance robust, practical VLN models. The code is\navailable at https://crystalsixone.github.io/vln_pe.github.io/."
                },
                "authors": [
                    {
                        "name": "Liuyi Wang"
                    },
                    {
                        "name": "Xinyuan Xia"
                    },
                    {
                        "name": "Hui Zhao"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Chengju Liu"
                    },
                    {
                        "name": "Qijun Chen"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12990v1",
                "updated": "2025-07-17T10:57:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    57,
                    49,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T10:57:49Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    57,
                    49,
                    3,
                    198,
                    0
                ],
                "title": "Teach Old SAEs New Domain Tricks with Boosting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teach Old SAEs New Domain Tricks with Boosting"
                },
                "summary": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs."
                },
                "authors": [
                    {
                        "name": "Nikita Koriagin"
                    },
                    {
                        "name": "Yaroslav Aksenov"
                    },
                    {
                        "name": "Daniil Laptev"
                    },
                    {
                        "name": "Gleb Gerasimov"
                    },
                    {
                        "name": "Nikita Balagansky"
                    },
                    {
                        "name": "Daniil Gavrilov"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Gavrilov"
                },
                "author": "Daniil Gavrilov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12988v1",
                "updated": "2025-07-17T10:54:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    54,
                    17,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T10:54:17Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    54,
                    17,
                    3,
                    198,
                    0
                ],
                "title": "Variance-Based Pruning for Accelerating and Compressing Trained Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variance-Based Pruning for Accelerating and Compressing Trained Networks"
                },
                "summary": "Increasingly expensive training of ever larger models such as Vision\nTransfomers motivate reusing the vast library of already trained\nstate-of-the-art networks. However, their latency, high computational costs and\nmemory demands pose significant challenges for deployment, especially on\nresource-constrained hardware. While structured pruning methods can reduce\nthese factors, they often require costly retraining, sometimes for up to\nhundreds of epochs, or even training from scratch to recover the lost accuracy\nresulting from the structural modifications. Maintaining the provided\nperformance of trained models after structured pruning and thereby avoiding\nextensive retraining remains a challenge. To solve this, we introduce\nVariance-Based Pruning, a simple and structured one-shot pruning technique for\nefficiently compressing networks, with minimal finetuning. Our approach first\ngathers activation statistics, which are used to select neurons for pruning.\nSimultaneously the mean activations are integrated back into the model to\npreserve a high degree of performance. On ImageNet-1k recognition tasks, we\ndemonstrate that directly after pruning DeiT-Base retains over 70% of its\noriginal performance and requires only 10 epochs of fine-tuning to regain 99%\nof the original accuracy while simultaneously reducing MACs by 35% and model\nsize by 36%, thus speeding up the model by 1.44x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasingly expensive training of ever larger models such as Vision\nTransfomers motivate reusing the vast library of already trained\nstate-of-the-art networks. However, their latency, high computational costs and\nmemory demands pose significant challenges for deployment, especially on\nresource-constrained hardware. While structured pruning methods can reduce\nthese factors, they often require costly retraining, sometimes for up to\nhundreds of epochs, or even training from scratch to recover the lost accuracy\nresulting from the structural modifications. Maintaining the provided\nperformance of trained models after structured pruning and thereby avoiding\nextensive retraining remains a challenge. To solve this, we introduce\nVariance-Based Pruning, a simple and structured one-shot pruning technique for\nefficiently compressing networks, with minimal finetuning. Our approach first\ngathers activation statistics, which are used to select neurons for pruning.\nSimultaneously the mean activations are integrated back into the model to\npreserve a high degree of performance. On ImageNet-1k recognition tasks, we\ndemonstrate that directly after pruning DeiT-Base retains over 70% of its\noriginal performance and requires only 10 epochs of fine-tuning to regain 99%\nof the original accuracy while simultaneously reducing MACs by 35% and model\nsize by 36%, thus speeding up the model by 1.44x."
                },
                "authors": [
                    {
                        "name": "Uranik Berisha"
                    },
                    {
                        "name": "Jens Mehnert"
                    },
                    {
                        "name": "Alexandru Paul Condurache"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paul Condurache"
                },
                "author": "Alexandru Paul Condurache",
                "arxiv_comment": "Accepted at IEEE/CVF International Conference on Computer Vision\n  (ICCV) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12981v1",
                "updated": "2025-07-17T10:33:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    33,
                    36,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T10:33:36Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    33,
                    36,
                    3,
                    198,
                    0
                ],
                "title": "MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with\n  Multiple Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with\n  Multiple Steps"
                },
                "summary": "This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas\ny Respuestas sobre Tablas en Espa\\~nol (Questions and Answers about Tables in\nSpanish). Our solution obtains answers to the questions by implementing Python\ncode generation with LLMs that is used to filter and process the table. This\nsolution evolves from the MRT implementation for the Semeval 2025 related task.\nThe process consists of multiple steps: analyzing and understanding the content\nof the table, selecting the useful columns, generating instructions in natural\nlanguage, translating these instructions to code, running it, and handling\npotential errors or exceptions. These steps use open-source LLMs and\nfine-grained optimized prompts for each step. With this approach, we achieved\nan accuracy score of 85\\% in the task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas\ny Respuestas sobre Tablas en Espa\\~nol (Questions and Answers about Tables in\nSpanish). Our solution obtains answers to the questions by implementing Python\ncode generation with LLMs that is used to filter and process the table. This\nsolution evolves from the MRT implementation for the Semeval 2025 related task.\nThe process consists of multiple steps: analyzing and understanding the content\nof the table, selecting the useful columns, generating instructions in natural\nlanguage, translating these instructions to code, running it, and handling\npotential errors or exceptions. These steps use open-source LLMs and\nfine-grained optimized prompts for each step. With this approach, we achieved\nan accuracy score of 85\\% in the task."
                },
                "authors": [
                    {
                        "name": "Maximiliano Hormazbal Lagos"
                    },
                    {
                        "name": "lvaro Bueno Sez"
                    },
                    {
                        "name": "Hctor Cerezo-Costas"
                    },
                    {
                        "name": "Pedro Alonso Doval"
                    },
                    {
                        "name": "Jorge Alcalde Vesteiro"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Alcalde Vesteiro"
                },
                "author": "Jorge Alcalde Vesteiro",
                "arxiv_comment": "Accepted as an official challenge paper in the PRESTA: Questions and\n  Answers over Tabular Data shared task at IberLEF 2025, colocated with the\n  41st SEPLN Conference in Zaragoza, Spain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20770v2",
                "updated": "2025-07-17T10:10:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    10,
                    10,
                    45,
                    3,
                    198,
                    0
                ],
                "published": "2025-05-27T06:21:56Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    21,
                    56,
                    1,
                    147,
                    0
                ],
                "title": "Can Large Language Models Predict Audio Effects Parameters from Natural\n  Language?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Predict Audio Effects Parameters from Natural\n  Language?"
                },
                "summary": "In music production, manipulating audio effects (Fx) parameters through\nnatural language has the potential to reduce technical barriers for\nnon-experts. We present LLM2Fx, a framework leveraging Large Language Models\n(LLMs) to predict Fx parameters directly from textual descriptions without\nrequiring task-specific training or fine-tuning. Our approach address the\ntext-to-effect parameter prediction (Text2Fx) task by mapping natural language\ndescriptions to the corresponding Fx parameters for equalization and\nreverberation. We demonstrate that LLMs can generate Fx parameters in a\nzero-shot manner that elucidates the relationship between timbre semantics and\naudio effects in music production. To enhance performance, we introduce three\ntypes of in-context examples: audio Digital Signal Processing (DSP) features,\nDSP function code, and few-shot examples. Our results demonstrate that\nLLM-based Fx parameter generation outperforms previous optimization approaches,\noffering competitive performance in translating natural language descriptions\nto appropriate Fx settings. Furthermore, LLMs can serve as text-driven\ninterfaces for audio production, paving the way for more intuitive and\naccessible music production tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In music production, manipulating audio effects (Fx) parameters through\nnatural language has the potential to reduce technical barriers for\nnon-experts. We present LLM2Fx, a framework leveraging Large Language Models\n(LLMs) to predict Fx parameters directly from textual descriptions without\nrequiring task-specific training or fine-tuning. Our approach address the\ntext-to-effect parameter prediction (Text2Fx) task by mapping natural language\ndescriptions to the corresponding Fx parameters for equalization and\nreverberation. We demonstrate that LLMs can generate Fx parameters in a\nzero-shot manner that elucidates the relationship between timbre semantics and\naudio effects in music production. To enhance performance, we introduce three\ntypes of in-context examples: audio Digital Signal Processing (DSP) features,\nDSP function code, and few-shot examples. Our results demonstrate that\nLLM-based Fx parameter generation outperforms previous optimization approaches,\noffering competitive performance in translating natural language descriptions\nto appropriate Fx settings. Furthermore, LLMs can serve as text-driven\ninterfaces for audio production, paving the way for more intuitive and\naccessible music production tools."
                },
                "authors": [
                    {
                        "name": "Seungheon Doh"
                    },
                    {
                        "name": "Junghyun Koo"
                    },
                    {
                        "name": "Marco A. Martnez-Ramrez"
                    },
                    {
                        "name": "Wei-Hsiang Liao"
                    },
                    {
                        "name": "Juhan Nam"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "Accepted for publication at The IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12948v1",
                "updated": "2025-07-17T09:40:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    40,
                    56,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T09:40:56Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    40,
                    56,
                    3,
                    198,
                    0
                ],
                "title": "Probabilistic Soundness Guarantees in LLM Reasoning Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Soundness Guarantees in LLM Reasoning Chains"
                },
                "summary": "In reasoning chains generated by large language models (LLMs), initial errors\noften propagate and undermine the reliability of the final conclusion. Current\nLLM-based error detection methods often fail to detect propagated errors\nbecause they do not properly account for how earlier errors might corrupt\njudgments of downstream reasoning. To better detect such propagated errors, we\nintroduce Autoregressive Reasoning Entailment Stability (ARES), a novel\nprobabilistic framework that prevents error propagation by judging each claim\nbased only on previously-assessed sound premises. This inductive method yields\na nuanced score for each step and provides certified statistical guarantees of\nits soundness, rather than a brittle binary label. ARES achieves\nstate-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2\npoints) and demonstrates superior robustness on very long synthetic reasoning\nchains, where it excels at detecting propagated errors (90.3% F1, +27.6\npoints).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In reasoning chains generated by large language models (LLMs), initial errors\noften propagate and undermine the reliability of the final conclusion. Current\nLLM-based error detection methods often fail to detect propagated errors\nbecause they do not properly account for how earlier errors might corrupt\njudgments of downstream reasoning. To better detect such propagated errors, we\nintroduce Autoregressive Reasoning Entailment Stability (ARES), a novel\nprobabilistic framework that prevents error propagation by judging each claim\nbased only on previously-assessed sound premises. This inductive method yields\na nuanced score for each step and provides certified statistical guarantees of\nits soundness, rather than a brittle binary label. ARES achieves\nstate-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2\npoints) and demonstrates superior robustness on very long synthetic reasoning\nchains, where it excels at detecting propagated errors (90.3% F1, +27.6\npoints)."
                },
                "authors": [
                    {
                        "name": "Weiqiu You"
                    },
                    {
                        "name": "Anton Xue"
                    },
                    {
                        "name": "Shreya Havaldar"
                    },
                    {
                        "name": "Delip Rao"
                    },
                    {
                        "name": "Helen Jin"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08161v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08161v4",
                "updated": "2025-07-17T09:34:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    34,
                    49,
                    3,
                    198,
                    0
                ],
                "published": "2025-03-11T08:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    26,
                    37,
                    1,
                    70,
                    0
                ],
                "title": "OASIS: Order-Augmented Strategy for Improved Code Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Order-Augmented Strategy for Improved Code Search"
                },
                "summary": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training."
                },
                "authors": [
                    {
                        "name": "Zuchen Gao"
                    },
                    {
                        "name": "Zizheng Zhan"
                    },
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Erxin Yu"
                    },
                    {
                        "name": "Ziqi Zhan"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08161v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08161v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12933v1",
                "updated": "2025-07-17T09:15:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    15,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T09:15:29Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    15,
                    29,
                    3,
                    198,
                    0
                ],
                "title": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training\n  Quantization"
                },
                "summary": "Diffusion models have achieved remarkable success in image generation but\ncome with significant computational costs, posing challenges for deployment in\nresource-constrained environments. Recent post-training quantization (PTQ)\nmethods have attempted to mitigate this issue by focusing on the iterative\nnature of diffusion models. However, these approaches often overlook outliers,\nleading to degraded performance at low bit-widths. In this paper, we propose a\nDMQ which combines Learned Equivalent Scaling (LES) and channel-wise\nPower-of-Two Scaling (PTS) to effectively address these challenges. Learned\nEquivalent Scaling optimizes channel-wise scaling factors to redistribute\nquantization difficulty between weights and activations, reducing overall\nquantization error. Recognizing that early denoising steps, despite having\nsmall quantization errors, crucially impact the final output due to error\naccumulation, we incorporate an adaptive timestep weighting scheme to\nprioritize these critical steps during learning. Furthermore, identifying that\nlayers such as skip connections exhibit high inter-channel variance, we\nintroduce channel-wise Power-of-Two Scaling for activations. To ensure robust\nselection of PTS factors even with small calibration set, we introduce a voting\nalgorithm that enhances reliability. Extensive experiments demonstrate that our\nmethod significantly outperforms existing works, especially at low bit-widths\nsuch as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image\ngeneration quality and model stability. The code is available at\nhttps://github.com/LeeDongYeun/dmq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in image generation but\ncome with significant computational costs, posing challenges for deployment in\nresource-constrained environments. Recent post-training quantization (PTQ)\nmethods have attempted to mitigate this issue by focusing on the iterative\nnature of diffusion models. However, these approaches often overlook outliers,\nleading to degraded performance at low bit-widths. In this paper, we propose a\nDMQ which combines Learned Equivalent Scaling (LES) and channel-wise\nPower-of-Two Scaling (PTS) to effectively address these challenges. Learned\nEquivalent Scaling optimizes channel-wise scaling factors to redistribute\nquantization difficulty between weights and activations, reducing overall\nquantization error. Recognizing that early denoising steps, despite having\nsmall quantization errors, crucially impact the final output due to error\naccumulation, we incorporate an adaptive timestep weighting scheme to\nprioritize these critical steps during learning. Furthermore, identifying that\nlayers such as skip connections exhibit high inter-channel variance, we\nintroduce channel-wise Power-of-Two Scaling for activations. To ensure robust\nselection of PTS factors even with small calibration set, we introduce a voting\nalgorithm that enhances reliability. Extensive experiments demonstrate that our\nmethod significantly outperforms existing works, especially at low bit-widths\nsuch as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image\ngeneration quality and model stability. The code is available at\nhttps://github.com/LeeDongYeun/dmq."
                },
                "authors": [
                    {
                        "name": "Dongyeun Lee"
                    },
                    {
                        "name": "Jiwan Hur"
                    },
                    {
                        "name": "Hyounguk Shon"
                    },
                    {
                        "name": "Jae Young Lee"
                    },
                    {
                        "name": "Junmo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Junmo Kim"
                },
                "author": "Junmo Kim",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12916v1",
                "updated": "2025-07-17T09:02:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    2,
                    4,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T09:02:04Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    2,
                    4,
                    3,
                    198,
                    0
                ],
                "title": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding\n  With Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding\n  With Large Language Models"
                },
                "summary": "Advancements in foundation models have made it possible to conduct\napplications in various downstream tasks. Especially, the new era has witnessed\na remarkable capability to extend Large Language Models (LLMs) for tackling\ntasks of 3D scene understanding. Current methods rely heavily on 3D point\nclouds, but the 3D point cloud reconstruction of an indoor scene often results\nin information loss. Some textureless planes or repetitive patterns are prone\nto omission and manifest as voids within the reconstructed 3D point clouds.\nBesides, objects with complex structures tend to introduce distortion of\ndetails caused by misalignments between the captured images and the dense\nreconstructed point clouds. 2D multi-view images present visual consistency\nwith 3D point clouds and provide more detailed representations of scene\ncomponents, which can naturally compensate for these deficiencies. Based on\nthese insights, we propose Argus, a novel 3D multimodal framework that\nleverages multi-view images for enhanced 3D scene understanding with LLMs. In\ngeneral, Argus can be treated as a 3D Large Multimodal Foundation Model\n(3D-LMM) since it takes various modalities as input(text instructions, 2D\nmulti-view images, and 3D point clouds) and expands the capability of LLMs to\ntackle 3D tasks. Argus involves fusing and integrating multi-view images and\ncamera poses into view-as-scene features, which interact with the 3D features\nto create comprehensive and detailed 3D-aware scene embeddings. Our approach\ncompensates for the information loss while reconstructing 3D point clouds and\nhelps LLMs better understand the 3D world. Extensive experiments demonstrate\nthat our method outperforms existing 3D-LMMs in various downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in foundation models have made it possible to conduct\napplications in various downstream tasks. Especially, the new era has witnessed\na remarkable capability to extend Large Language Models (LLMs) for tackling\ntasks of 3D scene understanding. Current methods rely heavily on 3D point\nclouds, but the 3D point cloud reconstruction of an indoor scene often results\nin information loss. Some textureless planes or repetitive patterns are prone\nto omission and manifest as voids within the reconstructed 3D point clouds.\nBesides, objects with complex structures tend to introduce distortion of\ndetails caused by misalignments between the captured images and the dense\nreconstructed point clouds. 2D multi-view images present visual consistency\nwith 3D point clouds and provide more detailed representations of scene\ncomponents, which can naturally compensate for these deficiencies. Based on\nthese insights, we propose Argus, a novel 3D multimodal framework that\nleverages multi-view images for enhanced 3D scene understanding with LLMs. In\ngeneral, Argus can be treated as a 3D Large Multimodal Foundation Model\n(3D-LMM) since it takes various modalities as input(text instructions, 2D\nmulti-view images, and 3D point clouds) and expands the capability of LLMs to\ntackle 3D tasks. Argus involves fusing and integrating multi-view images and\ncamera poses into view-as-scene features, which interact with the 3D features\nto create comprehensive and detailed 3D-aware scene embeddings. Our approach\ncompensates for the information loss while reconstructing 3D point clouds and\nhelps LLMs better understand the 3D world. Extensive experiments demonstrate\nthat our method outperforms existing 3D-LMMs in various downstream tasks."
                },
                "authors": [
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Xiaoyan Wang"
                    },
                    {
                        "name": "Ruifei Ma"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Zeju Li"
                    },
                    {
                        "name": "Xiangde Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangde Liu"
                },
                "author": "Xiangde Liu",
                "arxiv_doi": "10.1109/TNNLS.2025.3581411",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNNLS.2025.3581411",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.12916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by TNNLS2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15841v2",
                "updated": "2025-07-17T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    53,
                    48,
                    3,
                    198,
                    0
                ],
                "published": "2025-06-18T19:44:46Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    19,
                    44,
                    46,
                    2,
                    169,
                    0
                ],
                "title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient\n  Long-Horizon Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEM1: Learning to Synergize Memory and Reasoning for Efficient\n  Long-Horizon Agents"
                },
                "summary": "Modern language agents must operate over long-horizon, multi-turn\ninteractions, where they retrieve external information, adapt to observations,\nand answer interdependent queries. Yet, most LLM systems rely on full-context\nprompting, appending all past turns regardless of their relevance. This leads\nto unbounded memory growth, increased computational costs, and degraded\nreasoning performance on out-of-distribution input lengths. We introduce MEM1,\nan end-to-end reinforcement learning framework that enables agents to operate\nwith constant memory across long multi-turn tasks. At each turn, MEM1 updates a\ncompact shared internal state that jointly supports memory consolidation and\nreasoning. This state integrates prior memory with new observations from the\nenvironment while strategically discarding irrelevant or redundant information.\nTo support training in more realistic and compositional settings, we propose a\nsimple yet effective and scalable approach to constructing multi-turn\nenvironments by composing existing datasets into arbitrarily complex task\nsequences. Experiments across three domains, including internal retrieval QA,\nopen-domain web QA, and multi-turn web shopping, show that MEM1-7B improves\nperformance by 3.5x while reducing memory usage by 3.7x compared to\nQwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes\nbeyond the training horizon. Our results demonstrate the promise of\nreasoning-driven memory consolidation as a scalable alternative to existing\nsolutions for training long-horizon interactive agents, where both efficiency\nand performance are optimized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern language agents must operate over long-horizon, multi-turn\ninteractions, where they retrieve external information, adapt to observations,\nand answer interdependent queries. Yet, most LLM systems rely on full-context\nprompting, appending all past turns regardless of their relevance. This leads\nto unbounded memory growth, increased computational costs, and degraded\nreasoning performance on out-of-distribution input lengths. We introduce MEM1,\nan end-to-end reinforcement learning framework that enables agents to operate\nwith constant memory across long multi-turn tasks. At each turn, MEM1 updates a\ncompact shared internal state that jointly supports memory consolidation and\nreasoning. This state integrates prior memory with new observations from the\nenvironment while strategically discarding irrelevant or redundant information.\nTo support training in more realistic and compositional settings, we propose a\nsimple yet effective and scalable approach to constructing multi-turn\nenvironments by composing existing datasets into arbitrarily complex task\nsequences. Experiments across three domains, including internal retrieval QA,\nopen-domain web QA, and multi-turn web shopping, show that MEM1-7B improves\nperformance by 3.5x while reducing memory usage by 3.7x compared to\nQwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes\nbeyond the training horizon. Our results demonstrate the promise of\nreasoning-driven memory consolidation as a scalable alternative to existing\nsolutions for training long-horizon interactive agents, where both efficiency\nand performance are optimized."
                },
                "authors": [
                    {
                        "name": "Zijian Zhou"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Zhaoxuan Wu"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Alok Prakash"
                    },
                    {
                        "name": "Daniela Rus"
                    },
                    {
                        "name": "Jinhua Zhao"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Paul Pu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Paul Pu Liang"
                },
                "author": "Paul Pu Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13886v3",
                "updated": "2025-07-17T08:46:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    46,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-05-20T03:47:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    47,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General\n  Reasoning"
                },
                "summary": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic."
                },
                "authors": [
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Jixin Tang"
                    },
                    {
                        "name": "Hangcheng Li"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Yanbo Wen"
                    },
                    {
                        "name": "Fan Song"
                    },
                    {
                        "name": "Jiahao Zhan"
                    },
                    {
                        "name": "Yuyang Lu"
                    },
                    {
                        "name": "Chaoran Tao"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Jizhou Yu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "63 pages, 23 figures, submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12901v1",
                "updated": "2025-07-17T08:40:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    40,
                    45,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T08:40:45Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    40,
                    45,
                    3,
                    198,
                    0
                ],
                "title": "Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic\n  Chain-of-Thought Synthesis Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic\n  Chain-of-Thought Synthesis Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable general reasoning capabilities, holding significant potential for\napplications in the financial domain, a field that requires robust and reliable\nreasoning. It has been demonstrated that distilling high-quality\nchain-of-thought (CoT) rationales from advanced general reasoning models offers\na promising and efficient path to the financial reasoning model. However,\nexisting CoT synthesis methods suffer from shallow CoT sampling, leaving the\nquestion of how to construct a well-designed knowledge space for finance\nreasoning unexplored. In this paper, we present\n\\textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset\ncharacterized by its systematic CoT synthesis optimization. We first introduce\na comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge\nExtraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and\ndeep financial reasoning trajectories. Furthermore, a systematic investigation,\ntermed CoT Cube, is conducted to analyze critical factors that influence CoT\neffectiveness, such as necessity, length and synthesizer, yielding valuable\ninsights for high-quality financial CoT construction. Experiments demonstrate\nthat models trained on our Agentar-DeepFinance-300K achieve significant\nimprovements on financial benchmarks. We publicly release\nAgentar-DeepFinance-300K , hoping to advance the research in financial\nreasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable general reasoning capabilities, holding significant potential for\napplications in the financial domain, a field that requires robust and reliable\nreasoning. It has been demonstrated that distilling high-quality\nchain-of-thought (CoT) rationales from advanced general reasoning models offers\na promising and efficient path to the financial reasoning model. However,\nexisting CoT synthesis methods suffer from shallow CoT sampling, leaving the\nquestion of how to construct a well-designed knowledge space for finance\nreasoning unexplored. In this paper, we present\n\\textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset\ncharacterized by its systematic CoT synthesis optimization. We first introduce\na comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge\nExtraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and\ndeep financial reasoning trajectories. Furthermore, a systematic investigation,\ntermed CoT Cube, is conducted to analyze critical factors that influence CoT\neffectiveness, such as necessity, length and synthesizer, yielding valuable\ninsights for high-quality financial CoT construction. Experiments demonstrate\nthat models trained on our Agentar-DeepFinance-300K achieve significant\nimprovements on financial benchmarks. We publicly release\nAgentar-DeepFinance-300K , hoping to advance the research in financial\nreasoning models."
                },
                "authors": [
                    {
                        "name": "Xiaoke Zhao"
                    },
                    {
                        "name": "Zhaowen Zhou"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Lihong Wang"
                    },
                    {
                        "name": "Zhiyi Huang"
                    },
                    {
                        "name": "Kaiyuan Zheng"
                    },
                    {
                        "name": "Yanjun Zheng"
                    },
                    {
                        "name": "Xiyang Du"
                    },
                    {
                        "name": "Longfei Liao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Xiang Qi"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06208v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06208v3",
                "updated": "2025-07-17T08:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    39,
                    11,
                    3,
                    198,
                    0
                ],
                "published": "2024-11-09T15:12:43Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    15,
                    12,
                    43,
                    5,
                    314,
                    0
                ],
                "title": "IOPO: Empowering LLMs with Complex Instruction Following via\n  Input-Output Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IOPO: Empowering LLMs with Complex Instruction Following via\n  Input-Output Preference Optimization"
                },
                "summary": "In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively."
                },
                "authors": [
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Cheng Fu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06208v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06208v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04631v2",
                "updated": "2025-07-17T08:20:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    20,
                    16,
                    3,
                    198,
                    0
                ],
                "published": "2024-04-06T13:38:15Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    13,
                    38,
                    15,
                    5,
                    97,
                    0
                ],
                "title": "On the Limitations of Large Language Models (LLMs): False Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limitations of Large Language Models (LLMs): False Attribution"
                },
                "summary": "In this work, we introduce a new hallucination metric - Simple Hallucination\nIndex (SHI) and provide insight into one important limitation of the parametric\nknowledge of large language models (LLMs), i.e. false attribution. The task of\nautomatic author attribution for relatively small chunks of text is an\nimportant NLP task but can be challenging. We empirically evaluate the power of\n3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and\nLLaMA-2-13B). We acquired the top 10 most popular books of a month, according\nto Project Gutenberg, divided each one into equal chunks of 400 words, and\nprompted each LLM to predict the author. We then randomly sampled 162 chunks\nper book for human evaluation, based on the error margin of 7% and a confidence\nlevel of 95%. The average results show that Mixtral 8x7B has the highest\nprediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724,\n0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B.\nHowever, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as\nhigh as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong\nnegative correlation of accuracy and SHI, given by r, demonstrates the fidelity\nof the new hallucination metric, which may generalize to other tasks. We also\nshow that prediction accuracies correlate positively with the frequencies of\nWikipedia instances of the book titles instead of the downloads and we perform\nerror analyses of predictions. We publicly release the annotated chunks of data\nand our codes to aid the reproducibility and evaluation of other models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce a new hallucination metric - Simple Hallucination\nIndex (SHI) and provide insight into one important limitation of the parametric\nknowledge of large language models (LLMs), i.e. false attribution. The task of\nautomatic author attribution for relatively small chunks of text is an\nimportant NLP task but can be challenging. We empirically evaluate the power of\n3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and\nLLaMA-2-13B). We acquired the top 10 most popular books of a month, according\nto Project Gutenberg, divided each one into equal chunks of 400 words, and\nprompted each LLM to predict the author. We then randomly sampled 162 chunks\nper book for human evaluation, based on the error margin of 7% and a confidence\nlevel of 95%. The average results show that Mixtral 8x7B has the highest\nprediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724,\n0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B.\nHowever, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as\nhigh as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong\nnegative correlation of accuracy and SHI, given by r, demonstrates the fidelity\nof the new hallucination metric, which may generalize to other tasks. We also\nshow that prediction accuracies correlate positively with the frequencies of\nWikipedia instances of the book titles instead of the downloads and we perform\nerror analyses of predictions. We publicly release the annotated chunks of data\nand our codes to aid the reproducibility and evaluation of other models."
                },
                "authors": [
                    {
                        "name": "Tosin Adewumi"
                    },
                    {
                        "name": "Nudrat Habib"
                    },
                    {
                        "name": "Lama Alkhaled"
                    },
                    {
                        "name": "Elisa Barney"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Barney"
                },
                "author": "Elisa Barney",
                "arxiv_comment": "This paper was accepted for presentation by Recent Advances in NLP\n  (RANLP) 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12889v1",
                "updated": "2025-07-17T08:17:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    17,
                    35,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T08:17:35Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    17,
                    35,
                    3,
                    198,
                    0
                ],
                "title": "Camera-based implicit mind reading by capturing higher-order semantic\n  dynamics of human gaze within environmental context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera-based implicit mind reading by capturing higher-order semantic\n  dynamics of human gaze within environmental context"
                },
                "summary": "Emotion recognition,as a step toward mind reading,seeks to infer internal\nstates from external cues.Most existing methods rely on explicit signals-such\nas facial expressions,speech,or gestures-that reflect only bodily responses and\noverlook the influence of environmental context.These cues are often\nvoluntary,easy to mask,and insufficient for capturing deeper,implicit emotions.\nPhysiological signal-based approaches offer more direct access to internal\nstates but require complex sensors that compromise natural behavior and limit\nscalability.Gaze-based methods typically rely on static fixation analysis and\nfail to capture the rich,dynamic interactions between gaze and the\nenvironment,and thus cannot uncover the deep connection between emotion and\nimplicit behavior.To address these limitations,we propose a novel\ncamera-based,user-unaware emotion recognition approach that integrates gaze\nfixation patterns with environmental semantics and temporal dynamics.Leveraging\nstandard HD cameras,our method unobtrusively captures users'eye appearance and\nhead movements in natural settings-without the need for specialized hardware or\nactive user participation.From these visual cues,the system estimates gaze\ntrajectories over time and space, providing the basis for modeling the spatial,\nsemantic,and temporal dimensions of gaze behavior. This allows us to capture\nthe dynamic interplay between visual attention and the surrounding\nenvironment,revealing that emotions are not merely physiological responses but\ncomplex outcomes of human-environment interactions.The proposed approach\nenables user-unaware,real-time,and continuous emotion recognition,offering high\ngeneralizability and low deployment cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion recognition,as a step toward mind reading,seeks to infer internal\nstates from external cues.Most existing methods rely on explicit signals-such\nas facial expressions,speech,or gestures-that reflect only bodily responses and\noverlook the influence of environmental context.These cues are often\nvoluntary,easy to mask,and insufficient for capturing deeper,implicit emotions.\nPhysiological signal-based approaches offer more direct access to internal\nstates but require complex sensors that compromise natural behavior and limit\nscalability.Gaze-based methods typically rely on static fixation analysis and\nfail to capture the rich,dynamic interactions between gaze and the\nenvironment,and thus cannot uncover the deep connection between emotion and\nimplicit behavior.To address these limitations,we propose a novel\ncamera-based,user-unaware emotion recognition approach that integrates gaze\nfixation patterns with environmental semantics and temporal dynamics.Leveraging\nstandard HD cameras,our method unobtrusively captures users'eye appearance and\nhead movements in natural settings-without the need for specialized hardware or\nactive user participation.From these visual cues,the system estimates gaze\ntrajectories over time and space, providing the basis for modeling the spatial,\nsemantic,and temporal dimensions of gaze behavior. This allows us to capture\nthe dynamic interplay between visual attention and the surrounding\nenvironment,revealing that emotions are not merely physiological responses but\ncomplex outcomes of human-environment interactions.The proposed approach\nenables user-unaware,real-time,and continuous emotion recognition,offering high\ngeneralizability and low deployment cost."
                },
                "authors": [
                    {
                        "name": "Mengke Song"
                    },
                    {
                        "name": "Yuge Xie"
                    },
                    {
                        "name": "Qi Cui"
                    },
                    {
                        "name": "Luming Li"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Guotao Wang"
                    },
                    {
                        "name": "Chenglizhao Chen"
                    },
                    {
                        "name": "Shanchen Pang"
                    }
                ],
                "author_detail": {
                    "name": "Shanchen Pang"
                },
                "author": "Shanchen Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.16054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.16054v2",
                "updated": "2025-07-17T08:13:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    13,
                    7,
                    3,
                    198,
                    0
                ],
                "published": "2023-12-26T13:54:00Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    13,
                    54,
                    0,
                    1,
                    360,
                    0
                ],
                "title": "A Logically Consistent Chain-of-Thought Approach for Stance Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Logically Consistent Chain-of-Thought Approach for Stance Detection"
                },
                "summary": "Zero-shot stance detection (ZSSD) aims to detect stances toward unseen\ntargets. Incorporating background knowledge to enhance transferability between\nseen and unseen targets constitutes the primary approach of ZSSD. However,\nthese methods often struggle with a knowledge-task disconnect and lack logical\nconsistency in their predictions. To address these issues, we introduce a novel\napproach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which\nimproves stance detection by ensuring relevant and logically sound knowledge\nextraction. LC-CoT employs a three-step process. Initially, it assesses whether\nsupplementary external knowledge is necessary. Subsequently, it uses API calls\nto retrieve this knowledge, which can be processed by a separate LLM. Finally,\na manual exemplar guides the LLM to infer stance categories, using an if-then\nlogical structure to maintain relevance and logical coherence. This structured\napproach to eliciting background knowledge enhances the model's capability,\noutperforming traditional supervised methods without relying on labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot stance detection (ZSSD) aims to detect stances toward unseen\ntargets. Incorporating background knowledge to enhance transferability between\nseen and unseen targets constitutes the primary approach of ZSSD. However,\nthese methods often struggle with a knowledge-task disconnect and lack logical\nconsistency in their predictions. To address these issues, we introduce a novel\napproach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which\nimproves stance detection by ensuring relevant and logically sound knowledge\nextraction. LC-CoT employs a three-step process. Initially, it assesses whether\nsupplementary external knowledge is necessary. Subsequently, it uses API calls\nto retrieve this knowledge, which can be processed by a separate LLM. Finally,\na manual exemplar guides the LLM to infer stance categories, using an if-then\nlogical structure to maintain relevance and logical coherence. This structured\napproach to eliciting background knowledge enhances the model's capability,\noutperforming traditional supervised methods without relying on labeled data."
                },
                "authors": [
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Daijun Ding"
                    },
                    {
                        "name": "Liwen Jing"
                    },
                    {
                        "name": "Hu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hu Huang"
                },
                "author": "Hu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.16054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.16054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12885v1",
                "updated": "2025-07-17T08:10:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    10,
                    55,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T08:10:55Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    10,
                    55,
                    3,
                    198,
                    0
                ],
                "title": "VAR-MATH: Probing True Mathematical Reasoning in Large Language Models\n  via Symbolic Multi-Instance Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAR-MATH: Probing True Mathematical Reasoning in Large Language Models\n  via Symbolic Multi-Instance Benchmarks"
                },
                "summary": "Recent advances in reinforcement learning (RL) have led to substantial\nimprovements in the mathematical reasoning abilities of large language models\n(LLMs), as measured by standard benchmarks. However, these gains often persist\neven when models are trained with flawed signals, such as random or inverted\nrewards, raising a fundamental question: do such improvements reflect true\nreasoning, or are they merely artifacts of overfitting to benchmark-specific\npatterns? To address this question, we take an evaluation-centric perspective\nand identify two critical shortcomings in existing protocols. First,\n\\emph{benchmark contamination} arises from the public availability of test\nproblems, increasing the risk of data leakage. Second, \\emph{evaluation\nfragility} stems from the reliance on single-instance assessments, which are\nhighly sensitive to stochastic outputs and fail to capture reasoning\nconsistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic\nevaluation framework designed to probe genuine reasoning ability. By converting\nfixed numerical problems into symbolic templates and requiring models to solve\nmultiple instantiations of each, VAR-MATH enforces consistent reasoning across\nstructurally equivalent variants, thereby mitigating contamination and\nimproving evaluation robustness. We apply VAR-MATH to transform two popular\nbenchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and\nVAR-AIME24. Experimental results reveal substantial performance drops for\nRL-trained models on the variabilized versions, especially for smaller models,\nwith average declines of 48.0\\% on AMC23 and 58.3\\% on AIME24. These findings\nsuggest that many existing RL methods rely on superficial heuristics and fail\nto generalize beyond specific numerical forms. Overall, VAR-MATH offers a\nprincipled, contamination-resistant evaluation paradigm for mathematical\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have led to substantial\nimprovements in the mathematical reasoning abilities of large language models\n(LLMs), as measured by standard benchmarks. However, these gains often persist\neven when models are trained with flawed signals, such as random or inverted\nrewards, raising a fundamental question: do such improvements reflect true\nreasoning, or are they merely artifacts of overfitting to benchmark-specific\npatterns? To address this question, we take an evaluation-centric perspective\nand identify two critical shortcomings in existing protocols. First,\n\\emph{benchmark contamination} arises from the public availability of test\nproblems, increasing the risk of data leakage. Second, \\emph{evaluation\nfragility} stems from the reliance on single-instance assessments, which are\nhighly sensitive to stochastic outputs and fail to capture reasoning\nconsistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic\nevaluation framework designed to probe genuine reasoning ability. By converting\nfixed numerical problems into symbolic templates and requiring models to solve\nmultiple instantiations of each, VAR-MATH enforces consistent reasoning across\nstructurally equivalent variants, thereby mitigating contamination and\nimproving evaluation robustness. We apply VAR-MATH to transform two popular\nbenchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and\nVAR-AIME24. Experimental results reveal substantial performance drops for\nRL-trained models on the variabilized versions, especially for smaller models,\nwith average declines of 48.0\\% on AMC23 and 58.3\\% on AIME24. These findings\nsuggest that many existing RL methods rely on superficial heuristics and fail\nto generalize beyond specific numerical forms. Overall, VAR-MATH offers a\nprincipled, contamination-resistant evaluation paradigm for mathematical\nreasoning."
                },
                "authors": [
                    {
                        "name": "Jian Yao"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Kay Chen Tan"
                    }
                ],
                "author_detail": {
                    "name": "Kay Chen Tan"
                },
                "author": "Kay Chen Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21773v2",
                "updated": "2025-07-17T08:03:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    3,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-30T16:17:53Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    17,
                    53,
                    2,
                    120,
                    0
                ],
                "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness"
                },
                "summary": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision."
                },
                "authors": [
                    {
                        "name": "Junsheng Huang"
                    },
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Yucheng Huang"
                    },
                    {
                        "name": "Sandeep Polisetty"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "May Fung"
                    }
                ],
                "author_detail": {
                    "name": "May Fung"
                },
                "author": "May Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08898v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08898v3",
                "updated": "2025-07-17T08:01:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    8,
                    1,
                    44,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-11T05:15:35Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    5,
                    15,
                    35,
                    4,
                    192,
                    0
                ],
                "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast\n  Asian Languages for LLM Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast\n  Asian Languages for LLM Software Systems"
                },
                "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. We release our pre-trained model and\nbenchmark at https://github.com/awsm-research/SEALGuard to support further\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. We release our pre-trained model and\nbenchmark at https://github.com/awsm-research/SEALGuard to support further\nresearch."
                },
                "authors": [
                    {
                        "name": "Wenliang Shan"
                    },
                    {
                        "name": "Michael Fu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    }
                ],
                "author_detail": {
                    "name": "Chakkrit Tantithamthavorn"
                },
                "author": "Chakkrit Tantithamthavorn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08898v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08898v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09617v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09617v2",
                "updated": "2025-07-17T07:51:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    51,
                    28,
                    3,
                    198,
                    0
                ],
                "published": "2024-02-14T23:12:09Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    23,
                    12,
                    9,
                    2,
                    45,
                    0
                ],
                "title": "LLM-Enhanced User-Item Interactions: Leveraging Edge Information for\n  Optimized Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced User-Item Interactions: Leveraging Edge Information for\n  Optimized Recommendations"
                },
                "summary": "Graph recommendation methods, representing a connected interaction\nperspective, reformulate user-item interactions as graphs to leverage graph\nstructure and topology to recommend and have proved practical effectiveness at\nscale. Large language models, representing a textual generative perspective,\nexcel at modeling user languages, understanding behavioral contexts, capturing\nuser-item semantic relationships, analyzing textual sentiments, and generating\ncoherent and contextually relevant texts as recommendations. However, there is\na gap between the connected graph perspective and the text generation\nperspective as the task formulations are different. A research question arises:\nhow can we effectively integrate the two perspectives for more personalized\nrecsys? To fill this gap, we propose to incorporate graph-edge information into\nLLMs via prompt and attention innovations. We reformulate recommendations as a\nprobabilistic generative problem using prompts. We develop a framework to\nincorporate graph edge information from the prompt and attention mechanisms for\ngraph-structured LLM recommendations. We develop a new prompt design that\nbrings in both first-order and second-order graph relationships; we devise an\nimproved LLM attention mechanism to embed direct the spatial and connectivity\ninformation of edges. Our evaluation of real-world datasets demonstrates the\nframework's ability to understand connectivity information in graph data and to\nimprove the relevance and quality of recommendation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph recommendation methods, representing a connected interaction\nperspective, reformulate user-item interactions as graphs to leverage graph\nstructure and topology to recommend and have proved practical effectiveness at\nscale. Large language models, representing a textual generative perspective,\nexcel at modeling user languages, understanding behavioral contexts, capturing\nuser-item semantic relationships, analyzing textual sentiments, and generating\ncoherent and contextually relevant texts as recommendations. However, there is\na gap between the connected graph perspective and the text generation\nperspective as the task formulations are different. A research question arises:\nhow can we effectively integrate the two perspectives for more personalized\nrecsys? To fill this gap, we propose to incorporate graph-edge information into\nLLMs via prompt and attention innovations. We reformulate recommendations as a\nprobabilistic generative problem using prompts. We develop a framework to\nincorporate graph edge information from the prompt and attention mechanisms for\ngraph-structured LLM recommendations. We develop a new prompt design that\nbrings in both first-order and second-order graph relationships; we devise an\nimproved LLM attention mechanism to embed direct the spatial and connectivity\ninformation of edges. Our evaluation of real-world datasets demonstrates the\nframework's ability to understand connectivity information in graph data and to\nimprove the relevance and quality of recommendation results."
                },
                "authors": [
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Liang Wu"
                    },
                    {
                        "name": "Liangjie Hong"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09617v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12873v1",
                "updated": "2025-07-17T07:48:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    48,
                    5,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T07:48:05Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    48,
                    5,
                    3,
                    198,
                    0
                ],
                "title": "An Investigation of Ear-EEG Signals for a Novel Biometric Authentication\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Investigation of Ear-EEG Signals for a Novel Biometric Authentication\n  System"
                },
                "summary": "This work explores the feasibility of biometric authentication using EEG\nsignals acquired through in-ear devices, commonly referred to as ear-EEG.\nTraditional EEG-based biometric systems, while secure, often suffer from low\nusability due to cumbersome scalp-based electrode setups. In this study, we\npropose a novel and practical framework leveraging ear-EEG signals as a\nuser-friendly alternative for everyday biometric authentication. The system\nextracts an original combination of temporal and spectral features from ear-EEG\nsignals and feeds them into a fully connected deep neural network for subject\nidentification. Experimental results on the only currently available ear-EEG\ndataset suitable for different purposes, including biometric authentication,\ndemonstrate promising performance, with an average accuracy of 82\\% in a\nsubject identification scenario. These findings confirm the potential of\near-EEG as a viable and deployable direction for next-generation real-world\nbiometric systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the feasibility of biometric authentication using EEG\nsignals acquired through in-ear devices, commonly referred to as ear-EEG.\nTraditional EEG-based biometric systems, while secure, often suffer from low\nusability due to cumbersome scalp-based electrode setups. In this study, we\npropose a novel and practical framework leveraging ear-EEG signals as a\nuser-friendly alternative for everyday biometric authentication. The system\nextracts an original combination of temporal and spectral features from ear-EEG\nsignals and feeds them into a fully connected deep neural network for subject\nidentification. Experimental results on the only currently available ear-EEG\ndataset suitable for different purposes, including biometric authentication,\ndemonstrate promising performance, with an average accuracy of 82\\% in a\nsubject identification scenario. These findings confirm the potential of\near-EEG as a viable and deployable direction for next-generation real-world\nbiometric systems."
                },
                "authors": [
                    {
                        "name": "Danilo Avola"
                    },
                    {
                        "name": "Giancarlo Crocetti"
                    },
                    {
                        "name": "Gian Luca Foresti"
                    },
                    {
                        "name": "Daniele Pannone"
                    },
                    {
                        "name": "Claudio Piciarelli"
                    },
                    {
                        "name": "Amedeo Ranaldi"
                    }
                ],
                "author_detail": {
                    "name": "Amedeo Ranaldi"
                },
                "author": "Amedeo Ranaldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12872v1",
                "updated": "2025-07-17T07:45:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    45,
                    53,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T07:45:53Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    45,
                    53,
                    3,
                    198,
                    0
                ],
                "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case\n  Framework"
                },
                "summary": "Frontier AI systems are rapidly advancing in their capabilities to persuade,\ndeceive, and influence human behaviour, with current models already\ndemonstrating human-level persuasion and strategic deception in specific\ncontexts. Humans are often the weakest link in cybersecurity systems, and a\nmisaligned AI system deployed internally within a frontier company may seek to\nundermine human oversight by manipulating employees. Despite this growing\nthreat, manipulation attacks have received little attention, and no systematic\nframework exists for assessing and mitigating these risks. To address this, we\nprovide a detailed explanation of why manipulation attacks are a significant\nthreat and could lead to catastrophic outcomes. Additionally, we present a\nsafety case framework for manipulation risk, structured around three core lines\nof argument: inability, control, and trustworthiness. For each argument, we\nspecify evidence requirements, evaluation methodologies, and implementation\nconsiderations for direct application by AI companies. This paper provides the\nfirst systematic methodology for integrating manipulation risk into AI safety\ngovernance, offering AI companies a concrete foundation to assess and mitigate\nthese threats before deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier AI systems are rapidly advancing in their capabilities to persuade,\ndeceive, and influence human behaviour, with current models already\ndemonstrating human-level persuasion and strategic deception in specific\ncontexts. Humans are often the weakest link in cybersecurity systems, and a\nmisaligned AI system deployed internally within a frontier company may seek to\nundermine human oversight by manipulating employees. Despite this growing\nthreat, manipulation attacks have received little attention, and no systematic\nframework exists for assessing and mitigating these risks. To address this, we\nprovide a detailed explanation of why manipulation attacks are a significant\nthreat and could lead to catastrophic outcomes. Additionally, we present a\nsafety case framework for manipulation risk, structured around three core lines\nof argument: inability, control, and trustworthiness. For each argument, we\nspecify evidence requirements, evaluation methodologies, and implementation\nconsiderations for direct application by AI companies. This paper provides the\nfirst systematic methodology for integrating manipulation risk into AI safety\ngovernance, offering AI companies a concrete foundation to assess and mitigate\nthese threats before deployment."
                },
                "authors": [
                    {
                        "name": "Rishane Dassanayake"
                    },
                    {
                        "name": "Mario Demetroudi"
                    },
                    {
                        "name": "James Walpole"
                    },
                    {
                        "name": "Lindley Lentati"
                    },
                    {
                        "name": "Jason R. Brown"
                    },
                    {
                        "name": "Edward James Young"
                    }
                ],
                "author_detail": {
                    "name": "Edward James Young"
                },
                "author": "Edward James Young",
                "arxiv_comment": "24 pages (14 pages main text, 4 pages bibliography, 6 pages\n  appendices), 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12855v1",
                "updated": "2025-07-17T07:26:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    26,
                    22,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T07:26:22Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    26,
                    22,
                    3,
                    198,
                    0
                ],
                "title": "DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task\n  Demonstration Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task\n  Demonstration Learning"
                },
                "summary": "The integration of large language models (LLMs) with control systems has\ndemonstrated significant potential in various settings, such as task completion\nwith a robotic manipulator. A main reason for this success is the ability of\nLLMs to perform in-context learning, which, however, strongly relies on the\ndesign of task examples, closely related to the target tasks. Consequently,\nemploying LLMs to formulate optimal control problems often requires task\nexamples that contain explicit mathematical expressions, designed by trained\nengineers. Furthermore, there is often no principled way to evaluate for\nhallucination before task execution. To address these challenges, we propose\nDEMONSTRATE, a novel methodology that avoids the use of LLMs for complex\noptimization problem generations, and instead only relies on the embedding\nrepresentations of task descriptions. To do this, we leverage tools from\ninverse optimal control to replace in-context prompt examples with task\ndemonstrations, as well as the concept of multitask learning, which ensures\ntarget and example task similarity by construction. Given the fact that\nhardware demonstrations can easily be collected using teleoperation or guidance\nof the robot, our approach significantly reduces the reliance on engineering\nexpertise for designing in-context examples. Furthermore, the enforced\nmultitask structure enables learning from few demonstrations and assessment of\nhallucinations prior to task execution. We demonstrate the effectiveness of our\nmethod through simulation and hardware experiments involving a robotic arm\ntasked with tabletop manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) with control systems has\ndemonstrated significant potential in various settings, such as task completion\nwith a robotic manipulator. A main reason for this success is the ability of\nLLMs to perform in-context learning, which, however, strongly relies on the\ndesign of task examples, closely related to the target tasks. Consequently,\nemploying LLMs to formulate optimal control problems often requires task\nexamples that contain explicit mathematical expressions, designed by trained\nengineers. Furthermore, there is often no principled way to evaluate for\nhallucination before task execution. To address these challenges, we propose\nDEMONSTRATE, a novel methodology that avoids the use of LLMs for complex\noptimization problem generations, and instead only relies on the embedding\nrepresentations of task descriptions. To do this, we leverage tools from\ninverse optimal control to replace in-context prompt examples with task\ndemonstrations, as well as the concept of multitask learning, which ensures\ntarget and example task similarity by construction. Given the fact that\nhardware demonstrations can easily be collected using teleoperation or guidance\nof the robot, our approach significantly reduces the reliance on engineering\nexpertise for designing in-context examples. Furthermore, the enforced\nmultitask structure enables learning from few demonstrations and assessment of\nhallucinations prior to task execution. We demonstrate the effectiveness of our\nmethod through simulation and hardware experiments involving a robotic arm\ntasked with tabletop manipulation."
                },
                "authors": [
                    {
                        "name": "Rahel Rickenbach"
                    },
                    {
                        "name": "Bruce Lee"
                    },
                    {
                        "name": "Ren Zurbrgg"
                    },
                    {
                        "name": "Carmen Amo Alonso"
                    },
                    {
                        "name": "Melanie N. Zeilinger"
                    }
                ],
                "author_detail": {
                    "name": "Melanie N. Zeilinger"
                },
                "author": "Melanie N. Zeilinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12269v2",
                "updated": "2025-07-17T07:11:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    11,
                    14,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T14:19:44Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    19,
                    44,
                    2,
                    197,
                    0
                ],
                "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust\n  Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in\n  Extremely Preterm Infants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust\n  Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in\n  Extremely Preterm Infants"
                },
                "summary": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments."
                },
                "authors": [
                    {
                        "name": "Sybelle Goedicke-Fritz"
                    },
                    {
                        "name": "Michelle Bous"
                    },
                    {
                        "name": "Annika Engel"
                    },
                    {
                        "name": "Matthias Flotho"
                    },
                    {
                        "name": "Pascal Hirsch"
                    },
                    {
                        "name": "Hannah Wittig"
                    },
                    {
                        "name": "Dino Milanovic"
                    },
                    {
                        "name": "Dominik Mohr"
                    },
                    {
                        "name": "Mathias Kaspar"
                    },
                    {
                        "name": "Sogand Nemat"
                    },
                    {
                        "name": "Dorothea Kerner"
                    },
                    {
                        "name": "Arno Bcker"
                    },
                    {
                        "name": "Andreas Keller"
                    },
                    {
                        "name": "Sascha Meyer"
                    },
                    {
                        "name": "Michael Zemlin"
                    },
                    {
                        "name": "Philipp Flotho"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Flotho"
                },
                "arxiv_affiliation": "Helmholtz Institute for Pharmaceutical Research Saarland",
                "author": "Philipp Flotho",
                "arxiv_comment": "S.G.-F., M.B., and A.E. contributed equally to this work and share\n  first authorship. M.Z. and P.F. contributed equally to this work and share\n  senior authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12840v1",
                "updated": "2025-07-17T06:59:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    59,
                    52,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T06:59:52Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    59,
                    52,
                    3,
                    198,
                    0
                ],
                "title": "Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better\n  Understand Public Concerns about Vaccines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better\n  Understand Public Concerns about Vaccines"
                },
                "summary": "Vaccine hesitancy threatens public health, leading to delayed or rejected\nvaccines. Social media is a vital source for understanding public concerns, and\ntraditional methods like topic modelling often struggle to capture nuanced\nopinions. Though trained for query answering, large Language Models (LLMs)\noften miss current events and community concerns. Additionally, hallucinations\nin LLMs can compromise public health communication. To address these\nlimitations, we developed a tool (VaxPulse Query Corner) using the Retrieval\nAugmented Generation technique. It addresses complex queries about public\nvaccine concerns on various online platforms, aiding public health\nadministrators and stakeholders in understanding public concerns and\nimplementing targeted interventions to boost vaccine confidence. Analysing\n35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and\nrelevance (0.94).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vaccine hesitancy threatens public health, leading to delayed or rejected\nvaccines. Social media is a vital source for understanding public concerns, and\ntraditional methods like topic modelling often struggle to capture nuanced\nopinions. Though trained for query answering, large Language Models (LLMs)\noften miss current events and community concerns. Additionally, hallucinations\nin LLMs can compromise public health communication. To address these\nlimitations, we developed a tool (VaxPulse Query Corner) using the Retrieval\nAugmented Generation technique. It addresses complex queries about public\nvaccine concerns on various online platforms, aiding public health\nadministrators and stakeholders in understanding public concerns and\nimplementing targeted interventions to boost vaccine confidence. Analysing\n35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and\nrelevance (0.94)."
                },
                "authors": [
                    {
                        "name": "Muhammad Javed"
                    },
                    {
                        "name": "Sedigh Khademi Habibabadi"
                    },
                    {
                        "name": "Christopher Palmer"
                    },
                    {
                        "name": "Hazel Clothier"
                    },
                    {
                        "name": "Jim Buttery"
                    },
                    {
                        "name": "Gerardo Luis Dimaguila"
                    }
                ],
                "author_detail": {
                    "name": "Gerardo Luis Dimaguila"
                },
                "author": "Gerardo Luis Dimaguila",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15859v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15859v4",
                "updated": "2025-07-17T06:34:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    34,
                    41,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-21T10:16:56Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    10,
                    16,
                    56,
                    4,
                    52,
                    0
                ],
                "title": "AI Governance InternationaL Evaluation Index (AGILE Index) 2024",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Governance InternationaL Evaluation Index (AGILE Index) 2024"
                },
                "summary": "The rapid advancement of Artificial Intelligence (AI) technology is\nprofoundly transforming human society and concurrently presenting a series of\nethical, legal, and social issues. The effective governance of AI has become a\ncrucial global concern. Since 2022, the extensive deployment of generative AI,\nparticularly large language models, marked a new phase in AI governance.\nContinuous efforts are being made by the international community in actively\naddressing the novel challenges posed by these AI developments. As consensus on\ninternational governance continues to be established and put into action, the\npractical importance of conducting a global assessment of the state of AI\ngovernance is progressively coming to light. In this context, we initiated the\ndevelopment of the AI Governance InternationaL Evaluation Index (AGILE Index).\nAdhering to the design principle, \"the level of governance should match the\nlevel of development,\" the inaugural evaluation of the AGILE Index commences\nwith an exploration of four foundational pillars: the development level of AI,\nthe AI governance environment, the AI governance instruments, and the AI\ngovernance effectiveness. It covers 39 indicators across 18 dimensions to\ncomprehensively assess the AI governance level of 14 representative countries\nglobally. The index is utilized to delve into the status of AI governance to\ndate in 14 countries for the first batch of evaluation. The aim is to depict\nthe current state of AI governance in these countries through data scoring,\nassist them in identifying their governance stage and uncovering governance\nissues, and ultimately offer insights for the enhancement of their AI\ngovernance systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Artificial Intelligence (AI) technology is\nprofoundly transforming human society and concurrently presenting a series of\nethical, legal, and social issues. The effective governance of AI has become a\ncrucial global concern. Since 2022, the extensive deployment of generative AI,\nparticularly large language models, marked a new phase in AI governance.\nContinuous efforts are being made by the international community in actively\naddressing the novel challenges posed by these AI developments. As consensus on\ninternational governance continues to be established and put into action, the\npractical importance of conducting a global assessment of the state of AI\ngovernance is progressively coming to light. In this context, we initiated the\ndevelopment of the AI Governance InternationaL Evaluation Index (AGILE Index).\nAdhering to the design principle, \"the level of governance should match the\nlevel of development,\" the inaugural evaluation of the AGILE Index commences\nwith an exploration of four foundational pillars: the development level of AI,\nthe AI governance environment, the AI governance instruments, and the AI\ngovernance effectiveness. It covers 39 indicators across 18 dimensions to\ncomprehensively assess the AI governance level of 14 representative countries\nglobally. The index is utilized to delve into the status of AI governance to\ndate in 14 countries for the first batch of evaluation. The aim is to depict\nthe current state of AI governance in these countries through data scoring,\nassist them in identifying their governance stage and uncovering governance\nissues, and ultimately offer insights for the enhancement of their AI\ngovernance systems."
                },
                "authors": [
                    {
                        "name": "Yi Zeng"
                    },
                    {
                        "name": "Enmeng Lu"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Cunqing Huangfu"
                    },
                    {
                        "name": "Zizhe Ruan"
                    },
                    {
                        "name": "Ammar Younas"
                    },
                    {
                        "name": "Kang Sun"
                    },
                    {
                        "name": "Xuan Tang"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Hongjie Suo"
                    },
                    {
                        "name": "Dongqi Liang"
                    },
                    {
                        "name": "Zhengqiang Han"
                    },
                    {
                        "name": "Aorigele Bao"
                    },
                    {
                        "name": "Xiaoyang Guo"
                    },
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Jiawei Xie"
                    },
                    {
                        "name": "Yao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yao Liang"
                },
                "author": "Yao Liang",
                "arxiv_comment": "Evaluation Report. 85 pages, 30 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15859v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15859v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "A.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12820v1",
                "updated": "2025-07-17T06:24:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    24,
                    20,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T06:24:20Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    24,
                    20,
                    3,
                    198,
                    0
                ],
                "title": "Emotional Support with LLM-based Empathetic Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional Support with LLM-based Empathetic Dialogue Generation"
                },
                "summary": "Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems."
                },
                "authors": [
                    {
                        "name": "Shiquan Wang"
                    },
                    {
                        "name": "Ruiyu Fang"
                    },
                    {
                        "name": "Zhongjiang He"
                    },
                    {
                        "name": "Shuangyong Song"
                    },
                    {
                        "name": "Yongxiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongxiang Li"
                },
                "author": "Yongxiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12819v1",
                "updated": "2025-07-17T06:22:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    22,
                    49,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T06:22:49Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    22,
                    49,
                    3,
                    198,
                    0
                ],
                "title": "MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free\n  Zero-Shot Composed Image Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free\n  Zero-Shot Composed Image Retrieval"
                },
                "summary": "Composed Image Retrieval (CIR) is the task of retrieving a target image from\na gallery using a composed query consisting of a reference image and a\nmodification text. Among various CIR approaches, training-free zero-shot\nmethods based on pre-trained models are cost-effective but still face notable\nlimitations. For example, sequential VLM-LLM pipelines process each modality\nindependently, which often results in information loss and limits cross-modal\ninteraction. In contrast, methods based on multimodal large language models\n(MLLMs) often focus exclusively on applying changes indicated by the text,\nwithout fully utilizing the contextual visual information from the reference\nimage. To address these issues, we propose multi-faceted Chain-of-Thought with\nre-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes\nmulti-faceted Chain-of-Thought to guide the MLLM to balance explicit\nmodifications and contextual visual cues, generating two distinct captions: one\nfocused on modification and the other integrating comprehensive visual-textual\ncontext. The first caption is used to filter candidate images. Subsequently, we\ncombine these two captions and the reference image to perform multi-grained\nre-ranking. This two-stage approach facilitates precise retrieval by aligning\nwith the textual modification instructions while preserving the visual context\nof the reference image. Through extensive experiments, MCoT-RE achieves\nstate-of-the-art results among training-free methods, yielding improvements of\nup to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composed Image Retrieval (CIR) is the task of retrieving a target image from\na gallery using a composed query consisting of a reference image and a\nmodification text. Among various CIR approaches, training-free zero-shot\nmethods based on pre-trained models are cost-effective but still face notable\nlimitations. For example, sequential VLM-LLM pipelines process each modality\nindependently, which often results in information loss and limits cross-modal\ninteraction. In contrast, methods based on multimodal large language models\n(MLLMs) often focus exclusively on applying changes indicated by the text,\nwithout fully utilizing the contextual visual information from the reference\nimage. To address these issues, we propose multi-faceted Chain-of-Thought with\nre-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes\nmulti-faceted Chain-of-Thought to guide the MLLM to balance explicit\nmodifications and contextual visual cues, generating two distinct captions: one\nfocused on modification and the other integrating comprehensive visual-textual\ncontext. The first caption is used to filter candidate images. Subsequently, we\ncombine these two captions and the reference image to perform multi-grained\nre-ranking. This two-stage approach facilitates precise retrieval by aligning\nwith the textual modification instructions while preserving the visual context\nof the reference image. Through extensive experiments, MCoT-RE achieves\nstate-of-the-art results among training-free methods, yielding improvements of\nup to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR."
                },
                "authors": [
                    {
                        "name": "Jeong-Woo Park"
                    },
                    {
                        "name": "Seong-Whan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Whan Lee"
                },
                "author": "Seong-Whan Lee",
                "arxiv_comment": "6 pages, 4 figures, 2025 IEEE International Conference on Systems,\n  Man, and Cybernetics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12808v1",
                "updated": "2025-07-17T05:48:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    48,
                    45,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T05:48:45Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    48,
                    45,
                    3,
                    198,
                    0
                ],
                "title": "Large Language Models' Internal Perception of Symbolic Music",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models' Internal Perception of Symbolic Music"
                },
                "summary": "Large language models (LLMs) excel at modeling relationships between strings\nin natural language and have shown promise in extending to other symbolic\ndomains like coding or mathematics. However, the extent to which they\nimplicitly model symbolic music remains underexplored. This paper investigates\nhow LLMs represent musical concepts by generating symbolic music data from\ntextual prompts describing combinations of genres and styles, and evaluating\ntheir utility through recognition and generation tasks. We produce a dataset of\nLLM-generated MIDI files without relying on explicit musical training. We then\ntrain neural networks entirely on this LLM-generated MIDI dataset and perform\ngenre and style classification as well as melody completion, benchmarking their\nperformance against established models. Our results demonstrate that LLMs can\ninfer rudimentary musical structures and temporal relationships from text,\nhighlighting both their potential to implicitly encode musical patterns and\ntheir limitations due to a lack of explicit musical context, shedding light on\ntheir generative capabilities for symbolic music.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at modeling relationships between strings\nin natural language and have shown promise in extending to other symbolic\ndomains like coding or mathematics. However, the extent to which they\nimplicitly model symbolic music remains underexplored. This paper investigates\nhow LLMs represent musical concepts by generating symbolic music data from\ntextual prompts describing combinations of genres and styles, and evaluating\ntheir utility through recognition and generation tasks. We produce a dataset of\nLLM-generated MIDI files without relying on explicit musical training. We then\ntrain neural networks entirely on this LLM-generated MIDI dataset and perform\ngenre and style classification as well as melody completion, benchmarking their\nperformance against established models. Our results demonstrate that LLMs can\ninfer rudimentary musical structures and temporal relationships from text,\nhighlighting both their potential to implicitly encode musical patterns and\ntheir limitations due to a lack of explicit musical context, shedding light on\ntheir generative capabilities for symbolic music."
                },
                "authors": [
                    {
                        "name": "Andrew Shin"
                    },
                    {
                        "name": "Kunitake Kaneko"
                    }
                ],
                "author_detail": {
                    "name": "Kunitake Kaneko"
                },
                "author": "Kunitake Kaneko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09592v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09592v3",
                "updated": "2025-07-17T05:47:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    47,
                    22,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-13T11:48:24Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    11,
                    48,
                    24,
                    6,
                    194,
                    0
                ],
                "title": "THOR: Transformer Heuristics for On-Demand Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THOR: Transformer Heuristics for On-Demand Retrieval"
                },
                "summary": "We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)\nModule, designed and implemented by eSapiens, a secure, scalable engine that\ntransforms natural-language questions into verified, read-only SQL analytics\nfor enterprise databases. The Text-to-SQL module follows a decoupled\norchestration/execution architecture: a Supervisor Agent routes queries, Schema\nRetrieval dynamically injects table and column metadata, and a SQL Generation\nAgent emits single-statement SELECT queries protected by a read-only guardrail.\nAn integrated Self-Correction & Rating loop captures empty results, execution\nerrors, or low-quality outputs and triggers up to five LLM-driven regeneration\nattempts. Finally, a Result Interpretation Agent produces concise,\nhuman-readable insights and hands raw rows to the Insight & Intelligence engine\nfor visualization or forecasting.\n  Smoke tests across finance, sales, and operations scenarios demonstrate\nreliable ad-hoc querying and automated periodic reporting. By embedding schema\nawareness, fault-tolerant execution, and compliance guardrails, the THOR Module\nempowers non-technical users to access live data with zero-SQL simplicity and\nenterprise-grade safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)\nModule, designed and implemented by eSapiens, a secure, scalable engine that\ntransforms natural-language questions into verified, read-only SQL analytics\nfor enterprise databases. The Text-to-SQL module follows a decoupled\norchestration/execution architecture: a Supervisor Agent routes queries, Schema\nRetrieval dynamically injects table and column metadata, and a SQL Generation\nAgent emits single-statement SELECT queries protected by a read-only guardrail.\nAn integrated Self-Correction & Rating loop captures empty results, execution\nerrors, or low-quality outputs and triggers up to five LLM-driven regeneration\nattempts. Finally, a Result Interpretation Agent produces concise,\nhuman-readable insights and hands raw rows to the Insight & Intelligence engine\nfor visualization or forecasting.\n  Smoke tests across finance, sales, and operations scenarios demonstrate\nreliable ad-hoc querying and automated periodic reporting. By embedding schema\nawareness, fault-tolerant execution, and compliance guardrails, the THOR Module\nempowers non-technical users to access live data with zero-SQL simplicity and\nenterprise-grade safety."
                },
                "authors": [
                    {
                        "name": "Isaac Shi"
                    },
                    {
                        "name": "Zeyuan Li"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Wenli Wang"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09592v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09592v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12806v1",
                "updated": "2025-07-17T05:46:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    46,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T05:46:27Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    46,
                    27,
                    3,
                    198,
                    0
                ],
                "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models"
                },
                "summary": "The rapid rise of Large Language Models (LLMs)-based intelligent agents\nunderscores the need for robust, scalable evaluation frameworks. Existing\nmethods rely on static benchmarks and labor-intensive data collection, limiting\npractical assessment. We introduce \\oursystemname, an open-source Model Context\nProtocol (MCP)-based framework that automates end-to-end task generation and\ndeep evaluation of LLM agents across diverse domains. MCPEval standardizes\nmetrics, seamlessly integrates with native agent tools, and eliminates manual\neffort in building evaluation pipelines. Empirical results across five\nreal-world domains show its effectiveness in revealing nuanced, domain-specific\nperformance. We publicly release MCPEval\nhttps://github.com/SalesforceAIResearch/MCPEval to promote reproducible and\nstandardized LLM agent evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Large Language Models (LLMs)-based intelligent agents\nunderscores the need for robust, scalable evaluation frameworks. Existing\nmethods rely on static benchmarks and labor-intensive data collection, limiting\npractical assessment. We introduce \\oursystemname, an open-source Model Context\nProtocol (MCP)-based framework that automates end-to-end task generation and\ndeep evaluation of LLM agents across diverse domains. MCPEval standardizes\nmetrics, seamlessly integrates with native agent tools, and eliminates manual\neffort in building evaluation pipelines. Empirical results across five\nreal-world domains show its effectiveness in revealing nuanced, domain-specific\nperformance. We publicly release MCPEval\nhttps://github.com/SalesforceAIResearch/MCPEval to promote reproducible and\nstandardized LLM agent evaluation."
                },
                "authors": [
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Jielin Qiu"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Roshan Ram"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "https://github.com/SalesforceAIResearch/MCPEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07476v2",
                "updated": "2025-07-17T05:44:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    44,
                    40,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-10T07:04:25Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    7,
                    4,
                    25,
                    3,
                    191,
                    0
                ],
                "title": "A comparative study of physics capabilities of a liquid argon and a\n  water based liquid scintillator at DUNE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comparative study of physics capabilities of a liquid argon and a\n  water based liquid scintillator at DUNE"
                },
                "summary": "We present a comprehensive comparison of the physics sensitivities of a\nLiquid Argon Time Projection Chamber (LArTPC) and a Water-based Liquid\nScintillator (WbLS) detector, considering their potential deployment as the\nfourth far detector module in the DUNE facility. Using GLoBES-based\nsimulations, we evaluate their performance in measuring standard neutrino\noscillation parameters ($\\theta_{23}, \\delta_{13}$ and $\\Delta m^{2}_{31}$),\nboth in standard 3-neutrino case, as well as in presence of new physics\nscenarios involving light sterile neutrinos and neutral-current non-standard\ninteractions (NC NSI). Our findings show that THEIA (a WbLS-based detector)\nsignificantly outperforms LArTPC in resolving the CP phase $\\delta_{13}$,-\nespecially near maximal CP violation, and in lifting the octant degeneracy of\n$\\theta_{23}$ due to its superior energy resolution and ability to clearly\nidentify the second oscillation maximum. Furthermore, THEIA offers competitive\nreconstruction precision even with relatively moderate energy resolutions\n($7-10\\%/\\sqrt{E}$) and demonstrates enhanced robustness under new physics\nscenarios. These results support the physics-driven case for a hybrid DUNE\nconfiguration utilizing both LArTPC and WbLS technologies for optimized\nsensitivity across the full spectrum of neutrino oscillation and physics beyond\nthe standard model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive comparison of the physics sensitivities of a\nLiquid Argon Time Projection Chamber (LArTPC) and a Water-based Liquid\nScintillator (WbLS) detector, considering their potential deployment as the\nfourth far detector module in the DUNE facility. Using GLoBES-based\nsimulations, we evaluate their performance in measuring standard neutrino\noscillation parameters ($\\theta_{23}, \\delta_{13}$ and $\\Delta m^{2}_{31}$),\nboth in standard 3-neutrino case, as well as in presence of new physics\nscenarios involving light sterile neutrinos and neutral-current non-standard\ninteractions (NC NSI). Our findings show that THEIA (a WbLS-based detector)\nsignificantly outperforms LArTPC in resolving the CP phase $\\delta_{13}$,-\nespecially near maximal CP violation, and in lifting the octant degeneracy of\n$\\theta_{23}$ due to its superior energy resolution and ability to clearly\nidentify the second oscillation maximum. Furthermore, THEIA offers competitive\nreconstruction precision even with relatively moderate energy resolutions\n($7-10\\%/\\sqrt{E}$) and demonstrates enhanced robustness under new physics\nscenarios. These results support the physics-driven case for a hybrid DUNE\nconfiguration utilizing both LArTPC and WbLS technologies for optimized\nsensitivity across the full spectrum of neutrino oscillation and physics beyond\nthe standard model."
                },
                "authors": [
                    {
                        "name": "Nishat Fiza"
                    },
                    {
                        "name": "Suhyeon Kim"
                    },
                    {
                        "name": "Emar Masaku"
                    },
                    {
                        "name": "Mehedi Masud"
                    },
                    {
                        "name": "Hokyeong Nam"
                    },
                    {
                        "name": "Juseong Park"
                    },
                    {
                        "name": "Yujin Park"
                    },
                    {
                        "name": "Kim Siyeon"
                    }
                ],
                "author_detail": {
                    "name": "Kim Siyeon"
                },
                "author": "Kim Siyeon",
                "arxiv_comment": "We found that we should significantly change some of the results\n  based on the update of sensitivities and the guideline to use them from DUNE\n  Collaboration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20495v2",
                "updated": "2025-07-17T05:31:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    31,
                    7,
                    3,
                    198,
                    0
                ],
                "published": "2025-06-25T14:41:13Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    14,
                    41,
                    13,
                    2,
                    176,
                    0
                ],
                "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCode: Updating Code API Knowledge with Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode."
                },
                "authors": [
                    {
                        "name": "Haoze Wu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18699v2",
                "updated": "2025-07-17T05:29:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    5,
                    29,
                    9,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-25T23:22:12Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    23,
                    22,
                    12,
                    1,
                    56,
                    0
                ],
                "title": "MPO: An Efficient Post-Processing Framework for Mixing Diverse\n  Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPO: An Efficient Post-Processing Framework for Mixing Diverse\n  Preference Alignment"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has shown promise in\naligning large language models (LLMs). Yet its reliance on a singular reward\nmodel often overlooks the diversity of human preferences. Recent approaches\naddress this limitation by leveraging multi-dimensional feedback to fine-tune\ncorresponding reward models and train LLMs using reinforcement learning.\nHowever, the process is costly and unstable, especially given the competing and\nheterogeneous nature of human preferences. In this paper, we propose Mixing\nPreference Optimization (MPO), a post-processing framework for aggregating\nsingle-objective policies as an alternative to both multi-objective RLHF\n(MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it\nlog-linearly combines existing policies into a unified one with the weight of\neach policy computed via a batch stochastic mirror descent. Empirical results\ndemonstrate that MPO achieves balanced performance across diverse preferences,\noutperforming or matching existing models with significantly reduced\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has shown promise in\naligning large language models (LLMs). Yet its reliance on a singular reward\nmodel often overlooks the diversity of human preferences. Recent approaches\naddress this limitation by leveraging multi-dimensional feedback to fine-tune\ncorresponding reward models and train LLMs using reinforcement learning.\nHowever, the process is costly and unstable, especially given the competing and\nheterogeneous nature of human preferences. In this paper, we propose Mixing\nPreference Optimization (MPO), a post-processing framework for aggregating\nsingle-objective policies as an alternative to both multi-objective RLHF\n(MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it\nlog-linearly combines existing policies into a unified one with the weight of\neach policy computed via a batch stochastic mirror descent. Empirical results\ndemonstrate that MPO achieves balanced performance across diverse preferences,\noutperforming or matching existing models with significantly reduced\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Tianze Wang"
                    },
                    {
                        "name": "Dongnan Gui"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Linjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Zhang"
                },
                "author": "Linjun Zhang",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12782v1",
                "updated": "2025-07-17T04:48:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    4,
                    48,
                    54,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T04:48:54Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    4,
                    48,
                    54,
                    3,
                    198,
                    0
                ],
                "title": "Learning Robust Negation Text Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Robust Negation Text Representations"
                },
                "summary": "Despite rapid adoption of autoregressive large language models, smaller text\nencoders still play an important role in text understanding tasks that require\nrich contextualized representations. Negation is an important semantic function\nthat is still not properly captured by such methods, affecting many downstream\napplications relying on text embeddings. We propose a strategy to improve\nnegation robustness of text encoders, by distilling data from large language\nmodels using diverse patterns of negation and hedging. We adopt a standard\ncontrastive learning strategy to finetune a strong BERT-based model, and\nobserve large improvement in negation understanding capabilities while\nmaintaining competitive performance on general benchmarks. In addition, we also\nshow that our method can be adapted to LLMs, leading to improved performance on\nnegation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid adoption of autoregressive large language models, smaller text\nencoders still play an important role in text understanding tasks that require\nrich contextualized representations. Negation is an important semantic function\nthat is still not properly captured by such methods, affecting many downstream\napplications relying on text embeddings. We propose a strategy to improve\nnegation robustness of text encoders, by distilling data from large language\nmodels using diverse patterns of negation and hedging. We adopt a standard\ncontrastive learning strategy to finetune a strong BERT-based model, and\nobserve large improvement in negation understanding capabilities while\nmaintaining competitive performance on general benchmarks. In addition, we also\nshow that our method can be adapted to LLMs, leading to improved performance on\nnegation benchmarks."
                },
                "authors": [
                    {
                        "name": "Thinh Hung Truong"
                    },
                    {
                        "name": "Karin Verspoor"
                    },
                    {
                        "name": "Trevor Cohn"
                    },
                    {
                        "name": "Timothy Baldwin"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Baldwin"
                },
                "author": "Timothy Baldwin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12774v1",
                "updated": "2025-07-17T04:31:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    4,
                    31,
                    55,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T04:31:55Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    4,
                    31,
                    55,
                    3,
                    198,
                    0
                ],
                "title": "A Comprehensive Survey of Electronic Health Record Modeling: From Deep\n  Learning Approaches to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Electronic Health Record Modeling: From Deep\n  Learning Approaches to Large Language Models"
                },
                "summary": "Artificial intelligence (AI) has demonstrated significant potential in\ntransforming healthcare through the analysis and modeling of electronic health\nrecords (EHRs). However, the inherent heterogeneity, temporal irregularity, and\ndomain-specific nature of EHR data present unique challenges that differ\nfundamentally from those in vision and natural language tasks. This survey\noffers a comprehensive overview of recent advancements at the intersection of\ndeep learning, large language models (LLMs), and EHR modeling. We introduce a\nunified taxonomy that spans five key design dimensions: data-centric\napproaches, neural architecture design, learning-focused strategies, multimodal\nlearning, and LLM-based modeling systems. Within each dimension, we review\nrepresentative methods addressing data quality enhancement, structural and\ntemporal representation, self-supervised learning, and integration with\nclinical knowledge. We further highlight emerging trends such as foundation\nmodels, LLM-driven clinical agents, and EHR-to-text translation for downstream\nreasoning. Finally, we discuss open challenges in benchmarking, explainability,\nclinical alignment, and generalization across diverse clinical settings. This\nsurvey aims to provide a structured roadmap for advancing AI-driven EHR\nmodeling and clinical decision support. For a comprehensive list of EHR-related\nmethods, kindly refer to https://survey-on-tabular-data.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has demonstrated significant potential in\ntransforming healthcare through the analysis and modeling of electronic health\nrecords (EHRs). However, the inherent heterogeneity, temporal irregularity, and\ndomain-specific nature of EHR data present unique challenges that differ\nfundamentally from those in vision and natural language tasks. This survey\noffers a comprehensive overview of recent advancements at the intersection of\ndeep learning, large language models (LLMs), and EHR modeling. We introduce a\nunified taxonomy that spans five key design dimensions: data-centric\napproaches, neural architecture design, learning-focused strategies, multimodal\nlearning, and LLM-based modeling systems. Within each dimension, we review\nrepresentative methods addressing data quality enhancement, structural and\ntemporal representation, self-supervised learning, and integration with\nclinical knowledge. We further highlight emerging trends such as foundation\nmodels, LLM-driven clinical agents, and EHR-to-text translation for downstream\nreasoning. Finally, we discuss open challenges in benchmarking, explainability,\nclinical alignment, and generalization across diverse clinical settings. This\nsurvey aims to provide a structured roadmap for advancing AI-driven EHR\nmodeling and clinical decision support. For a comprehensive list of EHR-related\nmethods, kindly refer to https://survey-on-tabular-data.github.io/."
                },
                "authors": [
                    {
                        "name": "Weijieying Ren"
                    },
                    {
                        "name": "Jingxi Zhu"
                    },
                    {
                        "name": "Zehao Liu"
                    },
                    {
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "name": "Vasant Honavar"
                    }
                ],
                "author_detail": {
                    "name": "Vasant Honavar"
                },
                "author": "Vasant Honavar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03106v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03106v4",
                "updated": "2025-07-17T04:08:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    4,
                    8,
                    3,
                    3,
                    198,
                    0
                ],
                "published": "2025-06-03T17:39:02Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    39,
                    2,
                    1,
                    154,
                    0
                ],
                "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback"
                },
                "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of spontaneous self-reflection, and\npersistent failures. We then demonstrate that RL-finetuned models, even after\nexhibiting performance plateaus, can generate correct refinements on\npersistently failed problems by leveraging natural language feedback in the\nform of critiques. Building on this insight, we propose Critique-GRPO, an\nonline RL framework that integrates both natural language and numerical\nfeedback for effective policy optimization. Critique-GRPO enables LLMs to learn\nfrom initial responses and critique-guided self-refinements simultaneously\nwhile maintaining exploration. Additionally, we employ a shaping function to\namplify learning from correct, especially unfamiliar, refinements and penalize\nincorrect ones. Extensive experiments with Qwen2.5-7B-Base,\nQwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently\noutperforms supervised learning and RL-based fine-tuning methods across eight\nchallenging mathematical, STEM, and general reasoning tasks, improving average\npass@1 scores by approximately 4.4% and 3.8% on Qwen2.5-7B-Base and Qwen3-8B,\nrespectively. Notably, Critique-GRPO enables effective self-improvement through\nself-critiquing and weak-to-strong generalization, achieving consistent gains\nover GRPO, such as 16.7% and 10.0% pass@1 improvements on AIME 2024,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of spontaneous self-reflection, and\npersistent failures. We then demonstrate that RL-finetuned models, even after\nexhibiting performance plateaus, can generate correct refinements on\npersistently failed problems by leveraging natural language feedback in the\nform of critiques. Building on this insight, we propose Critique-GRPO, an\nonline RL framework that integrates both natural language and numerical\nfeedback for effective policy optimization. Critique-GRPO enables LLMs to learn\nfrom initial responses and critique-guided self-refinements simultaneously\nwhile maintaining exploration. Additionally, we employ a shaping function to\namplify learning from correct, especially unfamiliar, refinements and penalize\nincorrect ones. Extensive experiments with Qwen2.5-7B-Base,\nQwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently\noutperforms supervised learning and RL-based fine-tuning methods across eight\nchallenging mathematical, STEM, and general reasoning tasks, improving average\npass@1 scores by approximately 4.4% and 3.8% on Qwen2.5-7B-Base and Qwen3-8B,\nrespectively. Notably, Critique-GRPO enables effective self-improvement through\nself-critiquing and weak-to-strong generalization, achieving consistent gains\nover GRPO, such as 16.7% and 10.0% pass@1 improvements on AIME 2024,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Xiaoying Zhang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Kaituo Feng"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Chao Yang"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "52 pages, updated with new experimental results and implementation\n  details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03106v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03106v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21582v2",
                "updated": "2025-07-17T03:52:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    3,
                    52,
                    15,
                    3,
                    198,
                    0
                ],
                "published": "2025-06-17T05:24:58Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    5,
                    24,
                    58,
                    1,
                    168,
                    0
                ],
                "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation\n  of Text Analytics with Intelligent Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation\n  of Text Analytics with Intelligent Agents"
                },
                "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems."
                },
                "authors": [
                    {
                        "name": "Sam Yu-Te Lee"
                    },
                    {
                        "name": "Chengyang Ji"
                    },
                    {
                        "name": "Shicheng Wen"
                    },
                    {
                        "name": "Lifu Huang"
                    },
                    {
                        "name": "Dongyu Liu"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11988v2",
                "updated": "2025-07-17T03:34:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    3,
                    34,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T07:38:28Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    38,
                    28,
                    2,
                    197,
                    0
                ],
                "title": "Aime: Towards Fully-Autonomous Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aime: Towards Fully-Autonomous Multi-Agent Framework"
                },
                "summary": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Yexuan Shi"
                    },
                    {
                        "name": "Mingyu Wang"
                    },
                    {
                        "name": "Yunxiang Cao"
                    },
                    {
                        "name": "Hongjie Lai"
                    },
                    {
                        "name": "Junjian Lan"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Jie Geng"
                    },
                    {
                        "name": "Zhenan Li"
                    },
                    {
                        "name": "Zihao Xia"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Wenbo Duan"
                    },
                    {
                        "name": "Yuanshuo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanshuo Zhu"
                },
                "author": "Yuanshuo Zhu",
                "arxiv_comment": "14 pages, 1 figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16506v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16506v3",
                "updated": "2025-07-17T03:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    3,
                    22,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-04-23T08:33:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    33,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "A Comprehensive Survey of Synthetic Tabular Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Synthetic Tabular Data Generation"
                },
                "summary": "Tabular data is one of the most prevalent and important data formats in\nreal-world applications such as healthcare, finance, and education. However,\nits effective use in machine learning is often constrained by data scarcity,\nprivacy concerns, and class imbalance. Synthetic tabular data generation has\nemerged as a powerful solution, leveraging generative models to learn\nunderlying data distributions and produce realistic, privacy-preserving\nsamples. Although this area has seen growing attention, most existing surveys\nfocus narrowly on specific methods (e.g., GANs or privacy-enhancing\ntechniques), lacking a unified and comprehensive view that integrates recent\nadvances such as diffusion models and large language models (LLMs).\n  In this survey, we present a structured and in-depth review of synthetic\ntabular data generation methods. Specifically, the survey is organized into\nthree core components: (1) Background, which covers the overall generation\npipeline, including problem definitions, synthetic tabular data generation\nmethods, post processing, and evaluation; (2) Generation Methods, where we\ncategorize existing approaches into traditional generation methods, diffusion\nmodel methods, and LLM-based methods, and compare them in terms of\narchitecture, generation quality, and applicability; and (3) Applications and\nChallenges, which summarizes practical use cases, highlights common datasets,\nand discusses open challenges such as heterogeneity, data fidelity, and privacy\nprotection.\n  This survey aims to provide researchers and practitioners with a holistic\nunderstanding of the field and to highlight key directions for future work in\nsynthetic tabular data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data is one of the most prevalent and important data formats in\nreal-world applications such as healthcare, finance, and education. However,\nits effective use in machine learning is often constrained by data scarcity,\nprivacy concerns, and class imbalance. Synthetic tabular data generation has\nemerged as a powerful solution, leveraging generative models to learn\nunderlying data distributions and produce realistic, privacy-preserving\nsamples. Although this area has seen growing attention, most existing surveys\nfocus narrowly on specific methods (e.g., GANs or privacy-enhancing\ntechniques), lacking a unified and comprehensive view that integrates recent\nadvances such as diffusion models and large language models (LLMs).\n  In this survey, we present a structured and in-depth review of synthetic\ntabular data generation methods. Specifically, the survey is organized into\nthree core components: (1) Background, which covers the overall generation\npipeline, including problem definitions, synthetic tabular data generation\nmethods, post processing, and evaluation; (2) Generation Methods, where we\ncategorize existing approaches into traditional generation methods, diffusion\nmodel methods, and LLM-based methods, and compare them in terms of\narchitecture, generation quality, and applicability; and (3) Applications and\nChallenges, which summarizes practical use cases, highlights common datasets,\nand discusses open challenges such as heterogeneity, data fidelity, and privacy\nprotection.\n  This survey aims to provide researchers and practitioners with a holistic\nunderstanding of the field and to highlight key directions for future work in\nsynthetic tabular data generation."
                },
                "authors": [
                    {
                        "name": "Ruxue Shi"
                    },
                    {
                        "name": "Yili Wang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16506v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12753v1",
                "updated": "2025-07-17T03:14:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    3,
                    14,
                    37,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T03:14:37Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    3,
                    14,
                    37,
                    3,
                    198,
                    0
                ],
                "title": "osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps\n  and Large Language Models Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps\n  and Large Language Models Reasoning"
                },
                "summary": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features, achieving a high level of detail and\nguiding robots to find objects specified by open-vocabulary language queries.\nWhile the issue of scalability for such approaches has received some attention,\nanother fundamental problem is that high-detail object mapping quickly becomes\noutdated, as objects get moved around a lot. In this work, we develop a mapping\nand navigation system for object-goal navigation that, from the ground up,\nconsiders the possibilities that a queried object can have moved, or may not be\nmapped at all. Instead of striving for high-fidelity mapping detail, we\nconsider that the main purpose of a map is to provide environment grounding and\ncontext, which we combine with the semantic priors of LLMs to reason about\nobject locations and deploy an active, online approach to navigate to the\nobjects. Through simulated and real-world experiments we find that our approach\ntends to have higher retrieval success at shorter path lengths for static\nobjects and by far outperforms prior approaches in cases of dynamic or unmapped\nobject queries. We provide our code and dataset at:\nhttps://anonymous.4open.science/r/osmAG-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features, achieving a high level of detail and\nguiding robots to find objects specified by open-vocabulary language queries.\nWhile the issue of scalability for such approaches has received some attention,\nanother fundamental problem is that high-detail object mapping quickly becomes\noutdated, as objects get moved around a lot. In this work, we develop a mapping\nand navigation system for object-goal navigation that, from the ground up,\nconsiders the possibilities that a queried object can have moved, or may not be\nmapped at all. Instead of striving for high-fidelity mapping detail, we\nconsider that the main purpose of a map is to provide environment grounding and\ncontext, which we combine with the semantic priors of LLMs to reason about\nobject locations and deploy an active, online approach to navigate to the\nobjects. Through simulated and real-world experiments we find that our approach\ntends to have higher retrieval success at shorter path lengths for static\nobjects and by far outperforms prior approaches in cases of dynamic or unmapped\nobject queries. We provide our code and dataset at:\nhttps://anonymous.4open.science/r/osmAG-LLM."
                },
                "authors": [
                    {
                        "name": "Fujing Xie"
                    },
                    {
                        "name": "Sren Schwertfeger"
                    },
                    {
                        "name": "Hermann Blum"
                    }
                ],
                "author_detail": {
                    "name": "Hermann Blum"
                },
                "author": "Hermann Blum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12744v1",
                "updated": "2025-07-17T02:59:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    2,
                    59,
                    35,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T02:59:35Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    2,
                    59,
                    35,
                    3,
                    198,
                    0
                ],
                "title": "ASC-SW: Atrous strip convolution network with sliding windows for\n  visual-assisted map navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASC-SW: Atrous strip convolution network with sliding windows for\n  visual-assisted map navigation"
                },
                "summary": "With the rapid development of lightweight visual neural network\narchitectures, traditional high-performance vision models have undergone\nsignificant compression, greatly improving their computational efficiency and\nenergy consumption ratio. This makes them feasible for deployment on\nresource-constrained edge computing devices. We propose a visual-assisted\nnavigation framework called Atrous Strip Convolution-Sliding Window (ASC-SW),\nwhich leverages a depth camera and a lightweight visual neural network to\nassist map-based mobile robot navigation. This framework compensates for the\ninability of traditional light detection and range (LiDAR) sensors to detect\nground-level obstacles such as ground-level wires. We introduce a lightweight\nand efficient segmentation model, Atrous Strip Convolution Network (ASCnet),\nfor detecting deformable linear objects (DLOs). MobileNetV2 is used as the\nbackbone network, and Atrous Strip Convolution Spatial Pyramid Pooling (ASCSPP)\nis designed to extract DLO features more effectively. Atrous Strip Convolution\nis integrated into ASCSPP to accurately identify the linear structure of DLOs\nwith low computational cost. Additionally, a Sliding Window (SW)\npost-processing module is proposed to denoise the output in complex\nenvironments, improving recognition accuracy. Our method strikes a balance\nbetween inference speed and segmentation performance. It achieves a mean\nIntersection over Union (Miou) score of 75.3% on a self-built dataset and\nreaches 9.3 FPS inference speed on the Jetson Orin Nano edge device. Overall,\nour approach outperforms existing DLO detection models and has been\nsuccessfully validated on a physical robotic platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of lightweight visual neural network\narchitectures, traditional high-performance vision models have undergone\nsignificant compression, greatly improving their computational efficiency and\nenergy consumption ratio. This makes them feasible for deployment on\nresource-constrained edge computing devices. We propose a visual-assisted\nnavigation framework called Atrous Strip Convolution-Sliding Window (ASC-SW),\nwhich leverages a depth camera and a lightweight visual neural network to\nassist map-based mobile robot navigation. This framework compensates for the\ninability of traditional light detection and range (LiDAR) sensors to detect\nground-level obstacles such as ground-level wires. We introduce a lightweight\nand efficient segmentation model, Atrous Strip Convolution Network (ASCnet),\nfor detecting deformable linear objects (DLOs). MobileNetV2 is used as the\nbackbone network, and Atrous Strip Convolution Spatial Pyramid Pooling (ASCSPP)\nis designed to extract DLO features more effectively. Atrous Strip Convolution\nis integrated into ASCSPP to accurately identify the linear structure of DLOs\nwith low computational cost. Additionally, a Sliding Window (SW)\npost-processing module is proposed to denoise the output in complex\nenvironments, improving recognition accuracy. Our method strikes a balance\nbetween inference speed and segmentation performance. It achieves a mean\nIntersection over Union (Miou) score of 75.3% on a self-built dataset and\nreaches 9.3 FPS inference speed on the Jetson Orin Nano edge device. Overall,\nour approach outperforms existing DLO detection models and has been\nsuccessfully validated on a physical robotic platform."
                },
                "authors": [
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Fan Zhu"
                    },
                    {
                        "name": "Yaoyu Zhuang Zhinan Chen Jiefeng Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaoyu Zhuang Zhinan Chen Jiefeng Tang"
                },
                "author": "Yaoyu Zhuang Zhinan Chen Jiefeng Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15779v2",
                "updated": "2025-07-17T02:52:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    2,
                    52,
                    37,
                    3,
                    198,
                    0
                ],
                "published": "2025-03-20T01:41:28Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    1,
                    41,
                    28,
                    3,
                    79,
                    0
                ],
                "title": "Learning Universal Human Mobility Patterns with a Foundation Model for\n  Cross-domain Data Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Universal Human Mobility Patterns with a Foundation Model for\n  Cross-domain Data Fusion"
                },
                "summary": "Human mobility modeling is critical for urban planning and transportation\nmanagement, yet existing approaches often lack the integration capabilities\nneeded to handle diverse data sources. We present a foundation model framework\nfor universal human mobility patterns that leverages cross-domain data fusion\nand large language models to address these limitations. Our approach integrates\nmulti-modal data of distinct nature and spatio-temporal resolution, including\ngeographical, mobility, socio-demographic, and traffic information, to\nconstruct a privacy-preserving and semantically enriched human travel\ntrajectory dataset. Our framework demonstrates adaptability through domain\ntransfer techniques that ensure transferability across diverse urban contexts,\nas evidenced in case studies of Los Angeles (LA) and Egypt. The framework\nemploys LLMs for semantic enrichment of trajectory data, enabling comprehensive\nunderstanding of mobility patterns. Quantitative evaluation shows that our\ngenerated synthetic dataset accurately reproduces mobility patterns observed in\nempirical data. The practical utility of this foundation model approach is\ndemonstrated through large-scale traffic simulations for LA County, where\nresults align well with observed traffic data. On California's I-405 corridor,\nthe simulation yields a Mean Absolute Percentage Error of 5.85% for traffic\nvolume and 4.36% for speed compared to Caltrans PeMS observations, illustrating\nthe framework's potential for intelligent transportation systems and urban\nmobility applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human mobility modeling is critical for urban planning and transportation\nmanagement, yet existing approaches often lack the integration capabilities\nneeded to handle diverse data sources. We present a foundation model framework\nfor universal human mobility patterns that leverages cross-domain data fusion\nand large language models to address these limitations. Our approach integrates\nmulti-modal data of distinct nature and spatio-temporal resolution, including\ngeographical, mobility, socio-demographic, and traffic information, to\nconstruct a privacy-preserving and semantically enriched human travel\ntrajectory dataset. Our framework demonstrates adaptability through domain\ntransfer techniques that ensure transferability across diverse urban contexts,\nas evidenced in case studies of Los Angeles (LA) and Egypt. The framework\nemploys LLMs for semantic enrichment of trajectory data, enabling comprehensive\nunderstanding of mobility patterns. Quantitative evaluation shows that our\ngenerated synthetic dataset accurately reproduces mobility patterns observed in\nempirical data. The practical utility of this foundation model approach is\ndemonstrated through large-scale traffic simulations for LA County, where\nresults align well with observed traffic data. On California's I-405 corridor,\nthe simulation yields a Mean Absolute Percentage Error of 5.85% for traffic\nvolume and 4.36% for speed compared to Caltrans PeMS observations, illustrating\nthe framework's potential for intelligent transportation systems and urban\nmobility applications."
                },
                "authors": [
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Xishun Liao"
                    },
                    {
                        "name": "Yifan Liu"
                    },
                    {
                        "name": "Qinhua Jiang"
                    },
                    {
                        "name": "Chris Stanford"
                    },
                    {
                        "name": "Shangqing Cao"
                    },
                    {
                        "name": "Jiaqi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Ma"
                },
                "author": "Jiaqi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12741v1",
                "updated": "2025-07-17T02:49:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    2,
                    49,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T02:49:43Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    2,
                    49,
                    43,
                    3,
                    198,
                    0
                ],
                "title": "Public Evaluation on Potential Social Impacts of Fully Autonomous\n  Cybernetic Avatars for Physical Support in Daily-Life Environments:\n  Large-Scale Demonstration and Survey at Avatar Land",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public Evaluation on Potential Social Impacts of Fully Autonomous\n  Cybernetic Avatars for Physical Support in Daily-Life Environments:\n  Large-Scale Demonstration and Survey at Avatar Land"
                },
                "summary": "Cybernetic avatars (CAs) are key components of an avatar-symbiotic society,\nenabling individuals to overcome physical limitations through virtual agents\nand robotic assistants. While semi-autonomous CAs intermittently require human\nteleoperation and supervision, the deployment of fully autonomous CAs remains a\nchallenge. This study evaluates public perception and potential social impacts\nof fully autonomous CAs for physical support in daily life. To this end, we\nconducted a large-scale demonstration and survey during Avatar Land, a 19-day\npublic event in Osaka, Japan, where fully autonomous robotic CAs, alongside\nsemi-autonomous CAs, performed daily object retrieval tasks. Specifically, we\nanalyzed responses from 2,285 visitors who engaged with various CAs, including\na subset of 333 participants who interacted with fully autonomous CAs and\nshared their perceptions and concerns through a survey questionnaire. The\nsurvey results indicate interest in CAs for physical support in daily life and\nat work. However, concerns were raised regarding task execution reliability. In\ncontrast, cost and human-like interaction were not dominant concerns. Project\npage: https://lotfielhafi.github.io/FACA-Survey/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybernetic avatars (CAs) are key components of an avatar-symbiotic society,\nenabling individuals to overcome physical limitations through virtual agents\nand robotic assistants. While semi-autonomous CAs intermittently require human\nteleoperation and supervision, the deployment of fully autonomous CAs remains a\nchallenge. This study evaluates public perception and potential social impacts\nof fully autonomous CAs for physical support in daily life. To this end, we\nconducted a large-scale demonstration and survey during Avatar Land, a 19-day\npublic event in Osaka, Japan, where fully autonomous robotic CAs, alongside\nsemi-autonomous CAs, performed daily object retrieval tasks. Specifically, we\nanalyzed responses from 2,285 visitors who engaged with various CAs, including\na subset of 333 participants who interacted with fully autonomous CAs and\nshared their perceptions and concerns through a survey questionnaire. The\nsurvey results indicate interest in CAs for physical support in daily life and\nat work. However, concerns were raised regarding task execution reliability. In\ncontrast, cost and human-like interaction were not dominant concerns. Project\npage: https://lotfielhafi.github.io/FACA-Survey/."
                },
                "authors": [
                    {
                        "name": "Lotfi El Hafi"
                    },
                    {
                        "name": "Kazuma Onishi"
                    },
                    {
                        "name": "Shoichi Hasegawa"
                    },
                    {
                        "name": "Akira Oyama"
                    },
                    {
                        "name": "Tomochika Ishikawa"
                    },
                    {
                        "name": "Masashi Osada"
                    },
                    {
                        "name": "Carl Tornberg"
                    },
                    {
                        "name": "Ryoma Kado"
                    },
                    {
                        "name": "Kento Murata"
                    },
                    {
                        "name": "Saki Hashimoto"
                    },
                    {
                        "name": "Sebastian Carrera Villalobos"
                    },
                    {
                        "name": "Akira Taniguchi"
                    },
                    {
                        "name": "Gustavo Alfonso Garcia Ricardez"
                    },
                    {
                        "name": "Yoshinobu Hagiwara"
                    },
                    {
                        "name": "Tatsuya Aoki"
                    },
                    {
                        "name": "Kensuke Iwata"
                    },
                    {
                        "name": "Takato Horii"
                    },
                    {
                        "name": "Yukiko Horikawa"
                    },
                    {
                        "name": "Takahiro Miyashita"
                    },
                    {
                        "name": "Tadahiro Taniguchi"
                    },
                    {
                        "name": "Hiroshi Ishiguro"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshi Ishiguro"
                },
                "author": "Hiroshi Ishiguro",
                "arxiv_comment": "Accepted for presentation at the 2025 IEEE International Conference\n  on Advanced Robotics and its Social Impacts (ARSO), Osaka, Japan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12000v2",
                "updated": "2025-07-17T02:34:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    2,
                    34,
                    42,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T07:55:06Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    55,
                    6,
                    2,
                    197,
                    0
                ],
                "title": "DSSD: Efficient Edge-Device LLM Deployment and Collaborative Inference\n  via Distributed Split Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSSD: Efficient Edge-Device LLM Deployment and Collaborative Inference\n  via Distributed Split Speculative Decoding"
                },
                "summary": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing"
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12724v1",
                "updated": "2025-07-17T02:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    2,
                    2,
                    54,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T02:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    2,
                    2,
                    54,
                    3,
                    198,
                    0
                ],
                "title": "TransEvalnia: Reasoning-based Evaluation and Ranking of Translations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransEvalnia: Reasoning-based Evaluation and Ranking of Translations"
                },
                "summary": "We present TransEvalnia, a prompting-based translation evaluation and ranking\nsystem that uses reasoning in performing its evaluations and ranking. This\nsystem presents fine-grained evaluations based on a subset of the\nMultidimensional Quality Metrics (https://themqm.org/), returns an assessment\nof which translation it deems the best, and provides numerical scores for the\nvarious dimensions and for the overall translation. We show that TransEvalnia\nperforms as well as or better than the state-of-the-art MT-Ranker (Moosa et al.\n2024) on our own English-Japanese data as well as several language pairs from\nvarious WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and\nQwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations\nreturned are deemed highly acceptable to human raters, and that the scores\nassigned to the translations by Sonnet, as well as other LLMs, correlate well\nwith scores assigned by the human raters. We also note the sensitivity of our\nsystem -- as well as MT-Ranker -- to the order in which the translations are\npresented, and we propose methods to address this position bias. All data,\nincluding the system's evaluation and reasoning, human assessments, as well as\ncode is released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TransEvalnia, a prompting-based translation evaluation and ranking\nsystem that uses reasoning in performing its evaluations and ranking. This\nsystem presents fine-grained evaluations based on a subset of the\nMultidimensional Quality Metrics (https://themqm.org/), returns an assessment\nof which translation it deems the best, and provides numerical scores for the\nvarious dimensions and for the overall translation. We show that TransEvalnia\nperforms as well as or better than the state-of-the-art MT-Ranker (Moosa et al.\n2024) on our own English-Japanese data as well as several language pairs from\nvarious WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and\nQwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations\nreturned are deemed highly acceptable to human raters, and that the scores\nassigned to the translations by Sonnet, as well as other LLMs, correlate well\nwith scores assigned by the human raters. We also note the sensitivity of our\nsystem -- as well as MT-Ranker -- to the order in which the translations are\npresented, and we propose methods to address this position bias. All data,\nincluding the system's evaluation and reasoning, human assessments, as well as\ncode is released."
                },
                "authors": [
                    {
                        "name": "Richard Sproat"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Llion Jones"
                    }
                ],
                "author_detail": {
                    "name": "Llion Jones"
                },
                "author": "Llion Jones",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12716v1",
                "updated": "2025-07-17T01:47:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    1,
                    47,
                    37,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T01:47:37Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    1,
                    47,
                    37,
                    3,
                    198,
                    0
                ],
                "title": "MoistureMapper: An Autonomous Mobile Robot for High-Resolution Soil\n  Moisture Mapping at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoistureMapper: An Autonomous Mobile Robot for High-Resolution Soil\n  Moisture Mapping at Scale"
                },
                "summary": "Soil moisture is a quantity of interest in many application areas including\nagriculture and climate modeling. Existing methods are not suitable for scale\napplications due to large deployment costs in high-resolution sensing\napplications such as for variable irrigation. In this work, we design, build\nand field deploy an autonomous mobile robot, MoistureMapper, for soil moisture\nsensing. The robot is equipped with Time Domain Reflectometry (TDR) sensors and\na direct push drill mechanism for deploying the sensor to measure volumetric\nwater content in the soil. Additionally, we implement and evaluate multiple\nadaptive sampling strategies based on a Gaussian Process based modeling to\nbuild a spatial mapping of moisture distribution in the soil. We present\nresults from large scale computational simulations and proof-of-concept\ndeployment on the field. The adaptive sampling approach outperforms a greedy\nbenchmark approach and results in up to 30\\% reduction in travel distance and\n5\\% reduction in variance in the reconstructed moisture maps. Link to video\nshowing field experiments: https://youtu.be/S4bJ4tRzObg",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soil moisture is a quantity of interest in many application areas including\nagriculture and climate modeling. Existing methods are not suitable for scale\napplications due to large deployment costs in high-resolution sensing\napplications such as for variable irrigation. In this work, we design, build\nand field deploy an autonomous mobile robot, MoistureMapper, for soil moisture\nsensing. The robot is equipped with Time Domain Reflectometry (TDR) sensors and\na direct push drill mechanism for deploying the sensor to measure volumetric\nwater content in the soil. Additionally, we implement and evaluate multiple\nadaptive sampling strategies based on a Gaussian Process based modeling to\nbuild a spatial mapping of moisture distribution in the soil. We present\nresults from large scale computational simulations and proof-of-concept\ndeployment on the field. The adaptive sampling approach outperforms a greedy\nbenchmark approach and results in up to 30\\% reduction in travel distance and\n5\\% reduction in variance in the reconstructed moisture maps. Link to video\nshowing field experiments: https://youtu.be/S4bJ4tRzObg"
                },
                "authors": [
                    {
                        "name": "Nathaniel Rose"
                    },
                    {
                        "name": "Hannah Chuang"
                    },
                    {
                        "name": "Manuel A Andrade-Rodriguez"
                    },
                    {
                        "name": "Rishi Parashar"
                    },
                    {
                        "name": "Dani Or"
                    },
                    {
                        "name": "Parikshit Maini"
                    }
                ],
                "author_detail": {
                    "name": "Parikshit Maini"
                },
                "author": "Parikshit Maini",
                "arxiv_comment": "Accepted by 2025 IEEE 21st International Conference on Automation\n  Science and Engineering. 8 pages, 10 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12347v2",
                "updated": "2025-07-17T01:39:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    1,
                    39,
                    41,
                    3,
                    198,
                    0
                ],
                "published": "2025-03-16T04:00:32Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    4,
                    0,
                    32,
                    6,
                    75,
                    0
                ],
                "title": "Synthesizing Privacy-Preserving Text Data via Finetuning without\n  Finetuning Billion-Scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Privacy-Preserving Text Data via Finetuning without\n  Finetuning Billion-Scale LLMs"
                },
                "summary": "Synthetic data offers a promising path to train models while preserving data\nprivacy. Differentially private (DP) finetuning of large language models (LLMs)\nas data generator is effective, but is impractical when computation resources\nare limited. Meanwhile, prompt-based methods such as private evolution depend\nheavily on the manual prompts, and ineffectively use private information in\ntheir iterative data selection process. To overcome these limitations, we\npropose CTCL (Data Synthesis with ConTrollability and CLustering), a novel\nframework for generating privacy-preserving synthetic data without extensive\nprompt engineering or billion-scale LLM finetuning. CTCL pretrains a\nlightweight 140M conditional generator and a clustering-based topic model on\nlarge-scale public data. To further adapt to the private domain, the generator\nis DP finetuned on private data for fine-grained textual information, while the\ntopic model extracts a DP histogram representing distributional information.\nThe DP generator then samples according to the DP histogram to synthesize a\ndesired number of data examples. Evaluation across five diverse domains\ndemonstrates the effectiveness of our framework, particularly in the strong\nprivacy regime. Systematic ablation validates the design of each framework\ncomponent and highlights the scalability of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data offers a promising path to train models while preserving data\nprivacy. Differentially private (DP) finetuning of large language models (LLMs)\nas data generator is effective, but is impractical when computation resources\nare limited. Meanwhile, prompt-based methods such as private evolution depend\nheavily on the manual prompts, and ineffectively use private information in\ntheir iterative data selection process. To overcome these limitations, we\npropose CTCL (Data Synthesis with ConTrollability and CLustering), a novel\nframework for generating privacy-preserving synthetic data without extensive\nprompt engineering or billion-scale LLM finetuning. CTCL pretrains a\nlightweight 140M conditional generator and a clustering-based topic model on\nlarge-scale public data. To further adapt to the private domain, the generator\nis DP finetuned on private data for fine-grained textual information, while the\ntopic model extracts a DP histogram representing distributional information.\nThe DP generator then samples according to the DP histogram to synthesize a\ndesired number of data examples. Evaluation across five diverse domains\ndemonstrates the effectiveness of our framework, particularly in the strong\nprivacy regime. Systematic ablation validates the design of each framework\ncomponent and highlights the scalability of our approach."
                },
                "authors": [
                    {
                        "name": "Bowen Tan"
                    },
                    {
                        "name": "Zheng Xu"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Shanshan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shanshan Wu"
                },
                "author": "Shanshan Wu",
                "arxiv_comment": "Code available at https://github.com/tanyuqian/synthetic-private-data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10646v2",
                "updated": "2025-07-17T01:38:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    1,
                    38,
                    49,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-14T17:19:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    19,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based\n  Code Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based\n  Code Assistance"
                },
                "summary": "Programming assistants powered by large language models have transformed\nsoftware development, yet most benchmarks focus narrowly on code generation\ntasks. Recent efforts like InfiBench and StackEval attempt to address this gap\nusing Stack Overflow data but remain limited to single-turn interactions in\nisolated contexts, require significant manual curation, and fail to represent\ncomplete project environments. We introduce CodeAssistBench (CAB), the first\nbenchmark framework for evaluating multi-turn programming assistance in\nrealistic settings that address real-world questions about actual codebases.\nUnlike existing programming Q&A benchmarks, CAB automatically generates\nscalable datasets from question-related GitHub issues using configurable\nparameters (e.g., repository creation date, star count, programming languages),\nand includes automatic containerization of codebases for evaluation. It then\nevaluates models through simulated users in these containerized environments\nwith full codebase access. Using this framework, we constructed a test set of\n3,286 real-world programming questions across 231 repositories, spanning seven\nprogramming languages and diverse problem domains. Our evaluation of leading\nLLMs reveals a substantial capability gap: while models perform well on Stack\nOverflow questions with success rates of 70-83%, they resolve only up to 16.49%\nof CAB's recent issues. This discrepancy highlights the challenges of providing\nassistance in complex, project-specific contexts versus answering standalone\nquestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming assistants powered by large language models have transformed\nsoftware development, yet most benchmarks focus narrowly on code generation\ntasks. Recent efforts like InfiBench and StackEval attempt to address this gap\nusing Stack Overflow data but remain limited to single-turn interactions in\nisolated contexts, require significant manual curation, and fail to represent\ncomplete project environments. We introduce CodeAssistBench (CAB), the first\nbenchmark framework for evaluating multi-turn programming assistance in\nrealistic settings that address real-world questions about actual codebases.\nUnlike existing programming Q&A benchmarks, CAB automatically generates\nscalable datasets from question-related GitHub issues using configurable\nparameters (e.g., repository creation date, star count, programming languages),\nand includes automatic containerization of codebases for evaluation. It then\nevaluates models through simulated users in these containerized environments\nwith full codebase access. Using this framework, we constructed a test set of\n3,286 real-world programming questions across 231 repositories, spanning seven\nprogramming languages and diverse problem domains. Our evaluation of leading\nLLMs reveals a substantial capability gap: while models perform well on Stack\nOverflow questions with success rates of 70-83%, they resolve only up to 16.49%\nof CAB's recent issues. This discrepancy highlights the challenges of providing\nassistance in complex, project-specific contexts versus answering standalone\nquestions."
                },
                "authors": [
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Shweta Garg"
                    },
                    {
                        "name": "Baishakhi Ray"
                    },
                    {
                        "name": "Varun Kumar"
                    },
                    {
                        "name": "Anoop Deoras"
                    }
                ],
                "author_detail": {
                    "name": "Anoop Deoras"
                },
                "author": "Anoop Deoras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05028v2",
                "updated": "2025-07-17T01:36:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    1,
                    36,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2024-09-08T08:46:05Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    46,
                    5,
                    6,
                    252,
                    0
                ],
                "title": "GUI Test Migration via Abstraction and Concretization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI Test Migration via Abstraction and Concretization"
                },
                "summary": "GUI test migration aims to produce test cases with events and assertions to\ntest specific functionalities of a target app. Existing migration approaches\ntypically focus on the widget-mapping paradigm that maps widgets from source\napps to target apps. However, since different apps may implement the same\nfunctionality in different ways, direct mapping may result in incomplete or\nbuggy test cases, thus significantly impacting the effectiveness of testing\ntarget functionality and the practical applicability of migration approaches.\n  In this paper, we propose a new migration paradigm (i.e., the\nabstraction-concretization paradigm) that first abstracts the test logic for\nthe target functionality and then utilizes this logic to generate the concrete\nGUI test case. Furthermore, we introduce MACdroid, the first approach that\nmigrates GUI test cases based on this paradigm. Specifically, we propose an\nabstraction technique that utilizes source test cases from source apps\ntargeting the same functionality to extract a general test logic for that\nfunctionality. Then, we propose a concretization technique that utilizes the\ngeneral test logic to guide an LLM in generating the corresponding GUI test\ncase (including events and assertions) for the target app. We evaluate MACdroid\non two widely-used datasets (including 31 apps, 34 functionalities, and 123\ntest cases). On the FrUITeR dataset, the test cases generated by MACdroid\nsuccessfully test 64% of the target functionalities, improving the baselines by\n191%. On the Lin dataset, MACdroid successfully tests 75% of the target\nfunctionalities, outperforming the baselines by 42%. These results underscore\nthe effectiveness of MACdroid in GUI test migration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI test migration aims to produce test cases with events and assertions to\ntest specific functionalities of a target app. Existing migration approaches\ntypically focus on the widget-mapping paradigm that maps widgets from source\napps to target apps. However, since different apps may implement the same\nfunctionality in different ways, direct mapping may result in incomplete or\nbuggy test cases, thus significantly impacting the effectiveness of testing\ntarget functionality and the practical applicability of migration approaches.\n  In this paper, we propose a new migration paradigm (i.e., the\nabstraction-concretization paradigm) that first abstracts the test logic for\nthe target functionality and then utilizes this logic to generate the concrete\nGUI test case. Furthermore, we introduce MACdroid, the first approach that\nmigrates GUI test cases based on this paradigm. Specifically, we propose an\nabstraction technique that utilizes source test cases from source apps\ntargeting the same functionality to extract a general test logic for that\nfunctionality. Then, we propose a concretization technique that utilizes the\ngeneral test logic to guide an LLM in generating the corresponding GUI test\ncase (including events and assertions) for the target app. We evaluate MACdroid\non two widely-used datasets (including 31 apps, 34 functionalities, and 123\ntest cases). On the FrUITeR dataset, the test cases generated by MACdroid\nsuccessfully test 64% of the target functionalities, improving the baselines by\n191%. On the Lin dataset, MACdroid successfully tests 75% of the target\nfunctionalities, outperforming the baselines by 42%. These results underscore\nthe effectiveness of MACdroid in GUI test migration."
                },
                "authors": [
                    {
                        "name": "Yakun Zhang"
                    },
                    {
                        "name": "Chen Liu"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Jin Song Dong"
                    },
                    {
                        "name": "Dan Hao"
                    },
                    {
                        "name": "Lu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Zhang"
                },
                "author": "Lu Zhang",
                "arxiv_doi": "10.1145/3726525",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726525",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.05028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted for publication in ACM Transactions on\n  Software Engineering and Methodology (TOSEM) in 2025. The official\n  publication link is: https://dl.acm.org/doi/10.1145/3726525",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02946v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02946v6",
                "updated": "2025-07-17T01:19:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    1,
                    19,
                    42,
                    3,
                    198,
                    0
                ],
                "published": "2024-08-06T04:14:29Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    4,
                    14,
                    29,
                    1,
                    219,
                    0
                ],
                "title": "Scaling Trends for Data Poisoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Trends for Data Poisoning in LLMs"
                },
                "summary": "LLMs produce harmful and undesirable behavior when trained on datasets\ncontaining even a small fraction of poisoned data. We demonstrate that GPT\nmodels remain vulnerable to fine-tuning on poisoned data, even when safeguarded\nby moderation systems. Given the persistence of data poisoning vulnerabilities\nin today's most capable models, this paper investigates whether these risks\nincrease with model scaling. We evaluate three threat models -- malicious\nfine-tuning, imperfect data curation, and intentional data contamination --\nacross 24 frontier LLMs ranging from 1.5 to 72 billion parameters. Our\nexperiments reveal that larger LLMs are significantly more susceptible to data\npoisoning, learning harmful behaviors from even minimal exposure to harmful\ndata more quickly than smaller models. These findings underscore the need for\nleading AI companies to thoroughly red team fine-tuning APIs before public\nrelease and to develop more robust safeguards against data poisoning,\nparticularly as models continue to scale in size and capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs produce harmful and undesirable behavior when trained on datasets\ncontaining even a small fraction of poisoned data. We demonstrate that GPT\nmodels remain vulnerable to fine-tuning on poisoned data, even when safeguarded\nby moderation systems. Given the persistence of data poisoning vulnerabilities\nin today's most capable models, this paper investigates whether these risks\nincrease with model scaling. We evaluate three threat models -- malicious\nfine-tuning, imperfect data curation, and intentional data contamination --\nacross 24 frontier LLMs ranging from 1.5 to 72 billion parameters. Our\nexperiments reveal that larger LLMs are significantly more susceptible to data\npoisoning, learning harmful behaviors from even minimal exposure to harmful\ndata more quickly than smaller models. These findings underscore the need for\nleading AI companies to thoroughly red team fine-tuning APIs before public\nrelease and to develop more robust safeguards against data poisoning,\nparticularly as models continue to scale in size and capability."
                },
                "authors": [
                    {
                        "name": "Dillon Bowen"
                    },
                    {
                        "name": "Brendan Murphy"
                    },
                    {
                        "name": "Will Cai"
                    },
                    {
                        "name": "David Khachaturov"
                    },
                    {
                        "name": "Adam Gleave"
                    },
                    {
                        "name": "Kellin Pelrine"
                    }
                ],
                "author_detail": {
                    "name": "Kellin Pelrine"
                },
                "author": "Kellin Pelrine",
                "arxiv_comment": "This arXiv version of the paper originally included an initial\n  investigation of jailbreak-tuning, which can produce 60+ percentage point\n  increases in vulnerability elicitation compared with standard data poisoning.\n  Jailbreak-tuning has now been separated into a full independent paper, which\n  can be found at arXiv:2507.11630",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02946v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02946v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19232v2",
                "updated": "2025-07-17T01:08:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    1,
                    8,
                    34,
                    3,
                    198,
                    0
                ],
                "published": "2025-01-31T15:43:21Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    43,
                    21,
                    4,
                    31,
                    0
                ],
                "title": "LLM-RecG: A Semantic Bias-Aware Framework for Zero-Shot Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-RecG: A Semantic Bias-Aware Framework for Zero-Shot Sequential\n  Recommendation"
                },
                "summary": "Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without additional training or fine-tuning, addressing the\nlimitations of traditional models in sparse data environments. Recent\nadvancements in large language models (LLMs) have significantly enhanced ZCDSR\nby facilitating cross-domain knowledge transfer through rich, pretrained\nrepresentations. Despite this progress, domain semantic bias -- arising from\ndifferences in vocabulary and content focus between domains -- remains a\npersistent challenge, leading to misaligned item embeddings and reduced\ngeneralization across domains. To address this, we propose a novel semantic\nbias-aware framework that enhances LLM-based ZCDSR by improving cross-domain\nalignment at both the item and sequential levels. At the item level, we\nintroduce a generalization loss that aligns the embeddings of items across\ndomains (inter-domain compactness), while preserving the unique characteristics\nof each item within its own domain (intra-domain diversity). This ensures that\nitem embeddings can be transferred effectively between domains without\ncollapsing into overly generic or uniform representations. At the sequential\nlevel, we develop a method to transfer user behavioral patterns by clustering\nsource domain user sequences and applying attention-based aggregation during\ntarget domain inference. We dynamically adapt user embeddings to unseen\ndomains, enabling effective zero-shot recommendations without requiring\ntarget-domain interactions...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without additional training or fine-tuning, addressing the\nlimitations of traditional models in sparse data environments. Recent\nadvancements in large language models (LLMs) have significantly enhanced ZCDSR\nby facilitating cross-domain knowledge transfer through rich, pretrained\nrepresentations. Despite this progress, domain semantic bias -- arising from\ndifferences in vocabulary and content focus between domains -- remains a\npersistent challenge, leading to misaligned item embeddings and reduced\ngeneralization across domains. To address this, we propose a novel semantic\nbias-aware framework that enhances LLM-based ZCDSR by improving cross-domain\nalignment at both the item and sequential levels. At the item level, we\nintroduce a generalization loss that aligns the embeddings of items across\ndomains (inter-domain compactness), while preserving the unique characteristics\nof each item within its own domain (intra-domain diversity). This ensures that\nitem embeddings can be transferred effectively between domains without\ncollapsing into overly generic or uniform representations. At the sequential\nlevel, we develop a method to transfer user behavioral patterns by clustering\nsource domain user sequences and applying attention-based aggregation during\ntarget domain inference. We dynamically adapt user embeddings to unseen\ndomains, enabling effective zero-shot recommendations without requiring\ntarget-domain interactions..."
                },
                "authors": [
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Junting Wang"
                    },
                    {
                        "name": "Hari Sundaram"
                    },
                    {
                        "name": "Zhining Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhining Liu"
                },
                "author": "Zhining Liu",
                "arxiv_doi": "10.1145/3705328.3748077",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3705328.3748077",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.19232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, Recsys'25 Spotlight Oral",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16359v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16359v3",
                "updated": "2025-07-17T00:41:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    0,
                    41,
                    59,
                    3,
                    198,
                    0
                ],
                "published": "2025-06-19T14:35:23Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    14,
                    35,
                    23,
                    3,
                    170,
                    0
                ],
                "title": "SHREC: A Framework for Advancing Next-Generation Computational\n  Phenotyping with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHREC: A Framework for Advancing Next-Generation Computational\n  Phenotyping with Large Language Models"
                },
                "summary": "Objective: Computational phenotyping is a central informatics activity with\nresulting cohorts supporting a wide variety of applications. However, it is\ntime-intensive because of manual data review, limited automation, and\ndifficulties in adapting algorithms across sources. Since LLMs have\ndemonstrated promising capabilities for text classification, comprehension, and\ngeneration, we posit they will perform well at repetitive manual review tasks\ntraditionally performed by human experts. To support next-generation\ncomputational phenotyping methods, we developed SHREC, a framework for\ncomprehensive integration of LLMs into end-to-end phenotyping pipelines.\nMethods: We applied and tested the ability of three lightweight LLMs (Gemma2 27\nbillion, Mistral Small 24 billion, and Phi-4 14 billion) to classify concepts\nand phenotype patients using previously developed phenotypes for ARF\nrespiratory support therapies. Results: All models performed well on concept\nclassification, with the best model (Mistral) achieving an AUROC of 0.896\nacross all relevant concepts. For phenotyping, models demonstrated near-perfect\nspecificity for all phenotypes, and the top-performing model (Mistral) reached\nan average AUROC of 0.853 for single-therapy phenotypes, despite lower\nperformance on multi-therapy phenotypes. Conclusion: Current lightweight LLMs\ncan feasibly assist researchers with resource-intensive phenotyping tasks such\nas manual data review. There are several advantages of LLMs that support their\napplication to computational phenotyping, such as their ability to adapt to new\ntasks with prompt engineering alone and their ability to incorporate raw EHR\ndata. Future steps to advance next-generation phenotyping methods include\ndetermining optimal strategies for integrating biomedical data, exploring how\nLLMs reason, and advancing generative model methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: Computational phenotyping is a central informatics activity with\nresulting cohorts supporting a wide variety of applications. However, it is\ntime-intensive because of manual data review, limited automation, and\ndifficulties in adapting algorithms across sources. Since LLMs have\ndemonstrated promising capabilities for text classification, comprehension, and\ngeneration, we posit they will perform well at repetitive manual review tasks\ntraditionally performed by human experts. To support next-generation\ncomputational phenotyping methods, we developed SHREC, a framework for\ncomprehensive integration of LLMs into end-to-end phenotyping pipelines.\nMethods: We applied and tested the ability of three lightweight LLMs (Gemma2 27\nbillion, Mistral Small 24 billion, and Phi-4 14 billion) to classify concepts\nand phenotype patients using previously developed phenotypes for ARF\nrespiratory support therapies. Results: All models performed well on concept\nclassification, with the best model (Mistral) achieving an AUROC of 0.896\nacross all relevant concepts. For phenotyping, models demonstrated near-perfect\nspecificity for all phenotypes, and the top-performing model (Mistral) reached\nan average AUROC of 0.853 for single-therapy phenotypes, despite lower\nperformance on multi-therapy phenotypes. Conclusion: Current lightweight LLMs\ncan feasibly assist researchers with resource-intensive phenotyping tasks such\nas manual data review. There are several advantages of LLMs that support their\napplication to computational phenotyping, such as their ability to adapt to new\ntasks with prompt engineering alone and their ability to incorporate raw EHR\ndata. Future steps to advance next-generation phenotyping methods include\ndetermining optimal strategies for integrating biomedical data, exploring how\nLLMs reason, and advancing generative model methods."
                },
                "authors": [
                    {
                        "name": "Sarah Pungitore"
                    },
                    {
                        "name": "Shashank Yadav"
                    },
                    {
                        "name": "Molly Douglas"
                    },
                    {
                        "name": "Jarrod Mosier"
                    },
                    {
                        "name": "Vignesh Subbian"
                    }
                ],
                "author_detail": {
                    "name": "Vignesh Subbian"
                },
                "author": "Vignesh Subbian",
                "arxiv_comment": "Submitted to the npj Digital Medicine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16359v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16359v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]