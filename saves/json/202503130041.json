[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v1",
                "updated": "2025-03-11T17:30:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v3",
                "updated": "2025-03-11T16:35:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    59,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v3",
                "updated": "2025-03-11T13:13:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    },
                    {
                        "name": "Qihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qihua Zhou"
                },
                "author": "Qihua Zhou",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v1",
                "updated": "2025-03-11T13:10:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v5",
                "updated": "2025-03-11T09:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07545v1",
                "updated": "2025-03-10T17:12:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T17:12:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing, Predictions, and LLMs: Challenges and Open Problems"
                },
                "summary": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Rana Shahout"
                    }
                ],
                "author_detail": {
                    "name": "Rana Shahout"
                },
                "author": "Rana Shahout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07518v1",
                "updated": "2025-03-10T16:41:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:41:14Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "title": "TokenButler: Token Importance is Predictable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenButler: Token Importance is Predictable"
                },
                "summary": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler"
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Ahmed F AbouElhamayed"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07474v1",
                "updated": "2025-03-10T15:49:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v2",
                "updated": "2025-03-10T12:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    10,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v1",
                "updated": "2025-03-10T09:49:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07027v1",
                "updated": "2025-03-10T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer"
                },
                "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yirui Yuan"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v1",
                "updated": "2025-03-10T05:09:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v2",
                "updated": "2025-03-10T02:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    41,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v3",
                "updated": "2025-03-09T17:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    28,
                    6,
                    68,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v2",
                "updated": "2025-03-09T16:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    16,
                    14,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v1",
                "updated": "2025-03-09T12:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06545v1",
                "updated": "2025-03-09T10:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T10:31:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation"
                },
                "summary": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/JunyiWuCode/QuantCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06433v1",
                "updated": "2025-03-09T04:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T04:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seesaw: High-throughput LLM Inference via Model Re-sharding"
                },
                "summary": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine."
                },
                "authors": [
                    {
                        "name": "Qidong Su"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Muralidhar Andoorveedu"
                    },
                    {
                        "name": "Chenhao Jiang"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v3",
                "updated": "2025-03-09T02:19:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    19,
                    22,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v2",
                "updated": "2025-03-08T21:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    21,
                    55,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06302v1",
                "updated": "2025-03-08T18:30:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T18:30:54Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "title": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security"
                },
                "summary": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Dianwei Chen"
                    },
                    {
                        "name": "Xianfeng Yang"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v1",
                "updated": "2025-03-08T02:35:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v1",
                "updated": "2025-03-07T21:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v1",
                "updated": "2025-03-07T05:31:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04982v1",
                "updated": "2025-03-06T21:21:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:21:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression"
                },
                "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench."
                },
                "authors": [
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "This work has been accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04973v1",
                "updated": "2025-03-06T21:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:07:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning"
                },
                "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Fabio Petroni"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Müller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tamás Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tamás Jursonovics"
                },
                "author": "Tamás Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01323v1",
                "updated": "2025-03-03T09:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheQuant: Comprehensively Accelerated Diffusion Models"
                },
                "summary": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant ."
                },
                "authors": [
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Qingyi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyi Gu"
                },
                "author": "Qingyi Gu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01281v1",
                "updated": "2025-03-03T08:06:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T08:06:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System"
                },
                "summary": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Yaobin Wang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yingchen Song"
                    },
                    {
                        "name": "Huan Wu"
                    },
                    {
                        "name": "Qingfeng Wang"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v2",
                "updated": "2025-03-03T05:49:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    49,
                    41,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v1",
                "updated": "2025-03-02T18:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v2",
                "updated": "2025-03-02T14:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    14,
                    37,
                    53,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00695v1",
                "updated": "2025-03-02T02:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T02:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07295v2",
                "updated": "2025-03-02T01:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    1,
                    39,
                    57,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-09T16:21:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    21,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking"
                },
                "summary": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com."
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Rohan Gumaste"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Sasa Misailovic"
                    }
                ],
                "author_detail": {
                    "name": "Sasa Misailovic"
                },
                "author": "Sasa Misailovic",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00540v1",
                "updated": "2025-03-01T15:53:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T15:53:33Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "title": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval"
                },
                "summary": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models."
                },
                "authors": [
                    {
                        "name": "Shangzhe Di"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Bolin Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "Accepted to ICLR 2025. Code: https://github.com/Becomebright/ReKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00392v1",
                "updated": "2025-03-01T07:56:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T07:56:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving"
                },
                "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v6",
                "updated": "2025-03-01T05:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    5,
                    43,
                    19,
                    5,
                    60,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3706628.3708873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706628.3708873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00323v1",
                "updated": "2025-03-01T03:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T03:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLStore: Efficient Federated Learning Storage for non-training workloads"
                },
                "summary": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable."
                },
                "authors": [
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Samuel Fountain"
                    },
                    {
                        "name": "Ahmed M. Abdelmoniem"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "11 pages, 19 figures, 2 tables This paper has been accepted at the\n  The Eighth Annual Conference on Machine Learning and Systems (MLSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21079v1",
                "updated": "2025-02-28T14:11:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:11:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation"
                },
                "summary": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v1",
                "updated": "2025-02-27T23:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "Mingyuan, Jize, and Haozhen contributed equally, while Minjia,\n  Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15896v3",
                "updated": "2025-02-27T21:50:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    50,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2023-12-26T06:16:12Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    6,
                    16,
                    12,
                    1,
                    360,
                    0
                ],
                "title": "WWW: What, When, Where to Compute-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WWW: What, When, Where to Compute-in-Memory"
                },
                "summary": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Tanvi Sharma"
                    },
                    {
                        "name": "Mustafa Ali"
                    },
                    {
                        "name": "Indranil Chakraborty"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "added supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20547v1",
                "updated": "2025-02-27T21:42:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T21:42:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches"
                },
                "summary": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers."
                },
                "authors": [
                    {
                        "name": "Aurore Poirier"
                    },
                    {
                        "name": "Erven Rohou"
                    },
                    {
                        "name": "Manuel Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Serrano"
                },
                "arxiv_affiliation": "Inria - University of Côte d'Azur, France",
                "author": "Manuel Serrano",
                "arxiv_doi": "10.22152/programming-journal.org/2026/10/6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22152/programming-journal.org/2026/10/6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Art, Science, and Engineering of Programming, 2025, Vol. 10,\n  Issue 1, Article 6",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v2",
                "updated": "2025-02-27T06:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    39,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05783v1",
                "updated": "2025-02-26T21:03:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    21,
                    3,
                    2,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T21:03:02Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    21,
                    3,
                    2,
                    2,
                    57,
                    0
                ],
                "title": "Knowledge representation and scalable abstract reasoning for simulated\n  democracy in Unity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge representation and scalable abstract reasoning for simulated\n  democracy in Unity"
                },
                "summary": "We present a novel form of scalable knowledge representation about agents in\na simulated democracy, e-polis, where real users respond to social challenges\nassociated with democratic institutions, structured as Smart Spatial Types, a\nnew type of Smart Building that changes architectural form according to the\nphilosophical doctrine of a visitor. At the end of the game players vote on the\nSmart City that results from their collective choices. Our approach uses\ndeductive systems in an unusual way: by integrating a model of democracy with a\nmodel of a Smart City we are able to prove quality aspects of the simulated\ndemocracy in different urban and social settings, while adding ease and\nflexibility to the development. Second, we can infer and reason with abstract\nknowledge, which is a limitation of the Unity platform; third, our system\nenables real-time decision-making and adaptation of the game flow based on the\nplayer's abstract state, paving the road to explainability. Scalability is\nachieved by maintaining a dual-layer knowledge representation mechanism for\nreasoning about the simulated democracy that functions in a similar way to a\ntwo-level cache. The lower layer knows about the current state of the game by\ncontinually processing a high rate of events produced by the in-built physics\nengine of the Unity platform, e.g., it knows of the position of a player in\nspace, in terms of his coordinates x,y,z as well as their choices for each\nchallenge. The higher layer knows of easily-retrievable, user-defined abstract\nknowledge about current and historical states, e.g., it knows of the political\ndoctrine of a Smart Spatial Type, a player's philosophical doctrine, and the\ncollective philosophical doctrine of a community players with respect to\ncurrent social issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel form of scalable knowledge representation about agents in\na simulated democracy, e-polis, where real users respond to social challenges\nassociated with democratic institutions, structured as Smart Spatial Types, a\nnew type of Smart Building that changes architectural form according to the\nphilosophical doctrine of a visitor. At the end of the game players vote on the\nSmart City that results from their collective choices. Our approach uses\ndeductive systems in an unusual way: by integrating a model of democracy with a\nmodel of a Smart City we are able to prove quality aspects of the simulated\ndemocracy in different urban and social settings, while adding ease and\nflexibility to the development. Second, we can infer and reason with abstract\nknowledge, which is a limitation of the Unity platform; third, our system\nenables real-time decision-making and adaptation of the game flow based on the\nplayer's abstract state, paving the road to explainability. Scalability is\nachieved by maintaining a dual-layer knowledge representation mechanism for\nreasoning about the simulated democracy that functions in a similar way to a\ntwo-level cache. The lower layer knows about the current state of the game by\ncontinually processing a high rate of events produced by the in-built physics\nengine of the Unity platform, e.g., it knows of the position of a player in\nspace, in terms of his coordinates x,y,z as well as their choices for each\nchallenge. The higher layer knows of easily-retrievable, user-defined abstract\nknowledge about current and historical states, e.g., it knows of the political\ndoctrine of a Smart Spatial Type, a player's philosophical doctrine, and the\ncollective philosophical doctrine of a community players with respect to\ncurrent social issues."
                },
                "authors": [
                    {
                        "name": "Eleftheria Katsiri"
                    },
                    {
                        "name": "Alexandros Gazis"
                    },
                    {
                        "name": "Angelos Protopapas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Protopapas"
                },
                "author": "Angelos Protopapas",
                "arxiv_comment": "23 pages, 11 figures, 76 references. This article is under review at\n  WSEAS Transactions on Information Science and Applications from 02.2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.3; C.5.2; C.5.3; C.5.5; C.5.m; C.5.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v3",
                "updated": "2025-02-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    49,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "arxiv_comment": "Accepted by TMLR: https://openreview.net/forum?id=xXs2GKXPnH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v1",
                "updated": "2025-02-26T07:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v2",
                "updated": "2025-02-26T02:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    48,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18755v1",
                "updated": "2025-02-26T02:16:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T02:16:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type"
                },
                "summary": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator."
                },
                "authors": [
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Haoyan Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Renyang Guan"
                    },
                    {
                        "name": "Zhendong Hua"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v1",
                "updated": "2025-02-24T19:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17535v1",
                "updated": "2025-02-24T15:39:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?"
                },
                "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00022v1",
                "updated": "2025-02-24T02:57:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T02:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "title": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour"
                },
                "summary": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization."
                },
                "authors": [
                    {
                        "name": "Gopi Krishna Jha"
                    },
                    {
                        "name": "Sameh Gobriel"
                    },
                    {
                        "name": "Liubov Talamanova"
                    },
                    {
                        "name": "Alexander Kozlov"
                    },
                    {
                        "name": "Nilesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Jain"
                },
                "author": "Nilesh Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.08688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08688v1",
                "updated": "2025-03-11T17:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    53,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:59:53Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    53,
                    1,
                    70,
                    0
                ],
                "title": "Randomness, Not Representation: The Unreliability of Evaluating Cultural\n  Alignment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomness, Not Representation: The Unreliability of Evaluating Cultural\n  Alignment in LLMs"
                },
                "summary": "Research on the 'cultural alignment' of Large Language Models (LLMs) has\nemerged in response to growing interest in understanding representation across\ndiverse stakeholders. Current approaches to evaluating cultural alignment\nborrow social science methodologies but often overlook systematic robustness\nchecks. Here, we identify and test three assumptions behind current evaluation\nmethods: (1) Stability: that cultural alignment is a property of LLMs rather\nthan an artifact of evaluation design, (2) Extrapolability: that alignment with\none culture on a narrow set of issues predicts alignment with that culture on\nothers, and (3) Steerability: that LLMs can be reliably prompted to represent\nspecific cultural perspectives. Through experiments examining both explicit and\nimplicit preferences of leading LLMs, we find a high level of instability\nacross presentation formats, incoherence between evaluated versus held-out\ncultural dimensions, and erratic behavior under prompt steering. We show that\nthese inconsistencies can cause the results of an evaluation to be very\nsensitive to minor variations in methodology. Finally, we demonstrate in a case\nstudy on evaluation design that narrow experiments and a selective assessment\nof evidence can be used to paint an incomplete picture of LLMs' cultural\nalignment properties. Overall, these results highlight significant limitations\nof current approaches for evaluating the cultural alignment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on the 'cultural alignment' of Large Language Models (LLMs) has\nemerged in response to growing interest in understanding representation across\ndiverse stakeholders. Current approaches to evaluating cultural alignment\nborrow social science methodologies but often overlook systematic robustness\nchecks. Here, we identify and test three assumptions behind current evaluation\nmethods: (1) Stability: that cultural alignment is a property of LLMs rather\nthan an artifact of evaluation design, (2) Extrapolability: that alignment with\none culture on a narrow set of issues predicts alignment with that culture on\nothers, and (3) Steerability: that LLMs can be reliably prompted to represent\nspecific cultural perspectives. Through experiments examining both explicit and\nimplicit preferences of leading LLMs, we find a high level of instability\nacross presentation formats, incoherence between evaluated versus held-out\ncultural dimensions, and erratic behavior under prompt steering. We show that\nthese inconsistencies can cause the results of an evaluation to be very\nsensitive to minor variations in methodology. Finally, we demonstrate in a case\nstudy on evaluation design that narrow experiments and a selective assessment\nof evidence can be used to paint an incomplete picture of LLMs' cultural\nalignment properties. Overall, these results highlight significant limitations\nof current approaches for evaluating the cultural alignment of LLMs."
                },
                "authors": [
                    {
                        "name": "Ariba Khan"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Dylan Hadfield-Menell"
                    }
                ],
                "author_detail": {
                    "name": "Dylan Hadfield-Menell"
                },
                "author": "Dylan Hadfield-Menell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08686v1",
                "updated": "2025-03-11T17:59:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    46,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:59:46Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    46,
                    1,
                    70,
                    0
                ],
                "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models"
                },
                "summary": "Recent advancements in unified multimodal understanding and visual generation\n(or multimodal generation) models have been hindered by their quadratic\ncomputational complexity and dependence on large-scale training data. We\npresent OmniMamba, the first linear-architecture-based multimodal generation\nmodel that generates both text and images through a unified next-token\nprediction paradigm. The model fully leverages Mamba-2's high computational and\nmemory efficiency, extending its capabilities from text generation to\nmultimodal generation. To address the data inefficiency of existing unified\nmodels, we propose two key innovations: (1) decoupled vocabularies to guide\nmodality-specific generation, and (2) task-specific LoRA for\nparameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage\ntraining strategy to mitigate data imbalance between two tasks. Equipped with\nthese techniques, OmniMamba achieves competitive performance with JanusFlow\nwhile surpassing Show-o across benchmarks, despite being trained on merely 2M\nimage-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba\nstands out with outstanding inference efficiency, achieving up to a 119.2 times\nspeedup and 63% GPU memory reduction for long-sequence generation compared to\nTransformer-based counterparts. Code and models are released at\nhttps://github.com/hustvl/OmniMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in unified multimodal understanding and visual generation\n(or multimodal generation) models have been hindered by their quadratic\ncomputational complexity and dependence on large-scale training data. We\npresent OmniMamba, the first linear-architecture-based multimodal generation\nmodel that generates both text and images through a unified next-token\nprediction paradigm. The model fully leverages Mamba-2's high computational and\nmemory efficiency, extending its capabilities from text generation to\nmultimodal generation. To address the data inefficiency of existing unified\nmodels, we propose two key innovations: (1) decoupled vocabularies to guide\nmodality-specific generation, and (2) task-specific LoRA for\nparameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage\ntraining strategy to mitigate data imbalance between two tasks. Equipped with\nthese techniques, OmniMamba achieves competitive performance with JanusFlow\nwhile surpassing Show-o across benchmarks, despite being trained on merely 2M\nimage-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba\nstands out with outstanding inference efficiency, achieving up to a 119.2 times\nspeedup and 63% GPU memory reduction for long-sequence generation compared to\nTransformer-based counterparts. Code and models are released at\nhttps://github.com/hustvl/OmniMamba"
                },
                "authors": [
                    {
                        "name": "Jialv Zou"
                    },
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08685v1",
                "updated": "2025-03-11T17:59:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:59:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "\"Principal Components\" Enable A New Language of Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Principal Components\" Enable A New Language of Images"
                },
                "summary": "We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space -- a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, auto-regressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel visual tokenization framework that embeds a provable\nPCA-like structure into the latent token space. While existing visual\ntokenizers primarily optimize for reconstruction fidelity, they often neglect\nthe structural properties of the latent space -- a critical factor for both\ninterpretability and downstream tasks. Our method generates a 1D causal token\nsequence for images, where each successive token contributes non-overlapping\ninformation with mathematically guaranteed decreasing explained variance,\nanalogous to principal component analysis. This structural constraint ensures\nthe tokenizer extracts the most salient visual features first, with each\nsubsequent token adding diminishing yet complementary information.\nAdditionally, we identified and resolved a semantic-spectrum coupling effect\nthat causes the unwanted entanglement of high-level semantic content and\nlow-level spectral details in the tokens by leveraging a diffusion decoder.\nExperiments demonstrate that our approach achieves state-of-the-art\nreconstruction performance and enables better interpretability to align with\nthe human vision system. Moreover, auto-regressive models trained on our token\nsequences achieve performance comparable to current state-of-the-art methods\nwhile requiring fewer tokens for training and inference."
                },
                "authors": [
                    {
                        "name": "Xin Wen"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Ismail Elezi"
                    },
                    {
                        "name": "Jiankang Deng"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojuan Qi"
                },
                "author": "Xiaojuan Qi",
                "arxiv_comment": "The first two authors contributed equally, project page:\n  https://visual-gen.github.io/semanticist/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08684v1",
                "updated": "2025-03-11T17:59:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    0,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:59:00Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    0,
                    1,
                    70,
                    0
                ],
                "title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents"
                },
                "summary": "Previous studies have found that PLM-based retrieval models exhibit a\npreference for LLM-generated content, assigning higher relevance scores to\nthese documents even when their semantic quality is comparable to human-written\nones. This phenomenon, known as source bias, threatens the sustainable\ndevelopment of the information access ecosystem. However, the underlying causes\nof source bias remain unexplored. In this paper, we explain the process of\ninformation retrieval with a causal graph and discover that PLM-based\nretrievers learn perplexity features for relevance estimation, causing source\nbias by ranking the documents with low perplexity higher. Theoretical analysis\nfurther reveals that the phenomenon stems from the positive correlation between\nthe gradients of the loss functions in language modeling task and retrieval\ntask. Based on the analysis, a causal-inspired inference-time debiasing method\nis proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses\nthe bias effect of the perplexity and then separates the bias effect from the\noverall estimated relevance score. Experimental results across three domains\ndemonstrate the superior debiasing effectiveness of CDC, emphasizing the\nvalidity of our proposed explanatory framework. Source codes are available at\nhttps://github.com/WhyDwelledOnAi/Perplexity-Trap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous studies have found that PLM-based retrieval models exhibit a\npreference for LLM-generated content, assigning higher relevance scores to\nthese documents even when their semantic quality is comparable to human-written\nones. This phenomenon, known as source bias, threatens the sustainable\ndevelopment of the information access ecosystem. However, the underlying causes\nof source bias remain unexplored. In this paper, we explain the process of\ninformation retrieval with a causal graph and discover that PLM-based\nretrievers learn perplexity features for relevance estimation, causing source\nbias by ranking the documents with low perplexity higher. Theoretical analysis\nfurther reveals that the phenomenon stems from the positive correlation between\nthe gradients of the loss functions in language modeling task and retrieval\ntask. Based on the analysis, a causal-inspired inference-time debiasing method\nis proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses\nthe bias effect of the perplexity and then separates the bias effect from the\noverall estimated relevance score. Experimental results across three domains\ndemonstrate the superior debiasing effectiveness of CDC, emphasizing the\nvalidity of our proposed explanatory framework. Source codes are available at\nhttps://github.com/WhyDwelledOnAi/Perplexity-Trap."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Haiyuan Zhao"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08683v1",
                "updated": "2025-03-11T17:58:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    58,
                    42,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:58:42Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    58,
                    42,
                    1,
                    70,
                    0
                ],
                "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving"
                },
                "summary": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver."
                },
                "authors": [
                    {
                        "name": "Changxing Liu"
                    },
                    {
                        "name": "Genjia Liu"
                    },
                    {
                        "name": "Zijun Wang"
                    },
                    {
                        "name": "Jinchang Yang"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08681v1",
                "updated": "2025-03-11T17:57:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    57,
                    44,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:57:44Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    57,
                    44,
                    1,
                    70,
                    0
                ],
                "title": "Self-Taught Self-Correction for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Taught Self-Correction for Small Language Models"
                },
                "summary": "Although large language models (LLMs) have achieved remarkable performance\nacross various tasks, they remain prone to errors. A key challenge is enabling\nthem to self-correct. While prior research has relied on external tools or\nlarge proprietary models, this work explores self-correction in small language\nmodels (SLMs) through iterative fine-tuning using solely self-generated data.\nWe introduce the Self-Taught Self-Correction (STaSC) algorithm, which\nincorporates multiple algorithmic design choices. Experimental results on a\nquestion-answering task demonstrate that STaSC effectively learns\nself-correction, leading to significant performance improvements. Our analysis\nfurther provides insights into the mechanisms of self-correction and the impact\nof different design choices on learning dynamics and overall performance. To\nsupport future research, we release our user-friendly codebase and lightweight\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have achieved remarkable performance\nacross various tasks, they remain prone to errors. A key challenge is enabling\nthem to self-correct. While prior research has relied on external tools or\nlarge proprietary models, this work explores self-correction in small language\nmodels (SLMs) through iterative fine-tuning using solely self-generated data.\nWe introduce the Self-Taught Self-Correction (STaSC) algorithm, which\nincorporates multiple algorithmic design choices. Experimental results on a\nquestion-answering task demonstrate that STaSC effectively learns\nself-correction, leading to significant performance improvements. Our analysis\nfurther provides insights into the mechanisms of self-correction and the impact\nof different design choices on learning dynamics and overall performance. To\nsupport future research, we release our user-friendly codebase and lightweight\nmodels."
                },
                "authors": [
                    {
                        "name": "Viktor Moskvoretskii"
                    },
                    {
                        "name": "Chris Biemann"
                    },
                    {
                        "name": "Irina Nikishina"
                    }
                ],
                "author_detail": {
                    "name": "Irina Nikishina"
                },
                "author": "Irina Nikishina",
                "arxiv_comment": "Code is available at https://github.com/VityaVitalich/STASC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08678v1",
                "updated": "2025-03-11T17:56:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    56,
                    3,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    56,
                    3,
                    1,
                    70,
                    0
                ],
                "title": "GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D\n  Garment Reconstruction and Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GarmentCrafter: Progressive Novel View Synthesis for Single-View 3D\n  Garment Reconstruction and Editing"
                },
                "summary": "We introduce GarmentCrafter, a new approach that enables non-professional\nusers to create and modify 3D garments from a single-view image. While recent\nadvances in image generation have facilitated 2D garment design, creating and\nediting 3D garments remains challenging for non-professional users. Existing\nmethods for single-view 3D reconstruction often rely on pre-trained generative\nmodels to synthesize novel views conditioning on the reference image and camera\npose, yet they lack cross-view consistency, failing to capture the internal\nrelationships across different views. In this paper, we tackle this challenge\nthrough progressive depth prediction and image warping to approximate novel\nviews. Subsequently, we train a multi-view diffusion model to complete occluded\nand unknown clothing regions, informed by the evolving camera pose. By jointly\ninferring RGB and depth, GarmentCrafter enforces inter-view coherence and\nreconstructs precise geometries and fine details. Extensive experiments\ndemonstrate that our method achieves superior visual fidelity and inter-view\ncoherence compared to state-of-the-art single-view 3D garment reconstruction\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GarmentCrafter, a new approach that enables non-professional\nusers to create and modify 3D garments from a single-view image. While recent\nadvances in image generation have facilitated 2D garment design, creating and\nediting 3D garments remains challenging for non-professional users. Existing\nmethods for single-view 3D reconstruction often rely on pre-trained generative\nmodels to synthesize novel views conditioning on the reference image and camera\npose, yet they lack cross-view consistency, failing to capture the internal\nrelationships across different views. In this paper, we tackle this challenge\nthrough progressive depth prediction and image warping to approximate novel\nviews. Subsequently, we train a multi-view diffusion model to complete occluded\nand unknown clothing regions, informed by the evolving camera pose. By jointly\ninferring RGB and depth, GarmentCrafter enforces inter-view coherence and\nreconstructs precise geometries and fine details. Extensive experiments\ndemonstrate that our method achieves superior visual fidelity and inter-view\ncoherence compared to state-of-the-art single-view 3D garment reconstruction\nmethods."
                },
                "authors": [
                    {
                        "name": "Yuanhao Wang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Gonçalo Frazão"
                    },
                    {
                        "name": "Jinlong Yang"
                    },
                    {
                        "name": "Alexandru-Eugen Ichim"
                    },
                    {
                        "name": "Thabo Beeler"
                    },
                    {
                        "name": "Fernando De la Torre"
                    }
                ],
                "author_detail": {
                    "name": "Fernando De la Torre"
                },
                "author": "Fernando De la Torre",
                "arxiv_comment": "Project Page: https://humansensinglab.github.io/garment-crafter/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08665v1",
                "updated": "2025-03-11T17:51:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    51,
                    7,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:51:07Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    51,
                    7,
                    1,
                    70,
                    0
                ],
                "title": "REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder"
                },
                "summary": "We present a novel perspective on learning video embedders for generative\nmodeling: rather than requiring an exact reproduction of an input video, an\neffective embedder should focus on synthesizing visually plausible\nreconstructions. This relaxed criterion enables substantial improvements in\ncompression ratios without compromising the quality of downstream generative\nmodels. Specifically, we propose replacing the conventional encoder-decoder\nvideo embedder with an encoder-generator framework that employs a diffusion\ntransformer (DiT) to synthesize missing details from a compact latent space.\nTherein, we develop a dedicated latent conditioning module to condition the DiT\ndecoder on the encoded video latent embedding. Our experiments demonstrate that\nour approach enables superior encoding-decoding performance compared to\nstate-of-the-art methods, particularly as the compression ratio increases. To\ndemonstrate the efficacy of our approach, we report results from our video\nembedders achieving a temporal compression ratio of up to 32x (8x higher than\nleading video embedders) and validate the robustness of this ultra-compact\nlatent space for text-to-video generation, providing a significant efficiency\nboost in latent diffusion model training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel perspective on learning video embedders for generative\nmodeling: rather than requiring an exact reproduction of an input video, an\neffective embedder should focus on synthesizing visually plausible\nreconstructions. This relaxed criterion enables substantial improvements in\ncompression ratios without compromising the quality of downstream generative\nmodels. Specifically, we propose replacing the conventional encoder-decoder\nvideo embedder with an encoder-generator framework that employs a diffusion\ntransformer (DiT) to synthesize missing details from a compact latent space.\nTherein, we develop a dedicated latent conditioning module to condition the DiT\ndecoder on the encoded video latent embedding. Our experiments demonstrate that\nour approach enables superior encoding-decoding performance compared to\nstate-of-the-art methods, particularly as the compression ratio increases. To\ndemonstrate the efficacy of our approach, we report results from our video\nembedders achieving a temporal compression ratio of up to 32x (8x higher than\nleading video embedders) and validate the robustness of this ultra-compact\nlatent space for text-to-video generation, providing a significant efficiency\nboost in latent diffusion model training and inference."
                },
                "authors": [
                    {
                        "name": "Yitian Zhang"
                    },
                    {
                        "name": "Long Mai"
                    },
                    {
                        "name": "Aniruddha Mahapatra"
                    },
                    {
                        "name": "David Bourgin"
                    },
                    {
                        "name": "Yicong Hong"
                    },
                    {
                        "name": "Jonah Casebeer"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Yun Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yun Fu"
                },
                "author": "Yun Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08662v1",
                "updated": "2025-03-11T17:50:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    50,
                    44,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:50:44Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    50,
                    44,
                    1,
                    70,
                    0
                ],
                "title": "Exploring the Word Sense Disambiguation Capabilities of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Word Sense Disambiguation Capabilities of Large Language\n  Models"
                },
                "summary": "Word Sense Disambiguation (WSD) is a historical task in computational\nlinguistics that has received much attention over the years. However, with the\nadvent of Large Language Models (LLMs), interest in this task (in its classical\ndefinition) has decreased. In this study, we evaluate the performance of\nvarious LLMs on the WSD task. We extend a previous benchmark (XL-WSD) to\nre-design two subtasks suitable for LLM: 1) given a word in a sentence, the LLM\nmust generate the correct definition; 2) given a word in a sentence and a set\nof predefined meanings, the LLM must select the correct one. The extended\nbenchmark is built using the XL-WSD and BabelNet. The results indicate that\nLLMs perform well in zero-shot learning but cannot surpass current\nstate-of-the-art methods. However, a fine-tuned model with a medium number of\nparameters outperforms all other models, including the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word Sense Disambiguation (WSD) is a historical task in computational\nlinguistics that has received much attention over the years. However, with the\nadvent of Large Language Models (LLMs), interest in this task (in its classical\ndefinition) has decreased. In this study, we evaluate the performance of\nvarious LLMs on the WSD task. We extend a previous benchmark (XL-WSD) to\nre-design two subtasks suitable for LLM: 1) given a word in a sentence, the LLM\nmust generate the correct definition; 2) given a word in a sentence and a set\nof predefined meanings, the LLM must select the correct one. The extended\nbenchmark is built using the XL-WSD and BabelNet. The results indicate that\nLLMs perform well in zero-shot learning but cannot surpass current\nstate-of-the-art methods. However, a fine-tuned model with a medium number of\nparameters outperforms all other models, including the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Pierpaolo Basile"
                    },
                    {
                        "name": "Lucia Siciliani"
                    },
                    {
                        "name": "Elio Musacchio"
                    },
                    {
                        "name": "Giovanni Semeraro"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Semeraro"
                },
                "author": "Giovanni Semeraro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08655v1",
                "updated": "2025-03-11T17:46:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    46,
                    46,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:46:46Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    46,
                    46,
                    1,
                    70,
                    0
                ],
                "title": "On a new robust method of inference for general time series models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On a new robust method of inference for general time series models"
                },
                "summary": "In this article, we propose a novel logistic quasi-maximum likelihood\nestimation (LQMLE) for general parametric time series models. Compared to the\nclassical Gaussian QMLE and existing robust estimations, it enjoys many\ndistinctive advantages, such as robustness in respect of distributional\nmisspecification and heavy-tailedness of the innovation, more resiliency to\noutliers, smoothness and strict concavity of the log logistic quasi-likelihood\nfunction, and boundedness of the influence function among others. Under some\nmild conditions, we establish the strong consistency and asymptotic normality\nof the LQMLE. Moreover, we propose a new and vital parameter identifiability\ncondition to ensure desirable asymptotics of the LQMLE. Further, based on the\nLQMLE, we consider the Wald test and the Lagrange multiplier test for the\nunknown parameters, and derive the limiting distributions of the corresponding\ntest statistics. The applicability of our methodology is demonstrated by\nseveral time series models, including DAR, GARCH, ARMA-GARCH, DTARMACH, and\nEXPAR. Numerical simulation studies are carried out to assess the finite-sample\nperformance of our methodology, and an empirical example is analyzed to\nillustrate its usefulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we propose a novel logistic quasi-maximum likelihood\nestimation (LQMLE) for general parametric time series models. Compared to the\nclassical Gaussian QMLE and existing robust estimations, it enjoys many\ndistinctive advantages, such as robustness in respect of distributional\nmisspecification and heavy-tailedness of the innovation, more resiliency to\noutliers, smoothness and strict concavity of the log logistic quasi-likelihood\nfunction, and boundedness of the influence function among others. Under some\nmild conditions, we establish the strong consistency and asymptotic normality\nof the LQMLE. Moreover, we propose a new and vital parameter identifiability\ncondition to ensure desirable asymptotics of the LQMLE. Further, based on the\nLQMLE, we consider the Wald test and the Lagrange multiplier test for the\nunknown parameters, and derive the limiting distributions of the corresponding\ntest statistics. The applicability of our methodology is demonstrated by\nseveral time series models, including DAR, GARCH, ARMA-GARCH, DTARMACH, and\nEXPAR. Numerical simulation studies are carried out to assess the finite-sample\nperformance of our methodology, and an empirical example is analyzed to\nillustrate its usefulness."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Xinghao Qiao"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Howell Tong"
                    }
                ],
                "author_detail": {
                    "name": "Howell Tong"
                },
                "author": "Howell Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08653v1",
                "updated": "2025-03-11T17:44:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    44,
                    5,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:44:05Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    44,
                    5,
                    1,
                    70,
                    0
                ],
                "title": "Leveraging national forest inventory data to estimate forest carbon\n  density status and trends for small areas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging national forest inventory data to estimate forest carbon\n  density status and trends for small areas"
                },
                "summary": "National forest inventory (NFI) data are often costly to collect, which\ninhibits efforts to estimate parameters of interest for small spatial,\ntemporal, or biophysical domains. Traditionally, design-based estimators are\nused to estimate status of forest parameters of interest, but are unreliable\nfor small areas where data are sparse. Additionally, design-based estimates\nconstructed directly from the survey data are often unavailable when sample\nsizes are especially small. Traditional model-based small area estimation\napproaches, such as the Fay-Herriot (FH) model, rely on these direct estimates\nfor inference; hence, missing direct estimates preclude the use of such\napproaches. Here, we detail a Bayesian spatio-temporal small area estimation\nmodel that efficiently leverages sparse NFI data to estimate status and trends\nfor forest parameters. The proposed model bypasses the use of direct estimates\nand instead uses plot-level NFI measurements along with auxiliary data\nincluding remotely sensed tree canopy cover. We produce forest carbon estimates\nfrom the United States NFI over 14 years across the contiguous US (CONUS) and\nconduct a simulation study to assess our proposed model's accuracy, precision,\nand bias, compared to that of a design-based estimator. The proposed model\nprovides improved precision and accuracy over traditional estimation methods,\nand provides useful insights into county-level forest carbon dynamics across\nthe CONUS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "National forest inventory (NFI) data are often costly to collect, which\ninhibits efforts to estimate parameters of interest for small spatial,\ntemporal, or biophysical domains. Traditionally, design-based estimators are\nused to estimate status of forest parameters of interest, but are unreliable\nfor small areas where data are sparse. Additionally, design-based estimates\nconstructed directly from the survey data are often unavailable when sample\nsizes are especially small. Traditional model-based small area estimation\napproaches, such as the Fay-Herriot (FH) model, rely on these direct estimates\nfor inference; hence, missing direct estimates preclude the use of such\napproaches. Here, we detail a Bayesian spatio-temporal small area estimation\nmodel that efficiently leverages sparse NFI data to estimate status and trends\nfor forest parameters. The proposed model bypasses the use of direct estimates\nand instead uses plot-level NFI measurements along with auxiliary data\nincluding remotely sensed tree canopy cover. We produce forest carbon estimates\nfrom the United States NFI over 14 years across the contiguous US (CONUS) and\nconduct a simulation study to assess our proposed model's accuracy, precision,\nand bias, compared to that of a design-based estimator. The proposed model\nprovides improved precision and accuracy over traditional estimation methods,\nand provides useful insights into county-level forest carbon dynamics across\nthe CONUS."
                },
                "authors": [
                    {
                        "name": "Elliot S. Shannon"
                    },
                    {
                        "name": "Andrew O. Finley"
                    },
                    {
                        "name": "Paul B. May"
                    },
                    {
                        "name": "Grant M. Domke"
                    },
                    {
                        "name": "Hans-Erik Andersen"
                    },
                    {
                        "name": "George C. Gaines III"
                    },
                    {
                        "name": "Arne Nothdurft"
                    },
                    {
                        "name": "Sudipto Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Sudipto Banerjee"
                },
                "author": "Sudipto Banerjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17017v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17017v3",
                "updated": "2025-03-11T17:42:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    42,
                    55,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-26T01:00:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    1,
                    0,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On"
                },
                "summary": "Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task. Project page:\nhttps://zhenchenwan.github.io/TED-VITON/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task. Project page:\nhttps://zhenchenwan.github.io/TED-VITON/"
                },
                "authors": [
                    {
                        "name": "Zhenchen Wan"
                    },
                    {
                        "name": "Yanwu Xu"
                    },
                    {
                        "name": "Zhaoqing Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Gong"
                },
                "author": "Mingming Gong",
                "arxiv_comment": "Project page: https://github.com/ZhenchenWan/TED-VITON",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17017v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17017v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06759v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06759v3",
                "updated": "2025-03-11T17:37:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    37,
                    30,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-10T18:38:57Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    38,
                    57,
                    0,
                    41,
                    0
                ],
                "title": "Rationalization Models for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationalization Models for Text-to-SQL"
                },
                "summary": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability."
                },
                "authors": [
                    {
                        "name": "Gaetano Rossiello"
                    },
                    {
                        "name": "Nhan Pham"
                    },
                    {
                        "name": "Michael Glass"
                    },
                    {
                        "name": "Junkyu Lee"
                    },
                    {
                        "name": "Dharmashankar Subramanian"
                    }
                ],
                "author_detail": {
                    "name": "Dharmashankar Subramanian"
                },
                "author": "Dharmashankar Subramanian",
                "arxiv_comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06759v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06759v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08644v1",
                "updated": "2025-03-11T17:36:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    36,
                    53,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:36:53Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    36,
                    53,
                    1,
                    70,
                    0
                ],
                "title": "Exploiting Instruction-Following Retrievers for Malicious Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Instruction-Following Retrievers for Malicious Information\n  Retrieval"
                },
                "summary": "Instruction-following retrievers have been widely adopted alongside LLMs in\nreal-world applications, but little work has investigated the safety risks\nsurrounding their increasing search capabilities. We empirically study the\nability of retrievers to satisfy malicious queries, both when used directly and\nwhen used in a retrieval augmented generation-based setup. Concretely, we\ninvestigate six leading retrievers, including NV-Embed and LLM2Vec, and find\nthat given malicious requests, most retrievers can (for >50% of queries) select\nrelevant harmful passages. For example, LLM2Vec correctly selects passages for\n61.35% of our malicious queries. We further uncover an emerging risk with\ninstruction-following retrievers, where highly relevant harmful information can\nbe surfaced by exploiting their instruction-following capabilities. Finally, we\nshow that even safety-aligned LLMs, such as Llama3, can satisfy malicious\nrequests when provided with harmful retrieved passages in-context. In summary,\nour findings underscore the malicious misuse risks associated with increasing\nretriever capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following retrievers have been widely adopted alongside LLMs in\nreal-world applications, but little work has investigated the safety risks\nsurrounding their increasing search capabilities. We empirically study the\nability of retrievers to satisfy malicious queries, both when used directly and\nwhen used in a retrieval augmented generation-based setup. Concretely, we\ninvestigate six leading retrievers, including NV-Embed and LLM2Vec, and find\nthat given malicious requests, most retrievers can (for >50% of queries) select\nrelevant harmful passages. For example, LLM2Vec correctly selects passages for\n61.35% of our malicious queries. We further uncover an emerging risk with\ninstruction-following retrievers, where highly relevant harmful information can\nbe surfaced by exploiting their instruction-following capabilities. Finally, we\nshow that even safety-aligned LLMs, such as Llama3, can satisfy malicious\nrequests when provided with harmful retrieved passages in-context. In summary,\nour findings underscore the malicious misuse risks associated with increasing\nretriever capability."
                },
                "authors": [
                    {
                        "name": "Parishad BehnamGhader"
                    },
                    {
                        "name": "Nicholas Meade"
                    },
                    {
                        "name": "Siva Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Siva Reddy"
                },
                "author": "Siva Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08643v1",
                "updated": "2025-03-11T17:36:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    36,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:36:11Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    36,
                    11,
                    1,
                    70,
                    0
                ],
                "title": "Rethinking Diffusion Model in High Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Diffusion Model in High Dimension"
                },
                "summary": "Curse of Dimensionality is an unavoidable challenge in statistical\nprobability models, yet diffusion models seem to overcome this limitation,\nachieving impressive results in high-dimensional data generation. Diffusion\nmodels assume that they can learn the statistical properties of the underlying\nprobability distribution, enabling sampling from this distribution to generate\nrealistic samples. But is this really how they work? To address this question,\nthis paper conducts a detailed analysis of the objective function and inference\nmethods of diffusion models, leading to several important conclusions that help\nanswer the above question: 1) In high-dimensional sparse scenarios, the target\nof the objective function fitting degrades from a weighted sum of multiple\nsamples to a single sample. 2) The mainstream inference methods can all be\nrepresented within a simple unified framework, without requiring statistical\nconcepts such as Markov chains and SDEs. 3) Guided by this simple framework,\nmore efficient inference methods can be discovered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curse of Dimensionality is an unavoidable challenge in statistical\nprobability models, yet diffusion models seem to overcome this limitation,\nachieving impressive results in high-dimensional data generation. Diffusion\nmodels assume that they can learn the statistical properties of the underlying\nprobability distribution, enabling sampling from this distribution to generate\nrealistic samples. But is this really how they work? To address this question,\nthis paper conducts a detailed analysis of the objective function and inference\nmethods of diffusion models, leading to several important conclusions that help\nanswer the above question: 1) In high-dimensional sparse scenarios, the target\nof the objective function fitting degrades from a weighted sum of multiple\nsamples to a single sample. 2) The mainstream inference methods can all be\nrepresented within a simple unified framework, without requiring statistical\nconcepts such as Markov chains and SDEs. 3) Guided by this simple framework,\nmore efficient inference methods can be discovered."
                },
                "authors": [
                    {
                        "name": "Zhenxin Zheng"
                    },
                    {
                        "name": "Zhenjie Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhenjie Zheng"
                },
                "author": "Zhenjie Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05891v2",
                "updated": "2025-03-11T17:33:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    33,
                    51,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-07T19:24:59Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    19,
                    24,
                    59,
                    4,
                    66,
                    0
                ],
                "title": "MastermindEval: A Simple But Scalable Reasoning Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MastermindEval: A Simple But Scalable Reasoning Benchmark"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing."
                },
                "authors": [
                    {
                        "name": "Jonas Golde"
                    },
                    {
                        "name": "Patrick Haller"
                    },
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Alan Akbik"
                    }
                ],
                "author_detail": {
                    "name": "Alan Akbik"
                },
                "author": "Alan Akbik",
                "arxiv_comment": "9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and\n  Planning for Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17591v2",
                "updated": "2025-03-11T17:32:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    32,
                    22,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-24T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    16,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "Proactive Privacy Amnesia for Large Language Models: Safeguarding PII\n  with Negligible Impact on Model Utility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive Privacy Amnesia for Large Language Models: Safeguarding PII\n  with Negligible Impact on Model Utility"
                },
                "summary": "With the rise of large language models (LLMs), increasing research has\nrecognized their risk of leaking personally identifiable information (PII)\nunder malicious attacks. Although efforts have been made to protect PII in\nLLMs, existing methods struggle to balance privacy protection with maintaining\nmodel utility. In this paper, inspired by studies of amnesia in cognitive\nscience, we propose a novel approach, Proactive Privacy Amnesia (PPA), to\nsafeguard PII in LLMs while preserving their utility. This mechanism works by\nactively identifying and forgetting key memories most closely associated with\nPII in sequences, followed by a memory implanting using suitable substitute\nmemories to maintain the LLM's functionality. We conduct evaluations across\nmultiple models to protect common PII, such as phone numbers and physical\naddresses, against prevalent PII-targeted attacks, demonstrating the\nsuperiority of our method compared with other existing defensive techniques.\nThe results show that our PPA method completely eliminates the risk of phone\nnumber exposure by 100% and significantly reduces the risk of physical address\nexposure by 9.8% - 87.6%, all while maintaining comparable model utility\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs), increasing research has\nrecognized their risk of leaking personally identifiable information (PII)\nunder malicious attacks. Although efforts have been made to protect PII in\nLLMs, existing methods struggle to balance privacy protection with maintaining\nmodel utility. In this paper, inspired by studies of amnesia in cognitive\nscience, we propose a novel approach, Proactive Privacy Amnesia (PPA), to\nsafeguard PII in LLMs while preserving their utility. This mechanism works by\nactively identifying and forgetting key memories most closely associated with\nPII in sequences, followed by a memory implanting using suitable substitute\nmemories to maintain the LLM's functionality. We conduct evaluations across\nmultiple models to protect common PII, such as phone numbers and physical\naddresses, against prevalent PII-targeted attacks, demonstrating the\nsuperiority of our method compared with other existing defensive techniques.\nThe results show that our PPA method completely eliminates the risk of phone\nnumber exposure by 100% and significantly reduces the risk of physical address\nexposure by 9.8% - 87.6%, all while maintaining comparable model utility\nperformance."
                },
                "authors": [
                    {
                        "name": "Martin Kuo"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Minxue Tang"
                    },
                    {
                        "name": "Louis DiValentin"
                    },
                    {
                        "name": "Aolin Ding"
                    },
                    {
                        "name": "Jingwei Sun"
                    },
                    {
                        "name": "William Chen"
                    },
                    {
                        "name": "Amin Hass"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Hai Li"
                    }
                ],
                "author_detail": {
                    "name": "Hai Li"
                },
                "author": "Hai Li",
                "arxiv_comment": "ICLR'25 Poster. Project page and code is available at\n  https://ppa-iclr2025.my.canva.site/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v1",
                "updated": "2025-03-11T17:30:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07072v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07072v3",
                "updated": "2025-03-11T17:08:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    8,
                    5,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-10T22:07:02Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    22,
                    7,
                    2,
                    0,
                    41,
                    0
                ],
                "title": "IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models"
                },
                "summary": "Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair."
                },
                "authors": [
                    {
                        "name": "Sayem Mohammad Imtiaz"
                    },
                    {
                        "name": "Astha Singh"
                    },
                    {
                        "name": "Fraol Batole"
                    },
                    {
                        "name": "Hridesh Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Hridesh Rajan"
                },
                "author": "Hridesh Rajan",
                "arxiv_comment": "Accepted as full research paper at FSE'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07072v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07072v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07154v2",
                "updated": "2025-03-11T16:52:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    52,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-10T10:27:30Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    27,
                    30,
                    0,
                    69,
                    0
                ],
                "title": "Ideas in Inference-time Scaling can Benefit Generative Pre-training\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ideas in Inference-time Scaling can Benefit Generative Pre-training\n  Algorithms"
                },
                "summary": "Recent years have seen significant advancements in foundation models through\ngenerative pre-training, yet algorithmic innovation in this space has largely\nstagnated around autoregressive models for discrete signals and diffusion\nmodels for continuous signals. This stagnation creates a bottleneck that\nprevents us from fully unlocking the potential of rich multi-modal data, which\nin turn limits the progress on multimodal intelligence. We argue that an\ninference-first perspective, which prioritizes scaling efficiency during\ninference time across sequence length and refinement steps, can inspire novel\ngenerative pre-training algorithms. Using Inductive Moment Matching (IMM) as a\nconcrete example, we demonstrate how addressing limitations in diffusion\nmodels' inference process through targeted modifications yields a stable,\nsingle-stage algorithm that achieves superior sample quality with over an order\nof magnitude greater inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen significant advancements in foundation models through\ngenerative pre-training, yet algorithmic innovation in this space has largely\nstagnated around autoregressive models for discrete signals and diffusion\nmodels for continuous signals. This stagnation creates a bottleneck that\nprevents us from fully unlocking the potential of rich multi-modal data, which\nin turn limits the progress on multimodal intelligence. We argue that an\ninference-first perspective, which prioritizes scaling efficiency during\ninference time across sequence length and refinement steps, can inspire novel\ngenerative pre-training algorithms. Using Inductive Moment Matching (IMM) as a\nconcrete example, we demonstrate how addressing limitations in diffusion\nmodels' inference process through targeted modifications yields a stable,\nsingle-stage algorithm that achieves superior sample quality with over an order\nof magnitude greater inference efficiency."
                },
                "authors": [
                    {
                        "name": "Jiaming Song"
                    },
                    {
                        "name": "Linqi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Linqi Zhou"
                },
                "author": "Linqi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16862v2",
                "updated": "2025-03-11T16:51:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    51,
                    35,
                    1,
                    70,
                    0
                ],
                "published": "2024-03-25T15:26:32Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    15,
                    26,
                    32,
                    0,
                    85,
                    0
                ],
                "title": "INPC: Implicit Neural Point Clouds for Radiance Field Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INPC: Implicit Neural Point Clouds for Radiance Field Rendering"
                },
                "summary": "We introduce a new approach for reconstruction and novel view synthesis of\nunbounded real-world scenes. In contrast to previous methods using either\nvolumetric fields, grid-based models, or discrete point cloud proxies, we\npropose a hybrid scene representation, which implicitly encodes the geometry in\na continuous octree-based probability field and view-dependent appearance in a\nmulti-resolution hash grid. This allows for extraction of arbitrary explicit\npoint clouds, which can be rendered using rasterization. In doing so, we\ncombine the benefits of both worlds and retain favorable behavior during\noptimization: Our novel implicit point cloud representation and differentiable\nbilinear rasterizer enable fast rendering while preserving the fine geometric\ndetail captured by volumetric neural fields. Furthermore, this representation\ndoes not depend on priors like structure-from-motion point clouds. Our method\nachieves state-of-the-art image quality on common benchmarks. Furthermore, we\nachieve fast inference at interactive frame rates, and can convert our trained\nmodel into a large, explicit point cloud to further enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new approach for reconstruction and novel view synthesis of\nunbounded real-world scenes. In contrast to previous methods using either\nvolumetric fields, grid-based models, or discrete point cloud proxies, we\npropose a hybrid scene representation, which implicitly encodes the geometry in\na continuous octree-based probability field and view-dependent appearance in a\nmulti-resolution hash grid. This allows for extraction of arbitrary explicit\npoint clouds, which can be rendered using rasterization. In doing so, we\ncombine the benefits of both worlds and retain favorable behavior during\noptimization: Our novel implicit point cloud representation and differentiable\nbilinear rasterizer enable fast rendering while preserving the fine geometric\ndetail captured by volumetric neural fields. Furthermore, this representation\ndoes not depend on priors like structure-from-motion point clouds. Our method\nachieves state-of-the-art image quality on common benchmarks. Furthermore, we\nachieve fast inference at interactive frame rates, and can convert our trained\nmodel into a large, explicit point cloud to further enhance performance."
                },
                "authors": [
                    {
                        "name": "Florian Hahlbohm"
                    },
                    {
                        "name": "Linus Franke"
                    },
                    {
                        "name": "Moritz Kappel"
                    },
                    {
                        "name": "Susana Castillo"
                    },
                    {
                        "name": "Martin Eisemann"
                    },
                    {
                        "name": "Marc Stamminger"
                    },
                    {
                        "name": "Marcus Magnor"
                    }
                ],
                "author_detail": {
                    "name": "Marcus Magnor"
                },
                "author": "Marcus Magnor",
                "arxiv_comment": "Project page: https://fhahlbohm.github.io/inpc/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13333v2",
                "updated": "2025-03-11T16:50:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    50,
                    1,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-20T14:02:53Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    2,
                    53,
                    2,
                    325,
                    0
                ],
                "title": "Reanalyzing the ringdown signal of GW150914 using the F-statistic method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reanalyzing the ringdown signal of GW150914 using the F-statistic method"
                },
                "summary": "The ringdown phase of a gravitational wave (GW) signal from a binary black\nhole merger provides valuable insights into the properties of the final black\nhole and serves as a critical test of general relativity in the strong-field\nregime. A key aspect of this investigation is to determine whether the first\novertone mode exists in real GW data, as its presence would offer significant\nimplications for our understanding of general relativity under extreme\nconditions. To address this, we conducted a reanalysis of the ringdown signal\nfrom GW150914, using the newly proposed F-statistic method to search for the\nfirst overtone mode. Our results are consistent with those obtained through\nclassical time-domain Bayesian inference, indicating that there is no evidence\nof the first overtone mode in the ringdown signal of GW150914. However, our\nresults show the potentiality of utilizing the F-statistic methodology to\nunearth nuanced features within GW signals, thereby contributing novel insights\ninto black hole properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ringdown phase of a gravitational wave (GW) signal from a binary black\nhole merger provides valuable insights into the properties of the final black\nhole and serves as a critical test of general relativity in the strong-field\nregime. A key aspect of this investigation is to determine whether the first\novertone mode exists in real GW data, as its presence would offer significant\nimplications for our understanding of general relativity under extreme\nconditions. To address this, we conducted a reanalysis of the ringdown signal\nfrom GW150914, using the newly proposed F-statistic method to search for the\nfirst overtone mode. Our results are consistent with those obtained through\nclassical time-domain Bayesian inference, indicating that there is no evidence\nof the first overtone mode in the ringdown signal of GW150914. However, our\nresults show the potentiality of utilizing the F-statistic methodology to\nunearth nuanced features within GW signals, thereby contributing novel insights\ninto black hole properties."
                },
                "authors": [
                    {
                        "name": "Hai-Tian Wang"
                    },
                    {
                        "name": "Ziming Wang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Garvin Yim"
                    },
                    {
                        "name": "Lijing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lijing Shao"
                },
                "author": "Lijing Shao",
                "arxiv_doi": "10.1103/PhysRevD.111.064037",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.064037",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 3 figures, updated posterior data are now publicly available",
                "arxiv_journal_ref": "Phys. Rev. D 111, 064037 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08605v1",
                "updated": "2025-03-11T16:43:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    43,
                    45,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:43:45Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    43,
                    45,
                    1,
                    70,
                    0
                ],
                "title": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling"
                },
                "summary": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent advancements in text-to-video diffusion models enable\nhigh-quality short video generation from a single prompt, generating real-world\nlong videos in a single pass remains challenging due to limited data and high\ncomputational costs. To address this, several works propose tuning-free\napproaches, i.e., extending existing models for long video generation,\nspecifically using multiple prompts to allow for dynamic and controlled content\nchanges. However, these methods primarily focus on ensuring smooth transitions\nbetween adjacent frames, often leading to content drift and a gradual loss of\nsemantic coherence over longer sequences. To tackle such an issue, we propose\nSynchronized Coupled Sampling (SynCoS), a novel inference framework that\nsynchronizes denoising paths across the entire video, ensuring long-range\nconsistency across both adjacent and distant frames. Our approach combines two\ncomplementary sampling strategies: reverse and optimization-based sampling,\nwhich ensure seamless local transitions and enforce global coherence,\nrespectively. However, directly alternating between these samplings misaligns\ndenoising trajectories, disrupting prompt guidance and introducing unintended\ncontent changes as they operate independently. To resolve this, SynCoS\nsynchronizes them through a grounded timestep and a fixed baseline noise,\nensuring fully coupled sampling with aligned denoising paths. Extensive\nexperiments show that SynCoS significantly improves multi-event long video\ngeneration, achieving smoother transitions and superior long-range coherence,\noutperforming previous approaches both quantitatively and qualitatively."
                },
                "authors": [
                    {
                        "name": "Subin Kim"
                    },
                    {
                        "name": "Seoung Wug Oh"
                    },
                    {
                        "name": "Jui-Hsien Wang"
                    },
                    {
                        "name": "Joon-Young Lee"
                    },
                    {
                        "name": "Jinwoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Shin"
                },
                "author": "Jinwoo Shin",
                "arxiv_comment": "Project page with visuals: https://syncos2025.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08604v1",
                "updated": "2025-03-11T16:42:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    42,
                    36,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:42:36Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    42,
                    36,
                    1,
                    70,
                    0
                ],
                "title": "EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in\n  Open Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in\n  Open Environments"
                },
                "summary": "Developing autonomous home robots controlled by natural language has long\nbeen a pursuit of human. While advancements in large language models (LLMs) and\nembodied intelligence make this goal closer, several challenges persist: the\nlack of a unified benchmark for more complex robot tasks, limited evaluation\nmethods and metrics, data incompatibility between LLMs and mobile manipulation\ntrajectories. To address these issues, we introduce Embodied Mobile\nManipulation in Open Environments (EMMOE), which requires agents to interpret\nuser instructions and execute long-horizon everyday tasks in continuous space.\nEMMOE seamlessly integrates high-level and low-level embodied tasks into a\nunified framework, along with three new metrics for more diverse assessment.\nAdditionally, we collect EMMOE-100, which features in various task attributes,\ndetailed process annotations, re-plans after failures, and two sub-datasets for\nLLM training. Furthermore, we design HomieBot, a sophisticated agent system\nconsists of LLM with Direct Preference Optimization (DPO), light weighted\nnavigation and manipulation models, and multiple error detection mechanisms.\nFinally, we demonstrate HomieBot's performance and the evaluation of different\nmodels and policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing autonomous home robots controlled by natural language has long\nbeen a pursuit of human. While advancements in large language models (LLMs) and\nembodied intelligence make this goal closer, several challenges persist: the\nlack of a unified benchmark for more complex robot tasks, limited evaluation\nmethods and metrics, data incompatibility between LLMs and mobile manipulation\ntrajectories. To address these issues, we introduce Embodied Mobile\nManipulation in Open Environments (EMMOE), which requires agents to interpret\nuser instructions and execute long-horizon everyday tasks in continuous space.\nEMMOE seamlessly integrates high-level and low-level embodied tasks into a\nunified framework, along with three new metrics for more diverse assessment.\nAdditionally, we collect EMMOE-100, which features in various task attributes,\ndetailed process annotations, re-plans after failures, and two sub-datasets for\nLLM training. Furthermore, we design HomieBot, a sophisticated agent system\nconsists of LLM with Direct Preference Optimization (DPO), light weighted\nnavigation and manipulation models, and multiple error detection mechanisms.\nFinally, we demonstrate HomieBot's performance and the evaluation of different\nmodels and policies."
                },
                "authors": [
                    {
                        "name": "Dongping Li"
                    },
                    {
                        "name": "Tielong Cai"
                    },
                    {
                        "name": "Tianci Tang"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Katherine Rose Driggs-Campbell"
                    },
                    {
                        "name": "Gaoang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gaoang Wang"
                },
                "author": "Gaoang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11681v2",
                "updated": "2025-03-11T16:41:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    41,
                    52,
                    1,
                    70,
                    0
                ],
                "published": "2024-04-17T18:20:31Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    18,
                    20,
                    31,
                    2,
                    108,
                    0
                ],
                "title": "Evaluating Tenant-Landlord Tensions Using Generative AI on Online Tenant\n  Forums",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Tenant-Landlord Tensions Using Generative AI on Online Tenant\n  Forums"
                },
                "summary": "Tenant-landlord relationships exhibit a power asymmetry where landlords'\npower to evict the tenants at a low-cost results in their dominating status in\nsuch relationships. Tenant concerns are thus often unspoken, unresolved, or\nignored and this could lead to blatant conflicts as suppressed tenant concerns\naccumulate. Modern machine learning methods and Large Language Models (LLM)\nhave demonstrated immense abilities to perform language tasks. In this study,\nwe incorporate Latent Dirichlet Allocation (LDA) with GPT-4 to classify Reddit\npost data scraped from the subreddit r/Tenant, aiming to unveil trends in\ntenant concerns while exploring the adoption of LLMs and machine learning\nmethods in social science research. We find that tenant concerns in topics like\nfee dispute and utility issues are consistently dominant in all four states\nanalyzed while each state has other common tenant concerns special to itself.\nMoreover, we discover temporal trends in tenant concerns that provide important\nimplications regarding the impact of the pandemic and the Eviction Moratorium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tenant-landlord relationships exhibit a power asymmetry where landlords'\npower to evict the tenants at a low-cost results in their dominating status in\nsuch relationships. Tenant concerns are thus often unspoken, unresolved, or\nignored and this could lead to blatant conflicts as suppressed tenant concerns\naccumulate. Modern machine learning methods and Large Language Models (LLM)\nhave demonstrated immense abilities to perform language tasks. In this study,\nwe incorporate Latent Dirichlet Allocation (LDA) with GPT-4 to classify Reddit\npost data scraped from the subreddit r/Tenant, aiming to unveil trends in\ntenant concerns while exploring the adoption of LLMs and machine learning\nmethods in social science research. We find that tenant concerns in topics like\nfee dispute and utility issues are consistently dominant in all four states\nanalyzed while each state has other common tenant concerns special to itself.\nMoreover, we discover temporal trends in tenant concerns that provide important\nimplications regarding the impact of the pandemic and the Eviction Moratorium."
                },
                "authors": [
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Cheng Ren"
                    },
                    {
                        "name": "Timothy A Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Timothy A Thomas"
                },
                "author": "Timothy A Thomas",
                "arxiv_doi": "10.1007/s42001-025-00378-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00378-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.11681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "J Comput Soc Sc 8, 50 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v3",
                "updated": "2025-03-11T16:35:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    59,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08600v1",
                "updated": "2025-03-11T16:35:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    8,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:35:08Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    8,
                    1,
                    70,
                    0
                ],
                "title": "NSF-SciFy: Mining the NSF Awards Database for Scientific Claims",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSF-SciFy: Mining the NSF Awards Database for Scientific Claims"
                },
                "summary": "We present NSF-SciFy, a large-scale dataset for scientific claim extraction\nderived from the National Science Foundation (NSF) awards database, comprising\nover 400K grant abstracts spanning five decades. While previous datasets relied\non published literature, we leverage grant abstracts which offer a unique\nadvantage: they capture claims at an earlier stage in the research lifecycle\nbefore publication takes effect. We also introduce a new task to distinguish\nbetween existing scientific claims and aspirational research intentions in\nproposals.Using zero-shot prompting with frontier large language models, we\njointly extract 114K scientific claims and 145K investigation proposals from\n16K grant abstracts in the materials science domain to create a focused subset\ncalled NSF-SciFy-MatSci. We use this dataset to evaluate 3 three key tasks: (1)\ntechnical to non-technical abstract generation, where models achieve high\nBERTScore (0.85+ F1); (2) scientific claim extraction, where fine-tuned models\noutperform base models by 100% relative improvement; and (3) investigation\nproposal extraction, showing 90%+ improvement with fine-tuning. We introduce\nnovel LLM-based evaluation metrics for robust assessment of claim/proposal\nextraction quality. As the largest scientific claim dataset to date -- with an\nestimated 2.8 million claims across all STEM disciplines funded by the NSF --\nNSF-SciFy enables new opportunities for claim verification and meta-scientific\nresearch. We publicly release all datasets, trained models, and evaluation code\nto facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NSF-SciFy, a large-scale dataset for scientific claim extraction\nderived from the National Science Foundation (NSF) awards database, comprising\nover 400K grant abstracts spanning five decades. While previous datasets relied\non published literature, we leverage grant abstracts which offer a unique\nadvantage: they capture claims at an earlier stage in the research lifecycle\nbefore publication takes effect. We also introduce a new task to distinguish\nbetween existing scientific claims and aspirational research intentions in\nproposals.Using zero-shot prompting with frontier large language models, we\njointly extract 114K scientific claims and 145K investigation proposals from\n16K grant abstracts in the materials science domain to create a focused subset\ncalled NSF-SciFy-MatSci. We use this dataset to evaluate 3 three key tasks: (1)\ntechnical to non-technical abstract generation, where models achieve high\nBERTScore (0.85+ F1); (2) scientific claim extraction, where fine-tuned models\noutperform base models by 100% relative improvement; and (3) investigation\nproposal extraction, showing 90%+ improvement with fine-tuning. We introduce\nnovel LLM-based evaluation metrics for robust assessment of claim/proposal\nextraction quality. As the largest scientific claim dataset to date -- with an\nestimated 2.8 million claims across all STEM disciplines funded by the NSF --\nNSF-SciFy enables new opportunities for claim verification and meta-scientific\nresearch. We publicly release all datasets, trained models, and evaluation code\nto facilitate further research."
                },
                "authors": [
                    {
                        "name": "Delip Rao"
                    },
                    {
                        "name": "Weiqiu You"
                    },
                    {
                        "name": "Eric Wong"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "11 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08585v1",
                "updated": "2025-03-11T16:21:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    21,
                    23,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:21:23Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    21,
                    23,
                    1,
                    70,
                    0
                ],
                "title": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video\n  Understanding"
                },
                "summary": "Despite advancements in multimodal large language models (MLLMs), current\napproaches struggle in medium-to-long video understanding due to frame and\ncontext length limitations. As a result, these models often depend on frame\nsampling, which risks missing key information over time and lacks task-specific\nrelevance. To address these challenges, we introduce HierarQ, a task-aware\nhierarchical Q-Former based framework that sequentially processes frames to\nbypass the need for frame sampling, while avoiding LLM's context length\nlimitations. We introduce a lightweight two-stream language-guided feature\nmodulator to incorporate task awareness in video understanding, with the entity\nstream capturing frame-level object information within a short context and the\nscene stream identifying their broader interactions over longer period of time.\nEach stream is supported by dedicated memory banks which enables our proposed\nHierachical Querying transformer (HierarQ) to effectively capture short and\nlong-term context. Extensive evaluations on 10 video benchmarks across video\nunderstanding, question answering, and captioning tasks demonstrate HierarQ's\nstate-of-the-art performance across most datasets, proving its robustness and\nefficiency for comprehensive video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in multimodal large language models (MLLMs), current\napproaches struggle in medium-to-long video understanding due to frame and\ncontext length limitations. As a result, these models often depend on frame\nsampling, which risks missing key information over time and lacks task-specific\nrelevance. To address these challenges, we introduce HierarQ, a task-aware\nhierarchical Q-Former based framework that sequentially processes frames to\nbypass the need for frame sampling, while avoiding LLM's context length\nlimitations. We introduce a lightweight two-stream language-guided feature\nmodulator to incorporate task awareness in video understanding, with the entity\nstream capturing frame-level object information within a short context and the\nscene stream identifying their broader interactions over longer period of time.\nEach stream is supported by dedicated memory banks which enables our proposed\nHierachical Querying transformer (HierarQ) to effectively capture short and\nlong-term context. Extensive evaluations on 10 video benchmarks across video\nunderstanding, question answering, and captioning tasks demonstrate HierarQ's\nstate-of-the-art performance across most datasets, proving its robustness and\nefficiency for comprehensive video analysis."
                },
                "authors": [
                    {
                        "name": "Shehreen Azad"
                    },
                    {
                        "name": "Vibhav Vineet"
                    },
                    {
                        "name": "Yogesh Singh Rawat"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Singh Rawat"
                },
                "author": "Yogesh Singh Rawat",
                "arxiv_comment": "Accepted in CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08583v1",
                "updated": "2025-03-11T16:18:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    18,
                    26,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:18:26Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    18,
                    26,
                    1,
                    70,
                    0
                ],
                "title": "The Case for Edge-On Binaries: An Avenue Toward Comparative Exoplanet\n  Demographics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Edge-On Binaries: An Avenue Toward Comparative Exoplanet\n  Demographics"
                },
                "summary": "Most Sun-like and higher-mass stars reside in systems that include one or\nmore gravitationally bound stellar companions. These systems offer an important\nprobe of planet formation in the most common stellar systems, while also\nproviding key insights into how gravitational perturbations and irradiation\ndifferences from a companion star alter the outcomes of planet formation.\nRecent dynamical clues have begun to emerge that reveal systematic, non-random\nstructure in the configurations of many planet-hosting binary systems: in\nclose- to moderate-separation ($s < 800$ au) binary star systems, the orbits of\nexoplanets around individual stellar components are preferentially aligned with\nthe orbital plane of their host stellar binary. In this work, we flip this\nnarrative and search for nearby, edge-on binary star systems that, due to this\npreferential alignment, are top candidates for radial velocity and transiting\nexoplanet searches. We present a sample of 591 moderate-separation, relatively\nbright ($G < 14$) Gaia-resolved binary star systems in likely near-edge-on\nconfigurations. Using a simulated population of exoplanets drawn from transit\nsurvey occurrence rate constraints, we provide an overview of the expected\nplanet yields from a targeted search in these systems. We describe the\nopportunities for comparative exoplanet demographics in the case that both\nstars can be inferred to host edge-on planetary systems - a configuration\ntoward which the presented sample may be biased, given recent observations of\norbit-orbit alignment in exoplanet-hosting binary systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most Sun-like and higher-mass stars reside in systems that include one or\nmore gravitationally bound stellar companions. These systems offer an important\nprobe of planet formation in the most common stellar systems, while also\nproviding key insights into how gravitational perturbations and irradiation\ndifferences from a companion star alter the outcomes of planet formation.\nRecent dynamical clues have begun to emerge that reveal systematic, non-random\nstructure in the configurations of many planet-hosting binary systems: in\nclose- to moderate-separation ($s < 800$ au) binary star systems, the orbits of\nexoplanets around individual stellar components are preferentially aligned with\nthe orbital plane of their host stellar binary. In this work, we flip this\nnarrative and search for nearby, edge-on binary star systems that, due to this\npreferential alignment, are top candidates for radial velocity and transiting\nexoplanet searches. We present a sample of 591 moderate-separation, relatively\nbright ($G < 14$) Gaia-resolved binary star systems in likely near-edge-on\nconfigurations. Using a simulated population of exoplanets drawn from transit\nsurvey occurrence rate constraints, we provide an overview of the expected\nplanet yields from a targeted search in these systems. We describe the\nopportunities for comparative exoplanet demographics in the case that both\nstars can be inferred to host edge-on planetary systems - a configuration\ntoward which the presented sample may be biased, given recent observations of\norbit-orbit alignment in exoplanet-hosting binary systems."
                },
                "authors": [
                    {
                        "name": "Joseph E. Hand"
                    },
                    {
                        "name": "Malena Rice"
                    },
                    {
                        "name": "Konstantin Gerbig"
                    }
                ],
                "author_detail": {
                    "name": "Konstantin Gerbig"
                },
                "author": "Konstantin Gerbig",
                "arxiv_comment": "10 pages, 4 figures, to be published in ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08582v1",
                "updated": "2025-03-11T16:16:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    16,
                    49,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:16:49Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    16,
                    49,
                    1,
                    70,
                    0
                ],
                "title": "Chatbots for Data Collection in Surveys: A Comparison of Four\n  Theory-Based Interview Probes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots for Data Collection in Surveys: A Comparison of Four\n  Theory-Based Interview Probes"
                },
                "summary": "Surveys are a widespread method for collecting data at scale, but their rigid\nstructure often limits the depth of qualitative insights obtained. While\ninterviews naturally yield richer responses, they are challenging to conduct\nacross diverse locations and large participant pools. To partially bridge this\ngap, we investigate the potential of using LLM-based chatbots to support\nqualitative data collection through interview probes embedded in surveys. We\nassess four theory-based interview probes: descriptive, idiographic,\nclarifying, and explanatory. Through a split-plot study design (N=64), we\ncompare the probes' impact on response quality and user experience across three\nkey stages of HCI research: exploration, requirements gathering, and\nevaluation. Our results show that probes facilitate the collection of\nhigh-quality survey data, with specific probes proving effective at different\nresearch stages. We contribute practical and methodological implications for\nusing chatbots as research tools to enrich qualitative data collection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveys are a widespread method for collecting data at scale, but their rigid\nstructure often limits the depth of qualitative insights obtained. While\ninterviews naturally yield richer responses, they are challenging to conduct\nacross diverse locations and large participant pools. To partially bridge this\ngap, we investigate the potential of using LLM-based chatbots to support\nqualitative data collection through interview probes embedded in surveys. We\nassess four theory-based interview probes: descriptive, idiographic,\nclarifying, and explanatory. Through a split-plot study design (N=64), we\ncompare the probes' impact on response quality and user experience across three\nkey stages of HCI research: exploration, requirements gathering, and\nevaluation. Our results show that probes facilitate the collection of\nhigh-quality survey data, with specific probes proving effective at different\nresearch stages. We contribute practical and methodological implications for\nusing chatbots as research tools to enrich qualitative data collection."
                },
                "authors": [
                    {
                        "name": "Rune M. Jacobsen"
                    },
                    {
                        "name": "Samuel Rhys Cox"
                    },
                    {
                        "name": "Carla F. Griggio"
                    },
                    {
                        "name": "Niels van Berkel"
                    }
                ],
                "author_detail": {
                    "name": "Niels van Berkel"
                },
                "author": "Niels van Berkel",
                "arxiv_doi": "10.1145/3706598.3714128",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714128",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.08582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "CHI Conference on Human Factors in Computing Systems (CHI '25), April\n  26-May 1, 2025, Yokohama,Japan",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.14172v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.14172v3",
                "updated": "2025-03-11T16:11:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    11,
                    14,
                    1,
                    70,
                    0
                ],
                "published": "2023-08-27T18:28:58Z",
                "published_parsed": [
                    2023,
                    8,
                    27,
                    18,
                    28,
                    58,
                    6,
                    239,
                    0
                ],
                "title": "Hypergraph Structure Inference From Data Under Smoothness Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypergraph Structure Inference From Data Under Smoothness Prior"
                },
                "summary": "Hypergraphs are important for processing data with higher-order relationships\ninvolving more than two entities. In scenarios where explicit hypergraphs are\nnot readily available, it is desirable to infer a meaningful hypergraph\nstructure from the node features to capture the intrinsic relations within the\ndata. However, existing methods either adopt simple pre-defined rules that fail\nto precisely capture the distribution of the potential hypergraph structure, or\nlearn a mapping between hypergraph structures and node features but require a\nlarge amount of labelled data, i.e., pre-existing hypergraph structures, for\ntraining. Both restrict their applications in practical scenarios. To fill this\ngap, we propose a novel smoothness prior that enables us to design a method to\ninfer the probability for each potential hyperedge without labelled data as\nsupervision. The proposed prior indicates features of nodes in a hyperedge are\nhighly correlated by the features of the hyperedge containing them. We use this\nprior to derive the relation between the hypergraph structure and the node\nfeatures via probabilistic modelling. This allows us to develop an unsupervised\ninference method to estimate the probability for each potential hyperedge via\nsolving an optimisation problem that has an analytical solution. Experiments on\nboth synthetic and real-world data demonstrate that our method can learn\nmeaningful hypergraph structures from data more efficiently than existing\nhypergraph structure inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypergraphs are important for processing data with higher-order relationships\ninvolving more than two entities. In scenarios where explicit hypergraphs are\nnot readily available, it is desirable to infer a meaningful hypergraph\nstructure from the node features to capture the intrinsic relations within the\ndata. However, existing methods either adopt simple pre-defined rules that fail\nto precisely capture the distribution of the potential hypergraph structure, or\nlearn a mapping between hypergraph structures and node features but require a\nlarge amount of labelled data, i.e., pre-existing hypergraph structures, for\ntraining. Both restrict their applications in practical scenarios. To fill this\ngap, we propose a novel smoothness prior that enables us to design a method to\ninfer the probability for each potential hyperedge without labelled data as\nsupervision. The proposed prior indicates features of nodes in a hyperedge are\nhighly correlated by the features of the hyperedge containing them. We use this\nprior to derive the relation between the hypergraph structure and the node\nfeatures via probabilistic modelling. This allows us to develop an unsupervised\ninference method to estimate the probability for each potential hyperedge via\nsolving an optimisation problem that has an analytical solution. Experiments on\nboth synthetic and real-world data demonstrate that our method can learn\nmeaningful hypergraph structures from data more efficiently than existing\nhypergraph structure inference methods."
                },
                "authors": [
                    {
                        "name": "Bohan Tang"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Xiaowen Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Dong"
                },
                "author": "Xiaowen Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.14172v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.14172v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.01717v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.01717v4",
                "updated": "2025-03-11T16:08:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    8,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2022-11-03T11:13:02Z",
                "published_parsed": [
                    2022,
                    11,
                    3,
                    11,
                    13,
                    2,
                    3,
                    307,
                    0
                ],
                "title": "Learning Hypergraphs From Signals With Dual Smoothness Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Hypergraphs From Signals With Dual Smoothness Prior"
                },
                "summary": "Hypergraph structure learning, which aims to learn the hypergraph structures\nfrom the observed signals to capture the intrinsic high-order relationships\namong the entities, becomes crucial when a hypergraph topology is not readily\navailable in the datasets. There are two challenges that lie at the heart of\nthis problem: 1) how to handle the huge search space of potential hyperedges,\nand 2) how to define meaningful criteria to measure the relationship between\nthe signals observed on nodes and the hypergraph structure. In this paper, for\nthe first challenge, we adopt the assumption that the ideal hypergraph\nstructure can be derived from a learnable graph structure that captures the\npairwise relations within signals. Further, we propose a hypergraph structure\nlearning framework HGSL with a novel dual smoothness prior that reveals a\nmapping between the observed node signals and the hypergraph structure, whereby\neach hyperedge corresponds to a subgraph with both node signal smoothness and\nedge signal smoothness in the learnable graph structure. Finally, we conduct\nextensive experiments to evaluate HGSL on both synthetic and real world\ndatasets. Experiments show that HGSL can efficiently infer meaningful\nhypergraph topologies from observed signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypergraph structure learning, which aims to learn the hypergraph structures\nfrom the observed signals to capture the intrinsic high-order relationships\namong the entities, becomes crucial when a hypergraph topology is not readily\navailable in the datasets. There are two challenges that lie at the heart of\nthis problem: 1) how to handle the huge search space of potential hyperedges,\nand 2) how to define meaningful criteria to measure the relationship between\nthe signals observed on nodes and the hypergraph structure. In this paper, for\nthe first challenge, we adopt the assumption that the ideal hypergraph\nstructure can be derived from a learnable graph structure that captures the\npairwise relations within signals. Further, we propose a hypergraph structure\nlearning framework HGSL with a novel dual smoothness prior that reveals a\nmapping between the observed node signals and the hypergraph structure, whereby\neach hyperedge corresponds to a subgraph with both node signal smoothness and\nedge signal smoothness in the learnable graph structure. Finally, we conduct\nextensive experiments to evaluate HGSL on both synthetic and real world\ndatasets. Experiments show that HGSL can efficiently infer meaningful\nhypergraph topologies from observed signals."
                },
                "authors": [
                    {
                        "name": "Bohan Tang"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Xiaowen Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Dong"
                },
                "author": "Xiaowen Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.01717v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.01717v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09778v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09778v4",
                "updated": "2025-03-11T16:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    7,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2023-12-15T13:30:04Z",
                "published_parsed": [
                    2023,
                    12,
                    15,
                    13,
                    30,
                    4,
                    4,
                    349,
                    0
                ],
                "title": "Hypergraph-MLP: Learning on Hypergraphs without Message Passing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypergraph-MLP: Learning on Hypergraphs without Message Passing"
                },
                "summary": "Hypergraphs are vital in modelling data with higher-order relations\ncontaining more than two entities, gaining prominence in machine learning and\nsignal processing. Many hypergraph neural networks leverage message passing\nover hypergraph structures to enhance node representation learning, yielding\nimpressive performances in tasks like hypergraph node classification. However,\nthese message-passing-based models face several challenges, including\noversmoothing as well as high latency and sensitivity to structural\nperturbations at inference time. To tackle those challenges, we propose an\nalternative approach where we integrate the information about hypergraph\nstructures into training supervision without explicit message passing, thus\nalso removing the reliance on it at inference. Specifically, we introduce\nHypergraph-MLP, a novel learning framework for hypergraph-structured data,\nwhere the learning model is a straightforward multilayer perceptron (MLP)\nsupervised by a loss function based on a notion of signal smoothness on\nhypergraphs. Experiments on hypergraph node classification tasks demonstrate\nthat Hypergraph-MLP achieves competitive performance compared to existing\nbaselines, and is considerably faster and more robust against structural\nperturbations at inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypergraphs are vital in modelling data with higher-order relations\ncontaining more than two entities, gaining prominence in machine learning and\nsignal processing. Many hypergraph neural networks leverage message passing\nover hypergraph structures to enhance node representation learning, yielding\nimpressive performances in tasks like hypergraph node classification. However,\nthese message-passing-based models face several challenges, including\noversmoothing as well as high latency and sensitivity to structural\nperturbations at inference time. To tackle those challenges, we propose an\nalternative approach where we integrate the information about hypergraph\nstructures into training supervision without explicit message passing, thus\nalso removing the reliance on it at inference. Specifically, we introduce\nHypergraph-MLP, a novel learning framework for hypergraph-structured data,\nwhere the learning model is a straightforward multilayer perceptron (MLP)\nsupervised by a loss function based on a notion of signal smoothness on\nhypergraphs. Experiments on hypergraph node classification tasks demonstrate\nthat Hypergraph-MLP achieves competitive performance compared to existing\nbaselines, and is considerably faster and more robust against structural\nperturbations at inference."
                },
                "authors": [
                    {
                        "name": "Bohan Tang"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Xiaowen Dong"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Dong"
                },
                "author": "Xiaowen Dong",
                "arxiv_comment": "Accepted by ICASSP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09778v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09778v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08569v1",
                "updated": "2025-03-11T15:59:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    59,
                    43,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:59:43Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    59,
                    43,
                    1,
                    70,
                    0
                ],
                "title": "DeepReview: Improving LLM-based Paper Review with Human-like Deep\n  Thinking Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepReview: Improving LLM-based Paper Review with Human-like Deep\n  Thinking Process"
                },
                "summary": "Large Language Models (LLMs) are increasingly utilized in scientific research\nassessment, particularly in automated paper review. However, existing LLM-based\nreview systems face significant challenges, including limited domain expertise,\nhallucinated reasoning, and a lack of structured evaluation. To address these\nlimitations, we introduce DeepReview, a multi-stage framework designed to\nemulate expert reviewers by incorporating structured analysis, literature\nretrieval, and evidence-based argumentation. Using DeepReview-13K, a curated\ndataset with structured annotations, we train DeepReviewer-14B, which\noutperforms CycleReviewer-70B with fewer tokens. In its best mode,\nDeepReviewer-14B achieves win rates of 88.21\\% and 80.20\\% against GPT-o1 and\nDeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper\nreview, with all resources publicly available. The code, model, dataset and\ndemo have be released in http://ai-researcher.net.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly utilized in scientific research\nassessment, particularly in automated paper review. However, existing LLM-based\nreview systems face significant challenges, including limited domain expertise,\nhallucinated reasoning, and a lack of structured evaluation. To address these\nlimitations, we introduce DeepReview, a multi-stage framework designed to\nemulate expert reviewers by incorporating structured analysis, literature\nretrieval, and evidence-based argumentation. Using DeepReview-13K, a curated\ndataset with structured annotations, we train DeepReviewer-14B, which\noutperforms CycleReviewer-70B with fewer tokens. In its best mode,\nDeepReviewer-14B achieves win rates of 88.21\\% and 80.20\\% against GPT-o1 and\nDeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper\nreview, with all resources publicly available. The code, model, dataset and\ndemo have be released in http://ai-researcher.net."
                },
                "authors": [
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14042v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14042v2",
                "updated": "2025-03-11T15:54:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    54,
                    17,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-18T16:55:42Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    55,
                    42,
                    2,
                    353,
                    0
                ],
                "title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds"
                },
                "summary": "Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and training dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained on a procedurally\ngenerated dataset of one million CAD sequences. CAD-Recode significantly\noutperforms existing methods across the DeepCAD, Fusion360 and real-world CC3D\ndatasets. Furthermore, we show that our CAD Python code output is interpretable\nby off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and training dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained on a procedurally\ngenerated dataset of one million CAD sequences. CAD-Recode significantly\noutperforms existing methods across the DeepCAD, Fusion360 and real-world CC3D\ndatasets. Furthermore, we show that our CAD Python code output is interpretable\nby off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds."
                },
                "authors": [
                    {
                        "name": "Danila Rukhovich"
                    },
                    {
                        "name": "Elona Dupont"
                    },
                    {
                        "name": "Dimitrios Mallis"
                    },
                    {
                        "name": "Kseniya Cherenkova"
                    },
                    {
                        "name": "Anis Kacem"
                    },
                    {
                        "name": "Djamila Aouada"
                    }
                ],
                "author_detail": {
                    "name": "Djamila Aouada"
                },
                "author": "Djamila Aouada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14042v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02800v3",
                "updated": "2025-03-11T15:47:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    47,
                    37,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-04T17:20:43Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    20,
                    43,
                    1,
                    63,
                    0
                ],
                "title": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration"
                },
                "summary": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 88.6% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 88.6% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries."
                },
                "authors": [
                    {
                        "name": "Alicia Russell-Gilbert"
                    },
                    {
                        "name": "Sudip Mittal"
                    },
                    {
                        "name": "Shahram Rahimi"
                    },
                    {
                        "name": "Maria Seale"
                    },
                    {
                        "name": "Joseph Jabour"
                    },
                    {
                        "name": "Thomas Arnold"
                    },
                    {
                        "name": "Joshua Church"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Church"
                },
                "author": "Joshua Church",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2411.00914",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "1.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08558v1",
                "updated": "2025-03-11T15:47:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    47,
                    12,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:47:12Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    47,
                    12,
                    1,
                    70,
                    0
                ],
                "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime\n  Failure Detection for Imitation Learning Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime\n  Failure Detection for Imitation Learning Policies"
                },
                "summary": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Tony Khuong Nguyen"
                    },
                    {
                        "name": "Emma Dixon"
                    },
                    {
                        "name": "Christopher Rodriguez"
                    },
                    {
                        "name": "Patrick Miller"
                    },
                    {
                        "name": "Robert Lee"
                    },
                    {
                        "name": "Paarth Shah"
                    },
                    {
                        "name": "Rares Ambrus"
                    },
                    {
                        "name": "Haruki Nishimura"
                    },
                    {
                        "name": "Masha Itkina"
                    }
                ],
                "author_detail": {
                    "name": "Masha Itkina"
                },
                "author": "Masha Itkina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00106v2",
                "updated": "2025-03-11T15:41:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    41,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-28T19:00:01Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    19,
                    0,
                    1,
                    4,
                    59,
                    0
                ],
                "title": "The THESAN-ZOOM project: Burst, quench, repeat -- unveiling the\n  evolution of high-redshift galaxies along the star-forming main sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The THESAN-ZOOM project: Burst, quench, repeat -- unveiling the\n  evolution of high-redshift galaxies along the star-forming main sequence"
                },
                "summary": "Characterizing the evolution of the star-forming main sequence (SFMS) at high\nredshift is crucial to contextualize the observed extreme properties of\ngalaxies in the early Universe. We present an analysis of the SFMS and its\nscatter in the THESAN-ZOOM simulations, where we find a redshift evolution of\nthe SFMS normalization scaling as $\\propto (1+z)^{2.64\\pm0.03}$, significantly\nstronger than is typically inferred from observations. We can reproduce the\nflatter observed evolution by filtering out weakly star-forming galaxies,\nimplying that current observational fits are biased due to a missing population\nof lulling galaxies or overestimated star-formation rates. We also explore\nstar-formation variability using the scatter of galaxies around the SFMS\n($\\sigma_{\\mathrm{MS}}$). At the population level, the scatter around the SFMS\nincreases with cosmic time, driven by the increased importance of long-term\nenvironmental effects in regulating star formation at later times. To study\nshort-term star-formation variability, or ''burstiness'', we isolate the\nscatter on timescales shorter than 50 Myr. The short-term scatter is larger at\nhigher redshift, indicating that star formation is indeed more bursty in the\nearly Universe. We identify two starburst modes: (i) externally driven, where\nrapid large-scale inflows trigger and fuel prolonged, extreme star formation\nepisodes, and (ii) internally driven, where cyclical ejection and re-accretion\nof the interstellar medium in low-mass galaxies drive bursts, even under\nrelatively steady large-scale inflow. Both modes occur at all redshifts, but\nthe increased burstiness of galaxies at higher redshift is due to the\nincreasing prevalence of the more extreme external mode of star formation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the evolution of the star-forming main sequence (SFMS) at high\nredshift is crucial to contextualize the observed extreme properties of\ngalaxies in the early Universe. We present an analysis of the SFMS and its\nscatter in the THESAN-ZOOM simulations, where we find a redshift evolution of\nthe SFMS normalization scaling as $\\propto (1+z)^{2.64\\pm0.03}$, significantly\nstronger than is typically inferred from observations. We can reproduce the\nflatter observed evolution by filtering out weakly star-forming galaxies,\nimplying that current observational fits are biased due to a missing population\nof lulling galaxies or overestimated star-formation rates. We also explore\nstar-formation variability using the scatter of galaxies around the SFMS\n($\\sigma_{\\mathrm{MS}}$). At the population level, the scatter around the SFMS\nincreases with cosmic time, driven by the increased importance of long-term\nenvironmental effects in regulating star formation at later times. To study\nshort-term star-formation variability, or ''burstiness'', we isolate the\nscatter on timescales shorter than 50 Myr. The short-term scatter is larger at\nhigher redshift, indicating that star formation is indeed more bursty in the\nearly Universe. We identify two starburst modes: (i) externally driven, where\nrapid large-scale inflows trigger and fuel prolonged, extreme star formation\nepisodes, and (ii) internally driven, where cyclical ejection and re-accretion\nof the interstellar medium in low-mass galaxies drive bursts, even under\nrelatively steady large-scale inflow. Both modes occur at all redshifts, but\nthe increased burstiness of galaxies at higher redshift is due to the\nincreasing prevalence of the more extreme external mode of star formation."
                },
                "authors": [
                    {
                        "name": "William McClymont"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Aaron Smith"
                    },
                    {
                        "name": "Rahul Kannan"
                    },
                    {
                        "name": "Ewald Puchwein"
                    },
                    {
                        "name": "Josh Borrow"
                    },
                    {
                        "name": "Enrico Garaldi"
                    },
                    {
                        "name": "Laura Keating"
                    },
                    {
                        "name": "Mark Vogelsberger"
                    },
                    {
                        "name": "Oliver Zier"
                    },
                    {
                        "name": "Xuejian Shen"
                    },
                    {
                        "name": "Filip Popovic"
                    },
                    {
                        "name": "Charlotte Simmonds"
                    }
                ],
                "author_detail": {
                    "name": "Charlotte Simmonds"
                },
                "author": "Charlotte Simmonds",
                "arxiv_comment": "19 pages, 16 figures, 1 table, submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08551v1",
                "updated": "2025-03-11T15:39:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    39,
                    43,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:39:43Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    39,
                    43,
                    1,
                    70,
                    0
                ],
                "title": "Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs"
                },
                "summary": "The difficulty of multiple-choice questions (MCQs) is a crucial factor for\neducational assessments. Predicting MCQ difficulty is challenging since it\nrequires understanding both the complexity of reaching the correct option and\nthe plausibility of distractors, i.e., incorrect options. In this paper, we\npropose a novel, two-stage method to predict the difficulty of MCQs. First, to\nbetter estimate the complexity of each MCQ, we use large language models (LLMs)\nto augment the reasoning steps required to reach each option. We use not just\nthe MCQ itself but also these reasoning steps as input to predict the\ndifficulty. Second, to capture the plausibility of distractors, we sample\nknowledge levels from a distribution to account for variation among students\nresponding to the MCQ. This setup, inspired by item response theory (IRT),\nenable us to estimate the likelihood of students selecting each (both correct\nand incorrect) option. We align these predictions with their ground truth\nvalues, using a Kullback-Leibler (KL) divergence-based regularization\nobjective, and use estimated likelihoods to predict MCQ difficulty. We evaluate\nour method on two real-world \\emph{math} MCQ and response datasets with ground\ntruth difficulty values estimated using IRT. Experimental results show that our\nmethod outperforms all baselines, up to a 28.3\\% reduction in mean squared\nerror and a 34.6\\% improvement in the coefficient of determination. We also\nqualitatively discuss how our novel method results in higher accuracy in\npredicting MCQ difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The difficulty of multiple-choice questions (MCQs) is a crucial factor for\neducational assessments. Predicting MCQ difficulty is challenging since it\nrequires understanding both the complexity of reaching the correct option and\nthe plausibility of distractors, i.e., incorrect options. In this paper, we\npropose a novel, two-stage method to predict the difficulty of MCQs. First, to\nbetter estimate the complexity of each MCQ, we use large language models (LLMs)\nto augment the reasoning steps required to reach each option. We use not just\nthe MCQ itself but also these reasoning steps as input to predict the\ndifficulty. Second, to capture the plausibility of distractors, we sample\nknowledge levels from a distribution to account for variation among students\nresponding to the MCQ. This setup, inspired by item response theory (IRT),\nenable us to estimate the likelihood of students selecting each (both correct\nand incorrect) option. We align these predictions with their ground truth\nvalues, using a Kullback-Leibler (KL) divergence-based regularization\nobjective, and use estimated likelihoods to predict MCQ difficulty. We evaluate\nour method on two real-world \\emph{math} MCQ and response datasets with ground\ntruth difficulty values estimated using IRT. Experimental results show that our\nmethod outperforms all baselines, up to a 28.3\\% reduction in mean squared\nerror and a 34.6\\% improvement in the coefficient of determination. We also\nqualitatively discuss how our novel method results in higher accuracy in\npredicting MCQ difficulty."
                },
                "authors": [
                    {
                        "name": "Wanyong Feng"
                    },
                    {
                        "name": "Peter Tran"
                    },
                    {
                        "name": "Stephen Sireci"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08550v1",
                "updated": "2025-03-11T15:36:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    36,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:36:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    36,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "Transferring Extreme Subword Style Using Ngram Model-Based Logit Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferring Extreme Subword Style Using Ngram Model-Based Logit Scaling"
                },
                "summary": "We present an ngram model-based logit scaling technique that effectively\ntransfers extreme subword stylistic variation to large language models at\ninference time. We demonstrate its efficacy by tracking the perplexity of\ngenerated text with respect to the ngram interpolated and original versions of\nan evaluation model. Minimizing the former measure while the latter approaches\nthe perplexity of a text produced by a target author or character lets us\nselect a sufficient degree of adaptation while retaining fluency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an ngram model-based logit scaling technique that effectively\ntransfers extreme subword stylistic variation to large language models at\ninference time. We demonstrate its efficacy by tracking the perplexity of\ngenerated text with respect to the ngram interpolated and original versions of\nan evaluation model. Minimizing the former measure while the latter approaches\nthe perplexity of a text produced by a target author or character lets us\nselect a sufficient degree of adaptation while retaining fluency."
                },
                "authors": [
                    {
                        "name": "Craig Messner"
                    },
                    {
                        "name": "Tom Lippincott"
                    }
                ],
                "author_detail": {
                    "name": "Tom Lippincott"
                },
                "author": "Tom Lippincott",
                "arxiv_comment": "Accepted for publication at NLP4DH 2025 @ NAACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08549v1",
                "updated": "2025-03-11T15:36:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    36,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:36:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    36,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research\n  Idea Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research\n  Idea Generation"
                },
                "summary": "Reading relevant scientific papers and analyzing research development trends\nis a critical step in generating new scientific ideas. However, the rapid\nincrease in the volume of research literature and the complex citation\nrelationships make it difficult for researchers to quickly analyze and derive\nmeaningful research trends. The development of large language models (LLMs) has\nprovided a novel approach for automatically summarizing papers and generating\ninnovative research ideas. However, existing paper-based idea generation\nmethods either simply input papers into LLMs via prompts or form logical chains\nof creative development based on citation relationships, without fully\nexploiting the semantic information embedded in these citations. Inspired by\nknowledge graphs and human cognitive processes, we propose a framework called\nthe Graph of AI Ideas (GoAI) for the AI research field, which is dominated by\nopen-access papers. This framework organizes relevant literature into entities\nwithin a knowledge graph and summarizes the semantic information contained in\ncitations into relations within the graph. This organization effectively\nreflects the relationships between two academic papers and the advancement of\nthe AI research field. Such organization aids LLMs in capturing the current\nprogress of research, thereby enhancing their creativity. Experimental results\ndemonstrate the effectiveness of our approach in generating novel, clear, and\neffective research ideas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading relevant scientific papers and analyzing research development trends\nis a critical step in generating new scientific ideas. However, the rapid\nincrease in the volume of research literature and the complex citation\nrelationships make it difficult for researchers to quickly analyze and derive\nmeaningful research trends. The development of large language models (LLMs) has\nprovided a novel approach for automatically summarizing papers and generating\ninnovative research ideas. However, existing paper-based idea generation\nmethods either simply input papers into LLMs via prompts or form logical chains\nof creative development based on citation relationships, without fully\nexploiting the semantic information embedded in these citations. Inspired by\nknowledge graphs and human cognitive processes, we propose a framework called\nthe Graph of AI Ideas (GoAI) for the AI research field, which is dominated by\nopen-access papers. This framework organizes relevant literature into entities\nwithin a knowledge graph and summarizes the semantic information contained in\ncitations into relations within the graph. This organization effectively\nreflects the relationships between two academic papers and the advancement of\nthe AI research field. Such organization aids LLMs in capturing the current\nprogress of research, thereby enhancing their creativity. Experimental results\ndemonstrate the effectiveness of our approach in generating novel, clear, and\neffective research ideas."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Zongyun Zhang"
                    },
                    {
                        "name": "Mingye Xie"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06089v2",
                "updated": "2025-03-11T15:34:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    34,
                    16,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-08T22:29:56Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    22,
                    29,
                    56,
                    6,
                    343,
                    0
                ],
                "title": "GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis"
                },
                "summary": "Text-to-image (T2I) generation has seen significant progress with diffusion\nmodels, enabling generation of photo-realistic images from text prompts.\nDespite this progress, existing methods still face challenges in following\ncomplex text prompts, especially those requiring compositional and multi-step\nreasoning. Given such complex instructions, SOTA models often make mistakes in\nfaithfully modeling object attributes, and relationships among them. In this\nwork, we present an alternate paradigm for T2I synthesis, decomposing the task\nof complex multi-step generation into three steps, (a) Generate: we first\ngenerate an image using existing diffusion models (b) Plan: we make use of\nMulti-Modal LLMs (MLLMs) to identify the mistakes in the generated image\nexpressed in terms of individual objects and their properties, and produce a\nsequence of corrective steps required in the form of an edit-plan. (c) Edit: we\nmake use of an existing text-guided image editing models to sequentially\nexecute our edit-plan over the generated image to get the desired image which\nis faithful to the original instruction. Our approach derives its strength from\nthe fact that it is modular in nature, is training free, and can be applied\nover any combination of image generation and editing models. As an added\ncontribution, we also develop a model capable of compositional editing, which\nfurther helps improve the overall accuracy of our proposed approach. Our method\nflexibly trades inference time compute with performance on compositional text\nprompts. We perform extensive experimental evaluation across 3 benchmarks and\n10 T2I models including DALLE-3 and the latest -- SD-3.5-Large. Our approach\nnot only improves the performance of the SOTA models, by upto 3 points, it also\nreduces the performance gap between weaker and stronger models.\n$\\href{https://dair-iitd.github.io/GraPE/}{https://dair-iitd.github.io/GraPE/}$",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation has seen significant progress with diffusion\nmodels, enabling generation of photo-realistic images from text prompts.\nDespite this progress, existing methods still face challenges in following\ncomplex text prompts, especially those requiring compositional and multi-step\nreasoning. Given such complex instructions, SOTA models often make mistakes in\nfaithfully modeling object attributes, and relationships among them. In this\nwork, we present an alternate paradigm for T2I synthesis, decomposing the task\nof complex multi-step generation into three steps, (a) Generate: we first\ngenerate an image using existing diffusion models (b) Plan: we make use of\nMulti-Modal LLMs (MLLMs) to identify the mistakes in the generated image\nexpressed in terms of individual objects and their properties, and produce a\nsequence of corrective steps required in the form of an edit-plan. (c) Edit: we\nmake use of an existing text-guided image editing models to sequentially\nexecute our edit-plan over the generated image to get the desired image which\nis faithful to the original instruction. Our approach derives its strength from\nthe fact that it is modular in nature, is training free, and can be applied\nover any combination of image generation and editing models. As an added\ncontribution, we also develop a model capable of compositional editing, which\nfurther helps improve the overall accuracy of our proposed approach. Our method\nflexibly trades inference time compute with performance on compositional text\nprompts. We perform extensive experimental evaluation across 3 benchmarks and\n10 T2I models including DALLE-3 and the latest -- SD-3.5-Large. Our approach\nnot only improves the performance of the SOTA models, by upto 3 points, it also\nreduces the performance gap between weaker and stronger models.\n$\\href{https://dair-iitd.github.io/GraPE/}{https://dair-iitd.github.io/GraPE/}$"
                },
                "authors": [
                    {
                        "name": "Ashish Goswami"
                    },
                    {
                        "name": "Satyam Kumar Modi"
                    },
                    {
                        "name": "Santhosh Rishi Deshineni"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Prathosh A. P"
                    },
                    {
                        "name": "Parag Singla"
                    }
                ],
                "author_detail": {
                    "name": "Parag Singla"
                },
                "author": "Parag Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08542v1",
                "updated": "2025-03-11T15:29:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    29,
                    55,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:29:55Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    29,
                    55,
                    1,
                    70,
                    0
                ],
                "title": "DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form\n  Question-Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form\n  Question-Answering"
                },
                "summary": "Evaluating Large Language Models (LLMs) free-form generated responses remains\na challenge due to their diverse and open-ended nature. Traditional supervised\nsignal-based automatic metrics fail to capture semantic equivalence or handle\nthe variability of open-ended responses, while human evaluation, though\nreliable, is resource-intensive. Leveraging LLMs as evaluators offers a\npromising alternative due to their strong language understanding and\ninstruction-following capabilities. Taking advantage of these capabilities, we\npropose the Dynamic Arbitration Framework for Evaluation (DAFE), which employs\ntwo primary LLM-as-judges and engages a third arbitrator only in cases of\ndisagreements. This selective arbitration prioritizes evaluation reliability\nwhile reducing unnecessary computational demands compared to conventional\nmajority voting. DAFE utilizes task-specific reference answers with dynamic\narbitration to enhance judgment accuracy, resulting in significant improvements\nin evaluation metrics such as Macro F1 and Cohen's Kappa. Through experiments,\nincluding a comprehensive human evaluation, we demonstrate DAFE's ability to\nprovide consistent, scalable, and resource-efficient assessments, establishing\nit as a robust framework for evaluating free-form model outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) free-form generated responses remains\na challenge due to their diverse and open-ended nature. Traditional supervised\nsignal-based automatic metrics fail to capture semantic equivalence or handle\nthe variability of open-ended responses, while human evaluation, though\nreliable, is resource-intensive. Leveraging LLMs as evaluators offers a\npromising alternative due to their strong language understanding and\ninstruction-following capabilities. Taking advantage of these capabilities, we\npropose the Dynamic Arbitration Framework for Evaluation (DAFE), which employs\ntwo primary LLM-as-judges and engages a third arbitrator only in cases of\ndisagreements. This selective arbitration prioritizes evaluation reliability\nwhile reducing unnecessary computational demands compared to conventional\nmajority voting. DAFE utilizes task-specific reference answers with dynamic\narbitration to enhance judgment accuracy, resulting in significant improvements\nin evaluation metrics such as Macro F1 and Cohen's Kappa. Through experiments,\nincluding a comprehensive human evaluation, we demonstrate DAFE's ability to\nprovide consistent, scalable, and resource-efficient assessments, establishing\nit as a robust framework for evaluating free-form model outputs."
                },
                "authors": [
                    {
                        "name": "Sher Badshah"
                    },
                    {
                        "name": "Hassan Sajjad"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sajjad"
                },
                "author": "Hassan Sajjad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08540v1",
                "updated": "2025-03-11T15:29:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    29,
                    0,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:29:00Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    29,
                    0,
                    1,
                    70,
                    0
                ],
                "title": "Mellow: a small audio language model for reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mellow: a small audio language model for reasoning"
                },
                "summary": "Multimodal Audio-Language Models (ALMs) can understand and reason over both\naudio and text. Typically, reasoning performance correlates with model size,\nwith the best results achieved by models exceeding 8 billion parameters.\nHowever, no prior work has explored enabling small audio-language models to\nperform reasoning tasks, despite the potential applications for edge devices.\nTo address this gap, we introduce Mellow, a small Audio-Language Model\nspecifically designed for reasoning. Mellow achieves state-of-the-art\nperformance among existing small audio-language models and surpasses several\nlarger models in reasoning capabilities. For instance, Mellow scores 52.11 on\nMMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times\nfewer parameters and being trained on 60 times less data (audio hrs). To train\nMellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded\nreasoning in models. It consists of a mixture of existing datasets (30% of the\ndata) and synthetically generated data (70%). The synthetic dataset is derived\nfrom audio captioning datasets, where Large Language Models (LLMs) generate\ndetailed and multiple-choice questions focusing on audio events, objects,\nacoustic scenes, signal properties, semantics, and listener emotions. To\nevaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks,\nassessing on both in-distribution and out-of-distribution data, including audio\nunderstanding, deductive reasoning, and comparative reasoning. Finally, we\nconduct extensive ablation studies to explore the impact of projection layer\nchoices, synthetic data generation methods, and language model pretraining on\nreasoning performance. Our training dataset, findings, and baseline pave the\nway for developing small ALMs capable of reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Audio-Language Models (ALMs) can understand and reason over both\naudio and text. Typically, reasoning performance correlates with model size,\nwith the best results achieved by models exceeding 8 billion parameters.\nHowever, no prior work has explored enabling small audio-language models to\nperform reasoning tasks, despite the potential applications for edge devices.\nTo address this gap, we introduce Mellow, a small Audio-Language Model\nspecifically designed for reasoning. Mellow achieves state-of-the-art\nperformance among existing small audio-language models and surpasses several\nlarger models in reasoning capabilities. For instance, Mellow scores 52.11 on\nMMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times\nfewer parameters and being trained on 60 times less data (audio hrs). To train\nMellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded\nreasoning in models. It consists of a mixture of existing datasets (30% of the\ndata) and synthetically generated data (70%). The synthetic dataset is derived\nfrom audio captioning datasets, where Large Language Models (LLMs) generate\ndetailed and multiple-choice questions focusing on audio events, objects,\nacoustic scenes, signal properties, semantics, and listener emotions. To\nevaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks,\nassessing on both in-distribution and out-of-distribution data, including audio\nunderstanding, deductive reasoning, and comparative reasoning. Finally, we\nconduct extensive ablation studies to explore the impact of projection layer\nchoices, synthetic data generation methods, and language model pretraining on\nreasoning performance. Our training dataset, findings, and baseline pave the\nway for developing small ALMs capable of reasoning."
                },
                "authors": [
                    {
                        "name": "Soham Deshmukh"
                    },
                    {
                        "name": "Satvik Dixit"
                    },
                    {
                        "name": "Rita Singh"
                    },
                    {
                        "name": "Bhiksha Raj"
                    }
                ],
                "author_detail": {
                    "name": "Bhiksha Raj"
                },
                "author": "Bhiksha Raj",
                "arxiv_comment": "Checkpoint and dataset available at:\n  https://github.com/soham97/mellow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08539v1",
                "updated": "2025-03-11T15:28:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    28,
                    9,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:28:09Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    28,
                    9,
                    1,
                    70,
                    0
                ],
                "title": "Desirable Unfamiliarity: Insights from Eye Movements on Engagement and\n  Readability of Dictation Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desirable Unfamiliarity: Insights from Eye Movements on Engagement and\n  Readability of Dictation Interfaces"
                },
                "summary": "Dictation interfaces support efficient text input, but the transcribed text\ncan be hard to read. To understand how users read and review dictated text, we\nconducted a controlled eye-tracking experiment with 20 participants to compare\nfive dictation interfaces: PLAIN (real-time transcription), AOC (periodic\ncorrections), RAKE (keyword highlights), GP-TSM (grammar-preserving\nhighlights), and SUMMARY (LLM-generated abstraction summary). The study\nanalyzed participants' gaze patterns during their speech composition and\nreviewing processes. The findings show that during composition, participants\nspent only 7--11% of their time actively reading, and they favored real-time\nfeedback and avoided distracting interface changes. During reviewing, although\nSUMMARY introduced unfamiliar words (requiring longer and more frequent\nfixation), they were easier to read (requiring fewer regressions). Participants\npreferred SUMMARY for the polished text that preserved fidelity to original\nmeanings. RAKE guided the reading of self-produced text better than GP-TSM.\nThese findings provide new ways to rethink the design of dictation interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dictation interfaces support efficient text input, but the transcribed text\ncan be hard to read. To understand how users read and review dictated text, we\nconducted a controlled eye-tracking experiment with 20 participants to compare\nfive dictation interfaces: PLAIN (real-time transcription), AOC (periodic\ncorrections), RAKE (keyword highlights), GP-TSM (grammar-preserving\nhighlights), and SUMMARY (LLM-generated abstraction summary). The study\nanalyzed participants' gaze patterns during their speech composition and\nreviewing processes. The findings show that during composition, participants\nspent only 7--11% of their time actively reading, and they favored real-time\nfeedback and avoided distracting interface changes. During reviewing, although\nSUMMARY introduced unfamiliar words (requiring longer and more frequent\nfixation), they were easier to read (requiring fewer regressions). Participants\npreferred SUMMARY for the polished text that preserved fidelity to original\nmeanings. RAKE guided the reading of self-produced text better than GP-TSM.\nThese findings provide new ways to rethink the design of dictation interfaces."
                },
                "authors": [
                    {
                        "name": "Zhaohui Liang"
                    },
                    {
                        "name": "Yonglin Chen"
                    },
                    {
                        "name": "Naser Al Madi"
                    },
                    {
                        "name": "Can Liu"
                    }
                ],
                "author_detail": {
                    "name": "Can Liu"
                },
                "author": "Can Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08538v1",
                "updated": "2025-03-11T15:28:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    28,
                    1,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:28:01Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    28,
                    1,
                    1,
                    70,
                    0
                ],
                "title": "Spectral distortion and polarization of the cosmic microwave background:\n  Measurement, challenges and perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral distortion and polarization of the cosmic microwave background:\n  Measurement, challenges and perspectives"
                },
                "summary": "The Cosmic Microwave Background (CMB) is a fundamental observational tool in\nmodern cosmology. The linear polarization of the CMB provides a crucial\nobservational tool for exploring new physics, including the inflationary\nparadigm and parity-violating phenomena. The spectral distortion of the CMB can\nbe used as a probe of the intracluster medium (ICM) of galaxy clusters (GCs)\nand to infer cosmological parameters. However, precise measurements are limited\nby foreground contamination and instrumental systematics.\n  This work examines the non-thermal energy budget in GCs with the non-thermal\nSunyaev-Zeldovich (ntSZ) effect. Using $Planck$ all-sky maps, we measure the\nntSZ effect in GCs hosting radio halos, enabling constraints on the\nvolume-averaged magnetic field strength within the ICM. We further assess how\nobservations from upcoming ground-based CMB experiments improve these\nconstraints.\n  The imprints of parity-violating physics on the polarization of the CMB are\npredicted theoretically and recently, several methods have been used to measure\nthe effect of cosmic birefringence from $Planck$ and $WMAP$ data. The\nmiscalibration of polarization-sensitive detectors is found to be one of the\nlimiting factors in this endeavour. We perform a relative and an absolute\npolarization angle calibration of the $Planck$ detectors using microwave and\nX-ray observations of the Crab nebula.\n  Additionally, we develop a simulation pipeline to generate time-ordered data\nfor an upcoming ground-based sub-mm telescope, analyzing the impact of\ncorrelated detector and atmospheric noise on CMB map reconstruction. Our\nfindings in this thesis emphasize the importance of broad-frequency coverage\nfor better foreground characterization and highlight the need for dedicated\nground-based calibration to achieve accurate CMB polarization measurements.\n[abridged]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cosmic Microwave Background (CMB) is a fundamental observational tool in\nmodern cosmology. The linear polarization of the CMB provides a crucial\nobservational tool for exploring new physics, including the inflationary\nparadigm and parity-violating phenomena. The spectral distortion of the CMB can\nbe used as a probe of the intracluster medium (ICM) of galaxy clusters (GCs)\nand to infer cosmological parameters. However, precise measurements are limited\nby foreground contamination and instrumental systematics.\n  This work examines the non-thermal energy budget in GCs with the non-thermal\nSunyaev-Zeldovich (ntSZ) effect. Using $Planck$ all-sky maps, we measure the\nntSZ effect in GCs hosting radio halos, enabling constraints on the\nvolume-averaged magnetic field strength within the ICM. We further assess how\nobservations from upcoming ground-based CMB experiments improve these\nconstraints.\n  The imprints of parity-violating physics on the polarization of the CMB are\npredicted theoretically and recently, several methods have been used to measure\nthe effect of cosmic birefringence from $Planck$ and $WMAP$ data. The\nmiscalibration of polarization-sensitive detectors is found to be one of the\nlimiting factors in this endeavour. We perform a relative and an absolute\npolarization angle calibration of the $Planck$ detectors using microwave and\nX-ray observations of the Crab nebula.\n  Additionally, we develop a simulation pipeline to generate time-ordered data\nfor an upcoming ground-based sub-mm telescope, analyzing the impact of\ncorrelated detector and atmospheric noise on CMB map reconstruction. Our\nfindings in this thesis emphasize the importance of broad-frequency coverage\nfor better foreground characterization and highlight the need for dedicated\nground-based calibration to achieve accurate CMB polarization measurements.\n[abridged]"
                },
                "authors": [
                    {
                        "name": "Vyoma Muralidhara"
                    }
                ],
                "author_detail": {
                    "name": "Vyoma Muralidhara"
                },
                "author": "Vyoma Muralidhara",
                "arxiv_doi": "10.5282/edoc.34768",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5282/edoc.34768",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.08538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "PhD thesis. Abstract is abridged for arXiv submission",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08537v1",
                "updated": "2025-03-11T15:27:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    27,
                    17,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:27:17Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    27,
                    17,
                    1,
                    70,
                    0
                ],
                "title": "Chemical reasoning in LLMs unlocks steerable synthesis planning and\n  reaction mechanism elucidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical reasoning in LLMs unlocks steerable synthesis planning and\n  reaction mechanism elucidation"
                },
                "summary": "While machine learning algorithms have been shown to excel at specific\nchemical tasks, they have struggled to capture the strategic thinking that\ncharacterizes expert chemical reasoning, limiting their widespread adoption.\nHere we demonstrate that large language models (LLMs) can serve as powerful\nchemical reasoning engines when integrated with traditional search algorithms,\nenabling a new approach to computer-aided chemistry that mirrors human expert\nthinking. Rather than using LLMs to directly manipulate chemical structures, we\nleverage their ability to evaluate chemical strategies and guide search\nalgorithms toward chemically meaningful solutions. We demonstrate this paradigm\nthrough two fundamental challenges: strategy-aware retrosynthetic planning and\nmechanism elucidation. In retrosynthetic planning, our method allows chemists\nto specify desired synthetic strategies in natural language to find routes that\nsatisfy these constraints in vast searches. In mechanism elucidation, LLMs\nguide the search for plausible reaction mechanisms by combining chemical\nprinciples with systematic exploration. Our approach shows strong performance\nacross diverse chemical tasks, with larger models demonstrating increasingly\nsophisticated chemical reasoning. Our approach establishes a new paradigm for\ncomputer-aided chemistry that combines the strategic understanding of LLMs with\nthe precision of traditional chemical tools, opening possibilities for more\nintuitive and powerful chemical reasoning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While machine learning algorithms have been shown to excel at specific\nchemical tasks, they have struggled to capture the strategic thinking that\ncharacterizes expert chemical reasoning, limiting their widespread adoption.\nHere we demonstrate that large language models (LLMs) can serve as powerful\nchemical reasoning engines when integrated with traditional search algorithms,\nenabling a new approach to computer-aided chemistry that mirrors human expert\nthinking. Rather than using LLMs to directly manipulate chemical structures, we\nleverage their ability to evaluate chemical strategies and guide search\nalgorithms toward chemically meaningful solutions. We demonstrate this paradigm\nthrough two fundamental challenges: strategy-aware retrosynthetic planning and\nmechanism elucidation. In retrosynthetic planning, our method allows chemists\nto specify desired synthetic strategies in natural language to find routes that\nsatisfy these constraints in vast searches. In mechanism elucidation, LLMs\nguide the search for plausible reaction mechanisms by combining chemical\nprinciples with systematic exploration. Our approach shows strong performance\nacross diverse chemical tasks, with larger models demonstrating increasingly\nsophisticated chemical reasoning. Our approach establishes a new paradigm for\ncomputer-aided chemistry that combines the strategic understanding of LLMs with\nthe precision of traditional chemical tools, opening possibilities for more\nintuitive and powerful chemical reasoning systems."
                },
                "authors": [
                    {
                        "name": "Andres M Bran"
                    },
                    {
                        "name": "Theo A Neukomm"
                    },
                    {
                        "name": "Daniel P Armstrong"
                    },
                    {
                        "name": "Zlatko Jončev"
                    },
                    {
                        "name": "Philippe Schwaller"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Schwaller"
                },
                "author": "Philippe Schwaller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01905v2",
                "updated": "2025-03-11T15:24:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    24,
                    13,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-28T13:30:10Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    30,
                    10,
                    4,
                    59,
                    0
                ],
                "title": "PaCA: Partial Connection Adaptation for Efficient Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaCA: Partial Connection Adaptation for Efficient Fine-Tuning"
                },
                "summary": "Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage\nand computational costs of fine-tuning large neural network models by training\nonly a few additional adapter parameters, rather than the entire model.\nHowever, the reduction in computational costs due to PEFT does not necessarily\ntranslate to a reduction in training time; although the computational costs of\nthe adapter layers are much smaller than the pretrained layers, it is well\nknown that those two types of layers are processed sequentially on GPUs,\nresulting in significant latency overhead. LoRA and its variants merge low-rank\nadapter matrices with pretrained weights during inference to avoid latency\noverhead, but during training, the pretrained weights remain frozen while the\nadapter matrices are continuously updated, preventing such merging. To mitigate\nthis issue, we propose Partial Connection Adaptation (PaCA), which fine-tunes\nrandomly selected partial connections within the pretrained weights instead of\nintroducing adapter layers in the model. PaCA not only enhances training speed\nby eliminating the time overhead due to the sequential processing of the\nadapter and pretrained layers but also reduces activation memory since only\npartial activations, rather than full activations, need to be stored for\ngradient computation. Compared to LoRA, PaCA reduces training time by 22% and\ntotal memory usage by 16%, while maintaining comparable accuracy across various\nfine-tuning scenarios, such as fine-tuning on the MMLU dataset and instruction\ntuning on the Oasst1 dataset. PaCA can also be combined with quantization,\nenabling the fine-tuning of large models such as LLaMA3.1-70B. In addition,\nPaCA enables training with 23% longer sequence and improves throughput by 16%\non both NVIDIA A100 GPU and INTEL Gaudi2 HPU compared to LoRA. The code is\navailable at https://github.com/WooSunghyeon/paca.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage\nand computational costs of fine-tuning large neural network models by training\nonly a few additional adapter parameters, rather than the entire model.\nHowever, the reduction in computational costs due to PEFT does not necessarily\ntranslate to a reduction in training time; although the computational costs of\nthe adapter layers are much smaller than the pretrained layers, it is well\nknown that those two types of layers are processed sequentially on GPUs,\nresulting in significant latency overhead. LoRA and its variants merge low-rank\nadapter matrices with pretrained weights during inference to avoid latency\noverhead, but during training, the pretrained weights remain frozen while the\nadapter matrices are continuously updated, preventing such merging. To mitigate\nthis issue, we propose Partial Connection Adaptation (PaCA), which fine-tunes\nrandomly selected partial connections within the pretrained weights instead of\nintroducing adapter layers in the model. PaCA not only enhances training speed\nby eliminating the time overhead due to the sequential processing of the\nadapter and pretrained layers but also reduces activation memory since only\npartial activations, rather than full activations, need to be stored for\ngradient computation. Compared to LoRA, PaCA reduces training time by 22% and\ntotal memory usage by 16%, while maintaining comparable accuracy across various\nfine-tuning scenarios, such as fine-tuning on the MMLU dataset and instruction\ntuning on the Oasst1 dataset. PaCA can also be combined with quantization,\nenabling the fine-tuning of large models such as LLaMA3.1-70B. In addition,\nPaCA enables training with 23% longer sequence and improves throughput by 16%\non both NVIDIA A100 GPU and INTEL Gaudi2 HPU compared to LoRA. The code is\navailable at https://github.com/WooSunghyeon/paca."
                },
                "authors": [
                    {
                        "name": "Sunghyeon Woo"
                    },
                    {
                        "name": "Sol Namkung"
                    },
                    {
                        "name": "Sunwoo Lee"
                    },
                    {
                        "name": "Inho Jeong"
                    },
                    {
                        "name": "Beomseok Kim"
                    },
                    {
                        "name": "Dongsuk Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Dongsuk Jeon"
                },
                "author": "Dongsuk Jeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08529v1",
                "updated": "2025-03-11T15:20:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    20,
                    1,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:20:01Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    20,
                    1,
                    1,
                    70,
                    0
                ],
                "title": "SignRep: Enhancing Self-Supervised Sign Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SignRep: Enhancing Self-Supervised Sign Representations"
                },
                "summary": "Sign language representation learning presents unique challenges due to the\ncomplex spatio-temporal nature of signs and the scarcity of labeled datasets.\nExisting methods often rely either on models pre-trained on general visual\ntasks, that lack sign-specific features, or use complex multimodal and\nmulti-branch architectures. To bridge this gap, we introduce a scalable,\nself-supervised framework for sign representation learning. We leverage\nimportant inductive (sign) priors during the training of our RGB model. To do\nthis, we leverage simple but important cues based on skeletons while\npretraining a masked autoencoder. These sign specific priors alongside feature\nregularization and an adversarial style agnostic loss provide a powerful\nbackbone. Notably, our model does not require skeletal keypoints during\ninference, avoiding the limitations of keypoint-based models during downstream\ntasks. When finetuned, we achieve state-of-the-art performance for sign\nrecognition on the WLASL, ASL-Citizen and NMFs-CSL datasets, using a simpler\narchitecture and with only a single-modality. Beyond recognition, our frozen\nmodel excels in sign dictionary retrieval and sign translation, surpassing\nstandard MAE pretraining and skeletal-based representations in retrieval. It\nalso reduces computational costs for training existing sign translation models\nwhile maintaining strong performance on Phoenix2014T, CSL-Daily and How2Sign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign language representation learning presents unique challenges due to the\ncomplex spatio-temporal nature of signs and the scarcity of labeled datasets.\nExisting methods often rely either on models pre-trained on general visual\ntasks, that lack sign-specific features, or use complex multimodal and\nmulti-branch architectures. To bridge this gap, we introduce a scalable,\nself-supervised framework for sign representation learning. We leverage\nimportant inductive (sign) priors during the training of our RGB model. To do\nthis, we leverage simple but important cues based on skeletons while\npretraining a masked autoencoder. These sign specific priors alongside feature\nregularization and an adversarial style agnostic loss provide a powerful\nbackbone. Notably, our model does not require skeletal keypoints during\ninference, avoiding the limitations of keypoint-based models during downstream\ntasks. When finetuned, we achieve state-of-the-art performance for sign\nrecognition on the WLASL, ASL-Citizen and NMFs-CSL datasets, using a simpler\narchitecture and with only a single-modality. Beyond recognition, our frozen\nmodel excels in sign dictionary retrieval and sign translation, surpassing\nstandard MAE pretraining and skeletal-based representations in retrieval. It\nalso reduces computational costs for training existing sign translation models\nwhile maintaining strong performance on Phoenix2014T, CSL-Daily and How2Sign."
                },
                "authors": [
                    {
                        "name": "Ryan Wong"
                    },
                    {
                        "name": "Necati Cihan Camgoz"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05628v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05628v5",
                "updated": "2025-03-12T05:54:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    54,
                    44,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-08T02:23:53Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    2,
                    23,
                    53,
                    1,
                    282,
                    0
                ],
                "title": "A Unified Framework for Motion Reasoning and Generation in Human\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Motion Reasoning and Generation in Human\n  Interaction"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural and contextually relevant text,\nenabling more human-like AI interactions. However, generating and understanding\ninteractive human-like motion, where multiple individuals engage in coordinated\nmovements, remains challenging due to the complexity of modeling these\ninteractions. Additionally, a unified and versatile model is needed to handle\ndiverse interactive scenarios, such as chat systems that dynamically adapt to\nuser instructions and assigned roles. To address these challenges, we introduce\nVIM, the Versatile Interactive Motion-language model, which integrates both\nlanguage and motion modalities to effectively understand, generate, and control\ninteractive motions in multi-turn conversational contexts. Unlike previous\nstudies that primarily focus on uni-directional tasks such as text-to-motion or\nmotion-to-text, VIM employs a unified architecture capable of simultaneously\nunderstanding and generating both motion and text modalities. Given the absence\nof an appropriate dataset to support this task, we introduce Inter-MT2, a\nlarge-scale instruction-tuning dataset containing 82.7K multi-turn interactive\nmotion instructions, covering 153K interactive motion samples. Inter-MT2 spans\ndiverse instructional scenarios, including motion editing, question answering,\nand story generation, leveraging off-the-shelf large language models and motion\ndiffusion models to construct a broad set of interactive motion instructions.\nWe extensively evaluate the versatility of VIM across multiple interactive\nmotion-related tasks, including motion-to-text, text-to-motion, reaction\ngeneration, motion editing, and reasoning about motion sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural and contextually relevant text,\nenabling more human-like AI interactions. However, generating and understanding\ninteractive human-like motion, where multiple individuals engage in coordinated\nmovements, remains challenging due to the complexity of modeling these\ninteractions. Additionally, a unified and versatile model is needed to handle\ndiverse interactive scenarios, such as chat systems that dynamically adapt to\nuser instructions and assigned roles. To address these challenges, we introduce\nVIM, the Versatile Interactive Motion-language model, which integrates both\nlanguage and motion modalities to effectively understand, generate, and control\ninteractive motions in multi-turn conversational contexts. Unlike previous\nstudies that primarily focus on uni-directional tasks such as text-to-motion or\nmotion-to-text, VIM employs a unified architecture capable of simultaneously\nunderstanding and generating both motion and text modalities. Given the absence\nof an appropriate dataset to support this task, we introduce Inter-MT2, a\nlarge-scale instruction-tuning dataset containing 82.7K multi-turn interactive\nmotion instructions, covering 153K interactive motion samples. Inter-MT2 spans\ndiverse instructional scenarios, including motion editing, question answering,\nand story generation, leveraging off-the-shelf large language models and motion\ndiffusion models to construct a broad set of interactive motion instructions.\nWe extensively evaluate the versatility of VIM across multiple interactive\nmotion-related tasks, including motion-to-text, text-to-motion, reaction\ngeneration, motion editing, and reasoning about motion sequences."
                },
                "authors": [
                    {
                        "name": "Jeongeun Park"
                    },
                    {
                        "name": "Sungjoon Choi"
                    },
                    {
                        "name": "Sangdoo Yun"
                    }
                ],
                "author_detail": {
                    "name": "Sangdoo Yun"
                },
                "author": "Sangdoo Yun",
                "arxiv_comment": "https://vim-motion-language.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05628v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05628v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08525v1",
                "updated": "2025-03-11T15:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:17:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training"
                },
                "summary": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes."
                },
                "authors": [
                    {
                        "name": "Tong Wei"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Junliang Xing"
                    },
                    {
                        "name": "Yuanchun Shi"
                    },
                    {
                        "name": "Zongqing Lu"
                    },
                    {
                        "name": "Deheng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Deheng Ye"
                },
                "author": "Deheng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08524v1",
                "updated": "2025-03-11T15:15:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    15,
                    54,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:15:54Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    15,
                    54,
                    1,
                    70,
                    0
                ],
                "title": "Position-Aware Depth Decay Decoding ($D^3$): Boosting Large Language\n  Model Inference Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position-Aware Depth Decay Decoding ($D^3$): Boosting Large Language\n  Model Inference Efficiency"
                },
                "summary": "Due to the large number of parameters, the inference phase of Large Language\nModels (LLMs) is resource-intensive. Unlike traditional model compression,\nwhich needs retraining, recent dynamic computation methods show that not all\ncomponents are required for inference, enabling a training-free pipeline. In\nthis paper, we focus on the dynamic depth of LLM generation. A token-position\naware layer skipping framework is proposed to save 1.5x times operations\nefficiently while maintaining performance. We first observed that tokens\npredicted later have lower perplexity and thus require less computation. Then,\nwe propose a training-free algorithm called Position-Aware Depth Decay Decoding\n($D^3$), which leverages a power-law decay function, $\\left\\lfloor L \\times\n(\\alpha^i) \\right\\rfloor$, to determine the number of layers to retain when\ngenerating token $T_i$. Remarkably, without any retraining, the $D^3$ achieves\nsuccess across a wide range of generation tasks for the first time. Experiments\non large language models (\\ie the Llama) with $7 \\sim 70$ billion parameters\nshow that $D^3$ can achieve an average 1.5x speedup compared with the\nfull-inference pipeline while maintaining comparable performance with nearly no\nperformance drop ($<1\\%$) on the GSM8K and BBH benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the large number of parameters, the inference phase of Large Language\nModels (LLMs) is resource-intensive. Unlike traditional model compression,\nwhich needs retraining, recent dynamic computation methods show that not all\ncomponents are required for inference, enabling a training-free pipeline. In\nthis paper, we focus on the dynamic depth of LLM generation. A token-position\naware layer skipping framework is proposed to save 1.5x times operations\nefficiently while maintaining performance. We first observed that tokens\npredicted later have lower perplexity and thus require less computation. Then,\nwe propose a training-free algorithm called Position-Aware Depth Decay Decoding\n($D^3$), which leverages a power-law decay function, $\\left\\lfloor L \\times\n(\\alpha^i) \\right\\rfloor$, to determine the number of layers to retain when\ngenerating token $T_i$. Remarkably, without any retraining, the $D^3$ achieves\nsuccess across a wide range of generation tasks for the first time. Experiments\non large language models (\\ie the Llama) with $7 \\sim 70$ billion parameters\nshow that $D^3$ can achieve an average 1.5x speedup compared with the\nfull-inference pipeline while maintaining comparable performance with nearly no\nperformance drop ($<1\\%$) on the GSM8K and BBH benchmarks."
                },
                "authors": [
                    {
                        "name": "Siqi Fan"
                    },
                    {
                        "name": "Xuezhi Fang"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Peng Han"
                    },
                    {
                        "name": "Shuo Shang"
                    },
                    {
                        "name": "Yequan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yequan Wang"
                },
                "author": "Yequan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.09727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.09727v3",
                "updated": "2025-03-11T15:10:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    10,
                    37,
                    1,
                    70,
                    0
                ],
                "published": "2023-02-20T02:38:36Z",
                "published_parsed": [
                    2023,
                    2,
                    20,
                    2,
                    38,
                    36,
                    0,
                    51,
                    0
                ],
                "title": "Statistical Inference for Linear Functionals of Online SGD in\n  High-dimensional Linear Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Linear Functionals of Online SGD in\n  High-dimensional Linear Regression"
                },
                "summary": "Stochastic gradient descent (SGD) has emerged as the quintessential method in\na data scientist's toolbox. Using SGD for high-stakes applications requires,\nhowever, careful quantification of the associated uncertainty. Towards that\nend, in this work, we establish a high-dimensional Central Limit Theorem (CLT)\nfor linear functionals of online SGD iterates for overparametrized\nleast-squares regression with non-isotropic Gaussian inputs. We first show that\na bias-corrected CLT holds when the number of iterations of the online SGD,\n$t$, grows sub-linearly in the dimensionality, $d$. In order to use the\ndeveloped result in practice, we further develop an online approach for\nestimating the variance term appearing in the CLT, and establish\nhigh-probability bounds for the developed online estimator. Together with the\nCLT result, this provides a fully online and data-driven way to numerically\nconstruct confidence intervals. This enables practical high-dimensional\nalgorithmic inference with SGD and to the best of our knowledge, is the first\nsuch result.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic gradient descent (SGD) has emerged as the quintessential method in\na data scientist's toolbox. Using SGD for high-stakes applications requires,\nhowever, careful quantification of the associated uncertainty. Towards that\nend, in this work, we establish a high-dimensional Central Limit Theorem (CLT)\nfor linear functionals of online SGD iterates for overparametrized\nleast-squares regression with non-isotropic Gaussian inputs. We first show that\na bias-corrected CLT holds when the number of iterations of the online SGD,\n$t$, grows sub-linearly in the dimensionality, $d$. In order to use the\ndeveloped result in practice, we further develop an online approach for\nestimating the variance term appearing in the CLT, and establish\nhigh-probability bounds for the developed online estimator. Together with the\nCLT result, this provides a fully online and data-driven way to numerically\nconstruct confidence intervals. This enables practical high-dimensional\nalgorithmic inference with SGD and to the best of our knowledge, is the first\nsuch result."
                },
                "authors": [
                    {
                        "name": "Bhavya Agrawalla"
                    },
                    {
                        "name": "Krishnakumar Balasubramanian"
                    },
                    {
                        "name": "Promit Ghosal"
                    }
                ],
                "author_detail": {
                    "name": "Promit Ghosal"
                },
                "author": "Promit Ghosal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.09727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.09727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15488v2",
                "updated": "2025-03-11T15:05:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    5,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-21T14:26:23Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    26,
                    23,
                    4,
                    52,
                    0
                ],
                "title": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection"
                },
                "summary": "Camera-based multi-view 3D detection has emerged as an attractive solution\nfor autonomous driving due to its low cost and broad applicability. However,\ndespite the strong performance of PETR-based methods in 3D perception\nbenchmarks, their direct INT8 quantization for onboard deployment leads to\ndrastic accuracy drops-up to 58.2% in mAP and 36.9% in NDS on the NuScenes\ndataset. In this work, we propose Q-PETR, a quantization-aware position\nembedding transformation that re-engineers key components of the PETR framework\nto reconcile the discrepancy between the dynamic ranges of positional encodings\nand image features, and to adapt the cross-attention mechanism for low-bit\ninference. By redesigning the positional encoding module and introducing an\nadaptive quantization strategy, Q-PETR maintains floating-point performance\nwith a performance degradation of less than 1% under standard 8-bit per-tensor\npost-training quantization. Moreover, compared to its FP32 counterpart, Q-PETR\nachieves a two-fold speedup and reduces memory usage by three times, thereby\noffering a deployment-friendly solution for resource-constrained onboard\ndevices. Extensive experiments across various PETR-series models validate the\nstrong generalization and practical benefits of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera-based multi-view 3D detection has emerged as an attractive solution\nfor autonomous driving due to its low cost and broad applicability. However,\ndespite the strong performance of PETR-based methods in 3D perception\nbenchmarks, their direct INT8 quantization for onboard deployment leads to\ndrastic accuracy drops-up to 58.2% in mAP and 36.9% in NDS on the NuScenes\ndataset. In this work, we propose Q-PETR, a quantization-aware position\nembedding transformation that re-engineers key components of the PETR framework\nto reconcile the discrepancy between the dynamic ranges of positional encodings\nand image features, and to adapt the cross-attention mechanism for low-bit\ninference. By redesigning the positional encoding module and introducing an\nadaptive quantization strategy, Q-PETR maintains floating-point performance\nwith a performance degradation of less than 1% under standard 8-bit per-tensor\npost-training quantization. Moreover, compared to its FP32 counterpart, Q-PETR\nachieves a two-fold speedup and reduces memory usage by three times, thereby\noffering a deployment-friendly solution for resource-constrained onboard\ndevices. Extensive experiments across various PETR-series models validate the\nstrong generalization and practical benefits of our approach."
                },
                "authors": [
                    {
                        "name": "Jiangyong Yu"
                    },
                    {
                        "name": "Changyong Shu"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Sifan Zhou"
                    },
                    {
                        "name": "Zichen Yu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08514v1",
                "updated": "2025-03-11T15:03:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    3,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:03:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    3,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "The Gaia Ultracool Dwarf Sample -- VI. Spectral Types and Properties of\n  51 Ultracool Dwarfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gaia Ultracool Dwarf Sample -- VI. Spectral Types and Properties of\n  51 Ultracool Dwarfs"
                },
                "summary": "Near-infrared spectra from the IRTF/SpeX and Blanco/ARCoIRIS\ntelescope/instrument combinations are used for spectroscopic classification, to\nmeasure radial velocities and for the inference of astrophysical properties of\n51 Gaia-selected nearby ultracool dwarfs. In this sample, 44 are newly\nclassified in the near infrared. All but one of the UCDs are within 100 pc, and\n37 lie within 50 pc. We find a total of 26 M-types, 24 L-types and one T-type\nin our sample. The positions of the majority of the UCDs on colour-magnitude\ndiagrams and with evolutionary cooling track plots indicate that they are\nlargely old and stellar in nature. There are a few UCDs of particular interest\nwhich lie away from expected trends, highlighting potential young, binary and\nthick disc/subdwarf UCDs. From spectral and kinematic analyses, we identify\nUCDs of particular interest for further investigation, including seven\npotentially young UCDs, three thick disc UCDs, one subdwarf, six wide binaries,\nand six unresolved binaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-infrared spectra from the IRTF/SpeX and Blanco/ARCoIRIS\ntelescope/instrument combinations are used for spectroscopic classification, to\nmeasure radial velocities and for the inference of astrophysical properties of\n51 Gaia-selected nearby ultracool dwarfs. In this sample, 44 are newly\nclassified in the near infrared. All but one of the UCDs are within 100 pc, and\n37 lie within 50 pc. We find a total of 26 M-types, 24 L-types and one T-type\nin our sample. The positions of the majority of the UCDs on colour-magnitude\ndiagrams and with evolutionary cooling track plots indicate that they are\nlargely old and stellar in nature. There are a few UCDs of particular interest\nwhich lie away from expected trends, highlighting potential young, binary and\nthick disc/subdwarf UCDs. From spectral and kinematic analyses, we identify\nUCDs of particular interest for further investigation, including seven\npotentially young UCDs, three thick disc UCDs, one subdwarf, six wide binaries,\nand six unresolved binaries."
                },
                "authors": [
                    {
                        "name": "Gemma Cheng"
                    },
                    {
                        "name": "H. R. A. Jones"
                    },
                    {
                        "name": "R. L. Smart"
                    },
                    {
                        "name": "Federico Marocco"
                    },
                    {
                        "name": "W. J. Cooper"
                    },
                    {
                        "name": "Adam Burgasser"
                    },
                    {
                        "name": "Juan Carlos Beamin"
                    },
                    {
                        "name": "D. J. Pinfield"
                    },
                    {
                        "name": "Jonathan Gagné"
                    },
                    {
                        "name": "Leslie Moranta"
                    }
                ],
                "author_detail": {
                    "name": "Leslie Moranta"
                },
                "author": "Leslie Moranta",
                "arxiv_comment": "35 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01865v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01865v3",
                "updated": "2025-03-11T15:03:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    3,
                    32,
                    1,
                    70,
                    0
                ],
                "published": "2024-03-04T09:21:10Z",
                "published_parsed": [
                    2024,
                    3,
                    4,
                    9,
                    21,
                    10,
                    0,
                    64,
                    0
                ],
                "title": "Out-of-distribution robustness for multivariate analysis via causal\n  regularisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution robustness for multivariate analysis via causal\n  regularisation"
                },
                "summary": "We propose a regularisation strategy of classical machine learning algorithms\nrooted in causality that ensures robustness against distribution shifts.\nBuilding upon the anchor regression framework, we demonstrate how incorporating\na straightforward regularisation term into the loss function of classical\nmultivariate analysis algorithms, such as (orthonormalized) partial least\nsquares, reduced-rank regression, and multiple linear regression, enables\nout-of-distribution generalisation. Our framework allows users to efficiently\nverify the compatibility of a loss function with the regularisation strategy.\nEstimators for selected algorithms are provided, showcasing consistency and\nefficacy in synthetic and real-world climate science problems. The empirical\nvalidation highlights the versatility of anchor regularisation, emphasizing its\ncompatibility with multivariate analysis approaches and its role in enhancing\nreplicability while guarding against distribution shifts. The extended anchor\nframework advances causal inference methodologies, addressing the need for\nreliable out-of-distribution generalisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a regularisation strategy of classical machine learning algorithms\nrooted in causality that ensures robustness against distribution shifts.\nBuilding upon the anchor regression framework, we demonstrate how incorporating\na straightforward regularisation term into the loss function of classical\nmultivariate analysis algorithms, such as (orthonormalized) partial least\nsquares, reduced-rank regression, and multiple linear regression, enables\nout-of-distribution generalisation. Our framework allows users to efficiently\nverify the compatibility of a loss function with the regularisation strategy.\nEstimators for selected algorithms are provided, showcasing consistency and\nefficacy in synthetic and real-world climate science problems. The empirical\nvalidation highlights the versatility of anchor regularisation, emphasizing its\ncompatibility with multivariate analysis approaches and its role in enhancing\nreplicability while guarding against distribution shifts. The extended anchor\nframework advances causal inference methodologies, addressing the need for\nreliable out-of-distribution generalisation."
                },
                "authors": [
                    {
                        "name": "Homer Durand"
                    },
                    {
                        "name": "Gherardo Varando"
                    },
                    {
                        "name": "Nathan Mankovich"
                    },
                    {
                        "name": "Gustau Camps-Valls"
                    }
                ],
                "author_detail": {
                    "name": "Gustau Camps-Valls"
                },
                "author": "Gustau Camps-Valls",
                "arxiv_comment": "26 pages, 15 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01865v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01865v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62Hxx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08510v1",
                "updated": "2025-03-11T15:00:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    0,
                    22,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:00:22Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    0,
                    22,
                    1,
                    70,
                    0
                ],
                "title": "External Knowledge Injection for CLIP-Based Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External Knowledge Injection for CLIP-Based Class-Incremental Learning"
                },
                "summary": "Class-Incremental Learning (CIL) enables learning systems to continuously\nadapt to evolving data streams. With the advancement of pre-training,\nleveraging pre-trained vision-language models (e.g., CLIP) offers a promising\nstarting point for CIL. However, CLIP makes decisions by matching visual\nembeddings to class names, overlooking the rich contextual information conveyed\nthrough language. For instance, the concept of ``cat'' can be decomposed into\nfeatures like tail, fur, and face for recognition. Besides, since the model is\ncontinually updated, these detailed features are overwritten in CIL, requiring\nexternal knowledge for compensation. In this paper, we introduce ExterNal\nknowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer\nfrom outside the dataset, we propose a dual-branch injection tuning framework\nthat encodes informative knowledge from both visual and textual modalities. The\nvisual branch is enhanced with data augmentation to enrich the visual features,\nwhile the textual branch leverages GPT-4 to rewrite discriminative descriptors.\nIn addition to this on-the-fly knowledge injection, we also implement\npost-tuning knowledge by re-ranking the prediction results during inference.\nWith the injected knowledge, the model can better capture informative features\nfor downstream tasks as data evolves. Extensive experiments demonstrate the\nstate-of-the-art performance of ENGINE. Code is available at:\nhttps://github.com/RenaissCode/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Class-Incremental Learning (CIL) enables learning systems to continuously\nadapt to evolving data streams. With the advancement of pre-training,\nleveraging pre-trained vision-language models (e.g., CLIP) offers a promising\nstarting point for CIL. However, CLIP makes decisions by matching visual\nembeddings to class names, overlooking the rich contextual information conveyed\nthrough language. For instance, the concept of ``cat'' can be decomposed into\nfeatures like tail, fur, and face for recognition. Besides, since the model is\ncontinually updated, these detailed features are overwritten in CIL, requiring\nexternal knowledge for compensation. In this paper, we introduce ExterNal\nknowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer\nfrom outside the dataset, we propose a dual-branch injection tuning framework\nthat encodes informative knowledge from both visual and textual modalities. The\nvisual branch is enhanced with data augmentation to enrich the visual features,\nwhile the textual branch leverages GPT-4 to rewrite discriminative descriptors.\nIn addition to this on-the-fly knowledge injection, we also implement\npost-tuning knowledge by re-ranking the prediction results during inference.\nWith the injected knowledge, the model can better capture informative features\nfor downstream tasks as data evolves. Extensive experiments demonstrate the\nstate-of-the-art performance of ENGINE. Code is available at:\nhttps://github.com/RenaissCode/ENGINE"
                },
                "authors": [
                    {
                        "name": "Da-Wei Zhou"
                    },
                    {
                        "name": "Kai-Wen Li"
                    },
                    {
                        "name": "Jingyi Ning"
                    },
                    {
                        "name": "Han-Jia Ye"
                    },
                    {
                        "name": "Lijun Zhang"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    }
                ],
                "author_detail": {
                    "name": "De-Chuan Zhan"
                },
                "author": "De-Chuan Zhan",
                "arxiv_comment": "Code is available at: https://github.com/RenaissCode/ENGINE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08508v1",
                "updated": "2025-03-11T14:57:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    57,
                    53,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:57:53Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    57,
                    53,
                    1,
                    70,
                    0
                ],
                "title": "LightPlanner: Unleashing the Reasoning Capabilities of Lightweight Large\n  Language Models in Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightPlanner: Unleashing the Reasoning Capabilities of Lightweight Large\n  Language Models in Task Planning"
                },
                "summary": "In recent years, lightweight large language models (LLMs) have garnered\nsignificant attention in the robotics field due to their low computational\nresource requirements and suitability for edge deployment. However, in task\nplanning -- particularly for complex tasks that involve dynamic semantic logic\nreasoning -- lightweight LLMs have underperformed. To address this limitation,\nwe propose a novel task planner, LightPlanner, which enhances the performance\nof lightweight LLMs in complex task planning by fully leveraging their\nreasoning capabilities. Unlike conventional planners that use fixed skill\ntemplates, LightPlanner controls robot actions via parameterized function\ncalls, dynamically generating parameter values. This approach allows for\nfine-grained skill control and improves task planning success rates in complex\nscenarios. Furthermore, we introduce hierarchical deep reasoning. Before\ngenerating each action decision step, LightPlanner thoroughly considers three\nlevels: action execution (feedback verification), semantic parsing (goal\nconsistency verification), and parameter generation (parameter validity\nverification). This ensures the correctness of subsequent action controls.\nAdditionally, we incorporate a memory module to store historical actions,\nthereby reducing context length and enhancing planning efficiency for long-term\ntasks. We train the LightPlanner-1.5B model on our LightPlan-40k dataset, which\ncomprises 40,000 action controls across tasks with 2 to 13 action steps.\nExperiments demonstrate that our model achieves the highest task success rate\ndespite having the smallest number of parameters. In tasks involving spatial\nsemantic reasoning, the success rate exceeds that of ReAct by 14.9 percent.\nMoreover, we demonstrate LightPlanner's potential to operate on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, lightweight large language models (LLMs) have garnered\nsignificant attention in the robotics field due to their low computational\nresource requirements and suitability for edge deployment. However, in task\nplanning -- particularly for complex tasks that involve dynamic semantic logic\nreasoning -- lightweight LLMs have underperformed. To address this limitation,\nwe propose a novel task planner, LightPlanner, which enhances the performance\nof lightweight LLMs in complex task planning by fully leveraging their\nreasoning capabilities. Unlike conventional planners that use fixed skill\ntemplates, LightPlanner controls robot actions via parameterized function\ncalls, dynamically generating parameter values. This approach allows for\nfine-grained skill control and improves task planning success rates in complex\nscenarios. Furthermore, we introduce hierarchical deep reasoning. Before\ngenerating each action decision step, LightPlanner thoroughly considers three\nlevels: action execution (feedback verification), semantic parsing (goal\nconsistency verification), and parameter generation (parameter validity\nverification). This ensures the correctness of subsequent action controls.\nAdditionally, we incorporate a memory module to store historical actions,\nthereby reducing context length and enhancing planning efficiency for long-term\ntasks. We train the LightPlanner-1.5B model on our LightPlan-40k dataset, which\ncomprises 40,000 action controls across tasks with 2 to 13 action steps.\nExperiments demonstrate that our model achieves the highest task success rate\ndespite having the smallest number of parameters. In tasks involving spatial\nsemantic reasoning, the success rate exceeds that of ReAct by 14.9 percent.\nMoreover, we demonstrate LightPlanner's potential to operate on edge devices."
                },
                "authors": [
                    {
                        "name": "Weijie Zhou"
                    },
                    {
                        "name": "Yi Peng"
                    },
                    {
                        "name": "Manli Tao"
                    },
                    {
                        "name": "Chaoyang Zhao"
                    },
                    {
                        "name": "Honghui Dong"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "arxiv_affiliation": "objecteye.Inc",
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08506v1",
                "updated": "2025-03-11T14:56:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    56,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:56:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    56,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper\n  Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper\n  Reviews"
                },
                "summary": "Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11067v2",
                "updated": "2025-03-11T14:50:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    50,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-10-14T20:25:49Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    20,
                    25,
                    49,
                    0,
                    288,
                    0
                ],
                "title": "Variational Inference in Location-Scale Families: Exact Recovery of the\n  Mean and Correlation Matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Inference in Location-Scale Families: Exact Recovery of the\n  Mean and Correlation Matrix"
                },
                "summary": "Given an intractable target density $p$, variational inference (VI) attempts\nto find the best approximation $q$ from a tractable family $Q$. This is\ntypically done by minimizing the exclusive Kullback-Leibler divergence,\n$\\text{KL}(q||p)$. In practice, $Q$ is not rich enough to contain $p$, and the\napproximation is misspecified even when it is a unique global minimizer of\n$\\text{KL}(q||p)$. In this paper, we analyze the robustness of VI to these\nmisspecifications when $p$ exhibits certain symmetries and $Q$ is a\nlocation-scale family that shares these symmetries. We prove strong guarantees\nfor VI not only under mild regularity conditions but also in the face of severe\nmisspecifications. Namely, we show that (i) VI recovers the mean of $p$ when\n$p$ exhibits an \\textit{even} symmetry, and (ii) it recovers the correlation\nmatrix of $p$ when in addition~$p$ exhibits an \\textit{elliptical} symmetry.\nThese guarantees hold for the mean even when $q$ is factorized and $p$ is not,\nand for the correlation matrix even when~$q$ and~$p$ behave differently in\ntheir tails. We analyze various regimes of Bayesian inference where these\nsymmetries are useful idealizations, and we also investigate experimentally how\nVI behaves in their absence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given an intractable target density $p$, variational inference (VI) attempts\nto find the best approximation $q$ from a tractable family $Q$. This is\ntypically done by minimizing the exclusive Kullback-Leibler divergence,\n$\\text{KL}(q||p)$. In practice, $Q$ is not rich enough to contain $p$, and the\napproximation is misspecified even when it is a unique global minimizer of\n$\\text{KL}(q||p)$. In this paper, we analyze the robustness of VI to these\nmisspecifications when $p$ exhibits certain symmetries and $Q$ is a\nlocation-scale family that shares these symmetries. We prove strong guarantees\nfor VI not only under mild regularity conditions but also in the face of severe\nmisspecifications. Namely, we show that (i) VI recovers the mean of $p$ when\n$p$ exhibits an \\textit{even} symmetry, and (ii) it recovers the correlation\nmatrix of $p$ when in addition~$p$ exhibits an \\textit{elliptical} symmetry.\nThese guarantees hold for the mean even when $q$ is factorized and $p$ is not,\nand for the correlation matrix even when~$q$ and~$p$ behave differently in\ntheir tails. We analyze various regimes of Bayesian inference where these\nsymmetries are useful idealizations, and we also investigate experimentally how\nVI behaves in their absence."
                },
                "authors": [
                    {
                        "name": "Charles C. Margossian"
                    },
                    {
                        "name": "Lawrence K. Saul"
                    }
                ],
                "author_detail": {
                    "name": "Lawrence K. Saul"
                },
                "author": "Lawrence K. Saul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00781v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00781v3",
                "updated": "2025-03-11T14:48:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    48,
                    56,
                    1,
                    70,
                    0
                ],
                "published": "2024-10-01T15:22:42Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    15,
                    22,
                    42,
                    1,
                    275,
                    0
                ],
                "title": "Modeling Neural Switching via Drift-Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Neural Switching via Drift-Diffusion Models"
                },
                "summary": "Neural encoding is a field in neuroscience that focuses on characterizing how\ninformation from stimuli is encoded in the spiking activity of neurons. When\nmore than one stimulus is present, a theory known as multiplexing posits that\nneurons temporally switch between encoding various stimuli, creating a\nfluctuating firing pattern. Here, we propose a new statistical framework to\nanalyze rate fluctuations and discern whether neurons employ multiplexing as a\nmeans of encoding multiple stimuli. We adopt a mechanistic approach to modeling\nmultiplexing by constructing a non-Markovian endogenous state-space model.\nSpecifically, we posit that multiplexing arises from competition between the\nstimuli, which are modeled as latent drift-diffusion processes. We propose a\nnew MCMC algorithm for conducting posterior inference on similar types of\nstate-space models, where typical state-space MCMC methods fail due to strong\ndependence between the parameters. In addition, we develop alternative models\nthat represent a wide class of alternative encoding theories and perform model\ncomparison using WAIC to determine whether the data suggest the occurrence\nmultiplexing over alternative theories of neural encoding. Using the proposed\nframework, we provide evidence of multiplexing within the inferior colliculus\nand novel insight into the switching dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural encoding is a field in neuroscience that focuses on characterizing how\ninformation from stimuli is encoded in the spiking activity of neurons. When\nmore than one stimulus is present, a theory known as multiplexing posits that\nneurons temporally switch between encoding various stimuli, creating a\nfluctuating firing pattern. Here, we propose a new statistical framework to\nanalyze rate fluctuations and discern whether neurons employ multiplexing as a\nmeans of encoding multiple stimuli. We adopt a mechanistic approach to modeling\nmultiplexing by constructing a non-Markovian endogenous state-space model.\nSpecifically, we posit that multiplexing arises from competition between the\nstimuli, which are modeled as latent drift-diffusion processes. We propose a\nnew MCMC algorithm for conducting posterior inference on similar types of\nstate-space models, where typical state-space MCMC methods fail due to strong\ndependence between the parameters. In addition, we develop alternative models\nthat represent a wide class of alternative encoding theories and perform model\ncomparison using WAIC to determine whether the data suggest the occurrence\nmultiplexing over alternative theories of neural encoding. Using the proposed\nframework, we provide evidence of multiplexing within the inferior colliculus\nand novel insight into the switching dynamics."
                },
                "authors": [
                    {
                        "name": "Nicholas Marco"
                    },
                    {
                        "name": "Jennifer M. Groh"
                    },
                    {
                        "name": "Surya T. Tokdar"
                    }
                ],
                "author_detail": {
                    "name": "Surya T. Tokdar"
                },
                "author": "Surya T. Tokdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00781v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00781v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08497v1",
                "updated": "2025-03-11T14:48:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    48,
                    1,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:48:01Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    48,
                    1,
                    1,
                    70,
                    0
                ],
                "title": "MMRL: Multi-Modal Representation Learning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMRL: Multi-Modal Representation Learning for Vision-Language Models"
                },
                "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have become essential\nfor transfer learning across diverse tasks. However, adapting these models with\nlimited few-shot data often leads to overfitting, diminishing their performance\non new tasks. To tackle this issue, we propose a novel Multi-Modal\nRepresentation Learning (MMRL) framework that introduces a shared, learnable,\nand modality-agnostic representation space. MMRL projects the space tokens to\ntext and image representation tokens, facilitating more effective multi-modal\ninteractions. Unlike previous approaches that solely optimize class token\nfeatures, MMRL integrates representation tokens at higher layers of the\nencoders--where dataset-specific features are more prominent--while preserving\ngeneralized knowledge in the lower layers. During training, both representation\nand class features are optimized, with trainable projection layer applied to\nthe representation tokens, whereas the class token projection layer remains\nfrozen to retain pre-trained knowledge. Furthermore, a regularization term is\nintroduced to align the class features and text features with the zero-shot\nfeatures from the frozen VLM, thereby safeguarding the model's generalization\ncapacity. For inference, a decoupling strategy is employed, wherein both\nrepresentation and class features are utilized for base classes, while only the\nclass features, which retain more generalized knowledge, are used for new\ntasks. Extensive experiments across 15 datasets demonstrate that MMRL\noutperforms state-of-the-art methods, achieving a balanced trade-off between\ntask-specific adaptation and generalization. Code is available at\nhttps://github.com/yunncheng/MMRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained Vision-Language Models (VLMs) have become essential\nfor transfer learning across diverse tasks. However, adapting these models with\nlimited few-shot data often leads to overfitting, diminishing their performance\non new tasks. To tackle this issue, we propose a novel Multi-Modal\nRepresentation Learning (MMRL) framework that introduces a shared, learnable,\nand modality-agnostic representation space. MMRL projects the space tokens to\ntext and image representation tokens, facilitating more effective multi-modal\ninteractions. Unlike previous approaches that solely optimize class token\nfeatures, MMRL integrates representation tokens at higher layers of the\nencoders--where dataset-specific features are more prominent--while preserving\ngeneralized knowledge in the lower layers. During training, both representation\nand class features are optimized, with trainable projection layer applied to\nthe representation tokens, whereas the class token projection layer remains\nfrozen to retain pre-trained knowledge. Furthermore, a regularization term is\nintroduced to align the class features and text features with the zero-shot\nfeatures from the frozen VLM, thereby safeguarding the model's generalization\ncapacity. For inference, a decoupling strategy is employed, wherein both\nrepresentation and class features are utilized for base classes, while only the\nclass features, which retain more generalized knowledge, are used for new\ntasks. Extensive experiments across 15 datasets demonstrate that MMRL\noutperforms state-of-the-art methods, achieving a balanced trade-off between\ntask-specific adaptation and generalization. Code is available at\nhttps://github.com/yunncheng/MMRL."
                },
                "authors": [
                    {
                        "name": "Yuncheng Guo"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08495v1",
                "updated": "2025-03-11T14:47:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    47,
                    24,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:47:24Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    47,
                    24,
                    1,
                    70,
                    0
                ],
                "title": "Enhancing Multi-Hop Fact Verification with Structured\n  Knowledge-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Hop Fact Verification with Structured\n  Knowledge-Augmented Large Language Models"
                },
                "summary": "The rapid development of social platforms exacerbates the dissemination of\nmisinformation, which stimulates the research in fact verification. Recent\nstudies tend to leverage semantic features to solve this problem as a\nsingle-hop task. However, the process of verifying a claim requires several\npieces of evidence with complicated inner logic and relations to verify the\ngiven claim in real-world situations. Recent studies attempt to improve both\nunderstanding and reasoning abilities to enhance the performance, but they\noverlook the crucial relations between entities that benefit models to\nunderstand better and facilitate the prediction. To emphasize the significance\nof relations, we resort to Large Language Models (LLMs) considering their\nexcellent understanding ability. Instead of other methods using LLMs as the\npredictor, we take them as relation extractors, for they do better in\nunderstanding rather than reasoning according to the experimental results.\nThus, to solve the challenges above, we propose a novel Structured\nKnowledge-Augmented LLM-based Network (LLM-SKAN) for multi-hop fact\nverification. Specifically, we utilize an LLM-driven Knowledge Extractor to\ncapture fine-grained information, including entities and their complicated\nrelations. Besides, we leverage a Knowledge-Augmented Relation Graph Fusion\nmodule to interact with each node and learn better claim-evidence\nrepresentations comprehensively. The experimental results on four common-used\ndatasets demonstrate the effectiveness and superiority of our model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of social platforms exacerbates the dissemination of\nmisinformation, which stimulates the research in fact verification. Recent\nstudies tend to leverage semantic features to solve this problem as a\nsingle-hop task. However, the process of verifying a claim requires several\npieces of evidence with complicated inner logic and relations to verify the\ngiven claim in real-world situations. Recent studies attempt to improve both\nunderstanding and reasoning abilities to enhance the performance, but they\noverlook the crucial relations between entities that benefit models to\nunderstand better and facilitate the prediction. To emphasize the significance\nof relations, we resort to Large Language Models (LLMs) considering their\nexcellent understanding ability. Instead of other methods using LLMs as the\npredictor, we take them as relation extractors, for they do better in\nunderstanding rather than reasoning according to the experimental results.\nThus, to solve the challenges above, we propose a novel Structured\nKnowledge-Augmented LLM-based Network (LLM-SKAN) for multi-hop fact\nverification. Specifically, we utilize an LLM-driven Knowledge Extractor to\ncapture fine-grained information, including entities and their complicated\nrelations. Besides, we leverage a Knowledge-Augmented Relation Graph Fusion\nmodule to interact with each node and learn better claim-evidence\nrepresentations comprehensively. The experimental results on four common-used\ndatasets demonstrate the effectiveness and superiority of our model."
                },
                "authors": [
                    {
                        "name": "Han Cao"
                    },
                    {
                        "name": "Lingwei Wei"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16888v2",
                "updated": "2025-03-11T14:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    46,
                    34,
                    1,
                    70,
                    0
                ],
                "published": "2024-10-22T10:46:36Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    10,
                    46,
                    36,
                    1,
                    296,
                    0
                ],
                "title": "Unsupervised Time Series Anomaly Prediction with Importance-based\n  Generative Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Time Series Anomaly Prediction with Importance-based\n  Generative Contrastive Learning"
                },
                "summary": "Time series anomaly prediction plays an essential role in many real-world\nscenarios, such as environmental prevention and prompt maintenance of\ncyber-physical systems. However, existing time series anomaly prediction\nmethods mainly require supervised training with plenty of manually labeled\ndata, which are difficult to obtain in practice. Besides, unseen anomalies can\noccur during inference, which could differ from the labeled training data and\nmake these models fail to predict such new anomalies. In this paper, we study a\nnovel problem of unsupervised time series anomaly prediction. We provide a\ntheoretical analysis and propose Importance-based Generative Contrastive\nLearning (IGCL) to address the aforementioned problems. IGCL distinguishes\nbetween normal and anomaly precursors, which are generated by our anomaly\nprecursor pattern generation module. To address the efficiency issues caused by\nthe potential complex anomaly precursor combinations, we propose a memory bank\nwith importance-based scores to adaptively store representative anomaly\nprecursors and generate more complicated anomaly precursors. Extensive\nexperiments on seven benchmark datasets show our method outperforms\nstate-of-the-art baselines on unsupervised time series anomaly prediction\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series anomaly prediction plays an essential role in many real-world\nscenarios, such as environmental prevention and prompt maintenance of\ncyber-physical systems. However, existing time series anomaly prediction\nmethods mainly require supervised training with plenty of manually labeled\ndata, which are difficult to obtain in practice. Besides, unseen anomalies can\noccur during inference, which could differ from the labeled training data and\nmake these models fail to predict such new anomalies. In this paper, we study a\nnovel problem of unsupervised time series anomaly prediction. We provide a\ntheoretical analysis and propose Importance-based Generative Contrastive\nLearning (IGCL) to address the aforementioned problems. IGCL distinguishes\nbetween normal and anomaly precursors, which are generated by our anomaly\nprecursor pattern generation module. To address the efficiency issues caused by\nthe potential complex anomaly precursor combinations, we propose a memory bank\nwith importance-based scores to adaptively store representative anomaly\nprecursors and generate more complicated anomaly precursors. Extensive\nexperiments on seven benchmark datasets show our method outperforms\nstate-of-the-art baselines on unsupervised time series anomaly prediction\nproblems."
                },
                "authors": [
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Zhihao Zhuang"
                    },
                    {
                        "name": "Chenjuan Guo"
                    },
                    {
                        "name": "Hao Miao"
                    },
                    {
                        "name": "Yunyao Cheng"
                    },
                    {
                        "name": "Bin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Yang"
                },
                "author": "Bin Yang",
                "arxiv_comment": "revised",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10392v2",
                "updated": "2025-03-11T14:42:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    42,
                    27,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-14T18:59:59Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    59,
                    4,
                    45,
                    0
                ],
                "title": "TSP3D: Text-guided Sparse Voxel Pruning for Efficient 3D Visual\n  Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TSP3D: Text-guided Sparse Voxel Pruning for Efficient 3D Visual\n  Grounding"
                },
                "summary": "In this paper, we propose an efficient multi-level convolution architecture\nfor 3D visual grounding. Conventional methods are difficult to meet the\nrequirements of real-time inference due to the two-stage or point-based\narchitecture. Inspired by the success of multi-level fully sparse convolutional\narchitecture in 3D object detection, we aim to build a new 3D visual grounding\nframework following this technical route. However, as in 3D visual grounding\ntask the 3D scene representation should be deeply interacted with text\nfeatures, sparse convolution-based architecture is inefficient for this\ninteraction due to the large amount of voxel features. To this end, we propose\ntext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D\nscene representation and text features in an efficient way by gradual region\npruning and target completion. Specifically, TGP iteratively sparsifies the 3D\nscene representation and thus efficiently interacts the voxel features with\ntext features by cross-attention. To mitigate the affect of pruning on delicate\ngeometric information, CBA adaptively fixes the over-pruned region by voxel\ncompletion with negligible computational overhead. Compared with previous\nsingle-stage methods, our method achieves top inference speed and surpasses\nprevious fastest method by 100\\% FPS. Our method also achieves state-of-the-art\naccuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on\nScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code\nis available at\n\\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an efficient multi-level convolution architecture\nfor 3D visual grounding. Conventional methods are difficult to meet the\nrequirements of real-time inference due to the two-stage or point-based\narchitecture. Inspired by the success of multi-level fully sparse convolutional\narchitecture in 3D object detection, we aim to build a new 3D visual grounding\nframework following this technical route. However, as in 3D visual grounding\ntask the 3D scene representation should be deeply interacted with text\nfeatures, sparse convolution-based architecture is inefficient for this\ninteraction due to the large amount of voxel features. To this end, we propose\ntext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D\nscene representation and text features in an efficient way by gradual region\npruning and target completion. Specifically, TGP iteratively sparsifies the 3D\nscene representation and thus efficiently interacts the voxel features with\ntext features by cross-attention. To mitigate the affect of pruning on delicate\ngeometric information, CBA adaptively fixes the over-pruned region by voxel\ncompletion with negligible computational overhead. Compared with previous\nsingle-stage methods, our method achieves top inference speed and surpasses\nprevious fastest method by 100\\% FPS. Our method also achieves state-of-the-art\naccuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on\nScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code\nis available at\n\\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}."
                },
                "authors": [
                    {
                        "name": "Wenxuan Guo"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Jianjiang Feng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted at CVPR2025 with a top score",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08486v1",
                "updated": "2025-03-11T14:40:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    40,
                    56,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:40:56Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    40,
                    56,
                    1,
                    70,
                    0
                ],
                "title": "Inferring Input Grammars from Code with Symbolic Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Input Grammars from Code with Symbolic Parsing"
                },
                "summary": "Generating effective test inputs for a software system requires that these\ninputs be valid, as they will otherwise be rejected without reaching actual\nfunctionality. In the absence of a specification for the input language, common\ntest generation techniques rely on sample inputs, which are abstracted into\nmatching grammars and/or evolved guided by test coverage. However, if sample\ninputs miss features of the input language, the chances of generating these\nfeatures randomly are slim.\n  In this work, we present the first technique for symbolically and\nautomatically mining input grammars from the code of recursive descent parsers.\nSo far, the complexity of parsers has made such a symbolic analysis challenging\nto impossible. Our realization of the symbolic parsing technique overcomes\nthese challenges by (1) associating each parser function parse_ELEM() with a\nnonterminal <ELEM>; (2) limiting recursive calls and loop iterations, such that\na symbolic analysis of parse_ELEM() needs to consider only a finite number of\npaths; and (3) for each path, create an expansion alternative for <ELEM>. Being\npurely static, symbolic parsing does not require seed inputs; as it mitigates\npath explosion, it scales to complex parsers.\n  Our evaluation promises symbolic parsing to be highly accurate. Applied on\nparsers for complex languages such as TINY-C or JSON, our STALAGMITE\nimplementation extracts grammars with an accuracy of 99--100%, widely improving\nover the state of the art despite requiring only the program code and no input\nsamples. The resulting grammars cover the entire input space, allowing for\ncomprehensive and effective test generation, reverse engineering, and\ndocumentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating effective test inputs for a software system requires that these\ninputs be valid, as they will otherwise be rejected without reaching actual\nfunctionality. In the absence of a specification for the input language, common\ntest generation techniques rely on sample inputs, which are abstracted into\nmatching grammars and/or evolved guided by test coverage. However, if sample\ninputs miss features of the input language, the chances of generating these\nfeatures randomly are slim.\n  In this work, we present the first technique for symbolically and\nautomatically mining input grammars from the code of recursive descent parsers.\nSo far, the complexity of parsers has made such a symbolic analysis challenging\nto impossible. Our realization of the symbolic parsing technique overcomes\nthese challenges by (1) associating each parser function parse_ELEM() with a\nnonterminal <ELEM>; (2) limiting recursive calls and loop iterations, such that\na symbolic analysis of parse_ELEM() needs to consider only a finite number of\npaths; and (3) for each path, create an expansion alternative for <ELEM>. Being\npurely static, symbolic parsing does not require seed inputs; as it mitigates\npath explosion, it scales to complex parsers.\n  Our evaluation promises symbolic parsing to be highly accurate. Applied on\nparsers for complex languages such as TINY-C or JSON, our STALAGMITE\nimplementation extracts grammars with an accuracy of 99--100%, widely improving\nover the state of the art despite requiring only the program code and no input\nsamples. The resulting grammars cover the entire input space, allowing for\ncomprehensive and effective test generation, reverse engineering, and\ndocumentation."
                },
                "authors": [
                    {
                        "name": "Leon Bettscheider"
                    },
                    {
                        "name": "Andreas Zeller"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zeller"
                },
                "author": "Andreas Zeller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N30 (Primary), 68W32 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2; D.2.5; D.2.1; F.3.1; F.3.2; F.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08485v1",
                "updated": "2025-03-11T14:37:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    37,
                    39,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:37:39Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    37,
                    39,
                    1,
                    70,
                    0
                ],
                "title": "TT-GaussOcc: Test-Time Compute for Self-Supervised Occupancy Prediction\n  via Spatio-Temporal Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TT-GaussOcc: Test-Time Compute for Self-Supervised Occupancy Prediction\n  via Spatio-Temporal Gaussian Splatting"
                },
                "summary": "Self-supervised 3D occupancy prediction offers a promising solution for\nunderstanding complex driving scenes without requiring costly 3D annotations.\nHowever, training dense voxel decoders to capture fine-grained geometry and\nsemantics can demand hundreds of GPU hours, and such models often fail to adapt\nto varying voxel resolutions or new classes without extensive retraining. To\novercome these limitations, we propose a practical and flexible test-time\noccupancy prediction framework termed TT-GaussOcc. Our approach incrementally\noptimizes time-aware 3D Gaussians instantiated from raw sensor streams at\nruntime, enabling voxelization at arbitrary user-specified resolution.\nSpecifically, TT-GaussOcc operates in a \"lift-move-voxel\" symphony: we first\n\"lift\" surrounding-view semantics obtained from 2D vision foundation models\n(VLMs) to instantiate Gaussians at non-empty 3D space; Next, we \"move\" dynamic\nGaussians from previous frames along estimated Gaussian scene flow to complete\nappearance and eliminate trailing artifacts of fast-moving objects, while\naccumulating static Gaussians to enforce temporal consistency; Finally, we\nmitigate inherent noises in semantic predictions and scene flow vectors by\nperiodically smoothing neighboring Gaussians during optimization, using\nproposed trilateral RBF kernels that jointly consider color, semantic, and\nspatial affinities. The historical static and current dynamic Gaussians are\nthen combined and voxelized to generate occupancy prediction. Extensive\nexperiments on Occ3D and nuCraft with varying voxel resolutions demonstrate\nthat TT-GaussOcc surpasses self-supervised baselines by 46% on mIoU without any\noffline training, and supports finer voxel resolutions at 2.6 FPS inference\nspeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised 3D occupancy prediction offers a promising solution for\nunderstanding complex driving scenes without requiring costly 3D annotations.\nHowever, training dense voxel decoders to capture fine-grained geometry and\nsemantics can demand hundreds of GPU hours, and such models often fail to adapt\nto varying voxel resolutions or new classes without extensive retraining. To\novercome these limitations, we propose a practical and flexible test-time\noccupancy prediction framework termed TT-GaussOcc. Our approach incrementally\noptimizes time-aware 3D Gaussians instantiated from raw sensor streams at\nruntime, enabling voxelization at arbitrary user-specified resolution.\nSpecifically, TT-GaussOcc operates in a \"lift-move-voxel\" symphony: we first\n\"lift\" surrounding-view semantics obtained from 2D vision foundation models\n(VLMs) to instantiate Gaussians at non-empty 3D space; Next, we \"move\" dynamic\nGaussians from previous frames along estimated Gaussian scene flow to complete\nappearance and eliminate trailing artifacts of fast-moving objects, while\naccumulating static Gaussians to enforce temporal consistency; Finally, we\nmitigate inherent noises in semantic predictions and scene flow vectors by\nperiodically smoothing neighboring Gaussians during optimization, using\nproposed trilateral RBF kernels that jointly consider color, semantic, and\nspatial affinities. The historical static and current dynamic Gaussians are\nthen combined and voxelized to generate occupancy prediction. Extensive\nexperiments on Occ3D and nuCraft with varying voxel resolutions demonstrate\nthat TT-GaussOcc surpasses self-supervised baselines by 46% on mIoU without any\noffline training, and supports finer voxel resolutions at 2.6 FPS inference\nspeed."
                },
                "authors": [
                    {
                        "name": "Fengyi Zhang"
                    },
                    {
                        "name": "Huitong Yang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yadan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yadan Luo"
                },
                "author": "Yadan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04479v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04479v3",
                "updated": "2025-03-11T14:28:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    28,
                    13,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-06T14:29:52Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    29,
                    52,
                    3,
                    65,
                    0
                ],
                "title": "ToolFuzz -- Automated Agent Tool Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolFuzz -- Automated Agent Tool Testing"
                },
                "summary": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents."
                },
                "authors": [
                    {
                        "name": "Ivan Milev"
                    },
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04479v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04479v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06716v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06716v2",
                "updated": "2025-03-11T14:23:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    23,
                    44,
                    1,
                    70,
                    0
                ],
                "published": "2024-03-11T13:40:46Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    13,
                    40,
                    46,
                    0,
                    71,
                    0
                ],
                "title": "Emergency Response Inference Mapping (ERIMap): A Bayesian network-based\n  method for dynamic observation processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency Response Inference Mapping (ERIMap): A Bayesian network-based\n  method for dynamic observation processing"
                },
                "summary": "In emergencies, high stake decisions often have to be made under time\npressure and strain. In order to support such decisions, information from\nvarious sources needs to be collected and processed rapidly. The information\navailable tends to be temporally and spatially variable, uncertain, and\nsometimes conflicting, leading to potential biases in decisions. Currently,\nthere is a lack of systematic approaches for information processing and\nsituation assessment which meet the particular demands of emergency situations.\nTo address this gap, we present a Bayesian network-based method called ERIMap\nthat is tailored to the complex information-scape during emergencies. The\nmethod enables the systematic and rapid processing of heterogeneous and\npotentially uncertain observations and draws inferences about key variables of\nan emergency. It thereby reduces complexity and cognitive load for decision\nmakers. The output of the ERIMap method is a dynamically evolving and spatially\nresolved map of beliefs about key variables of an emergency that is updated\neach time a new observation becomes available. The method is illustrated in a\ncase study in which an emergency response is triggered by an accident causing a\ngas leakage on a chemical plant site.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In emergencies, high stake decisions often have to be made under time\npressure and strain. In order to support such decisions, information from\nvarious sources needs to be collected and processed rapidly. The information\navailable tends to be temporally and spatially variable, uncertain, and\nsometimes conflicting, leading to potential biases in decisions. Currently,\nthere is a lack of systematic approaches for information processing and\nsituation assessment which meet the particular demands of emergency situations.\nTo address this gap, we present a Bayesian network-based method called ERIMap\nthat is tailored to the complex information-scape during emergencies. The\nmethod enables the systematic and rapid processing of heterogeneous and\npotentially uncertain observations and draws inferences about key variables of\nan emergency. It thereby reduces complexity and cognitive load for decision\nmakers. The output of the ERIMap method is a dynamically evolving and spatially\nresolved map of beliefs about key variables of an emergency that is updated\neach time a new observation becomes available. The method is illustrated in a\ncase study in which an emergency response is triggered by an accident causing a\ngas leakage on a chemical plant site."
                },
                "authors": [
                    {
                        "name": "Moritz Schneider"
                    },
                    {
                        "name": "Lukas Halekotte"
                    },
                    {
                        "name": "Tina Comes"
                    },
                    {
                        "name": "Daniel Lichte"
                    },
                    {
                        "name": "Frank Fiedrich"
                    }
                ],
                "author_detail": {
                    "name": "Frank Fiedrich"
                },
                "author": "Frank Fiedrich",
                "arxiv_doi": "10.1016/j.ress.2024.110640",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ress.2024.110640",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.06716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06716v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18363v3",
                "updated": "2025-03-11T14:19:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    19,
                    42,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-27T14:11:10Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    11,
                    10,
                    2,
                    332,
                    0
                ],
                "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding"
                },
                "summary": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After a three-stage training\napproach, ChatRex demonstrates strong perception and understanding performance,\nand the combination of these two capabilities also unlocks many attractive\napplications, demonstrating their complementary roles in MLLM. Code is\navailable at https://github.com/IDEA-Research/ChatRex.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After a three-stage training\napproach, ChatRex demonstrates strong perception and understanding performance,\nand the combination of these two capabilities also unlocks many attractive\napplications, demonstrating their complementary roles in MLLM. Code is\navailable at https://github.com/IDEA-Research/ChatRex."
                },
                "authors": [
                    {
                        "name": "Qing Jiang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Yuqin Yang"
                    },
                    {
                        "name": "Yuda Xiong"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Zhaoyang Zeng"
                    },
                    {
                        "name": "Tianhe Ren"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "35 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08467v1",
                "updated": "2025-03-11T14:15:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    15,
                    1,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:15:01Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    15,
                    1,
                    1,
                    70,
                    0
                ],
                "title": "Accelerating MoE Model Inference with Expert Sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating MoE Model Inference with Expert Sharding"
                },
                "summary": "Mixture of experts (MoE) models achieve state-of-the-art results in language\nmodeling but suffer from inefficient hardware utilization due to imbalanced\ntoken routing and communication overhead. While prior work has focused on\noptimizing MoE training and decoder architectures, inference for encoder-based\nMoE models in a multi-GPU with expert parallelism setting remains\nunderexplored. We introduce MoEShard, an inference system that achieves perfect\nload balancing through tensor sharding of MoE experts. Unlike existing\napproaches that rely on heuristic capacity factors or drop tokens, MoEShard\nevenly distributes computation across GPUs and ensures full token retention,\nmaximizing utilization regardless of routing skewness. We achieve this through\na strategic row- and column-wise decomposition of expert matrices. This reduces\nidle time and avoids bottlenecks caused by imbalanced expert assignments.\nFurthermore, MoEShard minimizes kernel launches by fusing decomposed expert\ncomputations, significantly improving throughput. We evaluate MoEShard against\nDeepSpeed on encoder-based architectures, demonstrating speedups of up to\n6.4$\\times$ in time to first token (TTFT). Our results show that tensor\nsharding, when properly applied to experts, is a viable and effective strategy\nfor efficient MoE inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of experts (MoE) models achieve state-of-the-art results in language\nmodeling but suffer from inefficient hardware utilization due to imbalanced\ntoken routing and communication overhead. While prior work has focused on\noptimizing MoE training and decoder architectures, inference for encoder-based\nMoE models in a multi-GPU with expert parallelism setting remains\nunderexplored. We introduce MoEShard, an inference system that achieves perfect\nload balancing through tensor sharding of MoE experts. Unlike existing\napproaches that rely on heuristic capacity factors or drop tokens, MoEShard\nevenly distributes computation across GPUs and ensures full token retention,\nmaximizing utilization regardless of routing skewness. We achieve this through\na strategic row- and column-wise decomposition of expert matrices. This reduces\nidle time and avoids bottlenecks caused by imbalanced expert assignments.\nFurthermore, MoEShard minimizes kernel launches by fusing decomposed expert\ncomputations, significantly improving throughput. We evaluate MoEShard against\nDeepSpeed on encoder-based architectures, demonstrating speedups of up to\n6.4$\\times$ in time to first token (TTFT). Our results show that tensor\nsharding, when properly applied to experts, is a viable and effective strategy\nfor efficient MoE inference."
                },
                "authors": [
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "André Loureiro Espírito Santo"
                    },
                    {
                        "name": "Martijn de Vos"
                    },
                    {
                        "name": "Milos Vujasinovic"
                    }
                ],
                "author_detail": {
                    "name": "Milos Vujasinovic"
                },
                "author": "Milos Vujasinovic",
                "arxiv_doi": "10.1145/3721146.3721938",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721938",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.08467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of the 5th Workshop on Machine Learning\n  and Systems (EuroMLSys 25)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15576v3",
                "updated": "2025-03-11T14:09:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    9,
                    50,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-20T05:17:06Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    5,
                    17,
                    6,
                    4,
                    355,
                    0
                ],
                "title": "QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped\n  Robot Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped\n  Robot Learning"
                },
                "summary": "This paper addresses the inherent inference latency challenges associated\nwith deploying multimodal large language models (MLLM) in quadruped\nvision-language-action (QUAR-VLA) tasks. Our investigation reveals that\nconventional parameter reduction techniques ultimately impair the performance\nof the language foundation model during the action instruction tuning phase,\nmaking them unsuitable for this purpose. We introduce a novel latency-free\nquadruped MLLM model, dubbed QUART-Online, designed to enhance inference\nefficiency without degrading the performance of the language foundation model.\nBy incorporating Action Chunk Discretization (ACD), we compress the original\naction representation space, mapping continuous action values onto a smaller\nset of discrete representative vectors while preserving critical information.\nSubsequently, we fine-tune the MLLM to integrate vision, language, and\ncompressed actions into a unified semantic space. Experimental results\ndemonstrate that QUART-Online operates in tandem with the existing MLLM system,\nachieving real-time inference in sync with the underlying controller frequency,\nsignificantly boosting the success rate across various tasks by 65%. Our\nproject page is https://quart-online.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the inherent inference latency challenges associated\nwith deploying multimodal large language models (MLLM) in quadruped\nvision-language-action (QUAR-VLA) tasks. Our investigation reveals that\nconventional parameter reduction techniques ultimately impair the performance\nof the language foundation model during the action instruction tuning phase,\nmaking them unsuitable for this purpose. We introduce a novel latency-free\nquadruped MLLM model, dubbed QUART-Online, designed to enhance inference\nefficiency without degrading the performance of the language foundation model.\nBy incorporating Action Chunk Discretization (ACD), we compress the original\naction representation space, mapping continuous action values onto a smaller\nset of discrete representative vectors while preserving critical information.\nSubsequently, we fine-tune the MLLM to integrate vision, language, and\ncompressed actions into a unified semantic space. Experimental results\ndemonstrate that QUART-Online operates in tandem with the existing MLLM system,\nachieving real-time inference in sync with the underlying controller frequency,\nsignificantly boosting the success rate across various tasks by 65%. Our\nproject page is https://quart-online.github.io."
                },
                "authors": [
                    {
                        "name": "Xinyang Tong"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Yiguo Fan"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Can Cui"
                    },
                    {
                        "name": "Mingyang Sun"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Hongyin Zhang"
                    },
                    {
                        "name": "Yonghao Dang"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Shangke Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Shangke Lyu"
                },
                "author": "Shangke Lyu",
                "arxiv_comment": "Accepted to ICRA 2025; Github page: https://quart-online.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05760v2",
                "updated": "2025-03-11T14:04:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    4,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-23T18:47:14Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    18,
                    47,
                    14,
                    6,
                    54,
                    0
                ],
                "title": "The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own"
                },
                "summary": "This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps://gradegpt.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps://gradegpt.github.io."
                },
                "authors": [
                    {
                        "name": "Gokul Puthumanaillam"
                    },
                    {
                        "name": "Melkior Ornik"
                    }
                ],
                "author_detail": {
                    "name": "Melkior Ornik"
                },
                "author": "Melkior Ornik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08452v1",
                "updated": "2025-03-11T14:01:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    1,
                    3,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:01:03Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    1,
                    3,
                    1,
                    70,
                    0
                ],
                "title": "KAP: MLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese\n  Non-Narrative Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAP: MLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese\n  Non-Narrative Documents"
                },
                "summary": "We propose Knowledge-Aware Preprocessing (KAP), a two-stage preprocessing\nframework tailored for Traditional Chinese non-narrative documents, designed to\nenhance retrieval accuracy in Hybrid Retrieval systems. Hybrid Retrieval, which\nintegrates Sparse Retrieval (e.g., BM25) and Dense Retrieval (e.g., vector\nembeddings), has become a widely adopted approach for improving search\neffectiveness. However, its performance heavily depends on the quality of input\ntext, which is often degraded when dealing with non-narrative documents such as\nPDFs containing financial statements, contractual clauses, and tables. KAP\naddresses these challenges by integrating Multimodal Large Language Models\n(MLLMs) with LLM-driven post-OCR processing, refining extracted text to reduce\nOCR noise, restore table structures, and optimize text format. By ensuring\nbetter compatibility with Hybrid Retrieval, KAP improves the accuracy of both\nSparse and Dense Retrieval methods without modifying the retrieval architecture\nitself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Knowledge-Aware Preprocessing (KAP), a two-stage preprocessing\nframework tailored for Traditional Chinese non-narrative documents, designed to\nenhance retrieval accuracy in Hybrid Retrieval systems. Hybrid Retrieval, which\nintegrates Sparse Retrieval (e.g., BM25) and Dense Retrieval (e.g., vector\nembeddings), has become a widely adopted approach for improving search\neffectiveness. However, its performance heavily depends on the quality of input\ntext, which is often degraded when dealing with non-narrative documents such as\nPDFs containing financial statements, contractual clauses, and tables. KAP\naddresses these challenges by integrating Multimodal Large Language Models\n(MLLMs) with LLM-driven post-OCR processing, refining extracted text to reduce\nOCR noise, restore table structures, and optimize text format. By ensuring\nbetter compatibility with Hybrid Retrieval, KAP improves the accuracy of both\nSparse and Dense Retrieval methods without modifying the retrieval architecture\nitself."
                },
                "authors": [
                    {
                        "name": "Hsin-Ling Hsu"
                    },
                    {
                        "name": "Ping-Sheng Lin"
                    },
                    {
                        "name": "Jing-Di Lin"
                    },
                    {
                        "name": "Jengnan Tzeng"
                    }
                ],
                "author_detail": {
                    "name": "Jengnan Tzeng"
                },
                "author": "Jengnan Tzeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08445v1",
                "updated": "2025-03-11T13:56:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    56,
                    26,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:56:26Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    56,
                    26,
                    1,
                    70,
                    0
                ],
                "title": "LLM-Pack: Intuitive Grocery Handling for Logistics Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Pack: Intuitive Grocery Handling for Logistics Applications"
                },
                "summary": "Robotics and automation are increasingly influential in logistics but remain\nlargely confined to traditional warehouses. In grocery retail, advancements\nsuch as cashier-less supermarkets exist, yet customers still manually pick and\npack groceries. While there has been a substantial focus in robotics on the bin\npicking problem, the task of packing objects and groceries has remained largely\nuntouched. However, packing grocery items in the right order is crucial for\npreventing product damage, e.g., heavy objects should not be placed on top of\nfragile ones. However, the exact criteria for the right packing order are hard\nto define, in particular given the huge variety of objects typically found in\nstores. In this paper, we introduce LLM-Pack, a novel approach for grocery\npacking. LLM-Pack leverages language and vision foundation models for\nidentifying groceries and generating a packing sequence that mimics human\npacking strategy. LLM-Pack does not require dedicated training to handle new\ngrocery items and its modularity allows easy upgrades of the underlying\nfoundation models. We extensively evaluate our approach to demonstrate its\nperformance. We will make the source code of LLMPack publicly available upon\nthe publication of this manuscript.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotics and automation are increasingly influential in logistics but remain\nlargely confined to traditional warehouses. In grocery retail, advancements\nsuch as cashier-less supermarkets exist, yet customers still manually pick and\npack groceries. While there has been a substantial focus in robotics on the bin\npicking problem, the task of packing objects and groceries has remained largely\nuntouched. However, packing grocery items in the right order is crucial for\npreventing product damage, e.g., heavy objects should not be placed on top of\nfragile ones. However, the exact criteria for the right packing order are hard\nto define, in particular given the huge variety of objects typically found in\nstores. In this paper, we introduce LLM-Pack, a novel approach for grocery\npacking. LLM-Pack leverages language and vision foundation models for\nidentifying groceries and generating a packing sequence that mimics human\npacking strategy. LLM-Pack does not require dedicated training to handle new\ngrocery items and its modularity allows easy upgrades of the underlying\nfoundation models. We extensively evaluate our approach to demonstrate its\nperformance. We will make the source code of LLMPack publicly available upon\nthe publication of this manuscript."
                },
                "authors": [
                    {
                        "name": "Yannik Blei"
                    },
                    {
                        "name": "Michael Krawez"
                    },
                    {
                        "name": "Tobias Jülg"
                    },
                    {
                        "name": "Pierre Krack"
                    },
                    {
                        "name": "Florian Walter"
                    },
                    {
                        "name": "Wolfram Burgard"
                    }
                ],
                "author_detail": {
                    "name": "Wolfram Burgard"
                },
                "author": "Wolfram Burgard",
                "arxiv_comment": "6 Pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08441v1",
                "updated": "2025-03-11T13:53:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    53,
                    6,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:53:06Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    53,
                    6,
                    1,
                    70,
                    0
                ],
                "title": "Utilizing localized fast radio bursts to constrain their progenitors and\n  the expansion history of the Universe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing localized fast radio bursts to constrain their progenitors and\n  the expansion history of the Universe"
                },
                "summary": "Fast radio bursts (FRBs) are increasingly being used for cosmological\napplications such as measuring the Hubble constant and baryon abundance. The\nincreasing number of localized FRBs and precise measurement of dispersion\nmeasure (DM) make them a suitable probe for such an approach. We use a sample\nof 110 localized FRBs as well as a small sub-sample of 24 FRBs with scattering\ntimescale measurements or limits. We infer the Hubble constant ($H_0$) and the\nDM distribution of the host galaxies simultaneously by fitting our model to the\nFRB DM measurements. With current data, our results are in agreement with both\nhigh and low redshift measurements of $H_0$, obtained using Cosmic Microwave\nBackground (CMB) and Type Ia supernovae data respectively. We project that with\nabout 200 localized FRBs, we would be in a position to distinguish between the\ntwo scenarios at 4$\\sigma$ confidence. In addition, the host DM is expected to\nbe related to star formation in the host galaxy and the stellar age of the\nprogenitors. Using our inferred host galaxy DMs, we are able to rule out (at 95\npercent confidence) a majority of localized FRB progenitors originating from\nyoung sources with stellar ages less than 10 Myr. This might reflect a large\npopulation of old sources or an observational bias against detecting FRBs from\nyoung sources, which may be associated with long scatter broadening times and\nlarge DM from their source environments. Indeed, we find that scatter\nbroadening times of FRBs are inconsistent with the Milky Way ISM, but at the\nsame time, do not appear to be strongly correlated with the FRBs' redshift or\nwith the SFR or stellar mass of their host galaxies. This suggests that\nscattering is dominated by the immediate environment of the sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast radio bursts (FRBs) are increasingly being used for cosmological\napplications such as measuring the Hubble constant and baryon abundance. The\nincreasing number of localized FRBs and precise measurement of dispersion\nmeasure (DM) make them a suitable probe for such an approach. We use a sample\nof 110 localized FRBs as well as a small sub-sample of 24 FRBs with scattering\ntimescale measurements or limits. We infer the Hubble constant ($H_0$) and the\nDM distribution of the host galaxies simultaneously by fitting our model to the\nFRB DM measurements. With current data, our results are in agreement with both\nhigh and low redshift measurements of $H_0$, obtained using Cosmic Microwave\nBackground (CMB) and Type Ia supernovae data respectively. We project that with\nabout 200 localized FRBs, we would be in a position to distinguish between the\ntwo scenarios at 4$\\sigma$ confidence. In addition, the host DM is expected to\nbe related to star formation in the host galaxy and the stellar age of the\nprogenitors. Using our inferred host galaxy DMs, we are able to rule out (at 95\npercent confidence) a majority of localized FRB progenitors originating from\nyoung sources with stellar ages less than 10 Myr. This might reflect a large\npopulation of old sources or an observational bias against detecting FRBs from\nyoung sources, which may be associated with long scatter broadening times and\nlarge DM from their source environments. Indeed, we find that scatter\nbroadening times of FRBs are inconsistent with the Milky Way ISM, but at the\nsame time, do not appear to be strongly correlated with the FRBs' redshift or\nwith the SFR or stellar mass of their host galaxies. This suggests that\nscattering is dominated by the immediate environment of the sources."
                },
                "authors": [
                    {
                        "name": "Sandeep Kumar Acharya"
                    },
                    {
                        "name": "Paz Beniamini"
                    }
                ],
                "author_detail": {
                    "name": "Paz Beniamini"
                },
                "author": "Paz Beniamini",
                "arxiv_comment": "Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08437v1",
                "updated": "2025-03-11T13:50:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    50,
                    37,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:50:37Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    50,
                    37,
                    1,
                    70,
                    0
                ],
                "title": "ICPR 2024 Competition on Rider Intention Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICPR 2024 Competition on Rider Intention Prediction"
                },
                "summary": "The recent surge in the vehicle market has led to an alarming increase in\nroad accidents. This underscores the critical importance of enhancing road\nsafety measures, particularly for vulnerable road users like motorcyclists.\nHence, we introduce the rider intention prediction (RIP) competition that aims\nto address challenges in rider safety by proactively predicting maneuvers\nbefore they occur, thereby strengthening rider safety. This capability enables\nthe riders to react to the potential incorrect maneuvers flagged by advanced\ndriver assistance systems (ADAS). We collect a new dataset, namely, rider\naction anticipation dataset (RAAD) for the competition consisting of two tasks:\nsingle-view RIP and multi-view RIP. The dataset incorporates a spectrum of\ntraffic conditions and challenging navigational maneuvers on roads with varying\nlighting conditions. For the competition, we received seventy-five\nregistrations and five team submissions for inference of which we compared the\nmethods of the top three performing teams on both the RIP tasks: one\nstate-space model (Mamba2) and two learning-based approaches (SVM and\nCNN-LSTM). The results indicate that the state-space model outperformed the\nother methods across the entire dataset, providing a balanced performance\nacross maneuver classes. The SVM-based RIP method showed the second-best\nperformance when using random sampling and SMOTE. However, the CNN-LSTM method\nunderperformed, primarily due to class imbalance issues, particularly\nstruggling with minority classes. This paper details the proposed RAAD dataset\nand provides a summary of the submissions for the RIP 2024 competition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge in the vehicle market has led to an alarming increase in\nroad accidents. This underscores the critical importance of enhancing road\nsafety measures, particularly for vulnerable road users like motorcyclists.\nHence, we introduce the rider intention prediction (RIP) competition that aims\nto address challenges in rider safety by proactively predicting maneuvers\nbefore they occur, thereby strengthening rider safety. This capability enables\nthe riders to react to the potential incorrect maneuvers flagged by advanced\ndriver assistance systems (ADAS). We collect a new dataset, namely, rider\naction anticipation dataset (RAAD) for the competition consisting of two tasks:\nsingle-view RIP and multi-view RIP. The dataset incorporates a spectrum of\ntraffic conditions and challenging navigational maneuvers on roads with varying\nlighting conditions. For the competition, we received seventy-five\nregistrations and five team submissions for inference of which we compared the\nmethods of the top three performing teams on both the RIP tasks: one\nstate-space model (Mamba2) and two learning-based approaches (SVM and\nCNN-LSTM). The results indicate that the state-space model outperformed the\nother methods across the entire dataset, providing a balanced performance\nacross maneuver classes. The SVM-based RIP method showed the second-best\nperformance when using random sampling and SMOTE. However, the CNN-LSTM method\nunderperformed, primarily due to class imbalance issues, particularly\nstruggling with minority classes. This paper details the proposed RAAD dataset\nand provides a summary of the submissions for the RIP 2024 competition."
                },
                "authors": [
                    {
                        "name": "Shankar Gangisetty"
                    },
                    {
                        "name": "Abdul Wasi"
                    },
                    {
                        "name": "Shyam Nandan Rai"
                    },
                    {
                        "name": "C. V. Jawahar"
                    },
                    {
                        "name": "Sajay Raj"
                    },
                    {
                        "name": "Manish Prajapati"
                    },
                    {
                        "name": "Ayesha Choudhary"
                    },
                    {
                        "name": "Aaryadev Chandra"
                    },
                    {
                        "name": "Dev Chandan"
                    },
                    {
                        "name": "Shireen Chand"
                    },
                    {
                        "name": "Suvaditya Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Suvaditya Mukherjee"
                },
                "author": "Suvaditya Mukherjee",
                "arxiv_doi": "10.1007/978-3-031-80139-6_3",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-80139-6_3",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.08437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19338v2",
                "updated": "2025-03-11T13:44:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    44,
                    27,
                    1,
                    70,
                    0
                ],
                "published": "2024-09-28T12:49:02Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    12,
                    49,
                    2,
                    5,
                    272,
                    0
                ],
                "title": "Decoding Echo Chambers: LLM-Powered Simulations Revealing Polarization\n  in Social Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Echo Chambers: LLM-Powered Simulations Revealing Polarization\n  in Social Networks"
                },
                "summary": "The impact of social media on critical issues such as echo chambers needs to\nbe addressed, as these phenomena can have disruptive consequences for our\nsociety. Traditional research often oversimplifies emotional tendencies and\nopinion evolution into numbers and formulas, neglecting that news and\ncommunication are conveyed through text, which limits these approaches. Hence,\nin this work, we propose an LLM-based simulation for the social opinion network\nto evaluate and counter polarization phenomena. We first construct three\ntypical network structures to simulate different characteristics of social\ninteractions. Then, agents interact based on recommendation algorithms and\nupdate their strategies through reasoning and analysis. By comparing these\ninteractions with the classic Bounded Confidence Model (BCM), the Friedkin\nJohnsen (FJ) model, and using echo chamber-related indices, we demonstrate the\neffectiveness of our framework in simulating opinion dynamics and reproducing\nphenomena such as opinion polarization and echo chambers. We propose two\nmitigation methods, active and passive nudges, that can help reduce echo\nchambers, specifically within language-based simulations. We hope our work will\noffer valuable insights and guidance for social polarization mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of social media on critical issues such as echo chambers needs to\nbe addressed, as these phenomena can have disruptive consequences for our\nsociety. Traditional research often oversimplifies emotional tendencies and\nopinion evolution into numbers and formulas, neglecting that news and\ncommunication are conveyed through text, which limits these approaches. Hence,\nin this work, we propose an LLM-based simulation for the social opinion network\nto evaluate and counter polarization phenomena. We first construct three\ntypical network structures to simulate different characteristics of social\ninteractions. Then, agents interact based on recommendation algorithms and\nupdate their strategies through reasoning and analysis. By comparing these\ninteractions with the classic Bounded Confidence Model (BCM), the Friedkin\nJohnsen (FJ) model, and using echo chamber-related indices, we demonstrate the\neffectiveness of our framework in simulating opinion dynamics and reproducing\nphenomena such as opinion polarization and echo chambers. We propose two\nmitigation methods, active and passive nudges, that can help reduce echo\nchambers, specifically within language-based simulations. We hope our work will\noffer valuable insights and guidance for social polarization mitigation."
                },
                "authors": [
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Zongfang Liu"
                    },
                    {
                        "name": "Dequan Yang"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10573v2",
                "updated": "2025-03-11T13:41:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    41,
                    59,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-15T20:46:58Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    20,
                    46,
                    58,
                    4,
                    320,
                    0
                ],
                "title": "Hysteresis Activation Function for Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hysteresis Activation Function for Efficient Inference"
                },
                "summary": "The widely used ReLU is favored for its hardware efficiency, {as the\nimplementation at inference is a one bit sign case,} yet suffers from issues\nsuch as the ``dying ReLU'' problem, where during training, neurons fail to\nactivate and constantly remain at zero, as highlighted by Lu et al. Traditional\napproaches to mitigate this issue often introduce more complex and less\nhardware-friendly activation functions. In this work, we propose a Hysteresis\nRectified Linear Unit (HeLU), an efficient activation function designed to\naddress the ``dying ReLU'' problem with minimal complexity. Unlike traditional\nactivation functions with fixed thresholds for training and inference, HeLU\nemploys a variable threshold that refines the backpropagation. This refined\nmechanism allows simpler activation functions to achieve competitive\nperformance comparable to their more complex counterparts without introducing\nunnecessary complexity or requiring inductive biases. Empirical evaluations\ndemonstrate that HeLU enhances model generalization across diverse datasets,\noffering a promising solution for efficient and effective inference suitable\nfor a wide range of neural network architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widely used ReLU is favored for its hardware efficiency, {as the\nimplementation at inference is a one bit sign case,} yet suffers from issues\nsuch as the ``dying ReLU'' problem, where during training, neurons fail to\nactivate and constantly remain at zero, as highlighted by Lu et al. Traditional\napproaches to mitigate this issue often introduce more complex and less\nhardware-friendly activation functions. In this work, we propose a Hysteresis\nRectified Linear Unit (HeLU), an efficient activation function designed to\naddress the ``dying ReLU'' problem with minimal complexity. Unlike traditional\nactivation functions with fixed thresholds for training and inference, HeLU\nemploys a variable threshold that refines the backpropagation. This refined\nmechanism allows simpler activation functions to achieve competitive\nperformance comparable to their more complex counterparts without introducing\nunnecessary complexity or requiring inductive biases. Empirical evaluations\ndemonstrate that HeLU enhances model generalization across diverse datasets,\noffering a promising solution for efficient and effective inference suitable\nfor a wide range of neural network architectures."
                },
                "authors": [
                    {
                        "name": "Moshe Kimhi"
                    },
                    {
                        "name": "Idan Kashani"
                    },
                    {
                        "name": "Avi Mendelson"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "arxiv_comment": "Accepted to 4th NeurIPS Efficient Natural Language and Speech\n  Processing Workshop (ENLSP-IV 2024)",
                "arxiv_journal_ref": "Proceedings of Machine Learning Research, Volume 262, Pages 414\n  422, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05431v2",
                "updated": "2025-03-11T13:36:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    36,
                    1,
                    1,
                    70,
                    0
                ],
                "published": "2025-01-09T18:43:37Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    18,
                    43,
                    37,
                    3,
                    9,
                    0
                ],
                "title": "A Disintegrating Rocky Planet with Prominent Comet-like Tails Around a\n  Bright Star",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Disintegrating Rocky Planet with Prominent Comet-like Tails Around a\n  Bright Star"
                },
                "summary": "We report the discovery of BD+05$\\,$4868$\\,$Ab, a transiting exoplanet\norbiting a bright ($V=10.16$) K-dwarf (TIC 466376085) with a period of 1.27\ndays. Observations from NASA's Transiting Exoplanet Survey Satellite (TESS)\nreveal variable transit depths and asymmetric transit profiles that are\ncharacteristic of comet-like tails formed by dusty effluents emanating from a\ndisintegrating planet. Unique to BD+05$\\,$4868$\\,$Ab is the presence of\nprominent dust tails in both the trailing and leading directions that\ncontribute to the extinction of starlight from the host star. By fitting the\nobserved transit profile and analytically modeling the drift of dust grains\nwithin both dust tails, we infer large grain sizes ($\\sim1-10\\,\\mu$m) and a\nmass loss rate of $10\\,M_{\\rm \\oplus}\\,$Gyr$^{-1}$, suggestive of a lunar-mass\nobject with a disintegration timescale of only several Myr. The host star is\nprobably older than the Sun and is accompanied by an M-dwarf companion at a\nprojected physical separation of 130 AU. The brightness of the host star,\ncombined with the planet's relatively deep transits ($0.8-2.0\\%$), presents\nBD+05$\\,$4868$\\,$Ab as a prime target for compositional studies of rocky\nexoplanets and investigations into the nature of catastrophically evaporating\nplanets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the discovery of BD+05$\\,$4868$\\,$Ab, a transiting exoplanet\norbiting a bright ($V=10.16$) K-dwarf (TIC 466376085) with a period of 1.27\ndays. Observations from NASA's Transiting Exoplanet Survey Satellite (TESS)\nreveal variable transit depths and asymmetric transit profiles that are\ncharacteristic of comet-like tails formed by dusty effluents emanating from a\ndisintegrating planet. Unique to BD+05$\\,$4868$\\,$Ab is the presence of\nprominent dust tails in both the trailing and leading directions that\ncontribute to the extinction of starlight from the host star. By fitting the\nobserved transit profile and analytically modeling the drift of dust grains\nwithin both dust tails, we infer large grain sizes ($\\sim1-10\\,\\mu$m) and a\nmass loss rate of $10\\,M_{\\rm \\oplus}\\,$Gyr$^{-1}$, suggestive of a lunar-mass\nobject with a disintegration timescale of only several Myr. The host star is\nprobably older than the Sun and is accompanied by an M-dwarf companion at a\nprojected physical separation of 130 AU. The brightness of the host star,\ncombined with the planet's relatively deep transits ($0.8-2.0\\%$), presents\nBD+05$\\,$4868$\\,$Ab as a prime target for compositional studies of rocky\nexoplanets and investigations into the nature of catastrophically evaporating\nplanets."
                },
                "authors": [
                    {
                        "name": "Marc Hon"
                    },
                    {
                        "name": "Saul Rappaport"
                    },
                    {
                        "name": "Avi Shporer"
                    },
                    {
                        "name": "Andrew Vanderburg"
                    },
                    {
                        "name": "Karen A. Collins"
                    },
                    {
                        "name": "Cristilyn N. Watkins"
                    },
                    {
                        "name": "Richard P. Schwarz"
                    },
                    {
                        "name": "Khalid Barkaoui"
                    },
                    {
                        "name": "Samuel W. Yee"
                    },
                    {
                        "name": "Joshua N. Winn"
                    },
                    {
                        "name": "Alex S. Polanski"
                    },
                    {
                        "name": "Emily A. Gilbert"
                    },
                    {
                        "name": "David R. Ciardi"
                    },
                    {
                        "name": "Jeroen Audenaert"
                    },
                    {
                        "name": "William Fong"
                    },
                    {
                        "name": "Jack Haviland"
                    },
                    {
                        "name": "Katharine Hesse"
                    },
                    {
                        "name": "Daniel Muthukrishna"
                    },
                    {
                        "name": "Glen Petitpas"
                    },
                    {
                        "name": "Ellie Hadjiyska Schmelzer"
                    },
                    {
                        "name": "Norio Narita"
                    },
                    {
                        "name": "Akihiko Fukui"
                    },
                    {
                        "name": "Sara Seager"
                    },
                    {
                        "name": "George R. Ricker"
                    }
                ],
                "author_detail": {
                    "name": "George R. Ricker"
                },
                "author": "George R. Ricker",
                "arxiv_comment": "26 pages, 18 figures. Accepted for publication in The Astrophysical\n  Journal Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08421v1",
                "updated": "2025-03-11T13:34:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    34,
                    35,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:34:35Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    34,
                    35,
                    1,
                    70,
                    0
                ],
                "title": "Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual\n  Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual\n  Labels"
                },
                "summary": "Unsupervised 3D object detection serves as an important solution for offline\n3D object annotation. However, due to the data sparsity and limited views, the\nclustering-based label fitting in unsupervised object detection often generates\nlow-quality pseudo-labels. Multi-agent collaborative dataset, which involves\nthe sharing of complementary observations among agents, holds the potential to\nbreak through this bottleneck. In this paper, we introduce a novel unsupervised\nmethod that learns to Detect Objects from Multi-Agent LiDAR scans, termed DOtA,\nwithout using labels from external. DOtA first uses the internally shared\nego-pose and ego-shape of collaborative agents to initialize the detector,\nleveraging the generalization performance of neural networks to infer\npreliminary labels. Subsequently,DOtA uses the complementary observations\nbetween agents to perform multi-scale encoding on preliminary labels, then\ndecodes high-quality and low-quality labels. These labels are further used as\nprompts to guide a correct feature learning process, thereby enhancing the\nperformance of the unsupervised object detection task. Extensive experiments on\nthe V2V4Real and OPV2V datasets show that our DOtA outperforms state-of-the-art\nunsupervised 3D object detection methods. Additionally, we also validate the\neffectiveness of the DOtA labels under various collaborative perception\nframeworks.The code is available at https://github.com/xmuqimingxia/DOtA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised 3D object detection serves as an important solution for offline\n3D object annotation. However, due to the data sparsity and limited views, the\nclustering-based label fitting in unsupervised object detection often generates\nlow-quality pseudo-labels. Multi-agent collaborative dataset, which involves\nthe sharing of complementary observations among agents, holds the potential to\nbreak through this bottleneck. In this paper, we introduce a novel unsupervised\nmethod that learns to Detect Objects from Multi-Agent LiDAR scans, termed DOtA,\nwithout using labels from external. DOtA first uses the internally shared\nego-pose and ego-shape of collaborative agents to initialize the detector,\nleveraging the generalization performance of neural networks to infer\npreliminary labels. Subsequently,DOtA uses the complementary observations\nbetween agents to perform multi-scale encoding on preliminary labels, then\ndecodes high-quality and low-quality labels. These labels are further used as\nprompts to guide a correct feature learning process, thereby enhancing the\nperformance of the unsupervised object detection task. Extensive experiments on\nthe V2V4Real and OPV2V datasets show that our DOtA outperforms state-of-the-art\nunsupervised 3D object detection methods. Additionally, we also validate the\neffectiveness of the DOtA labels under various collaborative perception\nframeworks.The code is available at https://github.com/xmuqimingxia/DOtA."
                },
                "authors": [
                    {
                        "name": "Qiming Xia"
                    },
                    {
                        "name": "Wenkai Lin"
                    },
                    {
                        "name": "Haoen Xiang"
                    },
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Chenglu Wen"
                    }
                ],
                "author_detail": {
                    "name": "Chenglu Wen"
                },
                "author": "Chenglu Wen",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14137v2",
                "updated": "2025-03-11T13:29:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    29,
                    47,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-21T14:01:42Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    14,
                    1,
                    42,
                    3,
                    326,
                    0
                ],
                "title": "VAGUE: Visual Contexts Clarify Ambiguous Expressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAGUE: Visual Contexts Clarify Ambiguous Expressions"
                },
                "summary": "Human communication often relies on visual cues to resolve ambiguity. While\nhumans can intuitively integrate these cues, AI systems often find it\nchallenging to engage in sophisticated multimodal reasoning. We introduce\nVAGUE, a benchmark evaluating multimodal AI systems' ability to integrate\nvisual context for intent disambiguation. VAGUE consists of 1.6K ambiguous\ntextual expressions, each paired with an image and multiple-choice\ninterpretations, where the correct answer is only apparent with visual context.\nThe dataset spans both staged, complex (Visual Commonsense Reasoning) and\nnatural, personal (Ego4D) scenes, ensuring diversity. Our experiments reveal\nthat existing multimodal AI models struggle to infer the speaker's true intent.\nWhile performance consistently improves from the introduction of more visual\ncues, the overall accuracy remains far below human performance, highlighting a\ncritical gap in multimodal reasoning. Analysis of failure cases demonstrates\nthat current models fail to distinguish true intent from superficial\ncorrelations in the visual scene, indicating that they perceive images but do\nnot effectively reason with them. We release our code and data at\nhttps://github.com/Hazel-Heejeong-Nam/VAGUE.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human communication often relies on visual cues to resolve ambiguity. While\nhumans can intuitively integrate these cues, AI systems often find it\nchallenging to engage in sophisticated multimodal reasoning. We introduce\nVAGUE, a benchmark evaluating multimodal AI systems' ability to integrate\nvisual context for intent disambiguation. VAGUE consists of 1.6K ambiguous\ntextual expressions, each paired with an image and multiple-choice\ninterpretations, where the correct answer is only apparent with visual context.\nThe dataset spans both staged, complex (Visual Commonsense Reasoning) and\nnatural, personal (Ego4D) scenes, ensuring diversity. Our experiments reveal\nthat existing multimodal AI models struggle to infer the speaker's true intent.\nWhile performance consistently improves from the introduction of more visual\ncues, the overall accuracy remains far below human performance, highlighting a\ncritical gap in multimodal reasoning. Analysis of failure cases demonstrates\nthat current models fail to distinguish true intent from superficial\ncorrelations in the visual scene, indicating that they perceive images but do\nnot effectively reason with them. We release our code and data at\nhttps://github.com/Hazel-Heejeong-Nam/VAGUE.git."
                },
                "authors": [
                    {
                        "name": "Heejeong Nam"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Keummin Ka"
                    },
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "31 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08415v1",
                "updated": "2025-03-11T13:24:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    24,
                    39,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:24:39Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    24,
                    39,
                    1,
                    70,
                    0
                ],
                "title": "TokenSim: Enabling Hardware and Software Exploration for Large Language\n  Model Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSim: Enabling Hardware and Software Exploration for Large Language\n  Model Inference Systems"
                },
                "summary": "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Guoyang Duan"
                    },
                    {
                        "name": "Tianle Xu"
                    },
                    {
                        "name": "Junchi Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Yongqiang Yao"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "arxiv_comment": "9 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13579v2",
                "updated": "2025-03-11T13:07:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    7,
                    9,
                    1,
                    70,
                    0
                ],
                "published": "2024-07-18T15:20:31Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    15,
                    20,
                    31,
                    3,
                    200,
                    0
                ],
                "title": "Towards Zero-Shot Multimodal Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Zero-Shot Multimodal Machine Translation"
                },
                "summary": "Current multimodal machine translation (MMT) systems rely on fully supervised\ndata (i.e models are trained on sentences with their translations and\naccompanying images). However, this type of data is costly to collect, limiting\nthe extension of MMT to other language pairs for which such data does not\nexist. In this work, we propose a method to bypass the need for fully\nsupervised data to train MMT systems, using multimodal English data only. Our\nmethod, called ZeroMMT, consists in adapting a strong text-only machine\ntranslation (MT) model by training it on a mixture of two objectives: visually\nconditioned masked language modelling and the Kullback-Leibler divergence\nbetween the original and new MMT outputs. We evaluate on standard MMT\nbenchmarks and the recently released CoMMuTE, a contrastive benchmark aiming to\nevaluate how well models use images to disambiguate English sentences. We\nobtain disambiguation performance close to state-of-the-art MMT models trained\nadditionally on fully supervised examples. To prove that our method generalizes\nto languages with no fully supervised training data available, we extend the\nCoMMuTE evaluation dataset to three new languages: Arabic, Russian and Chinese.\nWe further show that we can control the trade-off between disambiguation\ncapabilities and translation fidelity at inference time using classifier-free\nguidance and without any additional data. Our code, data and trained models are\npublicly accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multimodal machine translation (MMT) systems rely on fully supervised\ndata (i.e models are trained on sentences with their translations and\naccompanying images). However, this type of data is costly to collect, limiting\nthe extension of MMT to other language pairs for which such data does not\nexist. In this work, we propose a method to bypass the need for fully\nsupervised data to train MMT systems, using multimodal English data only. Our\nmethod, called ZeroMMT, consists in adapting a strong text-only machine\ntranslation (MT) model by training it on a mixture of two objectives: visually\nconditioned masked language modelling and the Kullback-Leibler divergence\nbetween the original and new MMT outputs. We evaluate on standard MMT\nbenchmarks and the recently released CoMMuTE, a contrastive benchmark aiming to\nevaluate how well models use images to disambiguate English sentences. We\nobtain disambiguation performance close to state-of-the-art MMT models trained\nadditionally on fully supervised examples. To prove that our method generalizes\nto languages with no fully supervised training data available, we extend the\nCoMMuTE evaluation dataset to three new languages: Arabic, Russian and Chinese.\nWe further show that we can control the trade-off between disambiguation\ncapabilities and translation fidelity at inference time using classifier-free\nguidance and without any additional data. Our code, data and trained models are\npublicly accessible."
                },
                "authors": [
                    {
                        "name": "Matthieu Futeral"
                    },
                    {
                        "name": "Cordelia Schmid"
                    },
                    {
                        "name": "Benoît Sagot"
                    },
                    {
                        "name": "Rachel Bawden"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Bawden"
                },
                "author": "Rachel Bawden",
                "arxiv_comment": "NAACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08404v1",
                "updated": "2025-03-11T13:06:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    6,
                    40,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:06:40Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    6,
                    40,
                    1,
                    70,
                    0
                ],
                "title": "Fact-checking with Generative AI: A Systematic Cross-Topic Examination\n  of LLMs Capacity to Detect Veracity of Political Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-checking with Generative AI: A Systematic Cross-Topic Examination\n  of LLMs Capacity to Detect Veracity of Political Information"
                },
                "summary": "The purpose of this study is to assess how large language models (LLMs) can\nbe used for fact-checking and contribute to the broader debate on the use of\nautomated means for veracity identification. To achieve this purpose, we use AI\nauditing methodology that systematically evaluates performance of five LLMs\n(ChatGPT 4, Llama 3 (70B), Llama 3.1 (405B), Claude 3.5 Sonnet, and Google\nGemini) using prompts regarding a large set of statements fact-checked by\nprofessional journalists (16,513). Specifically, we use topic modeling and\nregression analysis to investigate which factors (e.g. topic of the prompt or\nthe LLM type) affect evaluations of true, false, and mixed statements. Our\nfindings reveal that while ChatGPT 4 and Google Gemini achieved higher accuracy\nthan other models, overall performance across models remains modest. Notably,\nthe results indicate that models are better at identifying false statements,\nespecially on sensitive topics such as COVID-19, American political\ncontroversies, and social issues, suggesting possible guardrails that may\nenhance accuracy on these topics. The major implication of our findings is that\nthere are significant challenges for using LLMs for factchecking, including\nsignificant variation in performance across different LLMs and unequal quality\nof outputs for specific topics which can be attributed to deficits of training\ndata. Our research highlights the potential and limitations of LLMs in\npolitical fact-checking, suggesting potential avenues for further improvements\nin guardrails as well as fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The purpose of this study is to assess how large language models (LLMs) can\nbe used for fact-checking and contribute to the broader debate on the use of\nautomated means for veracity identification. To achieve this purpose, we use AI\nauditing methodology that systematically evaluates performance of five LLMs\n(ChatGPT 4, Llama 3 (70B), Llama 3.1 (405B), Claude 3.5 Sonnet, and Google\nGemini) using prompts regarding a large set of statements fact-checked by\nprofessional journalists (16,513). Specifically, we use topic modeling and\nregression analysis to investigate which factors (e.g. topic of the prompt or\nthe LLM type) affect evaluations of true, false, and mixed statements. Our\nfindings reveal that while ChatGPT 4 and Google Gemini achieved higher accuracy\nthan other models, overall performance across models remains modest. Notably,\nthe results indicate that models are better at identifying false statements,\nespecially on sensitive topics such as COVID-19, American political\ncontroversies, and social issues, suggesting possible guardrails that may\nenhance accuracy on these topics. The major implication of our findings is that\nthere are significant challenges for using LLMs for factchecking, including\nsignificant variation in performance across different LLMs and unequal quality\nof outputs for specific topics which can be attributed to deficits of training\ndata. Our research highlights the potential and limitations of LLMs in\npolitical fact-checking, suggesting potential avenues for further improvements\nin guardrails as well as fine-tuning."
                },
                "authors": [
                    {
                        "name": "Elizaveta Kuznetsova"
                    },
                    {
                        "name": "Ilaria Vitulano"
                    },
                    {
                        "name": "Mykola Makhortykh"
                    },
                    {
                        "name": "Martha Stolze"
                    },
                    {
                        "name": "Tomas Nagy"
                    },
                    {
                        "name": "Victoria Vziatysheva"
                    }
                ],
                "author_detail": {
                    "name": "Victoria Vziatysheva"
                },
                "author": "Victoria Vziatysheva",
                "arxiv_comment": "15 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08952v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08952v5",
                "updated": "2025-03-12T04:46:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    4,
                    46,
                    47,
                    2,
                    71,
                    0
                ],
                "published": "2024-07-12T03:15:01Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    3,
                    15,
                    1,
                    4,
                    194,
                    0
                ],
                "title": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection"
                },
                "summary": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Jiajun Zhu"
                    },
                    {
                        "name": "Xukai Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Yanghai Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08952v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08952v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08398v1",
                "updated": "2025-03-11T13:04:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    4,
                    5,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:04:05Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    4,
                    5,
                    1,
                    70,
                    0
                ],
                "title": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning"
                },
                "summary": "In this paper, we analyze and empirically show that the learned relevance for\nconventional information retrieval (IR) scenarios may be inconsistent in\nretrieval-augmented generation (RAG) scenarios. To bridge this gap, we\nintroduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the\nretriever to capture in-context relevance, enabling adaptation to the diverse\nand evolving needs. Extensive experiments across a wide range of tasks\ndemonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a\nconsistent improvement of 4.0% over the original retriever, consistently\noutperforming existing state-of-the-art retrievers by 2.1%. Additionally, our\nresults indicate that for some tasks, an end-to-end tuned 0.2B retriever can\nachieve improvements that surpass those of RAG-oriented or instruction-tuned 8B\nlarge language models (LLMs), highlighting the cost-effectiveness of our\napproach in enhancing RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we analyze and empirically show that the learned relevance for\nconventional information retrieval (IR) scenarios may be inconsistent in\nretrieval-augmented generation (RAG) scenarios. To bridge this gap, we\nintroduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the\nretriever to capture in-context relevance, enabling adaptation to the diverse\nand evolving needs. Extensive experiments across a wide range of tasks\ndemonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a\nconsistent improvement of 4.0% over the original retriever, consistently\noutperforming existing state-of-the-art retrievers by 2.1%. Additionally, our\nresults indicate that for some tasks, an end-to-end tuned 0.2B retriever can\nachieve improvements that surpass those of RAG-oriented or instruction-tuned 8B\nlarge language models (LLMs), highlighting the cost-effectiveness of our\napproach in enhancing RAG systems."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16450v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16450v3",
                "updated": "2025-03-11T12:52:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    52,
                    28,
                    1,
                    70,
                    0
                ],
                "published": "2024-05-26T06:33:48Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    33,
                    48,
                    6,
                    147,
                    0
                ],
                "title": "Synthesizing Programmatic Reinforcement Learning Policies with Large\n  Language Model Guided Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Programmatic Reinforcement Learning Policies with Large\n  Language Model Guided Search"
                },
                "summary": "Programmatic reinforcement learning (PRL) has been explored for representing\npolicies through programs as a means to achieve interpretability and\ngeneralization. Despite promising outcomes, current state-of-the-art PRL\nmethods are hindered by sample inefficiency, necessitating tens of millions of\nprogram-environment interactions. To tackle this challenge, we introduce a\nnovel LLM-guided search framework (LLM-GS). Our key insight is to leverage the\nprogramming expertise and common sense reasoning of LLMs to enhance the\nefficiency of assumption-free, random-guessing search methods. We address the\nchallenge of LLMs' inability to generate precise and grammatically correct\nprograms in domain-specific languages (DSLs) by proposing a Pythonic-DSL\nstrategy - an LLM is instructed to initially generate Python codes and then\nconvert them into DSL programs. To further optimize the LLM-generated programs,\nwe develop a search algorithm named Scheduled Hill Climbing, designed to\nefficiently explore the programmatic search space to improve the programs\nconsistently. Experimental results in the Karel domain demonstrate our LLM-GS\nframework's superior effectiveness and efficiency. Extensive ablation studies\nfurther verify the critical role of our Pythonic-DSL strategy and Scheduled\nHill Climbing algorithm. Moreover, we conduct experiments with two novel tasks,\nshowing that LLM-GS enables users without programming skills and knowledge of\nthe domain or DSL to describe the tasks in natural language to obtain\nperformant programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programmatic reinforcement learning (PRL) has been explored for representing\npolicies through programs as a means to achieve interpretability and\ngeneralization. Despite promising outcomes, current state-of-the-art PRL\nmethods are hindered by sample inefficiency, necessitating tens of millions of\nprogram-environment interactions. To tackle this challenge, we introduce a\nnovel LLM-guided search framework (LLM-GS). Our key insight is to leverage the\nprogramming expertise and common sense reasoning of LLMs to enhance the\nefficiency of assumption-free, random-guessing search methods. We address the\nchallenge of LLMs' inability to generate precise and grammatically correct\nprograms in domain-specific languages (DSLs) by proposing a Pythonic-DSL\nstrategy - an LLM is instructed to initially generate Python codes and then\nconvert them into DSL programs. To further optimize the LLM-generated programs,\nwe develop a search algorithm named Scheduled Hill Climbing, designed to\nefficiently explore the programmatic search space to improve the programs\nconsistently. Experimental results in the Karel domain demonstrate our LLM-GS\nframework's superior effectiveness and efficiency. Extensive ablation studies\nfurther verify the critical role of our Pythonic-DSL strategy and Scheduled\nHill Climbing algorithm. Moreover, we conduct experiments with two novel tasks,\nshowing that LLM-GS enables users without programming skills and knowledge of\nthe domain or DSL to describe the tasks in natural language to obtain\nperformant programs."
                },
                "authors": [
                    {
                        "name": "Max Liu"
                    },
                    {
                        "name": "Chan-Hung Yu"
                    },
                    {
                        "name": "Wei-Hsu Lee"
                    },
                    {
                        "name": "Cheng-Wei Hung"
                    },
                    {
                        "name": "Yen-Chun Chen"
                    },
                    {
                        "name": "Shao-Hua Sun"
                    }
                ],
                "author_detail": {
                    "name": "Shao-Hua Sun"
                },
                "author": "Shao-Hua Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16450v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16450v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07216v2",
                "updated": "2025-03-11T12:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    49,
                    15,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-10T11:55:50Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    11,
                    55,
                    50,
                    0,
                    69,
                    0
                ],
                "title": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates"
                },
                "summary": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Seanie Lee"
                    },
                    {
                        "name": "Byungjoo Kim"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08379v1",
                "updated": "2025-03-11T12:39:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    39,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T12:39:04Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    39,
                    4,
                    1,
                    70,
                    0
                ],
                "title": "JurisTCU: A Brazilian Portuguese Information Retrieval Dataset with\n  Query Relevance Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JurisTCU: A Brazilian Portuguese Information Retrieval Dataset with\n  Query Relevance Judgments"
                },
                "summary": "This paper introduces JurisTCU, a Brazilian Portuguese dataset for legal\ninformation retrieval (LIR). The dataset is freely available and consists of\n16,045 jurisprudential documents from the Brazilian Federal Court of Accounts,\nalong with 150 queries annotated with relevance judgments. It addresses the\nscarcity of Portuguese-language LIR datasets with query relevance annotations.\nThe queries are organized into three groups: real user keyword-based queries,\nsynthetic keyword-based queries, and synthetic question-based queries.\nRelevance judgments were produced through a hybrid approach combining LLM-based\nscoring with expert domain validation. We used JurisTCU in 14 experiments using\nlexical search (document expansion methods) and semantic search (BERT-based and\nOpenAI embeddings). We show that the document expansion methods significantly\nimprove the performance of standard BM25 search on this dataset, with\nimprovements exceeding 45% in P@10, R@10, and nDCG@10 metrics when evaluating\nshort keyword-based queries. Among the embedding models, the OpenAI models\nproduced the best results, with improvements of approximately 70% in P@10,\nR@10, and nDCG@10 metrics for short keyword-based queries, suggesting that\nthese dense embeddings capture semantic relationships in this domain,\nsurpassing the reliance on lexical terms. Besides offering a dataset for the\nPortuguese-language IR research community, suitable for evaluating search\nsystems, the results also contribute to enhancing a search system highly\nrelevant to Brazilian citizens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces JurisTCU, a Brazilian Portuguese dataset for legal\ninformation retrieval (LIR). The dataset is freely available and consists of\n16,045 jurisprudential documents from the Brazilian Federal Court of Accounts,\nalong with 150 queries annotated with relevance judgments. It addresses the\nscarcity of Portuguese-language LIR datasets with query relevance annotations.\nThe queries are organized into three groups: real user keyword-based queries,\nsynthetic keyword-based queries, and synthetic question-based queries.\nRelevance judgments were produced through a hybrid approach combining LLM-based\nscoring with expert domain validation. We used JurisTCU in 14 experiments using\nlexical search (document expansion methods) and semantic search (BERT-based and\nOpenAI embeddings). We show that the document expansion methods significantly\nimprove the performance of standard BM25 search on this dataset, with\nimprovements exceeding 45% in P@10, R@10, and nDCG@10 metrics when evaluating\nshort keyword-based queries. Among the embedding models, the OpenAI models\nproduced the best results, with improvements of approximately 70% in P@10,\nR@10, and nDCG@10 metrics for short keyword-based queries, suggesting that\nthese dense embeddings capture semantic relationships in this domain,\nsurpassing the reliance on lexical terms. Besides offering a dataset for the\nPortuguese-language IR research community, suitable for evaluating search\nsystems, the results also contribute to enhancing a search system highly\nrelevant to Brazilian citizens."
                },
                "authors": [
                    {
                        "name": "Leandro Carísio Fernandes"
                    },
                    {
                        "name": "Leandro dos Santos Ribeiro"
                    },
                    {
                        "name": "Marcos Vinícius Borela de Castro"
                    },
                    {
                        "name": "Leonardo Augusto da Silva Pacheco"
                    },
                    {
                        "name": "Edans Flávius de Oliveira Sandes"
                    }
                ],
                "author_detail": {
                    "name": "Edans Flávius de Oliveira Sandes"
                },
                "author": "Edans Flávius de Oliveira Sandes",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10439v2",
                "updated": "2025-03-11T12:19:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    19,
                    9,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-11T09:50:35Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    50,
                    35,
                    2,
                    346,
                    0
                ],
                "title": "CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs"
                },
                "summary": "Object goal navigation (ObjectNav) is a fundamental task in embodied AI,\nrequiring an agent to locate a target object in previously unseen environments.\nThis task is particularly challenging because it requires both perceptual and\ncognitive processes, including object recognition and decision-making. While\nsubstantial advancements in perception have been driven by the rapid\ndevelopment of visual foundation models, progress on the cognitive aspect\nremains constrained, primarily limited to either implicit learning through\nsimulator rollouts or explicit reliance on predefined heuristic rules. Inspired\nby neuroscientific findings demonstrating that humans maintain and dynamically\nupdate fine-grained cognitive states during object search tasks in novel\nenvironments, we propose CogNav, a framework designed to mimic this cognitive\nprocess using large language models. Specifically, we model the cognitive\nprocess using a finite state machine comprising fine-grained cognitive states,\nranging from exploration to identification. Transitions between states are\ndetermined by a large language model based on a dynamically constructed\nheterogeneous cognitive map, which contains spatial and semantic information\nabout the scene being explored. Extensive evaluations on the HM3D, MP3D, and\nRoboTHOR benchmarks demonstrate that our cognitive process modeling\nsignificantly improves the success rate of ObjectNav at least by relative 14%\nover the state-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object goal navigation (ObjectNav) is a fundamental task in embodied AI,\nrequiring an agent to locate a target object in previously unseen environments.\nThis task is particularly challenging because it requires both perceptual and\ncognitive processes, including object recognition and decision-making. While\nsubstantial advancements in perception have been driven by the rapid\ndevelopment of visual foundation models, progress on the cognitive aspect\nremains constrained, primarily limited to either implicit learning through\nsimulator rollouts or explicit reliance on predefined heuristic rules. Inspired\nby neuroscientific findings demonstrating that humans maintain and dynamically\nupdate fine-grained cognitive states during object search tasks in novel\nenvironments, we propose CogNav, a framework designed to mimic this cognitive\nprocess using large language models. Specifically, we model the cognitive\nprocess using a finite state machine comprising fine-grained cognitive states,\nranging from exploration to identification. Transitions between states are\ndetermined by a large language model based on a dynamically constructed\nheterogeneous cognitive map, which contains spatial and semantic information\nabout the scene being explored. Extensive evaluations on the HM3D, MP3D, and\nRoboTHOR benchmarks demonstrate that our cognitive process modeling\nsignificantly improves the success rate of ObjectNav at least by relative 14%\nover the state-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Yihan Cao"
                    },
                    {
                        "name": "Jiazhao Zhang"
                    },
                    {
                        "name": "Zhinan Yu"
                    },
                    {
                        "name": "Shuzhen Liu"
                    },
                    {
                        "name": "Zheng Qin"
                    },
                    {
                        "name": "Qin Zou"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Kai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Xu"
                },
                "author": "Kai Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05244v2",
                "updated": "2025-03-11T12:11:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    11,
                    0,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-07T08:56:20Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    56,
                    20,
                    4,
                    66,
                    0
                ],
                "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WritingBench: A Comprehensive Benchmark for Generative Writing"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing."
                },
                "authors": [
                    {
                        "name": "Yuning Wu"
                    },
                    {
                        "name": "Jiahao Mei"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "Shaopeng Lai"
                    },
                    {
                        "name": "Yuran Ren"
                    },
                    {
                        "name": "Zijia Wang"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Mengyue Wu"
                    },
                    {
                        "name": "Qin Jin"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06631v2",
                "updated": "2025-03-11T12:02:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    2,
                    1,
                    1,
                    70,
                    0
                ],
                "published": "2024-08-13T04:36:18Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    4,
                    36,
                    18,
                    1,
                    226,
                    0
                ],
                "title": "IFShip: Interpretable Fine-grained Ship Classification with Domain\n  Knowledge-Enhanced Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFShip: Interpretable Fine-grained Ship Classification with Domain\n  Knowledge-Enhanced Vision-Language Models"
                },
                "summary": "End-to-end interpretation currently dominates the remote sensing fine-grained\nship classification (RS-FGSC) task. However, the inference process remains\nuninterpretable, leading to criticisms of these models as \"black box\" systems.\nTo address this issue, we propose a domain knowledge-enhanced Chain-of-Thought\n(CoT) prompt generation mechanism, which is used to semi-automatically\nconstruct a task-specific instruction-following dataset, TITANIC-FGS. By\ntraining on TITANIC-FGS, we adapt general-domain vision-language models (VLMs)\nto the FGSC task, resulting in a model named IFShip. Building upon IFShip, we\ndevelop an FGSC visual chatbot that redefines the FGSC problem as a\nstep-by-step reasoning task and conveys the reasoning process in natural\nlanguage. Experimental results show that IFShip outperforms state-of-the-art\nFGSC algorithms in both interpretability and classification accuracy.\nFurthermore, compared to VLMs such as LLaVA and MiniGPT-4, IFShip demonstrates\nsuperior performance on the FGSC task. It provides an accurate chain of\nreasoning when fine-grained ship types are recognizable to the human eye and\noffers interpretable explanations when they are not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end interpretation currently dominates the remote sensing fine-grained\nship classification (RS-FGSC) task. However, the inference process remains\nuninterpretable, leading to criticisms of these models as \"black box\" systems.\nTo address this issue, we propose a domain knowledge-enhanced Chain-of-Thought\n(CoT) prompt generation mechanism, which is used to semi-automatically\nconstruct a task-specific instruction-following dataset, TITANIC-FGS. By\ntraining on TITANIC-FGS, we adapt general-domain vision-language models (VLMs)\nto the FGSC task, resulting in a model named IFShip. Building upon IFShip, we\ndevelop an FGSC visual chatbot that redefines the FGSC problem as a\nstep-by-step reasoning task and conveys the reasoning process in natural\nlanguage. Experimental results show that IFShip outperforms state-of-the-art\nFGSC algorithms in both interpretability and classification accuracy.\nFurthermore, compared to VLMs such as LLaVA and MiniGPT-4, IFShip demonstrates\nsuperior performance on the FGSC task. It provides an accurate chain of\nreasoning when fine-grained ship types are recognizable to the human eye and\noffers interpretable explanations when they are not."
                },
                "authors": [
                    {
                        "name": "Mingning Guo"
                    },
                    {
                        "name": "Mengwei Wu"
                    },
                    {
                        "name": "Yuxiang Shen"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Chao Tao"
                    }
                ],
                "author_detail": {
                    "name": "Chao Tao"
                },
                "author": "Chao Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08343v1",
                "updated": "2025-03-11T11:53:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    53,
                    21,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:53:21Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    53,
                    21,
                    1,
                    70,
                    0
                ],
                "title": "Flexible and Efficient Probabilistic PDE Solvers through Gaussian Markov\n  Random Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible and Efficient Probabilistic PDE Solvers through Gaussian Markov\n  Random Fields"
                },
                "summary": "Mechanistic knowledge about the physical world is virtually always expressed\nvia partial differential equations (PDEs). Recently, there has been a surge of\ninterest in probabilistic PDE solvers -- Bayesian statistical models mostly\nbased on Gaussian process (GP) priors which seamlessly combine empirical\nmeasurements and mechanistic knowledge. As such, they quantify uncertainties\narising from e.g. noisy or missing data, unknown PDE parameters or\ndiscretization error by design. Prior work has established connections to\nclassical PDE solvers and provided solid theoretical guarantees. However,\nscaling such methods to large-scale problems remains a fundamental challenge\nprimarily due to dense covariance matrices. Our approach addresses the\nscalability issues by leveraging the Markov property of many commonly used GP\npriors. It has been shown that such priors are solutions to stochastic PDEs\n(SPDEs) which when discretized allow for highly efficient GP regression through\nsparse linear algebra. In this work, we show how to leverage this prior class\nto make probabilistic PDE solvers practical, even for large-scale nonlinear\nPDEs, through greatly accelerated inference mechanisms. Additionally, our\napproach also allows for flexible and physically meaningful priors beyond what\ncan be modeled with covariance functions. Experiments confirm substantial\nspeedups and accelerated convergence of our physics-informed priors in\nnonlinear settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic knowledge about the physical world is virtually always expressed\nvia partial differential equations (PDEs). Recently, there has been a surge of\ninterest in probabilistic PDE solvers -- Bayesian statistical models mostly\nbased on Gaussian process (GP) priors which seamlessly combine empirical\nmeasurements and mechanistic knowledge. As such, they quantify uncertainties\narising from e.g. noisy or missing data, unknown PDE parameters or\ndiscretization error by design. Prior work has established connections to\nclassical PDE solvers and provided solid theoretical guarantees. However,\nscaling such methods to large-scale problems remains a fundamental challenge\nprimarily due to dense covariance matrices. Our approach addresses the\nscalability issues by leveraging the Markov property of many commonly used GP\npriors. It has been shown that such priors are solutions to stochastic PDEs\n(SPDEs) which when discretized allow for highly efficient GP regression through\nsparse linear algebra. In this work, we show how to leverage this prior class\nto make probabilistic PDE solvers practical, even for large-scale nonlinear\nPDEs, through greatly accelerated inference mechanisms. Additionally, our\napproach also allows for flexible and physically meaningful priors beyond what\ncan be modeled with covariance functions. Experiments confirm substantial\nspeedups and accelerated convergence of our physics-informed priors in\nnonlinear settings."
                },
                "authors": [
                    {
                        "name": "Tim Weiland"
                    },
                    {
                        "name": "Marvin Pförtner"
                    },
                    {
                        "name": "Philipp Hennig"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Hennig"
                },
                "author": "Philipp Hennig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08342v2",
                "updated": "2025-03-12T04:18:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    4,
                    18,
                    48,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-11T11:52:37Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    52,
                    37,
                    1,
                    70,
                    0
                ],
                "title": "Attention Reallocation: Towards Zero-cost and Controllable Hallucination\n  Mitigation of MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Reallocation: Towards Zero-cost and Controllable Hallucination\n  Mitigation of MLLMs"
                },
                "summary": "Multi-Modal Large Language Models (MLLMs) stand out in various tasks but\nstill struggle with hallucinations. While recent training-free mitigation\nmethods mostly introduce additional inference overhead via retrospection\nstrategy and contrastive decoding, we propose attention reallocation (AttnReal)\nto mitigate hallucinations with nearly zero extra cost. Our approach is\nmotivated by the key observations that, MLLM's unreasonable attention\ndistribution causes features to be dominated by historical output tokens, which\nfurther contributes to hallucinated responses because of the distribution gap\nbetween different token types. Based on the observations, AttnReal recycles\nexcessive attention from output tokens and reallocates it to visual tokens,\nwhich reduces MLLM's reliance on language priors and ensures the decoding\nprocess depends more on the visual inputs. More interestingly, we find that, by\ncontrolling the intensity of AttnReal, we can achieve a wide-range trade-off\nbetween the response faithfulness and overall performance. Comprehensive\nresults from different benchmarks validate the effectiveness of AttnReal across\nsix open-source MLLMs and three decoding strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Large Language Models (MLLMs) stand out in various tasks but\nstill struggle with hallucinations. While recent training-free mitigation\nmethods mostly introduce additional inference overhead via retrospection\nstrategy and contrastive decoding, we propose attention reallocation (AttnReal)\nto mitigate hallucinations with nearly zero extra cost. Our approach is\nmotivated by the key observations that, MLLM's unreasonable attention\ndistribution causes features to be dominated by historical output tokens, which\nfurther contributes to hallucinated responses because of the distribution gap\nbetween different token types. Based on the observations, AttnReal recycles\nexcessive attention from output tokens and reallocates it to visual tokens,\nwhich reduces MLLM's reliance on language priors and ensures the decoding\nprocess depends more on the visual inputs. More interestingly, we find that, by\ncontrolling the intensity of AttnReal, we can achieve a wide-range trade-off\nbetween the response faithfulness and overall performance. Comprehensive\nresults from different benchmarks validate the effectiveness of AttnReal across\nsix open-source MLLMs and three decoding strategies."
                },
                "authors": [
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08338v1",
                "updated": "2025-03-11T11:50:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    50,
                    36,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:50:36Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    50,
                    36,
                    1,
                    70,
                    0
                ],
                "title": "Trinity: A Modular Humanoid Robot AI System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trinity: A Modular Humanoid Robot AI System"
                },
                "summary": "In recent years, research on humanoid robots has garnered increasing\nattention. With breakthroughs in various types of artificial intelligence\nalgorithms, embodied intelligence, exemplified by humanoid robots, has been\nhighly anticipated. The advancements in reinforcement learning (RL) algorithms\nhave significantly improved the motion control and generalization capabilities\nof humanoid robots. Simultaneously, the groundbreaking progress in large\nlanguage models (LLM) and visual language models (VLM) has brought more\npossibilities and imagination to humanoid robots. LLM enables humanoid robots\nto understand complex tasks from language instructions and perform long-term\ntask planning, while VLM greatly enhances the robots' understanding and\ninteraction with their environment. This paper introduces\n\\textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that\nintegrates RL, LLM, and VLM. By combining these technologies, Trinity enables\nefficient control of humanoid robots in complex environments. This innovative\napproach not only enhances the capabilities but also opens new avenues for\nfuture research and applications of humanoid robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, research on humanoid robots has garnered increasing\nattention. With breakthroughs in various types of artificial intelligence\nalgorithms, embodied intelligence, exemplified by humanoid robots, has been\nhighly anticipated. The advancements in reinforcement learning (RL) algorithms\nhave significantly improved the motion control and generalization capabilities\nof humanoid robots. Simultaneously, the groundbreaking progress in large\nlanguage models (LLM) and visual language models (VLM) has brought more\npossibilities and imagination to humanoid robots. LLM enables humanoid robots\nto understand complex tasks from language instructions and perform long-term\ntask planning, while VLM greatly enhances the robots' understanding and\ninteraction with their environment. This paper introduces\n\\textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that\nintegrates RL, LLM, and VLM. By combining these technologies, Trinity enables\nefficient control of humanoid robots in complex environments. This innovative\napproach not only enhances the capabilities but also opens new avenues for\nfuture research and applications of humanoid robotics."
                },
                "authors": [
                    {
                        "name": "Jingkai Sun"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Gang Han"
                    },
                    {
                        "name": "Wen Zhao"
                    },
                    {
                        "name": "Zhe Yong"
                    },
                    {
                        "name": "Yan He"
                    },
                    {
                        "name": "Jiaxu Wang"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Yijie Guo"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08335v1",
                "updated": "2025-03-11T11:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    47,
                    48,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:47:48Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    47,
                    48,
                    1,
                    70,
                    0
                ],
                "title": "Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal\n  Videos"
                },
                "summary": "Learning multimodal video understanding typically relies on datasets\ncomprising video clips paired with manually annotated captions. However, this\nbecomes even more challenging when dealing with long-form videos, lasting from\nminutes to hours, in educational and news domains due to the need for more\nannotators with subject expertise. Hence, there arises a need for automated\nsolutions. Recent advancements in Large Language Models (LLMs) promise to\ncapture concise and informative content that allows the comprehension of entire\nvideos by leveraging Automatic Speech Recognition (ASR) and Optical Character\nRecognition (OCR) technologies. ASR provides textual content from audio, while\nOCR extracts textual content from specific frames. This paper introduces a\ndataset comprising long-form lectures and news videos. We present baseline\napproaches to understand their limitations on this dataset and advocate for\nexploring prompt engineering techniques to comprehend long-form multimodal\nvideo datasets comprehensively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning multimodal video understanding typically relies on datasets\ncomprising video clips paired with manually annotated captions. However, this\nbecomes even more challenging when dealing with long-form videos, lasting from\nminutes to hours, in educational and news domains due to the need for more\nannotators with subject expertise. Hence, there arises a need for automated\nsolutions. Recent advancements in Large Language Models (LLMs) promise to\ncapture concise and informative content that allows the comprehension of entire\nvideos by leveraging Automatic Speech Recognition (ASR) and Optical Character\nRecognition (OCR) technologies. ASR provides textual content from audio, while\nOCR extracts textual content from specific frames. This paper introduces a\ndataset comprising long-form lectures and news videos. We present baseline\napproaches to understand their limitations on this dataset and advocate for\nexploring prompt engineering techniques to comprehend long-form multimodal\nvideo datasets comprehensively."
                },
                "authors": [
                    {
                        "name": "Soumya Shamarao Jahagirdar"
                    },
                    {
                        "name": "Jayasree Saha"
                    },
                    {
                        "name": "C V Jawahar"
                    }
                ],
                "author_detail": {
                    "name": "C V Jawahar"
                },
                "author": "C V Jawahar",
                "arxiv_comment": "CVIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08332v1",
                "updated": "2025-03-11T11:45:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    45,
                    5,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:45:05Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    45,
                    5,
                    1,
                    70,
                    0
                ],
                "title": "MINT-Demo: Membership Inference Test Demonstrator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MINT-Demo: Membership Inference Test Demonstrator"
                },
                "summary": "We present the Membership Inference Test Demonstrator, to emphasize the need\nfor more transparent machine learning training processes. MINT is a technique\nfor experimentally determining whether certain data has been used during the\ntraining of machine learning models. We conduct experiments with popular face\nrecognition models and 5 public databases containing over 22M images. Promising\nresults, up to 89% accuracy are achieved, suggesting that it is possible to\nrecognize if an AI model has been trained with specific data. Finally, we\npresent a MINT platform as demonstrator of this technology aimed to promote\ntransparency in AI training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Membership Inference Test Demonstrator, to emphasize the need\nfor more transparent machine learning training processes. MINT is a technique\nfor experimentally determining whether certain data has been used during the\ntraining of machine learning models. We conduct experiments with popular face\nrecognition models and 5 public databases containing over 22M images. Promising\nresults, up to 89% accuracy are achieved, suggesting that it is possible to\nrecognize if an AI model has been trained with specific data. Finally, we\npresent a MINT platform as demonstrator of this technology aimed to promote\ntransparency in AI training."
                },
                "authors": [
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Ruben Vera-Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "Ruben Vera-Rodriguez"
                },
                "author": "Ruben Vera-Rodriguez",
                "arxiv_comment": "Demo Paper Presented at Demo Track CVPR 24' and at AAAI 25' AIGOV\n  workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.08688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08688v1",
                "updated": "2025-03-11T17:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    53,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:59:53Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    53,
                    1,
                    70,
                    0
                ],
                "title": "Randomness, Not Representation: The Unreliability of Evaluating Cultural\n  Alignment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomness, Not Representation: The Unreliability of Evaluating Cultural\n  Alignment in LLMs"
                },
                "summary": "Research on the 'cultural alignment' of Large Language Models (LLMs) has\nemerged in response to growing interest in understanding representation across\ndiverse stakeholders. Current approaches to evaluating cultural alignment\nborrow social science methodologies but often overlook systematic robustness\nchecks. Here, we identify and test three assumptions behind current evaluation\nmethods: (1) Stability: that cultural alignment is a property of LLMs rather\nthan an artifact of evaluation design, (2) Extrapolability: that alignment with\none culture on a narrow set of issues predicts alignment with that culture on\nothers, and (3) Steerability: that LLMs can be reliably prompted to represent\nspecific cultural perspectives. Through experiments examining both explicit and\nimplicit preferences of leading LLMs, we find a high level of instability\nacross presentation formats, incoherence between evaluated versus held-out\ncultural dimensions, and erratic behavior under prompt steering. We show that\nthese inconsistencies can cause the results of an evaluation to be very\nsensitive to minor variations in methodology. Finally, we demonstrate in a case\nstudy on evaluation design that narrow experiments and a selective assessment\nof evidence can be used to paint an incomplete picture of LLMs' cultural\nalignment properties. Overall, these results highlight significant limitations\nof current approaches for evaluating the cultural alignment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on the 'cultural alignment' of Large Language Models (LLMs) has\nemerged in response to growing interest in understanding representation across\ndiverse stakeholders. Current approaches to evaluating cultural alignment\nborrow social science methodologies but often overlook systematic robustness\nchecks. Here, we identify and test three assumptions behind current evaluation\nmethods: (1) Stability: that cultural alignment is a property of LLMs rather\nthan an artifact of evaluation design, (2) Extrapolability: that alignment with\none culture on a narrow set of issues predicts alignment with that culture on\nothers, and (3) Steerability: that LLMs can be reliably prompted to represent\nspecific cultural perspectives. Through experiments examining both explicit and\nimplicit preferences of leading LLMs, we find a high level of instability\nacross presentation formats, incoherence between evaluated versus held-out\ncultural dimensions, and erratic behavior under prompt steering. We show that\nthese inconsistencies can cause the results of an evaluation to be very\nsensitive to minor variations in methodology. Finally, we demonstrate in a case\nstudy on evaluation design that narrow experiments and a selective assessment\nof evidence can be used to paint an incomplete picture of LLMs' cultural\nalignment properties. Overall, these results highlight significant limitations\nof current approaches for evaluating the cultural alignment of LLMs."
                },
                "authors": [
                    {
                        "name": "Ariba Khan"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Dylan Hadfield-Menell"
                    }
                ],
                "author_detail": {
                    "name": "Dylan Hadfield-Menell"
                },
                "author": "Dylan Hadfield-Menell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08684v1",
                "updated": "2025-03-11T17:59:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    0,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:59:00Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    59,
                    0,
                    1,
                    70,
                    0
                ],
                "title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents"
                },
                "summary": "Previous studies have found that PLM-based retrieval models exhibit a\npreference for LLM-generated content, assigning higher relevance scores to\nthese documents even when their semantic quality is comparable to human-written\nones. This phenomenon, known as source bias, threatens the sustainable\ndevelopment of the information access ecosystem. However, the underlying causes\nof source bias remain unexplored. In this paper, we explain the process of\ninformation retrieval with a causal graph and discover that PLM-based\nretrievers learn perplexity features for relevance estimation, causing source\nbias by ranking the documents with low perplexity higher. Theoretical analysis\nfurther reveals that the phenomenon stems from the positive correlation between\nthe gradients of the loss functions in language modeling task and retrieval\ntask. Based on the analysis, a causal-inspired inference-time debiasing method\nis proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses\nthe bias effect of the perplexity and then separates the bias effect from the\noverall estimated relevance score. Experimental results across three domains\ndemonstrate the superior debiasing effectiveness of CDC, emphasizing the\nvalidity of our proposed explanatory framework. Source codes are available at\nhttps://github.com/WhyDwelledOnAi/Perplexity-Trap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous studies have found that PLM-based retrieval models exhibit a\npreference for LLM-generated content, assigning higher relevance scores to\nthese documents even when their semantic quality is comparable to human-written\nones. This phenomenon, known as source bias, threatens the sustainable\ndevelopment of the information access ecosystem. However, the underlying causes\nof source bias remain unexplored. In this paper, we explain the process of\ninformation retrieval with a causal graph and discover that PLM-based\nretrievers learn perplexity features for relevance estimation, causing source\nbias by ranking the documents with low perplexity higher. Theoretical analysis\nfurther reveals that the phenomenon stems from the positive correlation between\nthe gradients of the loss functions in language modeling task and retrieval\ntask. Based on the analysis, a causal-inspired inference-time debiasing method\nis proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses\nthe bias effect of the perplexity and then separates the bias effect from the\noverall estimated relevance score. Experimental results across three domains\ndemonstrate the superior debiasing effectiveness of CDC, emphasizing the\nvalidity of our proposed explanatory framework. Source codes are available at\nhttps://github.com/WhyDwelledOnAi/Perplexity-Trap."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Haiyuan Zhao"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08683v1",
                "updated": "2025-03-11T17:58:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    58,
                    42,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:58:42Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    58,
                    42,
                    1,
                    70,
                    0
                ],
                "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving"
                },
                "summary": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https://github.com/cxliu0314/CoLMDriver."
                },
                "authors": [
                    {
                        "name": "Changxing Liu"
                    },
                    {
                        "name": "Genjia Liu"
                    },
                    {
                        "name": "Zijun Wang"
                    },
                    {
                        "name": "Jinchang Yang"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08681v1",
                "updated": "2025-03-11T17:57:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    57,
                    44,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:57:44Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    57,
                    44,
                    1,
                    70,
                    0
                ],
                "title": "Self-Taught Self-Correction for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Taught Self-Correction for Small Language Models"
                },
                "summary": "Although large language models (LLMs) have achieved remarkable performance\nacross various tasks, they remain prone to errors. A key challenge is enabling\nthem to self-correct. While prior research has relied on external tools or\nlarge proprietary models, this work explores self-correction in small language\nmodels (SLMs) through iterative fine-tuning using solely self-generated data.\nWe introduce the Self-Taught Self-Correction (STaSC) algorithm, which\nincorporates multiple algorithmic design choices. Experimental results on a\nquestion-answering task demonstrate that STaSC effectively learns\nself-correction, leading to significant performance improvements. Our analysis\nfurther provides insights into the mechanisms of self-correction and the impact\nof different design choices on learning dynamics and overall performance. To\nsupport future research, we release our user-friendly codebase and lightweight\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have achieved remarkable performance\nacross various tasks, they remain prone to errors. A key challenge is enabling\nthem to self-correct. While prior research has relied on external tools or\nlarge proprietary models, this work explores self-correction in small language\nmodels (SLMs) through iterative fine-tuning using solely self-generated data.\nWe introduce the Self-Taught Self-Correction (STaSC) algorithm, which\nincorporates multiple algorithmic design choices. Experimental results on a\nquestion-answering task demonstrate that STaSC effectively learns\nself-correction, leading to significant performance improvements. Our analysis\nfurther provides insights into the mechanisms of self-correction and the impact\nof different design choices on learning dynamics and overall performance. To\nsupport future research, we release our user-friendly codebase and lightweight\nmodels."
                },
                "authors": [
                    {
                        "name": "Viktor Moskvoretskii"
                    },
                    {
                        "name": "Chris Biemann"
                    },
                    {
                        "name": "Irina Nikishina"
                    }
                ],
                "author_detail": {
                    "name": "Irina Nikishina"
                },
                "author": "Irina Nikishina",
                "arxiv_comment": "Code is available at https://github.com/VityaVitalich/STASC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08677v1",
                "updated": "2025-03-11T17:55:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    55,
                    27,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:55:27Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    55,
                    27,
                    1,
                    70,
                    0
                ],
                "title": "OmniPaint: Mastering Object-Oriented Editing via Disentangled\n  Insertion-Removal Inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniPaint: Mastering Object-Oriented Editing via Disentangled\n  Insertion-Removal Inpainting"
                },
                "summary": "Diffusion-based generative models have revolutionized object-oriented image\nediting, yet their deployment in realistic object removal and insertion remains\nhampered by challenges such as the intricate interplay of physical effects and\ninsufficient paired training data. In this work, we introduce OmniPaint, a\nunified framework that re-conceptualizes object removal and insertion as\ninterdependent processes rather than isolated tasks. Leveraging a pre-trained\ndiffusion prior along with a progressive training pipeline comprising initial\npaired sample optimization and subsequent large-scale unpaired refinement via\nCycleFlow, OmniPaint achieves precise foreground elimination and seamless\nobject insertion while faithfully preserving scene geometry and intrinsic\nproperties. Furthermore, our novel CFD metric offers a robust, reference-free\nevaluation of context consistency and object hallucination, establishing a new\nbenchmark for high-fidelity image editing. Project page:\nhttps://github.com/yeates/OmniPaint-Page/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based generative models have revolutionized object-oriented image\nediting, yet their deployment in realistic object removal and insertion remains\nhampered by challenges such as the intricate interplay of physical effects and\ninsufficient paired training data. In this work, we introduce OmniPaint, a\nunified framework that re-conceptualizes object removal and insertion as\ninterdependent processes rather than isolated tasks. Leveraging a pre-trained\ndiffusion prior along with a progressive training pipeline comprising initial\npaired sample optimization and subsequent large-scale unpaired refinement via\nCycleFlow, OmniPaint achieves precise foreground elimination and seamless\nobject insertion while faithfully preserving scene geometry and intrinsic\nproperties. Furthermore, our novel CFD metric offers a robust, reference-free\nevaluation of context consistency and object hallucination, establishing a new\nbenchmark for high-fidelity image editing. Project page:\nhttps://github.com/yeates/OmniPaint-Page/"
                },
                "authors": [
                    {
                        "name": "Yongsheng Yu"
                    },
                    {
                        "name": "Ziyun Zeng"
                    },
                    {
                        "name": "Haitian Zheng"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08662v1",
                "updated": "2025-03-11T17:50:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    50,
                    44,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:50:44Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    50,
                    44,
                    1,
                    70,
                    0
                ],
                "title": "Exploring the Word Sense Disambiguation Capabilities of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Word Sense Disambiguation Capabilities of Large Language\n  Models"
                },
                "summary": "Word Sense Disambiguation (WSD) is a historical task in computational\nlinguistics that has received much attention over the years. However, with the\nadvent of Large Language Models (LLMs), interest in this task (in its classical\ndefinition) has decreased. In this study, we evaluate the performance of\nvarious LLMs on the WSD task. We extend a previous benchmark (XL-WSD) to\nre-design two subtasks suitable for LLM: 1) given a word in a sentence, the LLM\nmust generate the correct definition; 2) given a word in a sentence and a set\nof predefined meanings, the LLM must select the correct one. The extended\nbenchmark is built using the XL-WSD and BabelNet. The results indicate that\nLLMs perform well in zero-shot learning but cannot surpass current\nstate-of-the-art methods. However, a fine-tuned model with a medium number of\nparameters outperforms all other models, including the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Word Sense Disambiguation (WSD) is a historical task in computational\nlinguistics that has received much attention over the years. However, with the\nadvent of Large Language Models (LLMs), interest in this task (in its classical\ndefinition) has decreased. In this study, we evaluate the performance of\nvarious LLMs on the WSD task. We extend a previous benchmark (XL-WSD) to\nre-design two subtasks suitable for LLM: 1) given a word in a sentence, the LLM\nmust generate the correct definition; 2) given a word in a sentence and a set\nof predefined meanings, the LLM must select the correct one. The extended\nbenchmark is built using the XL-WSD and BabelNet. The results indicate that\nLLMs perform well in zero-shot learning but cannot surpass current\nstate-of-the-art methods. However, a fine-tuned model with a medium number of\nparameters outperforms all other models, including the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Pierpaolo Basile"
                    },
                    {
                        "name": "Lucia Siciliani"
                    },
                    {
                        "name": "Elio Musacchio"
                    },
                    {
                        "name": "Giovanni Semeraro"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Semeraro"
                },
                "author": "Giovanni Semeraro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17017v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17017v3",
                "updated": "2025-03-11T17:42:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    42,
                    55,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-26T01:00:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    1,
                    0,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On"
                },
                "summary": "Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task. Project page:\nhttps://zhenchenwan.github.io/TED-VITON/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional\nefficacy in generating realistic images and preserving garment details, largely\nattributed to the robust generative capabilities of text-to-image (T2I)\ndiffusion backbones. However, the T2I models that underpin these methods have\nbecome outdated, thereby limiting the potential for further improvement in VTO.\nAdditionally, current methods face notable challenges in accurately rendering\ntext on garments without distortion and preserving fine-grained details, such\nas textures and material fidelity. The emergence of Diffusion Transformer (DiT)\nbased T2I models has showcased impressive performance and offers a promising\nopportunity for advancing VTO. Directly applying existing VTO techniques to\ntransformer-based T2I models is ineffective due to substantial architectural\ndifferences, which hinder their ability to fully leverage the models' advanced\ncapabilities for improved text generation. To address these challenges and\nunlock the full potential of DiT-based T2I models for VTO, we propose\nTED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter\nfor enhancing garment-specific features, a Text Preservation Loss to ensure\naccurate and distortion-free text rendering, and a constraint mechanism to\ngenerate prompts by optimizing Large Language Model (LLM). These innovations\nenable state-of-the-art (SOTA) performance in visual quality and text fidelity,\nestablishing a new benchmark for VTO task. Project page:\nhttps://zhenchenwan.github.io/TED-VITON/"
                },
                "authors": [
                    {
                        "name": "Zhenchen Wan"
                    },
                    {
                        "name": "Yanwu Xu"
                    },
                    {
                        "name": "Zhaoqing Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Mingming Gong"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Gong"
                },
                "author": "Mingming Gong",
                "arxiv_comment": "Project page: https://github.com/ZhenchenWan/TED-VITON",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17017v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17017v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06759v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06759v3",
                "updated": "2025-03-11T17:37:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    37,
                    30,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-10T18:38:57Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    38,
                    57,
                    0,
                    41,
                    0
                ],
                "title": "Rationalization Models for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationalization Models for Text-to-SQL"
                },
                "summary": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability."
                },
                "authors": [
                    {
                        "name": "Gaetano Rossiello"
                    },
                    {
                        "name": "Nhan Pham"
                    },
                    {
                        "name": "Michael Glass"
                    },
                    {
                        "name": "Junkyu Lee"
                    },
                    {
                        "name": "Dharmashankar Subramanian"
                    }
                ],
                "author_detail": {
                    "name": "Dharmashankar Subramanian"
                },
                "author": "Dharmashankar Subramanian",
                "arxiv_comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06759v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06759v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08644v1",
                "updated": "2025-03-11T17:36:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    36,
                    53,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:36:53Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    36,
                    53,
                    1,
                    70,
                    0
                ],
                "title": "Exploiting Instruction-Following Retrievers for Malicious Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Instruction-Following Retrievers for Malicious Information\n  Retrieval"
                },
                "summary": "Instruction-following retrievers have been widely adopted alongside LLMs in\nreal-world applications, but little work has investigated the safety risks\nsurrounding their increasing search capabilities. We empirically study the\nability of retrievers to satisfy malicious queries, both when used directly and\nwhen used in a retrieval augmented generation-based setup. Concretely, we\ninvestigate six leading retrievers, including NV-Embed and LLM2Vec, and find\nthat given malicious requests, most retrievers can (for >50% of queries) select\nrelevant harmful passages. For example, LLM2Vec correctly selects passages for\n61.35% of our malicious queries. We further uncover an emerging risk with\ninstruction-following retrievers, where highly relevant harmful information can\nbe surfaced by exploiting their instruction-following capabilities. Finally, we\nshow that even safety-aligned LLMs, such as Llama3, can satisfy malicious\nrequests when provided with harmful retrieved passages in-context. In summary,\nour findings underscore the malicious misuse risks associated with increasing\nretriever capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following retrievers have been widely adopted alongside LLMs in\nreal-world applications, but little work has investigated the safety risks\nsurrounding their increasing search capabilities. We empirically study the\nability of retrievers to satisfy malicious queries, both when used directly and\nwhen used in a retrieval augmented generation-based setup. Concretely, we\ninvestigate six leading retrievers, including NV-Embed and LLM2Vec, and find\nthat given malicious requests, most retrievers can (for >50% of queries) select\nrelevant harmful passages. For example, LLM2Vec correctly selects passages for\n61.35% of our malicious queries. We further uncover an emerging risk with\ninstruction-following retrievers, where highly relevant harmful information can\nbe surfaced by exploiting their instruction-following capabilities. Finally, we\nshow that even safety-aligned LLMs, such as Llama3, can satisfy malicious\nrequests when provided with harmful retrieved passages in-context. In summary,\nour findings underscore the malicious misuse risks associated with increasing\nretriever capability."
                },
                "authors": [
                    {
                        "name": "Parishad BehnamGhader"
                    },
                    {
                        "name": "Nicholas Meade"
                    },
                    {
                        "name": "Siva Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Siva Reddy"
                },
                "author": "Siva Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05891v2",
                "updated": "2025-03-11T17:33:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    33,
                    51,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-07T19:24:59Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    19,
                    24,
                    59,
                    4,
                    66,
                    0
                ],
                "title": "MastermindEval: A Simple But Scalable Reasoning Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MastermindEval: A Simple But Scalable Reasoning Benchmark"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing."
                },
                "authors": [
                    {
                        "name": "Jonas Golde"
                    },
                    {
                        "name": "Patrick Haller"
                    },
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Alan Akbik"
                    }
                ],
                "author_detail": {
                    "name": "Alan Akbik"
                },
                "author": "Alan Akbik",
                "arxiv_comment": "9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and\n  Planning for Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17591v2",
                "updated": "2025-03-11T17:32:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    32,
                    22,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-24T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    16,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "Proactive Privacy Amnesia for Large Language Models: Safeguarding PII\n  with Negligible Impact on Model Utility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive Privacy Amnesia for Large Language Models: Safeguarding PII\n  with Negligible Impact on Model Utility"
                },
                "summary": "With the rise of large language models (LLMs), increasing research has\nrecognized their risk of leaking personally identifiable information (PII)\nunder malicious attacks. Although efforts have been made to protect PII in\nLLMs, existing methods struggle to balance privacy protection with maintaining\nmodel utility. In this paper, inspired by studies of amnesia in cognitive\nscience, we propose a novel approach, Proactive Privacy Amnesia (PPA), to\nsafeguard PII in LLMs while preserving their utility. This mechanism works by\nactively identifying and forgetting key memories most closely associated with\nPII in sequences, followed by a memory implanting using suitable substitute\nmemories to maintain the LLM's functionality. We conduct evaluations across\nmultiple models to protect common PII, such as phone numbers and physical\naddresses, against prevalent PII-targeted attacks, demonstrating the\nsuperiority of our method compared with other existing defensive techniques.\nThe results show that our PPA method completely eliminates the risk of phone\nnumber exposure by 100% and significantly reduces the risk of physical address\nexposure by 9.8% - 87.6%, all while maintaining comparable model utility\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs), increasing research has\nrecognized their risk of leaking personally identifiable information (PII)\nunder malicious attacks. Although efforts have been made to protect PII in\nLLMs, existing methods struggle to balance privacy protection with maintaining\nmodel utility. In this paper, inspired by studies of amnesia in cognitive\nscience, we propose a novel approach, Proactive Privacy Amnesia (PPA), to\nsafeguard PII in LLMs while preserving their utility. This mechanism works by\nactively identifying and forgetting key memories most closely associated with\nPII in sequences, followed by a memory implanting using suitable substitute\nmemories to maintain the LLM's functionality. We conduct evaluations across\nmultiple models to protect common PII, such as phone numbers and physical\naddresses, against prevalent PII-targeted attacks, demonstrating the\nsuperiority of our method compared with other existing defensive techniques.\nThe results show that our PPA method completely eliminates the risk of phone\nnumber exposure by 100% and significantly reduces the risk of physical address\nexposure by 9.8% - 87.6%, all while maintaining comparable model utility\nperformance."
                },
                "authors": [
                    {
                        "name": "Martin Kuo"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Minxue Tang"
                    },
                    {
                        "name": "Louis DiValentin"
                    },
                    {
                        "name": "Aolin Ding"
                    },
                    {
                        "name": "Jingwei Sun"
                    },
                    {
                        "name": "William Chen"
                    },
                    {
                        "name": "Amin Hass"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Hai Li"
                    }
                ],
                "author_detail": {
                    "name": "Hai Li"
                },
                "author": "Hai Li",
                "arxiv_comment": "ICLR'25 Poster. Project page and code is available at\n  https://ppa-iclr2025.my.canva.site/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v1",
                "updated": "2025-03-11T17:30:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10794v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10794v3",
                "updated": "2025-03-11T17:21:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    21,
                    0,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-16T13:04:52Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    4,
                    52,
                    5,
                    321,
                    0
                ],
                "title": "Going Beyond Conventional OOD Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going Beyond Conventional OOD Detection"
                },
                "summary": "Out-of-distribution (OOD) detection is critical to ensure the safe deployment\nof deep learning models in critical applications. Deep learning models can\noften misidentify OOD samples as in-distribution (ID) samples. This\nvulnerability worsens in the presence of spurious correlation in the training\nset. Likewise, in fine-grained classification settings, detection of\nfine-grained OOD samples becomes inherently challenging due to their high\nsimilarity to ID samples. However, current research on OOD detection has\nlargely ignored these challenging scenarios, focusing instead on relatively\neasier (conventional) cases. In this work, we present a unified Approach to\nSpurious, fine-grained, and Conventional OOD Detection (ASCOOD). First, we\npropose synthesizing virtual outliers from ID data by approximating the\ndestruction of invariant features. To this end, we identify invariant features\nwith the pixel attribution method using the model being learned. This approach\neliminates the burden of curating external OOD datasets. Then, we\nsimultaneously incentivize ID classification and predictive uncertainty towards\nvirtual outliers leveraging standardized feature representation. Our approach\neffectively mitigates the impact of spurious correlations and encourages\ncapturing fine-grained attributes. Extensive experiments across seven datasets\ndemonstrate the merit of ASCOOD in spurious, fine-grained, and conventional\nsettings. The code is available at: https://github.com/sudarshanregmi/ASCOOD/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is critical to ensure the safe deployment\nof deep learning models in critical applications. Deep learning models can\noften misidentify OOD samples as in-distribution (ID) samples. This\nvulnerability worsens in the presence of spurious correlation in the training\nset. Likewise, in fine-grained classification settings, detection of\nfine-grained OOD samples becomes inherently challenging due to their high\nsimilarity to ID samples. However, current research on OOD detection has\nlargely ignored these challenging scenarios, focusing instead on relatively\neasier (conventional) cases. In this work, we present a unified Approach to\nSpurious, fine-grained, and Conventional OOD Detection (ASCOOD). First, we\npropose synthesizing virtual outliers from ID data by approximating the\ndestruction of invariant features. To this end, we identify invariant features\nwith the pixel attribution method using the model being learned. This approach\neliminates the burden of curating external OOD datasets. Then, we\nsimultaneously incentivize ID classification and predictive uncertainty towards\nvirtual outliers leveraging standardized feature representation. Our approach\neffectively mitigates the impact of spurious correlations and encourages\ncapturing fine-grained attributes. Extensive experiments across seven datasets\ndemonstrate the merit of ASCOOD in spurious, fine-grained, and conventional\nsettings. The code is available at: https://github.com/sudarshanregmi/ASCOOD/"
                },
                "authors": [
                    {
                        "name": "Sudarshan Regmi"
                    }
                ],
                "author_detail": {
                    "name": "Sudarshan Regmi"
                },
                "author": "Sudarshan Regmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10794v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10794v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07072v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07072v3",
                "updated": "2025-03-11T17:08:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    8,
                    5,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-10T22:07:02Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    22,
                    7,
                    2,
                    0,
                    41,
                    0
                ],
                "title": "IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large\n  Language Models"
                },
                "summary": "Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not a day goes by without hearing about the impressive feats of large\nlanguage models (LLMs), and equally, not a day passes without hearing about\ntheir challenges. LLMs are notoriously vulnerable to biases in their dataset,\nleading to issues such as toxicity. While domain-adaptive training has been\nemployed to mitigate these issues, these techniques often address all model\nparameters indiscriminately during the repair process, resulting in poor repair\nquality and reduced model versatility. In this paper, we introduce a novel\ndynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach\nselectively targets the most error-prone sections of the model for repair.\nSpecifically, we propose dynamically slicing the model's most sensitive layers\nthat require immediate attention, concentrating repair efforts on those areas.\nThis method enables more effective repairs with potentially less impact on the\nmodel's overall performance by altering a smaller portion of the model. We\nevaluated our technique on three models from the GPT2 and GPT-Neo families,\nwith parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our\nresults show that IRepair repairs errors 43.6% more effectively while causing\n46% less disruption to general performance compared to the closest baseline,\ndirect preference optimization. Our empirical analysis also reveals that errors\nare more concentrated in a smaller section of the model, with the top 20% of\nlayers exhibiting 773% more error density than the remaining 80\\%. This\nhighlights the need for selective repair. Additionally, we demonstrate that a\ndynamic selection approach is essential for addressing errors dispersed\nthroughout the model, ensuring a robust and efficient repair."
                },
                "authors": [
                    {
                        "name": "Sayem Mohammad Imtiaz"
                    },
                    {
                        "name": "Astha Singh"
                    },
                    {
                        "name": "Fraol Batole"
                    },
                    {
                        "name": "Hridesh Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Hridesh Rajan"
                },
                "author": "Hridesh Rajan",
                "arxiv_comment": "Accepted as full research paper at FSE'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07072v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07072v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08604v1",
                "updated": "2025-03-11T16:42:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    42,
                    36,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:42:36Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    42,
                    36,
                    1,
                    70,
                    0
                ],
                "title": "EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in\n  Open Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in\n  Open Environments"
                },
                "summary": "Developing autonomous home robots controlled by natural language has long\nbeen a pursuit of human. While advancements in large language models (LLMs) and\nembodied intelligence make this goal closer, several challenges persist: the\nlack of a unified benchmark for more complex robot tasks, limited evaluation\nmethods and metrics, data incompatibility between LLMs and mobile manipulation\ntrajectories. To address these issues, we introduce Embodied Mobile\nManipulation in Open Environments (EMMOE), which requires agents to interpret\nuser instructions and execute long-horizon everyday tasks in continuous space.\nEMMOE seamlessly integrates high-level and low-level embodied tasks into a\nunified framework, along with three new metrics for more diverse assessment.\nAdditionally, we collect EMMOE-100, which features in various task attributes,\ndetailed process annotations, re-plans after failures, and two sub-datasets for\nLLM training. Furthermore, we design HomieBot, a sophisticated agent system\nconsists of LLM with Direct Preference Optimization (DPO), light weighted\nnavigation and manipulation models, and multiple error detection mechanisms.\nFinally, we demonstrate HomieBot's performance and the evaluation of different\nmodels and policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing autonomous home robots controlled by natural language has long\nbeen a pursuit of human. While advancements in large language models (LLMs) and\nembodied intelligence make this goal closer, several challenges persist: the\nlack of a unified benchmark for more complex robot tasks, limited evaluation\nmethods and metrics, data incompatibility between LLMs and mobile manipulation\ntrajectories. To address these issues, we introduce Embodied Mobile\nManipulation in Open Environments (EMMOE), which requires agents to interpret\nuser instructions and execute long-horizon everyday tasks in continuous space.\nEMMOE seamlessly integrates high-level and low-level embodied tasks into a\nunified framework, along with three new metrics for more diverse assessment.\nAdditionally, we collect EMMOE-100, which features in various task attributes,\ndetailed process annotations, re-plans after failures, and two sub-datasets for\nLLM training. Furthermore, we design HomieBot, a sophisticated agent system\nconsists of LLM with Direct Preference Optimization (DPO), light weighted\nnavigation and manipulation models, and multiple error detection mechanisms.\nFinally, we demonstrate HomieBot's performance and the evaluation of different\nmodels and policies."
                },
                "authors": [
                    {
                        "name": "Dongping Li"
                    },
                    {
                        "name": "Tielong Cai"
                    },
                    {
                        "name": "Tianci Tang"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Katherine Rose Driggs-Campbell"
                    },
                    {
                        "name": "Gaoang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gaoang Wang"
                },
                "author": "Gaoang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11681v2",
                "updated": "2025-03-11T16:41:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    41,
                    52,
                    1,
                    70,
                    0
                ],
                "published": "2024-04-17T18:20:31Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    18,
                    20,
                    31,
                    2,
                    108,
                    0
                ],
                "title": "Evaluating Tenant-Landlord Tensions Using Generative AI on Online Tenant\n  Forums",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Tenant-Landlord Tensions Using Generative AI on Online Tenant\n  Forums"
                },
                "summary": "Tenant-landlord relationships exhibit a power asymmetry where landlords'\npower to evict the tenants at a low-cost results in their dominating status in\nsuch relationships. Tenant concerns are thus often unspoken, unresolved, or\nignored and this could lead to blatant conflicts as suppressed tenant concerns\naccumulate. Modern machine learning methods and Large Language Models (LLM)\nhave demonstrated immense abilities to perform language tasks. In this study,\nwe incorporate Latent Dirichlet Allocation (LDA) with GPT-4 to classify Reddit\npost data scraped from the subreddit r/Tenant, aiming to unveil trends in\ntenant concerns while exploring the adoption of LLMs and machine learning\nmethods in social science research. We find that tenant concerns in topics like\nfee dispute and utility issues are consistently dominant in all four states\nanalyzed while each state has other common tenant concerns special to itself.\nMoreover, we discover temporal trends in tenant concerns that provide important\nimplications regarding the impact of the pandemic and the Eviction Moratorium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tenant-landlord relationships exhibit a power asymmetry where landlords'\npower to evict the tenants at a low-cost results in their dominating status in\nsuch relationships. Tenant concerns are thus often unspoken, unresolved, or\nignored and this could lead to blatant conflicts as suppressed tenant concerns\naccumulate. Modern machine learning methods and Large Language Models (LLM)\nhave demonstrated immense abilities to perform language tasks. In this study,\nwe incorporate Latent Dirichlet Allocation (LDA) with GPT-4 to classify Reddit\npost data scraped from the subreddit r/Tenant, aiming to unveil trends in\ntenant concerns while exploring the adoption of LLMs and machine learning\nmethods in social science research. We find that tenant concerns in topics like\nfee dispute and utility issues are consistently dominant in all four states\nanalyzed while each state has other common tenant concerns special to itself.\nMoreover, we discover temporal trends in tenant concerns that provide important\nimplications regarding the impact of the pandemic and the Eviction Moratorium."
                },
                "authors": [
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Cheng Ren"
                    },
                    {
                        "name": "Timothy A Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Timothy A Thomas"
                },
                "author": "Timothy A Thomas",
                "arxiv_doi": "10.1007/s42001-025-00378-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00378-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.11681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "J Comput Soc Sc 8, 50 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v3",
                "updated": "2025-03-11T16:35:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    59,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08601v1",
                "updated": "2025-03-11T16:35:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    22,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:35:22Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    22,
                    1,
                    70,
                    0
                ],
                "title": "LiSu: A Dataset and Method for LiDAR Surface Normal Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiSu: A Dataset and Method for LiDAR Surface Normal Estimation"
                },
                "summary": "While surface normals are widely used to analyse 3D scene geometry, surface\nnormal estimation from LiDAR point clouds remains severely underexplored. This\nis caused by the lack of large-scale annotated datasets on the one hand, and\nlack of methods that can robustly handle the sparse and often noisy LiDAR data\nin a reasonable time on the other hand. We address these limitations using a\ntraffic simulation engine and present LiSu, the first large-scale, synthetic\nLiDAR point cloud dataset with ground truth surface normal annotations,\neliminating the need for tedious manual labeling. Additionally, we propose a\nnovel method that exploits the spatiotemporal characteristics of autonomous\ndriving data to enhance surface normal estimation accuracy. By incorporating\ntwo regularization terms, we enforce spatial consistency among neighboring\npoints and temporal smoothness across consecutive LiDAR frames. These\nregularizers are particularly effective in self-training settings, where they\nmitigate the impact of noisy pseudo-labels, enabling robust real-world\ndeployment. We demonstrate the effectiveness of our method on LiSu, achieving\nstate-of-the-art performance in LiDAR surface normal estimation. Moreover, we\nshowcase its full potential in addressing the challenging task of\nsynthetic-to-real domain adaptation, leading to improved neural surface\nreconstruction on real-world data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While surface normals are widely used to analyse 3D scene geometry, surface\nnormal estimation from LiDAR point clouds remains severely underexplored. This\nis caused by the lack of large-scale annotated datasets on the one hand, and\nlack of methods that can robustly handle the sparse and often noisy LiDAR data\nin a reasonable time on the other hand. We address these limitations using a\ntraffic simulation engine and present LiSu, the first large-scale, synthetic\nLiDAR point cloud dataset with ground truth surface normal annotations,\neliminating the need for tedious manual labeling. Additionally, we propose a\nnovel method that exploits the spatiotemporal characteristics of autonomous\ndriving data to enhance surface normal estimation accuracy. By incorporating\ntwo regularization terms, we enforce spatial consistency among neighboring\npoints and temporal smoothness across consecutive LiDAR frames. These\nregularizers are particularly effective in self-training settings, where they\nmitigate the impact of noisy pseudo-labels, enabling robust real-world\ndeployment. We demonstrate the effectiveness of our method on LiSu, achieving\nstate-of-the-art performance in LiDAR surface normal estimation. Moreover, we\nshowcase its full potential in addressing the challenging task of\nsynthetic-to-real domain adaptation, leading to improved neural surface\nreconstruction on real-world data."
                },
                "authors": [
                    {
                        "name": "Dušan Malić"
                    },
                    {
                        "name": "Christian Fruhwirth-Reisinger"
                    },
                    {
                        "name": "Samuel Schulter"
                    },
                    {
                        "name": "Horst Possegger"
                    }
                ],
                "author_detail": {
                    "name": "Horst Possegger"
                },
                "author": "Horst Possegger",
                "arxiv_comment": "Accepted at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08600v1",
                "updated": "2025-03-11T16:35:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    8,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:35:08Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    8,
                    1,
                    70,
                    0
                ],
                "title": "NSF-SciFy: Mining the NSF Awards Database for Scientific Claims",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSF-SciFy: Mining the NSF Awards Database for Scientific Claims"
                },
                "summary": "We present NSF-SciFy, a large-scale dataset for scientific claim extraction\nderived from the National Science Foundation (NSF) awards database, comprising\nover 400K grant abstracts spanning five decades. While previous datasets relied\non published literature, we leverage grant abstracts which offer a unique\nadvantage: they capture claims at an earlier stage in the research lifecycle\nbefore publication takes effect. We also introduce a new task to distinguish\nbetween existing scientific claims and aspirational research intentions in\nproposals.Using zero-shot prompting with frontier large language models, we\njointly extract 114K scientific claims and 145K investigation proposals from\n16K grant abstracts in the materials science domain to create a focused subset\ncalled NSF-SciFy-MatSci. We use this dataset to evaluate 3 three key tasks: (1)\ntechnical to non-technical abstract generation, where models achieve high\nBERTScore (0.85+ F1); (2) scientific claim extraction, where fine-tuned models\noutperform base models by 100% relative improvement; and (3) investigation\nproposal extraction, showing 90%+ improvement with fine-tuning. We introduce\nnovel LLM-based evaluation metrics for robust assessment of claim/proposal\nextraction quality. As the largest scientific claim dataset to date -- with an\nestimated 2.8 million claims across all STEM disciplines funded by the NSF --\nNSF-SciFy enables new opportunities for claim verification and meta-scientific\nresearch. We publicly release all datasets, trained models, and evaluation code\nto facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NSF-SciFy, a large-scale dataset for scientific claim extraction\nderived from the National Science Foundation (NSF) awards database, comprising\nover 400K grant abstracts spanning five decades. While previous datasets relied\non published literature, we leverage grant abstracts which offer a unique\nadvantage: they capture claims at an earlier stage in the research lifecycle\nbefore publication takes effect. We also introduce a new task to distinguish\nbetween existing scientific claims and aspirational research intentions in\nproposals.Using zero-shot prompting with frontier large language models, we\njointly extract 114K scientific claims and 145K investigation proposals from\n16K grant abstracts in the materials science domain to create a focused subset\ncalled NSF-SciFy-MatSci. We use this dataset to evaluate 3 three key tasks: (1)\ntechnical to non-technical abstract generation, where models achieve high\nBERTScore (0.85+ F1); (2) scientific claim extraction, where fine-tuned models\noutperform base models by 100% relative improvement; and (3) investigation\nproposal extraction, showing 90%+ improvement with fine-tuning. We introduce\nnovel LLM-based evaluation metrics for robust assessment of claim/proposal\nextraction quality. As the largest scientific claim dataset to date -- with an\nestimated 2.8 million claims across all STEM disciplines funded by the NSF --\nNSF-SciFy enables new opportunities for claim verification and meta-scientific\nresearch. We publicly release all datasets, trained models, and evaluation code\nto facilitate further research."
                },
                "authors": [
                    {
                        "name": "Delip Rao"
                    },
                    {
                        "name": "Weiqiu You"
                    },
                    {
                        "name": "Eric Wong"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "11 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08589v1",
                "updated": "2025-03-11T16:25:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    25,
                    44,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:25:44Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    25,
                    44,
                    1,
                    70,
                    0
                ],
                "title": "Integration of nested cross-validation, automated hyperparameter\n  optimization, high-performance computing to reduce and quantify the variance\n  of test performance estimation of deep learning models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of nested cross-validation, automated hyperparameter\n  optimization, high-performance computing to reduce and quantify the variance\n  of test performance estimation of deep learning models"
                },
                "summary": "The variability and biases in the real-world performance benchmarking of deep\nlearning models for medical imaging compromise their trustworthiness for\nreal-world deployment. The common approach of holding out a single fixed test\nset fails to quantify the variance in the estimation of test performance\nmetrics. This study introduces NACHOS (Nested and Automated Cross-validation\nand Hyperparameter Optimization using Supercomputing) to reduce and quantify\nthe variance of test performance metrics of deep learning models. NACHOS\nintegrates Nested Cross-Validation (NCV) and Automated Hyperparameter\nOptimization (AHPO) within a parallelized high-performance computing (HPC)\nframework. NACHOS was demonstrated on a chest X-ray repository and an Optical\nCoherence Tomography (OCT) dataset under multiple data partitioning schemes.\nBeyond performance estimation, DACHOS (Deployment with Automated\nCross-validation and Hyperparameter Optimization using Supercomputing) is\nintroduced to leverage AHPO and cross-validation to build the final model on\nthe full dataset, improving expected deployment performance. The findings\nunderscore the importance of NCV in quantifying and reducing estimation\nvariance, AHPO in optimizing hyperparameters consistently across test folds,\nand HPC in ensuring computational feasibility. By integrating these\nmethodologies, NACHOS and DACHOS provide a scalable, reproducible, and\ntrustworthy framework for DL model evaluation and deployment in medical\nimaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The variability and biases in the real-world performance benchmarking of deep\nlearning models for medical imaging compromise their trustworthiness for\nreal-world deployment. The common approach of holding out a single fixed test\nset fails to quantify the variance in the estimation of test performance\nmetrics. This study introduces NACHOS (Nested and Automated Cross-validation\nand Hyperparameter Optimization using Supercomputing) to reduce and quantify\nthe variance of test performance metrics of deep learning models. NACHOS\nintegrates Nested Cross-Validation (NCV) and Automated Hyperparameter\nOptimization (AHPO) within a parallelized high-performance computing (HPC)\nframework. NACHOS was demonstrated on a chest X-ray repository and an Optical\nCoherence Tomography (OCT) dataset under multiple data partitioning schemes.\nBeyond performance estimation, DACHOS (Deployment with Automated\nCross-validation and Hyperparameter Optimization using Supercomputing) is\nintroduced to leverage AHPO and cross-validation to build the final model on\nthe full dataset, improving expected deployment performance. The findings\nunderscore the importance of NCV in quantifying and reducing estimation\nvariance, AHPO in optimizing hyperparameters consistently across test folds,\nand HPC in ensuring computational feasibility. By integrating these\nmethodologies, NACHOS and DACHOS provide a scalable, reproducible, and\ntrustworthy framework for DL model evaluation and deployment in medical\nimaging."
                },
                "authors": [
                    {
                        "name": "Paul Calle"
                    },
                    {
                        "name": "Averi Bates"
                    },
                    {
                        "name": "Justin C. Reynolds"
                    },
                    {
                        "name": "Yunlong Liu"
                    },
                    {
                        "name": "Haoyang Cui"
                    },
                    {
                        "name": "Sinaro Ly"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Qinghao Zhang"
                    },
                    {
                        "name": "Alberto J. de Armendi"
                    },
                    {
                        "name": "Shashank S. Shettar"
                    },
                    {
                        "name": "Kar Ming Fung"
                    },
                    {
                        "name": "Qinggong Tang"
                    },
                    {
                        "name": "Chongle Pan"
                    }
                ],
                "author_detail": {
                    "name": "Chongle Pan"
                },
                "author": "Chongle Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08585v1",
                "updated": "2025-03-11T16:21:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    21,
                    23,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:21:23Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    21,
                    23,
                    1,
                    70,
                    0
                ],
                "title": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video\n  Understanding"
                },
                "summary": "Despite advancements in multimodal large language models (MLLMs), current\napproaches struggle in medium-to-long video understanding due to frame and\ncontext length limitations. As a result, these models often depend on frame\nsampling, which risks missing key information over time and lacks task-specific\nrelevance. To address these challenges, we introduce HierarQ, a task-aware\nhierarchical Q-Former based framework that sequentially processes frames to\nbypass the need for frame sampling, while avoiding LLM's context length\nlimitations. We introduce a lightweight two-stream language-guided feature\nmodulator to incorporate task awareness in video understanding, with the entity\nstream capturing frame-level object information within a short context and the\nscene stream identifying their broader interactions over longer period of time.\nEach stream is supported by dedicated memory banks which enables our proposed\nHierachical Querying transformer (HierarQ) to effectively capture short and\nlong-term context. Extensive evaluations on 10 video benchmarks across video\nunderstanding, question answering, and captioning tasks demonstrate HierarQ's\nstate-of-the-art performance across most datasets, proving its robustness and\nefficiency for comprehensive video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in multimodal large language models (MLLMs), current\napproaches struggle in medium-to-long video understanding due to frame and\ncontext length limitations. As a result, these models often depend on frame\nsampling, which risks missing key information over time and lacks task-specific\nrelevance. To address these challenges, we introduce HierarQ, a task-aware\nhierarchical Q-Former based framework that sequentially processes frames to\nbypass the need for frame sampling, while avoiding LLM's context length\nlimitations. We introduce a lightweight two-stream language-guided feature\nmodulator to incorporate task awareness in video understanding, with the entity\nstream capturing frame-level object information within a short context and the\nscene stream identifying their broader interactions over longer period of time.\nEach stream is supported by dedicated memory banks which enables our proposed\nHierachical Querying transformer (HierarQ) to effectively capture short and\nlong-term context. Extensive evaluations on 10 video benchmarks across video\nunderstanding, question answering, and captioning tasks demonstrate HierarQ's\nstate-of-the-art performance across most datasets, proving its robustness and\nefficiency for comprehensive video analysis."
                },
                "authors": [
                    {
                        "name": "Shehreen Azad"
                    },
                    {
                        "name": "Vibhav Vineet"
                    },
                    {
                        "name": "Yogesh Singh Rawat"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Singh Rawat"
                },
                "author": "Yogesh Singh Rawat",
                "arxiv_comment": "Accepted in CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08582v1",
                "updated": "2025-03-11T16:16:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    16,
                    49,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T16:16:49Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    16,
                    49,
                    1,
                    70,
                    0
                ],
                "title": "Chatbots for Data Collection in Surveys: A Comparison of Four\n  Theory-Based Interview Probes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots for Data Collection in Surveys: A Comparison of Four\n  Theory-Based Interview Probes"
                },
                "summary": "Surveys are a widespread method for collecting data at scale, but their rigid\nstructure often limits the depth of qualitative insights obtained. While\ninterviews naturally yield richer responses, they are challenging to conduct\nacross diverse locations and large participant pools. To partially bridge this\ngap, we investigate the potential of using LLM-based chatbots to support\nqualitative data collection through interview probes embedded in surveys. We\nassess four theory-based interview probes: descriptive, idiographic,\nclarifying, and explanatory. Through a split-plot study design (N=64), we\ncompare the probes' impact on response quality and user experience across three\nkey stages of HCI research: exploration, requirements gathering, and\nevaluation. Our results show that probes facilitate the collection of\nhigh-quality survey data, with specific probes proving effective at different\nresearch stages. We contribute practical and methodological implications for\nusing chatbots as research tools to enrich qualitative data collection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveys are a widespread method for collecting data at scale, but their rigid\nstructure often limits the depth of qualitative insights obtained. While\ninterviews naturally yield richer responses, they are challenging to conduct\nacross diverse locations and large participant pools. To partially bridge this\ngap, we investigate the potential of using LLM-based chatbots to support\nqualitative data collection through interview probes embedded in surveys. We\nassess four theory-based interview probes: descriptive, idiographic,\nclarifying, and explanatory. Through a split-plot study design (N=64), we\ncompare the probes' impact on response quality and user experience across three\nkey stages of HCI research: exploration, requirements gathering, and\nevaluation. Our results show that probes facilitate the collection of\nhigh-quality survey data, with specific probes proving effective at different\nresearch stages. We contribute practical and methodological implications for\nusing chatbots as research tools to enrich qualitative data collection."
                },
                "authors": [
                    {
                        "name": "Rune M. Jacobsen"
                    },
                    {
                        "name": "Samuel Rhys Cox"
                    },
                    {
                        "name": "Carla F. Griggio"
                    },
                    {
                        "name": "Niels van Berkel"
                    }
                ],
                "author_detail": {
                    "name": "Niels van Berkel"
                },
                "author": "Niels van Berkel",
                "arxiv_doi": "10.1145/3706598.3714128",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714128",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.08582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "CHI Conference on Human Factors in Computing Systems (CHI '25), April\n  26-May 1, 2025, Yokohama,Japan",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08569v1",
                "updated": "2025-03-11T15:59:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    59,
                    43,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:59:43Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    59,
                    43,
                    1,
                    70,
                    0
                ],
                "title": "DeepReview: Improving LLM-based Paper Review with Human-like Deep\n  Thinking Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepReview: Improving LLM-based Paper Review with Human-like Deep\n  Thinking Process"
                },
                "summary": "Large Language Models (LLMs) are increasingly utilized in scientific research\nassessment, particularly in automated paper review. However, existing LLM-based\nreview systems face significant challenges, including limited domain expertise,\nhallucinated reasoning, and a lack of structured evaluation. To address these\nlimitations, we introduce DeepReview, a multi-stage framework designed to\nemulate expert reviewers by incorporating structured analysis, literature\nretrieval, and evidence-based argumentation. Using DeepReview-13K, a curated\ndataset with structured annotations, we train DeepReviewer-14B, which\noutperforms CycleReviewer-70B with fewer tokens. In its best mode,\nDeepReviewer-14B achieves win rates of 88.21\\% and 80.20\\% against GPT-o1 and\nDeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper\nreview, with all resources publicly available. The code, model, dataset and\ndemo have be released in http://ai-researcher.net.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly utilized in scientific research\nassessment, particularly in automated paper review. However, existing LLM-based\nreview systems face significant challenges, including limited domain expertise,\nhallucinated reasoning, and a lack of structured evaluation. To address these\nlimitations, we introduce DeepReview, a multi-stage framework designed to\nemulate expert reviewers by incorporating structured analysis, literature\nretrieval, and evidence-based argumentation. Using DeepReview-13K, a curated\ndataset with structured annotations, we train DeepReviewer-14B, which\noutperforms CycleReviewer-70B with fewer tokens. In its best mode,\nDeepReviewer-14B achieves win rates of 88.21\\% and 80.20\\% against GPT-o1 and\nDeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper\nreview, with all resources publicly available. The code, model, dataset and\ndemo have be released in http://ai-researcher.net."
                },
                "authors": [
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14042v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14042v2",
                "updated": "2025-03-11T15:54:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    54,
                    17,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-18T16:55:42Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    16,
                    55,
                    42,
                    2,
                    353,
                    0
                ],
                "title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds"
                },
                "summary": "Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and training dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained on a procedurally\ngenerated dataset of one million CAD sequences. CAD-Recode significantly\noutperforms existing methods across the DeepCAD, Fusion360 and real-world CC3D\ndatasets. Furthermore, we show that our CAD Python code output is interpretable\nby off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and training dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained on a procedurally\ngenerated dataset of one million CAD sequences. CAD-Recode significantly\noutperforms existing methods across the DeepCAD, Fusion360 and real-world CC3D\ndatasets. Furthermore, we show that our CAD Python code output is interpretable\nby off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds."
                },
                "authors": [
                    {
                        "name": "Danila Rukhovich"
                    },
                    {
                        "name": "Elona Dupont"
                    },
                    {
                        "name": "Dimitrios Mallis"
                    },
                    {
                        "name": "Kseniya Cherenkova"
                    },
                    {
                        "name": "Anis Kacem"
                    },
                    {
                        "name": "Djamila Aouada"
                    }
                ],
                "author_detail": {
                    "name": "Djamila Aouada"
                },
                "author": "Djamila Aouada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14042v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08564v1",
                "updated": "2025-03-11T15:53:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    53,
                    54,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:53:54Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    53,
                    54,
                    1,
                    70,
                    0
                ],
                "title": "MoE-Loco: Mixture of Experts for Multitask Locomotion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Loco: Mixture of Experts for Multitask Locomotion"
                },
                "summary": "We present MoE-Loco, a Mixture of Experts (MoE) framework for multitask\nlocomotion for legged robots. Our method enables a single policy to handle\ndiverse terrains, including bars, pits, stairs, slopes, and baffles, while\nsupporting quadrupedal and bipedal gaits. Using MoE, we mitigate the gradient\nconflicts that typically arise in multitask reinforcement learning, improving\nboth training efficiency and performance. Our experiments demonstrate that\ndifferent experts naturally specialize in distinct locomotion behaviors, which\ncan be leveraged for task migration and skill composition. We further validate\nour approach in both simulation and real-world deployment, showcasing its\nrobustness and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoE-Loco, a Mixture of Experts (MoE) framework for multitask\nlocomotion for legged robots. Our method enables a single policy to handle\ndiverse terrains, including bars, pits, stairs, slopes, and baffles, while\nsupporting quadrupedal and bipedal gaits. Using MoE, we mitigate the gradient\nconflicts that typically arise in multitask reinforcement learning, improving\nboth training efficiency and performance. Our experiments demonstrate that\ndifferent experts naturally specialize in distinct locomotion behaviors, which\ncan be leveraged for task migration and skill composition. We further validate\nour approach in both simulation and real-world deployment, showcasing its\nrobustness and adaptability."
                },
                "authors": [
                    {
                        "name": "Runhan Huang"
                    },
                    {
                        "name": "Shaoting Zhu"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Hang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhao"
                },
                "author": "Hang Zhao",
                "arxiv_comment": "8 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02800v3",
                "updated": "2025-03-11T15:47:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    47,
                    37,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-04T17:20:43Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    20,
                    43,
                    1,
                    63,
                    0
                ],
                "title": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration"
                },
                "summary": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 88.6% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 88.6% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries."
                },
                "authors": [
                    {
                        "name": "Alicia Russell-Gilbert"
                    },
                    {
                        "name": "Sudip Mittal"
                    },
                    {
                        "name": "Shahram Rahimi"
                    },
                    {
                        "name": "Maria Seale"
                    },
                    {
                        "name": "Joseph Jabour"
                    },
                    {
                        "name": "Thomas Arnold"
                    },
                    {
                        "name": "Joshua Church"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Church"
                },
                "author": "Joshua Church",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2411.00914",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "1.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08558v1",
                "updated": "2025-03-11T15:47:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    47,
                    12,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:47:12Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    47,
                    12,
                    1,
                    70,
                    0
                ],
                "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime\n  Failure Detection for Imitation Learning Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime\n  Failure Detection for Imitation Learning Policies"
                },
                "summary": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Tony Khuong Nguyen"
                    },
                    {
                        "name": "Emma Dixon"
                    },
                    {
                        "name": "Christopher Rodriguez"
                    },
                    {
                        "name": "Patrick Miller"
                    },
                    {
                        "name": "Robert Lee"
                    },
                    {
                        "name": "Paarth Shah"
                    },
                    {
                        "name": "Rares Ambrus"
                    },
                    {
                        "name": "Haruki Nishimura"
                    },
                    {
                        "name": "Masha Itkina"
                    }
                ],
                "author_detail": {
                    "name": "Masha Itkina"
                },
                "author": "Masha Itkina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08551v1",
                "updated": "2025-03-11T15:39:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    39,
                    43,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:39:43Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    39,
                    43,
                    1,
                    70,
                    0
                ],
                "title": "Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs"
                },
                "summary": "The difficulty of multiple-choice questions (MCQs) is a crucial factor for\neducational assessments. Predicting MCQ difficulty is challenging since it\nrequires understanding both the complexity of reaching the correct option and\nthe plausibility of distractors, i.e., incorrect options. In this paper, we\npropose a novel, two-stage method to predict the difficulty of MCQs. First, to\nbetter estimate the complexity of each MCQ, we use large language models (LLMs)\nto augment the reasoning steps required to reach each option. We use not just\nthe MCQ itself but also these reasoning steps as input to predict the\ndifficulty. Second, to capture the plausibility of distractors, we sample\nknowledge levels from a distribution to account for variation among students\nresponding to the MCQ. This setup, inspired by item response theory (IRT),\nenable us to estimate the likelihood of students selecting each (both correct\nand incorrect) option. We align these predictions with their ground truth\nvalues, using a Kullback-Leibler (KL) divergence-based regularization\nobjective, and use estimated likelihoods to predict MCQ difficulty. We evaluate\nour method on two real-world \\emph{math} MCQ and response datasets with ground\ntruth difficulty values estimated using IRT. Experimental results show that our\nmethod outperforms all baselines, up to a 28.3\\% reduction in mean squared\nerror and a 34.6\\% improvement in the coefficient of determination. We also\nqualitatively discuss how our novel method results in higher accuracy in\npredicting MCQ difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The difficulty of multiple-choice questions (MCQs) is a crucial factor for\neducational assessments. Predicting MCQ difficulty is challenging since it\nrequires understanding both the complexity of reaching the correct option and\nthe plausibility of distractors, i.e., incorrect options. In this paper, we\npropose a novel, two-stage method to predict the difficulty of MCQs. First, to\nbetter estimate the complexity of each MCQ, we use large language models (LLMs)\nto augment the reasoning steps required to reach each option. We use not just\nthe MCQ itself but also these reasoning steps as input to predict the\ndifficulty. Second, to capture the plausibility of distractors, we sample\nknowledge levels from a distribution to account for variation among students\nresponding to the MCQ. This setup, inspired by item response theory (IRT),\nenable us to estimate the likelihood of students selecting each (both correct\nand incorrect) option. We align these predictions with their ground truth\nvalues, using a Kullback-Leibler (KL) divergence-based regularization\nobjective, and use estimated likelihoods to predict MCQ difficulty. We evaluate\nour method on two real-world \\emph{math} MCQ and response datasets with ground\ntruth difficulty values estimated using IRT. Experimental results show that our\nmethod outperforms all baselines, up to a 28.3\\% reduction in mean squared\nerror and a 34.6\\% improvement in the coefficient of determination. We also\nqualitatively discuss how our novel method results in higher accuracy in\npredicting MCQ difficulty."
                },
                "authors": [
                    {
                        "name": "Wanyong Feng"
                    },
                    {
                        "name": "Peter Tran"
                    },
                    {
                        "name": "Stephen Sireci"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08549v1",
                "updated": "2025-03-11T15:36:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    36,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:36:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    36,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research\n  Idea Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph of AI Ideas: Leveraging Knowledge Graphs and LLMs for AI Research\n  Idea Generation"
                },
                "summary": "Reading relevant scientific papers and analyzing research development trends\nis a critical step in generating new scientific ideas. However, the rapid\nincrease in the volume of research literature and the complex citation\nrelationships make it difficult for researchers to quickly analyze and derive\nmeaningful research trends. The development of large language models (LLMs) has\nprovided a novel approach for automatically summarizing papers and generating\ninnovative research ideas. However, existing paper-based idea generation\nmethods either simply input papers into LLMs via prompts or form logical chains\nof creative development based on citation relationships, without fully\nexploiting the semantic information embedded in these citations. Inspired by\nknowledge graphs and human cognitive processes, we propose a framework called\nthe Graph of AI Ideas (GoAI) for the AI research field, which is dominated by\nopen-access papers. This framework organizes relevant literature into entities\nwithin a knowledge graph and summarizes the semantic information contained in\ncitations into relations within the graph. This organization effectively\nreflects the relationships between two academic papers and the advancement of\nthe AI research field. Such organization aids LLMs in capturing the current\nprogress of research, thereby enhancing their creativity. Experimental results\ndemonstrate the effectiveness of our approach in generating novel, clear, and\neffective research ideas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading relevant scientific papers and analyzing research development trends\nis a critical step in generating new scientific ideas. However, the rapid\nincrease in the volume of research literature and the complex citation\nrelationships make it difficult for researchers to quickly analyze and derive\nmeaningful research trends. The development of large language models (LLMs) has\nprovided a novel approach for automatically summarizing papers and generating\ninnovative research ideas. However, existing paper-based idea generation\nmethods either simply input papers into LLMs via prompts or form logical chains\nof creative development based on citation relationships, without fully\nexploiting the semantic information embedded in these citations. Inspired by\nknowledge graphs and human cognitive processes, we propose a framework called\nthe Graph of AI Ideas (GoAI) for the AI research field, which is dominated by\nopen-access papers. This framework organizes relevant literature into entities\nwithin a knowledge graph and summarizes the semantic information contained in\ncitations into relations within the graph. This organization effectively\nreflects the relationships between two academic papers and the advancement of\nthe AI research field. Such organization aids LLMs in capturing the current\nprogress of research, thereby enhancing their creativity. Experimental results\ndemonstrate the effectiveness of our approach in generating novel, clear, and\neffective research ideas."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Zongyun Zhang"
                    },
                    {
                        "name": "Mingye Xie"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06089v2",
                "updated": "2025-03-11T15:34:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    34,
                    16,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-08T22:29:56Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    22,
                    29,
                    56,
                    6,
                    343,
                    0
                ],
                "title": "GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis"
                },
                "summary": "Text-to-image (T2I) generation has seen significant progress with diffusion\nmodels, enabling generation of photo-realistic images from text prompts.\nDespite this progress, existing methods still face challenges in following\ncomplex text prompts, especially those requiring compositional and multi-step\nreasoning. Given such complex instructions, SOTA models often make mistakes in\nfaithfully modeling object attributes, and relationships among them. In this\nwork, we present an alternate paradigm for T2I synthesis, decomposing the task\nof complex multi-step generation into three steps, (a) Generate: we first\ngenerate an image using existing diffusion models (b) Plan: we make use of\nMulti-Modal LLMs (MLLMs) to identify the mistakes in the generated image\nexpressed in terms of individual objects and their properties, and produce a\nsequence of corrective steps required in the form of an edit-plan. (c) Edit: we\nmake use of an existing text-guided image editing models to sequentially\nexecute our edit-plan over the generated image to get the desired image which\nis faithful to the original instruction. Our approach derives its strength from\nthe fact that it is modular in nature, is training free, and can be applied\nover any combination of image generation and editing models. As an added\ncontribution, we also develop a model capable of compositional editing, which\nfurther helps improve the overall accuracy of our proposed approach. Our method\nflexibly trades inference time compute with performance on compositional text\nprompts. We perform extensive experimental evaluation across 3 benchmarks and\n10 T2I models including DALLE-3 and the latest -- SD-3.5-Large. Our approach\nnot only improves the performance of the SOTA models, by upto 3 points, it also\nreduces the performance gap between weaker and stronger models.\n$\\href{https://dair-iitd.github.io/GraPE/}{https://dair-iitd.github.io/GraPE/}$",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation has seen significant progress with diffusion\nmodels, enabling generation of photo-realistic images from text prompts.\nDespite this progress, existing methods still face challenges in following\ncomplex text prompts, especially those requiring compositional and multi-step\nreasoning. Given such complex instructions, SOTA models often make mistakes in\nfaithfully modeling object attributes, and relationships among them. In this\nwork, we present an alternate paradigm for T2I synthesis, decomposing the task\nof complex multi-step generation into three steps, (a) Generate: we first\ngenerate an image using existing diffusion models (b) Plan: we make use of\nMulti-Modal LLMs (MLLMs) to identify the mistakes in the generated image\nexpressed in terms of individual objects and their properties, and produce a\nsequence of corrective steps required in the form of an edit-plan. (c) Edit: we\nmake use of an existing text-guided image editing models to sequentially\nexecute our edit-plan over the generated image to get the desired image which\nis faithful to the original instruction. Our approach derives its strength from\nthe fact that it is modular in nature, is training free, and can be applied\nover any combination of image generation and editing models. As an added\ncontribution, we also develop a model capable of compositional editing, which\nfurther helps improve the overall accuracy of our proposed approach. Our method\nflexibly trades inference time compute with performance on compositional text\nprompts. We perform extensive experimental evaluation across 3 benchmarks and\n10 T2I models including DALLE-3 and the latest -- SD-3.5-Large. Our approach\nnot only improves the performance of the SOTA models, by upto 3 points, it also\nreduces the performance gap between weaker and stronger models.\n$\\href{https://dair-iitd.github.io/GraPE/}{https://dair-iitd.github.io/GraPE/}$"
                },
                "authors": [
                    {
                        "name": "Ashish Goswami"
                    },
                    {
                        "name": "Satyam Kumar Modi"
                    },
                    {
                        "name": "Santhosh Rishi Deshineni"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Prathosh A. P"
                    },
                    {
                        "name": "Parag Singla"
                    }
                ],
                "author_detail": {
                    "name": "Parag Singla"
                },
                "author": "Parag Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07896v2",
                "updated": "2025-03-11T15:31:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    31,
                    55,
                    1,
                    70,
                    0
                ],
                "published": "2023-12-13T04:53:12Z",
                "published_parsed": [
                    2023,
                    12,
                    13,
                    4,
                    53,
                    12,
                    2,
                    347,
                    0
                ],
                "title": "From Classification to Optimization: Slicing and Resource Management\n  with TRACTOR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Classification to Optimization: Slicing and Resource Management\n  with TRACTOR"
                },
                "summary": "5G and beyond networks promise advancements in bandwidth, latency, and\nconnectivity. The Open Radio Access Network (O-RAN) framework enhances\nflexibility through network slicing and closed-loop RAN control. Central to\nthis evolution is integrating machine learning (ML) for dynamic network\ncontrol. This paper presents a framework to optimize O-RAN operation. First, we\nbuild and share a robust O-RAN dataset from real-world traffic captured across\ndiverse locations and mobility scenarios, replicated within a full-stack\nsrsRAN-based O-RAN system using the Colosseum RF emulator. This dataset\nsupports ML training and deployment. We then introduce a traffic classification\napproach leveraging various ML models, demonstrating rapid training, testing,\nand refinement to improve accuracy. With up to 99% offline accuracy and 92%\nonline accuracy for specific slices, our framework adapts efficiently to\ndifferent models and network conditions. Finally, we present a physical\nresource block (PRB) assignment optimization strategy using reinforcement\nlearning to refine resource allocation. Our learned policy achieves a mean\nperformance score (0.631), surpassing a manually configured expert policy\n(0.609) and a random baseline (0.588), demonstrating improved PRB utilization.\nMore importantly, our approach exhibits lower variability, with the Coefficient\nof Variation (CV) reduced by up to an order of magnitude in three out of four\ncases, ensuring more consistent performance. Our contributions, including\nopen-source tools and datasets, accelerate O-RAN and ML-driven network control\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5G and beyond networks promise advancements in bandwidth, latency, and\nconnectivity. The Open Radio Access Network (O-RAN) framework enhances\nflexibility through network slicing and closed-loop RAN control. Central to\nthis evolution is integrating machine learning (ML) for dynamic network\ncontrol. This paper presents a framework to optimize O-RAN operation. First, we\nbuild and share a robust O-RAN dataset from real-world traffic captured across\ndiverse locations and mobility scenarios, replicated within a full-stack\nsrsRAN-based O-RAN system using the Colosseum RF emulator. This dataset\nsupports ML training and deployment. We then introduce a traffic classification\napproach leveraging various ML models, demonstrating rapid training, testing,\nand refinement to improve accuracy. With up to 99% offline accuracy and 92%\nonline accuracy for specific slices, our framework adapts efficiently to\ndifferent models and network conditions. Finally, we present a physical\nresource block (PRB) assignment optimization strategy using reinforcement\nlearning to refine resource allocation. Our learned policy achieves a mean\nperformance score (0.631), surpassing a manually configured expert policy\n(0.609) and a random baseline (0.588), demonstrating improved PRB utilization.\nMore importantly, our approach exhibits lower variability, with the Coefficient\nof Variation (CV) reduced by up to an order of magnitude in three out of four\ncases, ensuring more consistent performance. Our contributions, including\nopen-source tools and datasets, accelerate O-RAN and ML-driven network control\nresearch."
                },
                "authors": [
                    {
                        "name": "Joshua Groen"
                    },
                    {
                        "name": "Zixian Yang"
                    },
                    {
                        "name": "Divyadharshini Muruganandham"
                    },
                    {
                        "name": "Mauro Belgiovine"
                    },
                    {
                        "name": "Lei Ying"
                    },
                    {
                        "name": "Kaushik Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Chowdhury"
                },
                "author": "Kaushik Chowdhury",
                "arxiv_comment": "Journal version of TRACTOR: Traffic Analysis and Classification Tool\n  for Open RAN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.07896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08542v1",
                "updated": "2025-03-11T15:29:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    29,
                    55,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:29:55Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    29,
                    55,
                    1,
                    70,
                    0
                ],
                "title": "DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form\n  Question-Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form\n  Question-Answering"
                },
                "summary": "Evaluating Large Language Models (LLMs) free-form generated responses remains\na challenge due to their diverse and open-ended nature. Traditional supervised\nsignal-based automatic metrics fail to capture semantic equivalence or handle\nthe variability of open-ended responses, while human evaluation, though\nreliable, is resource-intensive. Leveraging LLMs as evaluators offers a\npromising alternative due to their strong language understanding and\ninstruction-following capabilities. Taking advantage of these capabilities, we\npropose the Dynamic Arbitration Framework for Evaluation (DAFE), which employs\ntwo primary LLM-as-judges and engages a third arbitrator only in cases of\ndisagreements. This selective arbitration prioritizes evaluation reliability\nwhile reducing unnecessary computational demands compared to conventional\nmajority voting. DAFE utilizes task-specific reference answers with dynamic\narbitration to enhance judgment accuracy, resulting in significant improvements\nin evaluation metrics such as Macro F1 and Cohen's Kappa. Through experiments,\nincluding a comprehensive human evaluation, we demonstrate DAFE's ability to\nprovide consistent, scalable, and resource-efficient assessments, establishing\nit as a robust framework for evaluating free-form model outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) free-form generated responses remains\na challenge due to their diverse and open-ended nature. Traditional supervised\nsignal-based automatic metrics fail to capture semantic equivalence or handle\nthe variability of open-ended responses, while human evaluation, though\nreliable, is resource-intensive. Leveraging LLMs as evaluators offers a\npromising alternative due to their strong language understanding and\ninstruction-following capabilities. Taking advantage of these capabilities, we\npropose the Dynamic Arbitration Framework for Evaluation (DAFE), which employs\ntwo primary LLM-as-judges and engages a third arbitrator only in cases of\ndisagreements. This selective arbitration prioritizes evaluation reliability\nwhile reducing unnecessary computational demands compared to conventional\nmajority voting. DAFE utilizes task-specific reference answers with dynamic\narbitration to enhance judgment accuracy, resulting in significant improvements\nin evaluation metrics such as Macro F1 and Cohen's Kappa. Through experiments,\nincluding a comprehensive human evaluation, we demonstrate DAFE's ability to\nprovide consistent, scalable, and resource-efficient assessments, establishing\nit as a robust framework for evaluating free-form model outputs."
                },
                "authors": [
                    {
                        "name": "Sher Badshah"
                    },
                    {
                        "name": "Hassan Sajjad"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sajjad"
                },
                "author": "Hassan Sajjad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08540v1",
                "updated": "2025-03-11T15:29:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    29,
                    0,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:29:00Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    29,
                    0,
                    1,
                    70,
                    0
                ],
                "title": "Mellow: a small audio language model for reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mellow: a small audio language model for reasoning"
                },
                "summary": "Multimodal Audio-Language Models (ALMs) can understand and reason over both\naudio and text. Typically, reasoning performance correlates with model size,\nwith the best results achieved by models exceeding 8 billion parameters.\nHowever, no prior work has explored enabling small audio-language models to\nperform reasoning tasks, despite the potential applications for edge devices.\nTo address this gap, we introduce Mellow, a small Audio-Language Model\nspecifically designed for reasoning. Mellow achieves state-of-the-art\nperformance among existing small audio-language models and surpasses several\nlarger models in reasoning capabilities. For instance, Mellow scores 52.11 on\nMMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times\nfewer parameters and being trained on 60 times less data (audio hrs). To train\nMellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded\nreasoning in models. It consists of a mixture of existing datasets (30% of the\ndata) and synthetically generated data (70%). The synthetic dataset is derived\nfrom audio captioning datasets, where Large Language Models (LLMs) generate\ndetailed and multiple-choice questions focusing on audio events, objects,\nacoustic scenes, signal properties, semantics, and listener emotions. To\nevaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks,\nassessing on both in-distribution and out-of-distribution data, including audio\nunderstanding, deductive reasoning, and comparative reasoning. Finally, we\nconduct extensive ablation studies to explore the impact of projection layer\nchoices, synthetic data generation methods, and language model pretraining on\nreasoning performance. Our training dataset, findings, and baseline pave the\nway for developing small ALMs capable of reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Audio-Language Models (ALMs) can understand and reason over both\naudio and text. Typically, reasoning performance correlates with model size,\nwith the best results achieved by models exceeding 8 billion parameters.\nHowever, no prior work has explored enabling small audio-language models to\nperform reasoning tasks, despite the potential applications for edge devices.\nTo address this gap, we introduce Mellow, a small Audio-Language Model\nspecifically designed for reasoning. Mellow achieves state-of-the-art\nperformance among existing small audio-language models and surpasses several\nlarger models in reasoning capabilities. For instance, Mellow scores 52.11 on\nMMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times\nfewer parameters and being trained on 60 times less data (audio hrs). To train\nMellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded\nreasoning in models. It consists of a mixture of existing datasets (30% of the\ndata) and synthetically generated data (70%). The synthetic dataset is derived\nfrom audio captioning datasets, where Large Language Models (LLMs) generate\ndetailed and multiple-choice questions focusing on audio events, objects,\nacoustic scenes, signal properties, semantics, and listener emotions. To\nevaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks,\nassessing on both in-distribution and out-of-distribution data, including audio\nunderstanding, deductive reasoning, and comparative reasoning. Finally, we\nconduct extensive ablation studies to explore the impact of projection layer\nchoices, synthetic data generation methods, and language model pretraining on\nreasoning performance. Our training dataset, findings, and baseline pave the\nway for developing small ALMs capable of reasoning."
                },
                "authors": [
                    {
                        "name": "Soham Deshmukh"
                    },
                    {
                        "name": "Satvik Dixit"
                    },
                    {
                        "name": "Rita Singh"
                    },
                    {
                        "name": "Bhiksha Raj"
                    }
                ],
                "author_detail": {
                    "name": "Bhiksha Raj"
                },
                "author": "Bhiksha Raj",
                "arxiv_comment": "Checkpoint and dataset available at:\n  https://github.com/soham97/mellow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08539v1",
                "updated": "2025-03-11T15:28:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    28,
                    9,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:28:09Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    28,
                    9,
                    1,
                    70,
                    0
                ],
                "title": "Desirable Unfamiliarity: Insights from Eye Movements on Engagement and\n  Readability of Dictation Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desirable Unfamiliarity: Insights from Eye Movements on Engagement and\n  Readability of Dictation Interfaces"
                },
                "summary": "Dictation interfaces support efficient text input, but the transcribed text\ncan be hard to read. To understand how users read and review dictated text, we\nconducted a controlled eye-tracking experiment with 20 participants to compare\nfive dictation interfaces: PLAIN (real-time transcription), AOC (periodic\ncorrections), RAKE (keyword highlights), GP-TSM (grammar-preserving\nhighlights), and SUMMARY (LLM-generated abstraction summary). The study\nanalyzed participants' gaze patterns during their speech composition and\nreviewing processes. The findings show that during composition, participants\nspent only 7--11% of their time actively reading, and they favored real-time\nfeedback and avoided distracting interface changes. During reviewing, although\nSUMMARY introduced unfamiliar words (requiring longer and more frequent\nfixation), they were easier to read (requiring fewer regressions). Participants\npreferred SUMMARY for the polished text that preserved fidelity to original\nmeanings. RAKE guided the reading of self-produced text better than GP-TSM.\nThese findings provide new ways to rethink the design of dictation interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dictation interfaces support efficient text input, but the transcribed text\ncan be hard to read. To understand how users read and review dictated text, we\nconducted a controlled eye-tracking experiment with 20 participants to compare\nfive dictation interfaces: PLAIN (real-time transcription), AOC (periodic\ncorrections), RAKE (keyword highlights), GP-TSM (grammar-preserving\nhighlights), and SUMMARY (LLM-generated abstraction summary). The study\nanalyzed participants' gaze patterns during their speech composition and\nreviewing processes. The findings show that during composition, participants\nspent only 7--11% of their time actively reading, and they favored real-time\nfeedback and avoided distracting interface changes. During reviewing, although\nSUMMARY introduced unfamiliar words (requiring longer and more frequent\nfixation), they were easier to read (requiring fewer regressions). Participants\npreferred SUMMARY for the polished text that preserved fidelity to original\nmeanings. RAKE guided the reading of self-produced text better than GP-TSM.\nThese findings provide new ways to rethink the design of dictation interfaces."
                },
                "authors": [
                    {
                        "name": "Zhaohui Liang"
                    },
                    {
                        "name": "Yonglin Chen"
                    },
                    {
                        "name": "Naser Al Madi"
                    },
                    {
                        "name": "Can Liu"
                    }
                ],
                "author_detail": {
                    "name": "Can Liu"
                },
                "author": "Can Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08537v1",
                "updated": "2025-03-11T15:27:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    27,
                    17,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:27:17Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    27,
                    17,
                    1,
                    70,
                    0
                ],
                "title": "Chemical reasoning in LLMs unlocks steerable synthesis planning and\n  reaction mechanism elucidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical reasoning in LLMs unlocks steerable synthesis planning and\n  reaction mechanism elucidation"
                },
                "summary": "While machine learning algorithms have been shown to excel at specific\nchemical tasks, they have struggled to capture the strategic thinking that\ncharacterizes expert chemical reasoning, limiting their widespread adoption.\nHere we demonstrate that large language models (LLMs) can serve as powerful\nchemical reasoning engines when integrated with traditional search algorithms,\nenabling a new approach to computer-aided chemistry that mirrors human expert\nthinking. Rather than using LLMs to directly manipulate chemical structures, we\nleverage their ability to evaluate chemical strategies and guide search\nalgorithms toward chemically meaningful solutions. We demonstrate this paradigm\nthrough two fundamental challenges: strategy-aware retrosynthetic planning and\nmechanism elucidation. In retrosynthetic planning, our method allows chemists\nto specify desired synthetic strategies in natural language to find routes that\nsatisfy these constraints in vast searches. In mechanism elucidation, LLMs\nguide the search for plausible reaction mechanisms by combining chemical\nprinciples with systematic exploration. Our approach shows strong performance\nacross diverse chemical tasks, with larger models demonstrating increasingly\nsophisticated chemical reasoning. Our approach establishes a new paradigm for\ncomputer-aided chemistry that combines the strategic understanding of LLMs with\nthe precision of traditional chemical tools, opening possibilities for more\nintuitive and powerful chemical reasoning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While machine learning algorithms have been shown to excel at specific\nchemical tasks, they have struggled to capture the strategic thinking that\ncharacterizes expert chemical reasoning, limiting their widespread adoption.\nHere we demonstrate that large language models (LLMs) can serve as powerful\nchemical reasoning engines when integrated with traditional search algorithms,\nenabling a new approach to computer-aided chemistry that mirrors human expert\nthinking. Rather than using LLMs to directly manipulate chemical structures, we\nleverage their ability to evaluate chemical strategies and guide search\nalgorithms toward chemically meaningful solutions. We demonstrate this paradigm\nthrough two fundamental challenges: strategy-aware retrosynthetic planning and\nmechanism elucidation. In retrosynthetic planning, our method allows chemists\nto specify desired synthetic strategies in natural language to find routes that\nsatisfy these constraints in vast searches. In mechanism elucidation, LLMs\nguide the search for plausible reaction mechanisms by combining chemical\nprinciples with systematic exploration. Our approach shows strong performance\nacross diverse chemical tasks, with larger models demonstrating increasingly\nsophisticated chemical reasoning. Our approach establishes a new paradigm for\ncomputer-aided chemistry that combines the strategic understanding of LLMs with\nthe precision of traditional chemical tools, opening possibilities for more\nintuitive and powerful chemical reasoning systems."
                },
                "authors": [
                    {
                        "name": "Andres M Bran"
                    },
                    {
                        "name": "Theo A Neukomm"
                    },
                    {
                        "name": "Daniel P Armstrong"
                    },
                    {
                        "name": "Zlatko Jončev"
                    },
                    {
                        "name": "Philippe Schwaller"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Schwaller"
                },
                "author": "Philippe Schwaller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05628v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05628v5",
                "updated": "2025-03-12T05:54:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    54,
                    44,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-08T02:23:53Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    2,
                    23,
                    53,
                    1,
                    282,
                    0
                ],
                "title": "A Unified Framework for Motion Reasoning and Generation in Human\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Motion Reasoning and Generation in Human\n  Interaction"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural and contextually relevant text,\nenabling more human-like AI interactions. However, generating and understanding\ninteractive human-like motion, where multiple individuals engage in coordinated\nmovements, remains challenging due to the complexity of modeling these\ninteractions. Additionally, a unified and versatile model is needed to handle\ndiverse interactive scenarios, such as chat systems that dynamically adapt to\nuser instructions and assigned roles. To address these challenges, we introduce\nVIM, the Versatile Interactive Motion-language model, which integrates both\nlanguage and motion modalities to effectively understand, generate, and control\ninteractive motions in multi-turn conversational contexts. Unlike previous\nstudies that primarily focus on uni-directional tasks such as text-to-motion or\nmotion-to-text, VIM employs a unified architecture capable of simultaneously\nunderstanding and generating both motion and text modalities. Given the absence\nof an appropriate dataset to support this task, we introduce Inter-MT2, a\nlarge-scale instruction-tuning dataset containing 82.7K multi-turn interactive\nmotion instructions, covering 153K interactive motion samples. Inter-MT2 spans\ndiverse instructional scenarios, including motion editing, question answering,\nand story generation, leveraging off-the-shelf large language models and motion\ndiffusion models to construct a broad set of interactive motion instructions.\nWe extensively evaluate the versatility of VIM across multiple interactive\nmotion-related tasks, including motion-to-text, text-to-motion, reaction\ngeneration, motion editing, and reasoning about motion sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural and contextually relevant text,\nenabling more human-like AI interactions. However, generating and understanding\ninteractive human-like motion, where multiple individuals engage in coordinated\nmovements, remains challenging due to the complexity of modeling these\ninteractions. Additionally, a unified and versatile model is needed to handle\ndiverse interactive scenarios, such as chat systems that dynamically adapt to\nuser instructions and assigned roles. To address these challenges, we introduce\nVIM, the Versatile Interactive Motion-language model, which integrates both\nlanguage and motion modalities to effectively understand, generate, and control\ninteractive motions in multi-turn conversational contexts. Unlike previous\nstudies that primarily focus on uni-directional tasks such as text-to-motion or\nmotion-to-text, VIM employs a unified architecture capable of simultaneously\nunderstanding and generating both motion and text modalities. Given the absence\nof an appropriate dataset to support this task, we introduce Inter-MT2, a\nlarge-scale instruction-tuning dataset containing 82.7K multi-turn interactive\nmotion instructions, covering 153K interactive motion samples. Inter-MT2 spans\ndiverse instructional scenarios, including motion editing, question answering,\nand story generation, leveraging off-the-shelf large language models and motion\ndiffusion models to construct a broad set of interactive motion instructions.\nWe extensively evaluate the versatility of VIM across multiple interactive\nmotion-related tasks, including motion-to-text, text-to-motion, reaction\ngeneration, motion editing, and reasoning about motion sequences."
                },
                "authors": [
                    {
                        "name": "Jeongeun Park"
                    },
                    {
                        "name": "Sungjoon Choi"
                    },
                    {
                        "name": "Sangdoo Yun"
                    }
                ],
                "author_detail": {
                    "name": "Sangdoo Yun"
                },
                "author": "Sangdoo Yun",
                "arxiv_comment": "https://vim-motion-language.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05628v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05628v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08525v1",
                "updated": "2025-03-11T15:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:17:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training"
                },
                "summary": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively\nscaled up chain-of-thought (CoT) reasoning in large language models (LLMs).\nYet, its efficacy in training vision-language model (VLM) agents for\ngoal-directed action reasoning in visual environments is less established. This\nwork investigates this problem through extensive experiments on complex card\ngames, such as 24 points, and embodied tasks from ALFWorld. We find that when\nrewards are based solely on action outcomes, RL fails to incentivize CoT\nreasoning in VLMs, instead leading to a phenomenon we termed thought collapse,\ncharacterized by a rapid loss of diversity in the agent's thoughts,\nstate-irrelevant and incomplete reasoning, and subsequent invalid actions,\nresulting in negative rewards. To counteract thought collapse, we highlight the\nnecessity of process guidance and propose an automated corrector that evaluates\nand refines the agent's reasoning at each RL step. This simple and scalable GTR\n(Guided Thought Reinforcement) framework trains reasoning and action\nsimultaneously without the need for dense, per-step human labeling. Our\nexperiments demonstrate that GTR significantly enhances the performance and\ngeneralization of the LLaVA-7b model across various visual environments,\nachieving 3-5 times higher task success rates compared to SoTA models with\nnotably smaller model sizes."
                },
                "authors": [
                    {
                        "name": "Tong Wei"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Junliang Xing"
                    },
                    {
                        "name": "Yuanchun Shi"
                    },
                    {
                        "name": "Zongqing Lu"
                    },
                    {
                        "name": "Deheng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Deheng Ye"
                },
                "author": "Deheng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08524v1",
                "updated": "2025-03-11T15:15:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    15,
                    54,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T15:15:54Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    15,
                    54,
                    1,
                    70,
                    0
                ],
                "title": "Position-Aware Depth Decay Decoding ($D^3$): Boosting Large Language\n  Model Inference Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position-Aware Depth Decay Decoding ($D^3$): Boosting Large Language\n  Model Inference Efficiency"
                },
                "summary": "Due to the large number of parameters, the inference phase of Large Language\nModels (LLMs) is resource-intensive. Unlike traditional model compression,\nwhich needs retraining, recent dynamic computation methods show that not all\ncomponents are required for inference, enabling a training-free pipeline. In\nthis paper, we focus on the dynamic depth of LLM generation. A token-position\naware layer skipping framework is proposed to save 1.5x times operations\nefficiently while maintaining performance. We first observed that tokens\npredicted later have lower perplexity and thus require less computation. Then,\nwe propose a training-free algorithm called Position-Aware Depth Decay Decoding\n($D^3$), which leverages a power-law decay function, $\\left\\lfloor L \\times\n(\\alpha^i) \\right\\rfloor$, to determine the number of layers to retain when\ngenerating token $T_i$. Remarkably, without any retraining, the $D^3$ achieves\nsuccess across a wide range of generation tasks for the first time. Experiments\non large language models (\\ie the Llama) with $7 \\sim 70$ billion parameters\nshow that $D^3$ can achieve an average 1.5x speedup compared with the\nfull-inference pipeline while maintaining comparable performance with nearly no\nperformance drop ($<1\\%$) on the GSM8K and BBH benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the large number of parameters, the inference phase of Large Language\nModels (LLMs) is resource-intensive. Unlike traditional model compression,\nwhich needs retraining, recent dynamic computation methods show that not all\ncomponents are required for inference, enabling a training-free pipeline. In\nthis paper, we focus on the dynamic depth of LLM generation. A token-position\naware layer skipping framework is proposed to save 1.5x times operations\nefficiently while maintaining performance. We first observed that tokens\npredicted later have lower perplexity and thus require less computation. Then,\nwe propose a training-free algorithm called Position-Aware Depth Decay Decoding\n($D^3$), which leverages a power-law decay function, $\\left\\lfloor L \\times\n(\\alpha^i) \\right\\rfloor$, to determine the number of layers to retain when\ngenerating token $T_i$. Remarkably, without any retraining, the $D^3$ achieves\nsuccess across a wide range of generation tasks for the first time. Experiments\non large language models (\\ie the Llama) with $7 \\sim 70$ billion parameters\nshow that $D^3$ can achieve an average 1.5x speedup compared with the\nfull-inference pipeline while maintaining comparable performance with nearly no\nperformance drop ($<1\\%$) on the GSM8K and BBH benchmarks."
                },
                "authors": [
                    {
                        "name": "Siqi Fan"
                    },
                    {
                        "name": "Xuezhi Fang"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Peng Han"
                    },
                    {
                        "name": "Shuo Shang"
                    },
                    {
                        "name": "Yequan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yequan Wang"
                },
                "author": "Yequan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15488v2",
                "updated": "2025-03-11T15:05:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    5,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-21T14:26:23Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    14,
                    26,
                    23,
                    4,
                    52,
                    0
                ],
                "title": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D\n  Object Detection"
                },
                "summary": "Camera-based multi-view 3D detection has emerged as an attractive solution\nfor autonomous driving due to its low cost and broad applicability. However,\ndespite the strong performance of PETR-based methods in 3D perception\nbenchmarks, their direct INT8 quantization for onboard deployment leads to\ndrastic accuracy drops-up to 58.2% in mAP and 36.9% in NDS on the NuScenes\ndataset. In this work, we propose Q-PETR, a quantization-aware position\nembedding transformation that re-engineers key components of the PETR framework\nto reconcile the discrepancy between the dynamic ranges of positional encodings\nand image features, and to adapt the cross-attention mechanism for low-bit\ninference. By redesigning the positional encoding module and introducing an\nadaptive quantization strategy, Q-PETR maintains floating-point performance\nwith a performance degradation of less than 1% under standard 8-bit per-tensor\npost-training quantization. Moreover, compared to its FP32 counterpart, Q-PETR\nachieves a two-fold speedup and reduces memory usage by three times, thereby\noffering a deployment-friendly solution for resource-constrained onboard\ndevices. Extensive experiments across various PETR-series models validate the\nstrong generalization and practical benefits of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera-based multi-view 3D detection has emerged as an attractive solution\nfor autonomous driving due to its low cost and broad applicability. However,\ndespite the strong performance of PETR-based methods in 3D perception\nbenchmarks, their direct INT8 quantization for onboard deployment leads to\ndrastic accuracy drops-up to 58.2% in mAP and 36.9% in NDS on the NuScenes\ndataset. In this work, we propose Q-PETR, a quantization-aware position\nembedding transformation that re-engineers key components of the PETR framework\nto reconcile the discrepancy between the dynamic ranges of positional encodings\nand image features, and to adapt the cross-attention mechanism for low-bit\ninference. By redesigning the positional encoding module and introducing an\nadaptive quantization strategy, Q-PETR maintains floating-point performance\nwith a performance degradation of less than 1% under standard 8-bit per-tensor\npost-training quantization. Moreover, compared to its FP32 counterpart, Q-PETR\nachieves a two-fold speedup and reduces memory usage by three times, thereby\noffering a deployment-friendly solution for resource-constrained onboard\ndevices. Extensive experiments across various PETR-series models validate the\nstrong generalization and practical benefits of our approach."
                },
                "authors": [
                    {
                        "name": "Jiangyong Yu"
                    },
                    {
                        "name": "Changyong Shu"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Sifan Zhou"
                    },
                    {
                        "name": "Zichen Yu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08508v1",
                "updated": "2025-03-11T14:57:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    57,
                    53,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:57:53Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    57,
                    53,
                    1,
                    70,
                    0
                ],
                "title": "LightPlanner: Unleashing the Reasoning Capabilities of Lightweight Large\n  Language Models in Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightPlanner: Unleashing the Reasoning Capabilities of Lightweight Large\n  Language Models in Task Planning"
                },
                "summary": "In recent years, lightweight large language models (LLMs) have garnered\nsignificant attention in the robotics field due to their low computational\nresource requirements and suitability for edge deployment. However, in task\nplanning -- particularly for complex tasks that involve dynamic semantic logic\nreasoning -- lightweight LLMs have underperformed. To address this limitation,\nwe propose a novel task planner, LightPlanner, which enhances the performance\nof lightweight LLMs in complex task planning by fully leveraging their\nreasoning capabilities. Unlike conventional planners that use fixed skill\ntemplates, LightPlanner controls robot actions via parameterized function\ncalls, dynamically generating parameter values. This approach allows for\nfine-grained skill control and improves task planning success rates in complex\nscenarios. Furthermore, we introduce hierarchical deep reasoning. Before\ngenerating each action decision step, LightPlanner thoroughly considers three\nlevels: action execution (feedback verification), semantic parsing (goal\nconsistency verification), and parameter generation (parameter validity\nverification). This ensures the correctness of subsequent action controls.\nAdditionally, we incorporate a memory module to store historical actions,\nthereby reducing context length and enhancing planning efficiency for long-term\ntasks. We train the LightPlanner-1.5B model on our LightPlan-40k dataset, which\ncomprises 40,000 action controls across tasks with 2 to 13 action steps.\nExperiments demonstrate that our model achieves the highest task success rate\ndespite having the smallest number of parameters. In tasks involving spatial\nsemantic reasoning, the success rate exceeds that of ReAct by 14.9 percent.\nMoreover, we demonstrate LightPlanner's potential to operate on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, lightweight large language models (LLMs) have garnered\nsignificant attention in the robotics field due to their low computational\nresource requirements and suitability for edge deployment. However, in task\nplanning -- particularly for complex tasks that involve dynamic semantic logic\nreasoning -- lightweight LLMs have underperformed. To address this limitation,\nwe propose a novel task planner, LightPlanner, which enhances the performance\nof lightweight LLMs in complex task planning by fully leveraging their\nreasoning capabilities. Unlike conventional planners that use fixed skill\ntemplates, LightPlanner controls robot actions via parameterized function\ncalls, dynamically generating parameter values. This approach allows for\nfine-grained skill control and improves task planning success rates in complex\nscenarios. Furthermore, we introduce hierarchical deep reasoning. Before\ngenerating each action decision step, LightPlanner thoroughly considers three\nlevels: action execution (feedback verification), semantic parsing (goal\nconsistency verification), and parameter generation (parameter validity\nverification). This ensures the correctness of subsequent action controls.\nAdditionally, we incorporate a memory module to store historical actions,\nthereby reducing context length and enhancing planning efficiency for long-term\ntasks. We train the LightPlanner-1.5B model on our LightPlan-40k dataset, which\ncomprises 40,000 action controls across tasks with 2 to 13 action steps.\nExperiments demonstrate that our model achieves the highest task success rate\ndespite having the smallest number of parameters. In tasks involving spatial\nsemantic reasoning, the success rate exceeds that of ReAct by 14.9 percent.\nMoreover, we demonstrate LightPlanner's potential to operate on edge devices."
                },
                "authors": [
                    {
                        "name": "Weijie Zhou"
                    },
                    {
                        "name": "Yi Peng"
                    },
                    {
                        "name": "Manli Tao"
                    },
                    {
                        "name": "Chaoyang Zhao"
                    },
                    {
                        "name": "Honghui Dong"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "arxiv_affiliation": "objecteye.Inc",
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08506v1",
                "updated": "2025-03-11T14:56:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    56,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:56:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    56,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper\n  Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper\n  Reviews"
                },
                "summary": "Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08495v1",
                "updated": "2025-03-11T14:47:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    47,
                    24,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:47:24Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    47,
                    24,
                    1,
                    70,
                    0
                ],
                "title": "Enhancing Multi-Hop Fact Verification with Structured\n  Knowledge-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Hop Fact Verification with Structured\n  Knowledge-Augmented Large Language Models"
                },
                "summary": "The rapid development of social platforms exacerbates the dissemination of\nmisinformation, which stimulates the research in fact verification. Recent\nstudies tend to leverage semantic features to solve this problem as a\nsingle-hop task. However, the process of verifying a claim requires several\npieces of evidence with complicated inner logic and relations to verify the\ngiven claim in real-world situations. Recent studies attempt to improve both\nunderstanding and reasoning abilities to enhance the performance, but they\noverlook the crucial relations between entities that benefit models to\nunderstand better and facilitate the prediction. To emphasize the significance\nof relations, we resort to Large Language Models (LLMs) considering their\nexcellent understanding ability. Instead of other methods using LLMs as the\npredictor, we take them as relation extractors, for they do better in\nunderstanding rather than reasoning according to the experimental results.\nThus, to solve the challenges above, we propose a novel Structured\nKnowledge-Augmented LLM-based Network (LLM-SKAN) for multi-hop fact\nverification. Specifically, we utilize an LLM-driven Knowledge Extractor to\ncapture fine-grained information, including entities and their complicated\nrelations. Besides, we leverage a Knowledge-Augmented Relation Graph Fusion\nmodule to interact with each node and learn better claim-evidence\nrepresentations comprehensively. The experimental results on four common-used\ndatasets demonstrate the effectiveness and superiority of our model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of social platforms exacerbates the dissemination of\nmisinformation, which stimulates the research in fact verification. Recent\nstudies tend to leverage semantic features to solve this problem as a\nsingle-hop task. However, the process of verifying a claim requires several\npieces of evidence with complicated inner logic and relations to verify the\ngiven claim in real-world situations. Recent studies attempt to improve both\nunderstanding and reasoning abilities to enhance the performance, but they\noverlook the crucial relations between entities that benefit models to\nunderstand better and facilitate the prediction. To emphasize the significance\nof relations, we resort to Large Language Models (LLMs) considering their\nexcellent understanding ability. Instead of other methods using LLMs as the\npredictor, we take them as relation extractors, for they do better in\nunderstanding rather than reasoning according to the experimental results.\nThus, to solve the challenges above, we propose a novel Structured\nKnowledge-Augmented LLM-based Network (LLM-SKAN) for multi-hop fact\nverification. Specifically, we utilize an LLM-driven Knowledge Extractor to\ncapture fine-grained information, including entities and their complicated\nrelations. Besides, we leverage a Knowledge-Augmented Relation Graph Fusion\nmodule to interact with each node and learn better claim-evidence\nrepresentations comprehensively. The experimental results on four common-used\ndatasets demonstrate the effectiveness and superiority of our model."
                },
                "authors": [
                    {
                        "name": "Han Cao"
                    },
                    {
                        "name": "Lingwei Wei"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04479v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04479v3",
                "updated": "2025-03-11T14:28:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    28,
                    13,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-06T14:29:52Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    14,
                    29,
                    52,
                    3,
                    65,
                    0
                ],
                "title": "ToolFuzz -- Automated Agent Tool Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolFuzz -- Automated Agent Tool Testing"
                },
                "summary": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents."
                },
                "authors": [
                    {
                        "name": "Ivan Milev"
                    },
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04479v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04479v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13514v2",
                "updated": "2025-03-11T14:22:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    22,
                    17,
                    1,
                    70,
                    0
                ],
                "published": "2024-10-17T13:02:06Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    13,
                    2,
                    6,
                    3,
                    291,
                    0
                ],
                "title": "GraphSCENE: On-Demand Critical Scenario Generation for Autonomous\n  Vehicles in Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSCENE: On-Demand Critical Scenario Generation for Autonomous\n  Vehicles in Simulation"
                },
                "summary": "Testing and validating Autonomous Vehicle (AV) performance in safety-critical\nand diverse scenarios is crucial before real-world deployment. However,\nmanually creating such scenarios in simulation remains a significant and\ntime-consuming challenge. This work introduces a novel method that generates\ndynamic temporal scene graphs corresponding to diverse traffic scenarios,\non-demand, tailored to user-defined preferences, such as AV actions, sets of\ndynamic agents, and criticality levels. A temporal Graph Neural Network (GNN)\nmodel learns to predict relationships between ego-vehicle, agents, and static\nstructures, guided by real-world spatiotemporal interaction patterns and\nconstrained by an ontology that restricts predictions to semantically valid\nlinks. Our model consistently outperforms the baselines in accurately\ngenerating links corresponding to the requested scenarios. We render the\npredicted scenarios in simulation to further demonstrate their effectiveness as\ntesting environments for AV agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing and validating Autonomous Vehicle (AV) performance in safety-critical\nand diverse scenarios is crucial before real-world deployment. However,\nmanually creating such scenarios in simulation remains a significant and\ntime-consuming challenge. This work introduces a novel method that generates\ndynamic temporal scene graphs corresponding to diverse traffic scenarios,\non-demand, tailored to user-defined preferences, such as AV actions, sets of\ndynamic agents, and criticality levels. A temporal Graph Neural Network (GNN)\nmodel learns to predict relationships between ego-vehicle, agents, and static\nstructures, guided by real-world spatiotemporal interaction patterns and\nconstrained by an ontology that restricts predictions to semantically valid\nlinks. Our model consistently outperforms the baselines in accurately\ngenerating links corresponding to the requested scenarios. We render the\npredicted scenarios in simulation to further demonstrate their effectiveness as\ntesting environments for AV agents."
                },
                "authors": [
                    {
                        "name": "Efimia Panagiotaki"
                    },
                    {
                        "name": "Georgi Pramatarov"
                    },
                    {
                        "name": "Lars Kunze"
                    },
                    {
                        "name": "Daniele De Martini"
                    }
                ],
                "author_detail": {
                    "name": "Daniele De Martini"
                },
                "author": "Daniele De Martini",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18363v3",
                "updated": "2025-03-11T14:19:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    19,
                    42,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-27T14:11:10Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    11,
                    10,
                    2,
                    332,
                    0
                ],
                "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding"
                },
                "summary": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After a three-stage training\napproach, ChatRex demonstrates strong perception and understanding performance,\nand the combination of these two capabilities also unlocks many attractive\napplications, demonstrating their complementary roles in MLLM. Code is\navailable at https://github.com/IDEA-Research/ChatRex.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After a three-stage training\napproach, ChatRex demonstrates strong perception and understanding performance,\nand the combination of these two capabilities also unlocks many attractive\napplications, demonstrating their complementary roles in MLLM. Code is\navailable at https://github.com/IDEA-Research/ChatRex."
                },
                "authors": [
                    {
                        "name": "Qing Jiang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Yuqin Yang"
                    },
                    {
                        "name": "Yuda Xiong"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Zhaoyang Zeng"
                    },
                    {
                        "name": "Tianhe Ren"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "35 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05760v2",
                "updated": "2025-03-11T14:04:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    4,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-23T18:47:14Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    18,
                    47,
                    14,
                    6,
                    54,
                    0
                ],
                "title": "The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own"
                },
                "summary": "This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps://gradegpt.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps://gradegpt.github.io."
                },
                "authors": [
                    {
                        "name": "Gokul Puthumanaillam"
                    },
                    {
                        "name": "Melkior Ornik"
                    }
                ],
                "author_detail": {
                    "name": "Melkior Ornik"
                },
                "author": "Melkior Ornik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08452v1",
                "updated": "2025-03-11T14:01:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    1,
                    3,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:01:03Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    1,
                    3,
                    1,
                    70,
                    0
                ],
                "title": "KAP: MLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese\n  Non-Narrative Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAP: MLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese\n  Non-Narrative Documents"
                },
                "summary": "We propose Knowledge-Aware Preprocessing (KAP), a two-stage preprocessing\nframework tailored for Traditional Chinese non-narrative documents, designed to\nenhance retrieval accuracy in Hybrid Retrieval systems. Hybrid Retrieval, which\nintegrates Sparse Retrieval (e.g., BM25) and Dense Retrieval (e.g., vector\nembeddings), has become a widely adopted approach for improving search\neffectiveness. However, its performance heavily depends on the quality of input\ntext, which is often degraded when dealing with non-narrative documents such as\nPDFs containing financial statements, contractual clauses, and tables. KAP\naddresses these challenges by integrating Multimodal Large Language Models\n(MLLMs) with LLM-driven post-OCR processing, refining extracted text to reduce\nOCR noise, restore table structures, and optimize text format. By ensuring\nbetter compatibility with Hybrid Retrieval, KAP improves the accuracy of both\nSparse and Dense Retrieval methods without modifying the retrieval architecture\nitself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Knowledge-Aware Preprocessing (KAP), a two-stage preprocessing\nframework tailored for Traditional Chinese non-narrative documents, designed to\nenhance retrieval accuracy in Hybrid Retrieval systems. Hybrid Retrieval, which\nintegrates Sparse Retrieval (e.g., BM25) and Dense Retrieval (e.g., vector\nembeddings), has become a widely adopted approach for improving search\neffectiveness. However, its performance heavily depends on the quality of input\ntext, which is often degraded when dealing with non-narrative documents such as\nPDFs containing financial statements, contractual clauses, and tables. KAP\naddresses these challenges by integrating Multimodal Large Language Models\n(MLLMs) with LLM-driven post-OCR processing, refining extracted text to reduce\nOCR noise, restore table structures, and optimize text format. By ensuring\nbetter compatibility with Hybrid Retrieval, KAP improves the accuracy of both\nSparse and Dense Retrieval methods without modifying the retrieval architecture\nitself."
                },
                "authors": [
                    {
                        "name": "Hsin-Ling Hsu"
                    },
                    {
                        "name": "Ping-Sheng Lin"
                    },
                    {
                        "name": "Jing-Di Lin"
                    },
                    {
                        "name": "Jengnan Tzeng"
                    }
                ],
                "author_detail": {
                    "name": "Jengnan Tzeng"
                },
                "author": "Jengnan Tzeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08445v1",
                "updated": "2025-03-11T13:56:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    56,
                    26,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:56:26Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    56,
                    26,
                    1,
                    70,
                    0
                ],
                "title": "LLM-Pack: Intuitive Grocery Handling for Logistics Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Pack: Intuitive Grocery Handling for Logistics Applications"
                },
                "summary": "Robotics and automation are increasingly influential in logistics but remain\nlargely confined to traditional warehouses. In grocery retail, advancements\nsuch as cashier-less supermarkets exist, yet customers still manually pick and\npack groceries. While there has been a substantial focus in robotics on the bin\npicking problem, the task of packing objects and groceries has remained largely\nuntouched. However, packing grocery items in the right order is crucial for\npreventing product damage, e.g., heavy objects should not be placed on top of\nfragile ones. However, the exact criteria for the right packing order are hard\nto define, in particular given the huge variety of objects typically found in\nstores. In this paper, we introduce LLM-Pack, a novel approach for grocery\npacking. LLM-Pack leverages language and vision foundation models for\nidentifying groceries and generating a packing sequence that mimics human\npacking strategy. LLM-Pack does not require dedicated training to handle new\ngrocery items and its modularity allows easy upgrades of the underlying\nfoundation models. We extensively evaluate our approach to demonstrate its\nperformance. We will make the source code of LLMPack publicly available upon\nthe publication of this manuscript.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotics and automation are increasingly influential in logistics but remain\nlargely confined to traditional warehouses. In grocery retail, advancements\nsuch as cashier-less supermarkets exist, yet customers still manually pick and\npack groceries. While there has been a substantial focus in robotics on the bin\npicking problem, the task of packing objects and groceries has remained largely\nuntouched. However, packing grocery items in the right order is crucial for\npreventing product damage, e.g., heavy objects should not be placed on top of\nfragile ones. However, the exact criteria for the right packing order are hard\nto define, in particular given the huge variety of objects typically found in\nstores. In this paper, we introduce LLM-Pack, a novel approach for grocery\npacking. LLM-Pack leverages language and vision foundation models for\nidentifying groceries and generating a packing sequence that mimics human\npacking strategy. LLM-Pack does not require dedicated training to handle new\ngrocery items and its modularity allows easy upgrades of the underlying\nfoundation models. We extensively evaluate our approach to demonstrate its\nperformance. We will make the source code of LLMPack publicly available upon\nthe publication of this manuscript."
                },
                "authors": [
                    {
                        "name": "Yannik Blei"
                    },
                    {
                        "name": "Michael Krawez"
                    },
                    {
                        "name": "Tobias Jülg"
                    },
                    {
                        "name": "Pierre Krack"
                    },
                    {
                        "name": "Florian Walter"
                    },
                    {
                        "name": "Wolfram Burgard"
                    }
                ],
                "author_detail": {
                    "name": "Wolfram Burgard"
                },
                "author": "Wolfram Burgard",
                "arxiv_comment": "6 Pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19338v2",
                "updated": "2025-03-11T13:44:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    44,
                    27,
                    1,
                    70,
                    0
                ],
                "published": "2024-09-28T12:49:02Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    12,
                    49,
                    2,
                    5,
                    272,
                    0
                ],
                "title": "Decoding Echo Chambers: LLM-Powered Simulations Revealing Polarization\n  in Social Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Echo Chambers: LLM-Powered Simulations Revealing Polarization\n  in Social Networks"
                },
                "summary": "The impact of social media on critical issues such as echo chambers needs to\nbe addressed, as these phenomena can have disruptive consequences for our\nsociety. Traditional research often oversimplifies emotional tendencies and\nopinion evolution into numbers and formulas, neglecting that news and\ncommunication are conveyed through text, which limits these approaches. Hence,\nin this work, we propose an LLM-based simulation for the social opinion network\nto evaluate and counter polarization phenomena. We first construct three\ntypical network structures to simulate different characteristics of social\ninteractions. Then, agents interact based on recommendation algorithms and\nupdate their strategies through reasoning and analysis. By comparing these\ninteractions with the classic Bounded Confidence Model (BCM), the Friedkin\nJohnsen (FJ) model, and using echo chamber-related indices, we demonstrate the\neffectiveness of our framework in simulating opinion dynamics and reproducing\nphenomena such as opinion polarization and echo chambers. We propose two\nmitigation methods, active and passive nudges, that can help reduce echo\nchambers, specifically within language-based simulations. We hope our work will\noffer valuable insights and guidance for social polarization mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of social media on critical issues such as echo chambers needs to\nbe addressed, as these phenomena can have disruptive consequences for our\nsociety. Traditional research often oversimplifies emotional tendencies and\nopinion evolution into numbers and formulas, neglecting that news and\ncommunication are conveyed through text, which limits these approaches. Hence,\nin this work, we propose an LLM-based simulation for the social opinion network\nto evaluate and counter polarization phenomena. We first construct three\ntypical network structures to simulate different characteristics of social\ninteractions. Then, agents interact based on recommendation algorithms and\nupdate their strategies through reasoning and analysis. By comparing these\ninteractions with the classic Bounded Confidence Model (BCM), the Friedkin\nJohnsen (FJ) model, and using echo chamber-related indices, we demonstrate the\neffectiveness of our framework in simulating opinion dynamics and reproducing\nphenomena such as opinion polarization and echo chambers. We propose two\nmitigation methods, active and passive nudges, that can help reduce echo\nchambers, specifically within language-based simulations. We hope our work will\noffer valuable insights and guidance for social polarization mitigation."
                },
                "authors": [
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Zongfang Liu"
                    },
                    {
                        "name": "Dequan Yang"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08415v1",
                "updated": "2025-03-11T13:24:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    24,
                    39,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:24:39Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    24,
                    39,
                    1,
                    70,
                    0
                ],
                "title": "TokenSim: Enabling Hardware and Software Exploration for Large Language\n  Model Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSim: Enabling Hardware and Software Exploration for Large Language\n  Model Inference Systems"
                },
                "summary": "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Guoyang Duan"
                    },
                    {
                        "name": "Tianle Xu"
                    },
                    {
                        "name": "Junchi Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Yongqiang Yao"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "arxiv_comment": "9 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08404v1",
                "updated": "2025-03-11T13:06:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    6,
                    40,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:06:40Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    6,
                    40,
                    1,
                    70,
                    0
                ],
                "title": "Fact-checking with Generative AI: A Systematic Cross-Topic Examination\n  of LLMs Capacity to Detect Veracity of Political Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-checking with Generative AI: A Systematic Cross-Topic Examination\n  of LLMs Capacity to Detect Veracity of Political Information"
                },
                "summary": "The purpose of this study is to assess how large language models (LLMs) can\nbe used for fact-checking and contribute to the broader debate on the use of\nautomated means for veracity identification. To achieve this purpose, we use AI\nauditing methodology that systematically evaluates performance of five LLMs\n(ChatGPT 4, Llama 3 (70B), Llama 3.1 (405B), Claude 3.5 Sonnet, and Google\nGemini) using prompts regarding a large set of statements fact-checked by\nprofessional journalists (16,513). Specifically, we use topic modeling and\nregression analysis to investigate which factors (e.g. topic of the prompt or\nthe LLM type) affect evaluations of true, false, and mixed statements. Our\nfindings reveal that while ChatGPT 4 and Google Gemini achieved higher accuracy\nthan other models, overall performance across models remains modest. Notably,\nthe results indicate that models are better at identifying false statements,\nespecially on sensitive topics such as COVID-19, American political\ncontroversies, and social issues, suggesting possible guardrails that may\nenhance accuracy on these topics. The major implication of our findings is that\nthere are significant challenges for using LLMs for factchecking, including\nsignificant variation in performance across different LLMs and unequal quality\nof outputs for specific topics which can be attributed to deficits of training\ndata. Our research highlights the potential and limitations of LLMs in\npolitical fact-checking, suggesting potential avenues for further improvements\nin guardrails as well as fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The purpose of this study is to assess how large language models (LLMs) can\nbe used for fact-checking and contribute to the broader debate on the use of\nautomated means for veracity identification. To achieve this purpose, we use AI\nauditing methodology that systematically evaluates performance of five LLMs\n(ChatGPT 4, Llama 3 (70B), Llama 3.1 (405B), Claude 3.5 Sonnet, and Google\nGemini) using prompts regarding a large set of statements fact-checked by\nprofessional journalists (16,513). Specifically, we use topic modeling and\nregression analysis to investigate which factors (e.g. topic of the prompt or\nthe LLM type) affect evaluations of true, false, and mixed statements. Our\nfindings reveal that while ChatGPT 4 and Google Gemini achieved higher accuracy\nthan other models, overall performance across models remains modest. Notably,\nthe results indicate that models are better at identifying false statements,\nespecially on sensitive topics such as COVID-19, American political\ncontroversies, and social issues, suggesting possible guardrails that may\nenhance accuracy on these topics. The major implication of our findings is that\nthere are significant challenges for using LLMs for factchecking, including\nsignificant variation in performance across different LLMs and unequal quality\nof outputs for specific topics which can be attributed to deficits of training\ndata. Our research highlights the potential and limitations of LLMs in\npolitical fact-checking, suggesting potential avenues for further improvements\nin guardrails as well as fine-tuning."
                },
                "authors": [
                    {
                        "name": "Elizaveta Kuznetsova"
                    },
                    {
                        "name": "Ilaria Vitulano"
                    },
                    {
                        "name": "Mykola Makhortykh"
                    },
                    {
                        "name": "Martha Stolze"
                    },
                    {
                        "name": "Tomas Nagy"
                    },
                    {
                        "name": "Victoria Vziatysheva"
                    }
                ],
                "author_detail": {
                    "name": "Victoria Vziatysheva"
                },
                "author": "Victoria Vziatysheva",
                "arxiv_comment": "15 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08952v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08952v5",
                "updated": "2025-03-12T04:46:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    4,
                    46,
                    47,
                    2,
                    71,
                    0
                ],
                "published": "2024-07-12T03:15:01Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    3,
                    15,
                    1,
                    4,
                    194,
                    0
                ],
                "title": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection"
                },
                "summary": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Jiajun Zhu"
                    },
                    {
                        "name": "Xukai Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Yanghai Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08952v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08952v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08398v1",
                "updated": "2025-03-11T13:04:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    4,
                    5,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:04:05Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    4,
                    5,
                    1,
                    70,
                    0
                ],
                "title": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning"
                },
                "summary": "In this paper, we analyze and empirically show that the learned relevance for\nconventional information retrieval (IR) scenarios may be inconsistent in\nretrieval-augmented generation (RAG) scenarios. To bridge this gap, we\nintroduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the\nretriever to capture in-context relevance, enabling adaptation to the diverse\nand evolving needs. Extensive experiments across a wide range of tasks\ndemonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a\nconsistent improvement of 4.0% over the original retriever, consistently\noutperforming existing state-of-the-art retrievers by 2.1%. Additionally, our\nresults indicate that for some tasks, an end-to-end tuned 0.2B retriever can\nachieve improvements that surpass those of RAG-oriented or instruction-tuned 8B\nlarge language models (LLMs), highlighting the cost-effectiveness of our\napproach in enhancing RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we analyze and empirically show that the learned relevance for\nconventional information retrieval (IR) scenarios may be inconsistent in\nretrieval-augmented generation (RAG) scenarios. To bridge this gap, we\nintroduce OpenRAG, a RAG framework that is optimized end-to-end by tuning the\nretriever to capture in-context relevance, enabling adaptation to the diverse\nand evolving needs. Extensive experiments across a wide range of tasks\ndemonstrate that OpenRAG, by tuning a retriever end-to-end, leads to a\nconsistent improvement of 4.0% over the original retriever, consistently\noutperforming existing state-of-the-art retrievers by 2.1%. Additionally, our\nresults indicate that for some tasks, an end-to-end tuned 0.2B retriever can\nachieve improvements that surpass those of RAG-oriented or instruction-tuned 8B\nlarge language models (LLMs), highlighting the cost-effectiveness of our\napproach in enhancing RAG systems."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06082v2",
                "updated": "2025-03-11T12:55:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    55,
                    6,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-08T22:05:38Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    22,
                    5,
                    38,
                    6,
                    343,
                    0
                ],
                "title": "Are foundation models for computer vision good conformal predictors?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are foundation models for computer vision good conformal predictors?"
                },
                "summary": "Recent advances in self-supervision and contrastive learning have brought the\nperformance of foundation models to unprecedented levels in a variety of tasks.\nFueled by this progress, these models are becoming the prevailing approach for\na wide array of real-world vision problems, including risk-sensitive and\nhigh-stakes applications. However, ensuring safe deployment in these scenarios\nrequires a more comprehensive understanding of their uncertainty modeling\ncapabilities, which has been barely explored. In this work, we delve into the\nbehaviour of vision and vision-language foundation models under Conformal\nPrediction (CP), a statistical framework that provides theoretical guarantees\nof marginal coverage of the true class. Across extensive experiments including\npopular vision classification benchmarks, well-known foundation vision models,\nand three CP methods, our findings reveal that foundation models are\nwell-suited for conformalization procedures, particularly those integrating\nVision Transformers. We also show that calibrating the confidence predictions\nof these models, a popular strategy to improve their uncertainty\nquantification, actually leads to efficiency degradation of the conformal set\non adaptive CP methods. Furthermore, few-shot adaptation of Vision-Language\nModels (VLMs) to downstream tasks, whose popularity is surging, enhances\nconformal scores compared to zero-shot predictions. Last, our empirical study\nexposes APS as particularly promising in the context of vision foundation\nmodels, as it does not violate the marginal coverage guarantees across multiple\nchallenging, yet realistic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in self-supervision and contrastive learning have brought the\nperformance of foundation models to unprecedented levels in a variety of tasks.\nFueled by this progress, these models are becoming the prevailing approach for\na wide array of real-world vision problems, including risk-sensitive and\nhigh-stakes applications. However, ensuring safe deployment in these scenarios\nrequires a more comprehensive understanding of their uncertainty modeling\ncapabilities, which has been barely explored. In this work, we delve into the\nbehaviour of vision and vision-language foundation models under Conformal\nPrediction (CP), a statistical framework that provides theoretical guarantees\nof marginal coverage of the true class. Across extensive experiments including\npopular vision classification benchmarks, well-known foundation vision models,\nand three CP methods, our findings reveal that foundation models are\nwell-suited for conformalization procedures, particularly those integrating\nVision Transformers. We also show that calibrating the confidence predictions\nof these models, a popular strategy to improve their uncertainty\nquantification, actually leads to efficiency degradation of the conformal set\non adaptive CP methods. Furthermore, few-shot adaptation of Vision-Language\nModels (VLMs) to downstream tasks, whose popularity is surging, enhances\nconformal scores compared to zero-shot predictions. Last, our empirical study\nexposes APS as particularly promising in the context of vision foundation\nmodels, as it does not violate the marginal coverage guarantees across multiple\nchallenging, yet realistic scenarios."
                },
                "authors": [
                    {
                        "name": "Leo Fillioux"
                    },
                    {
                        "name": "Julio Silva-Rodríguez"
                    },
                    {
                        "name": "Ismail Ben Ayed"
                    },
                    {
                        "name": "Paul-Henry Cournède"
                    },
                    {
                        "name": "Maria Vakalopoulou"
                    },
                    {
                        "name": "Stergios Christodoulidis"
                    },
                    {
                        "name": "Jose Dolz"
                    }
                ],
                "author_detail": {
                    "name": "Jose Dolz"
                },
                "author": "Jose Dolz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16450v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16450v3",
                "updated": "2025-03-11T12:52:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    52,
                    28,
                    1,
                    70,
                    0
                ],
                "published": "2024-05-26T06:33:48Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    33,
                    48,
                    6,
                    147,
                    0
                ],
                "title": "Synthesizing Programmatic Reinforcement Learning Policies with Large\n  Language Model Guided Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Programmatic Reinforcement Learning Policies with Large\n  Language Model Guided Search"
                },
                "summary": "Programmatic reinforcement learning (PRL) has been explored for representing\npolicies through programs as a means to achieve interpretability and\ngeneralization. Despite promising outcomes, current state-of-the-art PRL\nmethods are hindered by sample inefficiency, necessitating tens of millions of\nprogram-environment interactions. To tackle this challenge, we introduce a\nnovel LLM-guided search framework (LLM-GS). Our key insight is to leverage the\nprogramming expertise and common sense reasoning of LLMs to enhance the\nefficiency of assumption-free, random-guessing search methods. We address the\nchallenge of LLMs' inability to generate precise and grammatically correct\nprograms in domain-specific languages (DSLs) by proposing a Pythonic-DSL\nstrategy - an LLM is instructed to initially generate Python codes and then\nconvert them into DSL programs. To further optimize the LLM-generated programs,\nwe develop a search algorithm named Scheduled Hill Climbing, designed to\nefficiently explore the programmatic search space to improve the programs\nconsistently. Experimental results in the Karel domain demonstrate our LLM-GS\nframework's superior effectiveness and efficiency. Extensive ablation studies\nfurther verify the critical role of our Pythonic-DSL strategy and Scheduled\nHill Climbing algorithm. Moreover, we conduct experiments with two novel tasks,\nshowing that LLM-GS enables users without programming skills and knowledge of\nthe domain or DSL to describe the tasks in natural language to obtain\nperformant programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programmatic reinforcement learning (PRL) has been explored for representing\npolicies through programs as a means to achieve interpretability and\ngeneralization. Despite promising outcomes, current state-of-the-art PRL\nmethods are hindered by sample inefficiency, necessitating tens of millions of\nprogram-environment interactions. To tackle this challenge, we introduce a\nnovel LLM-guided search framework (LLM-GS). Our key insight is to leverage the\nprogramming expertise and common sense reasoning of LLMs to enhance the\nefficiency of assumption-free, random-guessing search methods. We address the\nchallenge of LLMs' inability to generate precise and grammatically correct\nprograms in domain-specific languages (DSLs) by proposing a Pythonic-DSL\nstrategy - an LLM is instructed to initially generate Python codes and then\nconvert them into DSL programs. To further optimize the LLM-generated programs,\nwe develop a search algorithm named Scheduled Hill Climbing, designed to\nefficiently explore the programmatic search space to improve the programs\nconsistently. Experimental results in the Karel domain demonstrate our LLM-GS\nframework's superior effectiveness and efficiency. Extensive ablation studies\nfurther verify the critical role of our Pythonic-DSL strategy and Scheduled\nHill Climbing algorithm. Moreover, we conduct experiments with two novel tasks,\nshowing that LLM-GS enables users without programming skills and knowledge of\nthe domain or DSL to describe the tasks in natural language to obtain\nperformant programs."
                },
                "authors": [
                    {
                        "name": "Max Liu"
                    },
                    {
                        "name": "Chan-Hung Yu"
                    },
                    {
                        "name": "Wei-Hsu Lee"
                    },
                    {
                        "name": "Cheng-Wei Hung"
                    },
                    {
                        "name": "Yen-Chun Chen"
                    },
                    {
                        "name": "Shao-Hua Sun"
                    }
                ],
                "author_detail": {
                    "name": "Shao-Hua Sun"
                },
                "author": "Shao-Hua Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16450v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16450v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08379v1",
                "updated": "2025-03-11T12:39:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    39,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T12:39:04Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    39,
                    4,
                    1,
                    70,
                    0
                ],
                "title": "JurisTCU: A Brazilian Portuguese Information Retrieval Dataset with\n  Query Relevance Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JurisTCU: A Brazilian Portuguese Information Retrieval Dataset with\n  Query Relevance Judgments"
                },
                "summary": "This paper introduces JurisTCU, a Brazilian Portuguese dataset for legal\ninformation retrieval (LIR). The dataset is freely available and consists of\n16,045 jurisprudential documents from the Brazilian Federal Court of Accounts,\nalong with 150 queries annotated with relevance judgments. It addresses the\nscarcity of Portuguese-language LIR datasets with query relevance annotations.\nThe queries are organized into three groups: real user keyword-based queries,\nsynthetic keyword-based queries, and synthetic question-based queries.\nRelevance judgments were produced through a hybrid approach combining LLM-based\nscoring with expert domain validation. We used JurisTCU in 14 experiments using\nlexical search (document expansion methods) and semantic search (BERT-based and\nOpenAI embeddings). We show that the document expansion methods significantly\nimprove the performance of standard BM25 search on this dataset, with\nimprovements exceeding 45% in P@10, R@10, and nDCG@10 metrics when evaluating\nshort keyword-based queries. Among the embedding models, the OpenAI models\nproduced the best results, with improvements of approximately 70% in P@10,\nR@10, and nDCG@10 metrics for short keyword-based queries, suggesting that\nthese dense embeddings capture semantic relationships in this domain,\nsurpassing the reliance on lexical terms. Besides offering a dataset for the\nPortuguese-language IR research community, suitable for evaluating search\nsystems, the results also contribute to enhancing a search system highly\nrelevant to Brazilian citizens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces JurisTCU, a Brazilian Portuguese dataset for legal\ninformation retrieval (LIR). The dataset is freely available and consists of\n16,045 jurisprudential documents from the Brazilian Federal Court of Accounts,\nalong with 150 queries annotated with relevance judgments. It addresses the\nscarcity of Portuguese-language LIR datasets with query relevance annotations.\nThe queries are organized into three groups: real user keyword-based queries,\nsynthetic keyword-based queries, and synthetic question-based queries.\nRelevance judgments were produced through a hybrid approach combining LLM-based\nscoring with expert domain validation. We used JurisTCU in 14 experiments using\nlexical search (document expansion methods) and semantic search (BERT-based and\nOpenAI embeddings). We show that the document expansion methods significantly\nimprove the performance of standard BM25 search on this dataset, with\nimprovements exceeding 45% in P@10, R@10, and nDCG@10 metrics when evaluating\nshort keyword-based queries. Among the embedding models, the OpenAI models\nproduced the best results, with improvements of approximately 70% in P@10,\nR@10, and nDCG@10 metrics for short keyword-based queries, suggesting that\nthese dense embeddings capture semantic relationships in this domain,\nsurpassing the reliance on lexical terms. Besides offering a dataset for the\nPortuguese-language IR research community, suitable for evaluating search\nsystems, the results also contribute to enhancing a search system highly\nrelevant to Brazilian citizens."
                },
                "authors": [
                    {
                        "name": "Leandro Carísio Fernandes"
                    },
                    {
                        "name": "Leandro dos Santos Ribeiro"
                    },
                    {
                        "name": "Marcos Vinícius Borela de Castro"
                    },
                    {
                        "name": "Leonardo Augusto da Silva Pacheco"
                    },
                    {
                        "name": "Edans Flávius de Oliveira Sandes"
                    }
                ],
                "author_detail": {
                    "name": "Edans Flávius de Oliveira Sandes"
                },
                "author": "Edans Flávius de Oliveira Sandes",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05336v2",
                "updated": "2025-03-11T12:31:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    31,
                    22,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-07T11:23:48Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    23,
                    48,
                    4,
                    66,
                    0
                ],
                "title": "Toward an Evaluation Science for Generative AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward an Evaluation Science for Generative AI Systems"
                },
                "summary": "There is an increasing imperative to anticipate and understand the\nperformance and safety of generative AI systems in real-world deployment\ncontexts. However, the current evaluation ecosystem is insufficient: Commonly\nused static benchmarks face validity challenges, and ad hoc case-by-case audits\nrarely scale. In this piece, we advocate for maturing an evaluation science for\ngenerative AI systems. While generative AI creates unique challenges for system\nsafety engineering and measurement science, the field can draw valuable\ninsights from the development of safety evaluation practices in other fields,\nincluding transportation, aerospace, and pharmaceutical engineering. In\nparticular, we present three key lessons: Evaluation metrics must be applicable\nto real-world performance, metrics must be iteratively refined, and evaluation\ninstitutions and norms must be established. Applying these insights, we outline\na concrete path toward a more rigorous approach for evaluating generative AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an increasing imperative to anticipate and understand the\nperformance and safety of generative AI systems in real-world deployment\ncontexts. However, the current evaluation ecosystem is insufficient: Commonly\nused static benchmarks face validity challenges, and ad hoc case-by-case audits\nrarely scale. In this piece, we advocate for maturing an evaluation science for\ngenerative AI systems. While generative AI creates unique challenges for system\nsafety engineering and measurement science, the field can draw valuable\ninsights from the development of safety evaluation practices in other fields,\nincluding transportation, aerospace, and pharmaceutical engineering. In\nparticular, we present three key lessons: Evaluation metrics must be applicable\nto real-world performance, metrics must be iteratively refined, and evaluation\ninstitutions and norms must be established. Applying these insights, we outline\na concrete path toward a more rigorous approach for evaluating generative AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Laura Weidinger"
                    },
                    {
                        "name": "Deb Raji"
                    },
                    {
                        "name": "Hanna Wallach"
                    },
                    {
                        "name": "Margaret Mitchell"
                    },
                    {
                        "name": "Angelina Wang"
                    },
                    {
                        "name": "Olawale Salaudeen"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Deep Ganguli"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "William Isaac"
                    }
                ],
                "author_detail": {
                    "name": "William Isaac"
                },
                "author": "William Isaac",
                "arxiv_comment": "First two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10439v2",
                "updated": "2025-03-11T12:19:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    19,
                    9,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-11T09:50:35Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    50,
                    35,
                    2,
                    346,
                    0
                ],
                "title": "CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs"
                },
                "summary": "Object goal navigation (ObjectNav) is a fundamental task in embodied AI,\nrequiring an agent to locate a target object in previously unseen environments.\nThis task is particularly challenging because it requires both perceptual and\ncognitive processes, including object recognition and decision-making. While\nsubstantial advancements in perception have been driven by the rapid\ndevelopment of visual foundation models, progress on the cognitive aspect\nremains constrained, primarily limited to either implicit learning through\nsimulator rollouts or explicit reliance on predefined heuristic rules. Inspired\nby neuroscientific findings demonstrating that humans maintain and dynamically\nupdate fine-grained cognitive states during object search tasks in novel\nenvironments, we propose CogNav, a framework designed to mimic this cognitive\nprocess using large language models. Specifically, we model the cognitive\nprocess using a finite state machine comprising fine-grained cognitive states,\nranging from exploration to identification. Transitions between states are\ndetermined by a large language model based on a dynamically constructed\nheterogeneous cognitive map, which contains spatial and semantic information\nabout the scene being explored. Extensive evaluations on the HM3D, MP3D, and\nRoboTHOR benchmarks demonstrate that our cognitive process modeling\nsignificantly improves the success rate of ObjectNav at least by relative 14%\nover the state-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object goal navigation (ObjectNav) is a fundamental task in embodied AI,\nrequiring an agent to locate a target object in previously unseen environments.\nThis task is particularly challenging because it requires both perceptual and\ncognitive processes, including object recognition and decision-making. While\nsubstantial advancements in perception have been driven by the rapid\ndevelopment of visual foundation models, progress on the cognitive aspect\nremains constrained, primarily limited to either implicit learning through\nsimulator rollouts or explicit reliance on predefined heuristic rules. Inspired\nby neuroscientific findings demonstrating that humans maintain and dynamically\nupdate fine-grained cognitive states during object search tasks in novel\nenvironments, we propose CogNav, a framework designed to mimic this cognitive\nprocess using large language models. Specifically, we model the cognitive\nprocess using a finite state machine comprising fine-grained cognitive states,\nranging from exploration to identification. Transitions between states are\ndetermined by a large language model based on a dynamically constructed\nheterogeneous cognitive map, which contains spatial and semantic information\nabout the scene being explored. Extensive evaluations on the HM3D, MP3D, and\nRoboTHOR benchmarks demonstrate that our cognitive process modeling\nsignificantly improves the success rate of ObjectNav at least by relative 14%\nover the state-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Yihan Cao"
                    },
                    {
                        "name": "Jiazhao Zhang"
                    },
                    {
                        "name": "Zhinan Yu"
                    },
                    {
                        "name": "Shuzhen Liu"
                    },
                    {
                        "name": "Zheng Qin"
                    },
                    {
                        "name": "Qin Zou"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Kai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Xu"
                },
                "author": "Kai Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05244v2",
                "updated": "2025-03-11T12:11:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    11,
                    0,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-07T08:56:20Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    8,
                    56,
                    20,
                    4,
                    66,
                    0
                ],
                "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WritingBench: A Comprehensive Benchmark for Generative Writing"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing."
                },
                "authors": [
                    {
                        "name": "Yuning Wu"
                    },
                    {
                        "name": "Jiahao Mei"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Chenliang Li"
                    },
                    {
                        "name": "Shaopeng Lai"
                    },
                    {
                        "name": "Yuran Ren"
                    },
                    {
                        "name": "Zijia Wang"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Mengyue Wu"
                    },
                    {
                        "name": "Qin Jin"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15933v2",
                "updated": "2025-03-11T12:08:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    8,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-24T17:39:39Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    17,
                    39,
                    39,
                    6,
                    329,
                    0
                ],
                "title": "Bringing the Context Back into Object Recognition, Robustly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bringing the Context Back into Object Recognition, Robustly"
                },
                "summary": "In object recognition, both the subject of interest (referred to as\nforeground, FG, for simplicity) and its surrounding context (background, BG)\nmay play an important role. However, standard supervised learning often leads\nto unintended over-reliance on the BG, limiting model robustness in real-world\ndeployment settings. The problem is mainly addressed by suppressing the BG,\nsacrificing context information for improved generalization.\n  We propose \"Localize to Recognize Robustly\" (L2R2), a novel recognition\napproach which exploits the benefits of context-aware classification while\nmaintaining robustness to distribution shifts. L2R2 leverages advances in\nzero-shot detection to localize the FG before recognition. It improves the\nperformance of both standard recognition with supervised training, as well as\nmultimodal zero-shot recognition with VLMs, while being robust to long-tail BGs\nand distribution shifts. The results confirm localization before recognition is\npossible for a wide range of datasets and they highlight the limits of object\ndetection on others",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In object recognition, both the subject of interest (referred to as\nforeground, FG, for simplicity) and its surrounding context (background, BG)\nmay play an important role. However, standard supervised learning often leads\nto unintended over-reliance on the BG, limiting model robustness in real-world\ndeployment settings. The problem is mainly addressed by suppressing the BG,\nsacrificing context information for improved generalization.\n  We propose \"Localize to Recognize Robustly\" (L2R2), a novel recognition\napproach which exploits the benefits of context-aware classification while\nmaintaining robustness to distribution shifts. L2R2 leverages advances in\nzero-shot detection to localize the FG before recognition. It improves the\nperformance of both standard recognition with supervised training, as well as\nmultimodal zero-shot recognition with VLMs, while being robust to long-tail BGs\nand distribution shifts. The results confirm localization before recognition is\npossible for a wide range of datasets and they highlight the limits of object\ndetection on others"
                },
                "authors": [
                    {
                        "name": "Klara Janouskova"
                    },
                    {
                        "name": "Cristian Gavrus"
                    },
                    {
                        "name": "Jiri Matas"
                    }
                ],
                "author_detail": {
                    "name": "Jiri Matas"
                },
                "author": "Jiri Matas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08349v1",
                "updated": "2025-03-11T12:05:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    5,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T12:05:04Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    12,
                    5,
                    4,
                    1,
                    70,
                    0
                ],
                "title": "LiPS: Large-Scale Humanoid Robot Reinforcement Learning with\n  Parallel-Series Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiPS: Large-Scale Humanoid Robot Reinforcement Learning with\n  Parallel-Series Structures"
                },
                "summary": "In recent years, research on humanoid robots has garnered significant\nattention, particularly in reinforcement learning based control algorithms,\nwhich have achieved major breakthroughs. Compared to traditional model-based\ncontrol algorithms, reinforcement learning based algorithms demonstrate\nsubstantial advantages in handling complex tasks. Leveraging the large-scale\nparallel computing capabilities of GPUs, contemporary humanoid robots can\nundergo extensive parallel training in simulated environments. A physical\nsimulation platform capable of large-scale parallel training is crucial for the\ndevelopment of humanoid robots. As one of the most complex robot forms,\nhumanoid robots typically possess intricate mechanical structures, encompassing\nnumerous series and parallel mechanisms. However, many reinforcement learning\nbased humanoid robot control algorithms currently employ open-loop topologies\nduring training, deferring the conversion to series-parallel structures until\nthe sim2real phase. This approach is primarily due to the limitations of\nphysics engines, as current GPU-based physics engines often only support\nopen-loop topologies or have limited capabilities in simulating\nmulti-rigid-body closed-loop topologies. For enabling reinforcement\nlearning-based humanoid robot control algorithms to train in large-scale\nparallel environments, we propose a novel training method LiPS. By\nincorporating multi-rigid-body dynamics modeling in the simulation environment,\nwe significantly reduce the sim2real gap and the difficulty of converting to\nparallel structures during model deployment, thereby robustly supporting\nlarge-scale reinforcement learning for humanoid robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, research on humanoid robots has garnered significant\nattention, particularly in reinforcement learning based control algorithms,\nwhich have achieved major breakthroughs. Compared to traditional model-based\ncontrol algorithms, reinforcement learning based algorithms demonstrate\nsubstantial advantages in handling complex tasks. Leveraging the large-scale\nparallel computing capabilities of GPUs, contemporary humanoid robots can\nundergo extensive parallel training in simulated environments. A physical\nsimulation platform capable of large-scale parallel training is crucial for the\ndevelopment of humanoid robots. As one of the most complex robot forms,\nhumanoid robots typically possess intricate mechanical structures, encompassing\nnumerous series and parallel mechanisms. However, many reinforcement learning\nbased humanoid robot control algorithms currently employ open-loop topologies\nduring training, deferring the conversion to series-parallel structures until\nthe sim2real phase. This approach is primarily due to the limitations of\nphysics engines, as current GPU-based physics engines often only support\nopen-loop topologies or have limited capabilities in simulating\nmulti-rigid-body closed-loop topologies. For enabling reinforcement\nlearning-based humanoid robot control algorithms to train in large-scale\nparallel environments, we propose a novel training method LiPS. By\nincorporating multi-rigid-body dynamics modeling in the simulation environment,\nwe significantly reduce the sim2real gap and the difficulty of converting to\nparallel structures during model deployment, thereby robustly supporting\nlarge-scale reinforcement learning for humanoid robots."
                },
                "authors": [
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Gang Han"
                    },
                    {
                        "name": "Jingkai Sun"
                    },
                    {
                        "name": "Wen Zhao"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Jiaxu Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Lingfeng Zhang"
                    },
                    {
                        "name": "Yijie Guo"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08338v1",
                "updated": "2025-03-11T11:50:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    50,
                    36,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:50:36Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    50,
                    36,
                    1,
                    70,
                    0
                ],
                "title": "Trinity: A Modular Humanoid Robot AI System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trinity: A Modular Humanoid Robot AI System"
                },
                "summary": "In recent years, research on humanoid robots has garnered increasing\nattention. With breakthroughs in various types of artificial intelligence\nalgorithms, embodied intelligence, exemplified by humanoid robots, has been\nhighly anticipated. The advancements in reinforcement learning (RL) algorithms\nhave significantly improved the motion control and generalization capabilities\nof humanoid robots. Simultaneously, the groundbreaking progress in large\nlanguage models (LLM) and visual language models (VLM) has brought more\npossibilities and imagination to humanoid robots. LLM enables humanoid robots\nto understand complex tasks from language instructions and perform long-term\ntask planning, while VLM greatly enhances the robots' understanding and\ninteraction with their environment. This paper introduces\n\\textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that\nintegrates RL, LLM, and VLM. By combining these technologies, Trinity enables\nefficient control of humanoid robots in complex environments. This innovative\napproach not only enhances the capabilities but also opens new avenues for\nfuture research and applications of humanoid robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, research on humanoid robots has garnered increasing\nattention. With breakthroughs in various types of artificial intelligence\nalgorithms, embodied intelligence, exemplified by humanoid robots, has been\nhighly anticipated. The advancements in reinforcement learning (RL) algorithms\nhave significantly improved the motion control and generalization capabilities\nof humanoid robots. Simultaneously, the groundbreaking progress in large\nlanguage models (LLM) and visual language models (VLM) has brought more\npossibilities and imagination to humanoid robots. LLM enables humanoid robots\nto understand complex tasks from language instructions and perform long-term\ntask planning, while VLM greatly enhances the robots' understanding and\ninteraction with their environment. This paper introduces\n\\textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that\nintegrates RL, LLM, and VLM. By combining these technologies, Trinity enables\nefficient control of humanoid robots in complex environments. This innovative\napproach not only enhances the capabilities but also opens new avenues for\nfuture research and applications of humanoid robotics."
                },
                "authors": [
                    {
                        "name": "Jingkai Sun"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Gang Han"
                    },
                    {
                        "name": "Wen Zhao"
                    },
                    {
                        "name": "Zhe Yong"
                    },
                    {
                        "name": "Yan He"
                    },
                    {
                        "name": "Jiaxu Wang"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Yijie Guo"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08335v1",
                "updated": "2025-03-11T11:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    47,
                    48,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:47:48Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    47,
                    48,
                    1,
                    70,
                    0
                ],
                "title": "Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal\n  Videos"
                },
                "summary": "Learning multimodal video understanding typically relies on datasets\ncomprising video clips paired with manually annotated captions. However, this\nbecomes even more challenging when dealing with long-form videos, lasting from\nminutes to hours, in educational and news domains due to the need for more\nannotators with subject expertise. Hence, there arises a need for automated\nsolutions. Recent advancements in Large Language Models (LLMs) promise to\ncapture concise and informative content that allows the comprehension of entire\nvideos by leveraging Automatic Speech Recognition (ASR) and Optical Character\nRecognition (OCR) technologies. ASR provides textual content from audio, while\nOCR extracts textual content from specific frames. This paper introduces a\ndataset comprising long-form lectures and news videos. We present baseline\napproaches to understand their limitations on this dataset and advocate for\nexploring prompt engineering techniques to comprehend long-form multimodal\nvideo datasets comprehensively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning multimodal video understanding typically relies on datasets\ncomprising video clips paired with manually annotated captions. However, this\nbecomes even more challenging when dealing with long-form videos, lasting from\nminutes to hours, in educational and news domains due to the need for more\nannotators with subject expertise. Hence, there arises a need for automated\nsolutions. Recent advancements in Large Language Models (LLMs) promise to\ncapture concise and informative content that allows the comprehension of entire\nvideos by leveraging Automatic Speech Recognition (ASR) and Optical Character\nRecognition (OCR) technologies. ASR provides textual content from audio, while\nOCR extracts textual content from specific frames. This paper introduces a\ndataset comprising long-form lectures and news videos. We present baseline\napproaches to understand their limitations on this dataset and advocate for\nexploring prompt engineering techniques to comprehend long-form multimodal\nvideo datasets comprehensively."
                },
                "authors": [
                    {
                        "name": "Soumya Shamarao Jahagirdar"
                    },
                    {
                        "name": "Jayasree Saha"
                    },
                    {
                        "name": "C V Jawahar"
                    }
                ],
                "author_detail": {
                    "name": "C V Jawahar"
                },
                "author": "C V Jawahar",
                "arxiv_comment": "CVIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08330v1",
                "updated": "2025-03-11T11:44:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    44,
                    29,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:44:29Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    44,
                    29,
                    1,
                    70,
                    0
                ],
                "title": "KiteRunner: Language-Driven Cooperative Local-Global Navigation Policy\n  with UAV Mapping in Outdoor Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KiteRunner: Language-Driven Cooperative Local-Global Navigation Policy\n  with UAV Mapping in Outdoor Environments"
                },
                "summary": "Autonomous navigation in open-world outdoor environments faces challenges in\nintegrating dynamic conditions, long-distance spatial reasoning, and semantic\nunderstanding. Traditional methods struggle to balance local planning, global\nplanning, and semantic task execution, while existing large language models\n(LLMs) enhance semantic comprehension but lack spatial reasoning capabilities.\nAlthough diffusion models excel in local optimization, they fall short in\nlarge-scale long-distance navigation. To address these gaps, this paper\nproposes KiteRunner, a language-driven cooperative local-global navigation\nstrategy that combines UAV orthophoto-based global planning with diffusion\nmodel-driven local path generation for long-distance navigation in open-world\nscenarios. Our method innovatively leverages real-time UAV orthophotography to\nconstruct a global probability map, providing traversability guidance for the\nlocal planner, while integrating large models like CLIP and GPT to interpret\nnatural language instructions. Experiments demonstrate that KiteRunner achieves\n5.6% and 12.8% improvements in path efficiency over state-of-the-art methods in\nstructured and unstructured environments, respectively, with significant\nreductions in human interventions and execution time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in open-world outdoor environments faces challenges in\nintegrating dynamic conditions, long-distance spatial reasoning, and semantic\nunderstanding. Traditional methods struggle to balance local planning, global\nplanning, and semantic task execution, while existing large language models\n(LLMs) enhance semantic comprehension but lack spatial reasoning capabilities.\nAlthough diffusion models excel in local optimization, they fall short in\nlarge-scale long-distance navigation. To address these gaps, this paper\nproposes KiteRunner, a language-driven cooperative local-global navigation\nstrategy that combines UAV orthophoto-based global planning with diffusion\nmodel-driven local path generation for long-distance navigation in open-world\nscenarios. Our method innovatively leverages real-time UAV orthophotography to\nconstruct a global probability map, providing traversability guidance for the\nlocal planner, while integrating large models like CLIP and GPT to interpret\nnatural language instructions. Experiments demonstrate that KiteRunner achieves\n5.6% and 12.8% improvements in path efficiency over state-of-the-art methods in\nstructured and unstructured environments, respectively, with significant\nreductions in human interventions and execution time."
                },
                "authors": [
                    {
                        "name": "Shibo Huang"
                    },
                    {
                        "name": "Chenfan Shi"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Hanlin Dong"
                    },
                    {
                        "name": "Jinpeng Mi"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Jianfeng Zhang"
                    },
                    {
                        "name": "Miao Ding"
                    },
                    {
                        "name": "Peidong Liang"
                    },
                    {
                        "name": "Xiong You"
                    },
                    {
                        "name": "Xian Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xian Wei"
                },
                "author": "Xian Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08323v1",
                "updated": "2025-03-11T11:34:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    34,
                    57,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:34:57Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    34,
                    57,
                    1,
                    70,
                    0
                ],
                "title": "Towards Scalable and Cross-Lingual Specialist Language Models for\n  Oncology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable and Cross-Lingual Specialist Language Models for\n  Oncology"
                },
                "summary": "Clinical oncology generates vast, unstructured data that often contain\ninconsistencies, missing information, and ambiguities, making it difficult to\nextract reliable insights for data-driven decision-making. General-purpose\nlarge language models (LLMs) struggle with these challenges due to their lack\nof domain-specific reasoning, including specialized clinical terminology,\ncontext-dependent interpretations, and multi-modal data integration. We address\nthese issues with an oncology-specialized, efficient, and adaptable NLP\nframework that combines instruction tuning, retrieval-augmented generation\n(RAG), and graph-based knowledge integration. Our lightweight models prove\neffective at oncology-specific tasks, such as named entity recognition (e.g.,\nidentifying cancer diagnoses), entity linking (e.g., linking entities to\nstandardized ontologies), TNM staging, document classification (e.g., cancer\nsubtype classification from pathology reports), and treatment response\nprediction. Our framework emphasizes adaptability and resource efficiency. We\ninclude minimal German instructions, collected at the University Hospital\nZurich (USZ), to test whether small amounts of non-English language data can\neffectively transfer knowledge across languages. This approach mirrors our\nmotivation for lightweight models, which balance strong performance with\nreduced computational costs, making them suitable for resource-limited\nhealthcare settings. We validated our models on oncology datasets,\ndemonstrating strong results in named entity recognition, relation extraction,\nand document classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical oncology generates vast, unstructured data that often contain\ninconsistencies, missing information, and ambiguities, making it difficult to\nextract reliable insights for data-driven decision-making. General-purpose\nlarge language models (LLMs) struggle with these challenges due to their lack\nof domain-specific reasoning, including specialized clinical terminology,\ncontext-dependent interpretations, and multi-modal data integration. We address\nthese issues with an oncology-specialized, efficient, and adaptable NLP\nframework that combines instruction tuning, retrieval-augmented generation\n(RAG), and graph-based knowledge integration. Our lightweight models prove\neffective at oncology-specific tasks, such as named entity recognition (e.g.,\nidentifying cancer diagnoses), entity linking (e.g., linking entities to\nstandardized ontologies), TNM staging, document classification (e.g., cancer\nsubtype classification from pathology reports), and treatment response\nprediction. Our framework emphasizes adaptability and resource efficiency. We\ninclude minimal German instructions, collected at the University Hospital\nZurich (USZ), to test whether small amounts of non-English language data can\neffectively transfer knowledge across languages. This approach mirrors our\nmotivation for lightweight models, which balance strong performance with\nreduced computational costs, making them suitable for resource-limited\nhealthcare settings. We validated our models on oncology datasets,\ndemonstrating strong results in named entity recognition, relation extraction,\nand document classification."
                },
                "authors": [
                    {
                        "name": "Morteza Rohanian"
                    },
                    {
                        "name": "Tarun Mehra"
                    },
                    {
                        "name": "Nicola Miglino"
                    },
                    {
                        "name": "Farhad Nooralahzadeh"
                    },
                    {
                        "name": "Michael Krauthammer"
                    },
                    {
                        "name": "Andreas Wicki"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Wicki"
                },
                "author": "Andreas Wicki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17384v2",
                "updated": "2025-03-11T11:31:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    31,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-26T12:43:19Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    12,
                    43,
                    19,
                    1,
                    331,
                    0
                ],
                "title": "Assessing Electricity Network Capacity Requirements for Industrial\n  Decarbonisation in Great Britain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Electricity Network Capacity Requirements for Industrial\n  Decarbonisation in Great Britain"
                },
                "summary": "Decarbonising the industrial sector is vital to reach net zero targets. The\ndeployment of industrial decarbonisation technologies is expected to increase\nindustrial electricity demand in many countries and this may require upgrades\nto the existing electricity network or new network investment. While the\ninfrastructure requirements to support the introduction of new fuels and\ntechnologies in industry, such as hydrogen and carbon capture, utilisation and\nstorage are often discussed, the need for investment to increase the capacity\nof the electricity network to meet increasing industrial electricity demands is\noften overlooked in the literature. This paper addresses this gap by\nquantifying the requirements for additional electricity network capacity to\nsupport the decarbonisation of industrial sectors across Great Britain (GB).\nThe Net Zero Industrial Pathways model is used to predict the future\nelectricity demand from industrial sites to 2050 which is then compared\nspatially to the available headroom across the distribution network in GB. The\nresults show that network headroom is sufficient to meet extra capacity demands\nfrom industrial sites over the period to 2030 in nearly all GB regions and\nnetwork scenarios. However, as electricity demand rises due to increased\nelectrification across all sectors and industrial decarbonisation accelerates\ntowards 2050, the network will need significant new capacity (71 GW + by 2050)\nparticularly in the central, south, and north-west regions of England, and\nWales. Without solving these network constraints, around 65% of industrial\nsites that are large point sources of emissions would be constrained in terms\nof electric capacity by 2040. These sites are responsible for 69% of industrial\npoint source emissions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decarbonising the industrial sector is vital to reach net zero targets. The\ndeployment of industrial decarbonisation technologies is expected to increase\nindustrial electricity demand in many countries and this may require upgrades\nto the existing electricity network or new network investment. While the\ninfrastructure requirements to support the introduction of new fuels and\ntechnologies in industry, such as hydrogen and carbon capture, utilisation and\nstorage are often discussed, the need for investment to increase the capacity\nof the electricity network to meet increasing industrial electricity demands is\noften overlooked in the literature. This paper addresses this gap by\nquantifying the requirements for additional electricity network capacity to\nsupport the decarbonisation of industrial sectors across Great Britain (GB).\nThe Net Zero Industrial Pathways model is used to predict the future\nelectricity demand from industrial sites to 2050 which is then compared\nspatially to the available headroom across the distribution network in GB. The\nresults show that network headroom is sufficient to meet extra capacity demands\nfrom industrial sites over the period to 2030 in nearly all GB regions and\nnetwork scenarios. However, as electricity demand rises due to increased\nelectrification across all sectors and industrial decarbonisation accelerates\ntowards 2050, the network will need significant new capacity (71 GW + by 2050)\nparticularly in the central, south, and north-west regions of England, and\nWales. Without solving these network constraints, around 65% of industrial\nsites that are large point sources of emissions would be constrained in terms\nof electric capacity by 2040. These sites are responsible for 69% of industrial\npoint source emissions."
                },
                "authors": [
                    {
                        "name": "Ahmed Gailani"
                    },
                    {
                        "name": "Peter Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Peter Taylor"
                },
                "author": "Peter Taylor",
                "arxiv_doi": "10.1016/j.enpol.2025.114559",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.enpol.2025.114559",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.17384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Volume 201, June 2025, 114559",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06150v2",
                "updated": "2025-03-11T11:28:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    28,
                    18,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T10:21:21Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    10,
                    21,
                    21,
                    5,
                    67,
                    0
                ],
                "title": "Do Fairness Interventions Come at the Cost of Privacy: Evaluations for\n  Binary Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Fairness Interventions Come at the Cost of Privacy: Evaluations for\n  Binary Classifiers"
                },
                "summary": "While in-processing fairness approaches show promise in mitigating biased\npredictions, their potential impact on privacy leakage remains under-explored.\nWe aim to address this gap by assessing the privacy risks of fairness-enhanced\nbinary classifiers via membership inference attacks (MIAs) and attribute\ninference attacks (AIAs). Surprisingly, our results reveal that enhancing\nfairness does not necessarily lead to privacy compromises. For example, these\nfairness interventions exhibit increased resilience against MIAs and AIAs. This\nis because fairness interventions tend to remove sensitive information among\nextracted features and reduce confidence scores for the majority of training\ndata for fairer predictions. However, during the evaluations, we uncover a\npotential threat mechanism that exploits prediction discrepancies between fair\nand biased models, leading to advanced attack results for both MIAs and AIAs.\nThis mechanism reveals potent vulnerabilities of fair models and poses\nsignificant privacy risks of current fairness methods. Extensive experiments\nacross multiple datasets, attack methods, and representative fairness\napproaches confirm our findings and demonstrate the efficacy of the uncovered\nmechanism. Our study exposes the under-explored privacy threats in fairness\nstudies, advocating for thorough evaluations of potential security\nvulnerabilities before model deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While in-processing fairness approaches show promise in mitigating biased\npredictions, their potential impact on privacy leakage remains under-explored.\nWe aim to address this gap by assessing the privacy risks of fairness-enhanced\nbinary classifiers via membership inference attacks (MIAs) and attribute\ninference attacks (AIAs). Surprisingly, our results reveal that enhancing\nfairness does not necessarily lead to privacy compromises. For example, these\nfairness interventions exhibit increased resilience against MIAs and AIAs. This\nis because fairness interventions tend to remove sensitive information among\nextracted features and reduce confidence scores for the majority of training\ndata for fairer predictions. However, during the evaluations, we uncover a\npotential threat mechanism that exploits prediction discrepancies between fair\nand biased models, leading to advanced attack results for both MIAs and AIAs.\nThis mechanism reveals potent vulnerabilities of fair models and poses\nsignificant privacy risks of current fairness methods. Extensive experiments\nacross multiple datasets, attack methods, and representative fairness\napproaches confirm our findings and demonstrate the efficacy of the uncovered\nmechanism. Our study exposes the under-explored privacy threats in fairness\nstudies, advocating for thorough evaluations of potential security\nvulnerabilities before model deployments."
                },
                "authors": [
                    {
                        "name": "Huan Tian"
                    },
                    {
                        "name": "Guangsheng Zhang"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Tianqing Zhu"
                    },
                    {
                        "name": "Ming Ding"
                    },
                    {
                        "name": "Wanlei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wanlei Zhou"
                },
                "author": "Wanlei Zhou",
                "arxiv_comment": "Accepted to IEEE Transactions on Dependable and Secure Computing\n  (TDSC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02189v2",
                "updated": "2025-03-11T11:22:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    22,
                    17,
                    1,
                    70,
                    0
                ],
                "published": "2024-10-03T04:07:51Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    4,
                    7,
                    51,
                    3,
                    277,
                    0
                ],
                "title": "Agent-Oriented Planning in Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-Oriented Planning in Multi-Agent Systems"
                },
                "summary": "Through the collaboration of multiple LLM-empowered agents possessing diverse\nexpertise and tools, multi-agent systems achieve impressive progress in solving\nreal-world problems. Given the user queries, the meta-agents, serving as the\nbrain within multi-agent systems, are required to decompose the queries into\nmultiple sub-tasks that can be allocated to suitable agents capable of solving\nthem, so-called agent-oriented planning. In this study, we identify three\ncritical design principles of agent-oriented planning, including solvability,\ncompleteness, and non-redundancy, to ensure that each sub-task can be\neffectively resolved, resulting in satisfactory responses to user queries.\nThese principles further inspire us to propose AOP, a novel framework for\nagent-oriented planning in multi-agent systems, leveraging a fast task\ndecomposition and allocation process followed by an effective and efficient\nevaluation via a reward model. According to the evaluation results, the\nmeta-agent is also responsible for promptly making necessary adjustments to\nsub-tasks and scheduling. Besides, we integrate a feedback loop into AOP to\nfurther enhance the effectiveness and robustness of such a problem-solving\nprocess. Extensive experiments demonstrate the advancement of AOP in solving\nreal-world problems compared to both single-agent systems and existing planning\nstrategies for multi-agent systems. The source code is available at\nhttps://github.com/lalaliat/Agent-Oriented-Planning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the collaboration of multiple LLM-empowered agents possessing diverse\nexpertise and tools, multi-agent systems achieve impressive progress in solving\nreal-world problems. Given the user queries, the meta-agents, serving as the\nbrain within multi-agent systems, are required to decompose the queries into\nmultiple sub-tasks that can be allocated to suitable agents capable of solving\nthem, so-called agent-oriented planning. In this study, we identify three\ncritical design principles of agent-oriented planning, including solvability,\ncompleteness, and non-redundancy, to ensure that each sub-task can be\neffectively resolved, resulting in satisfactory responses to user queries.\nThese principles further inspire us to propose AOP, a novel framework for\nagent-oriented planning in multi-agent systems, leveraging a fast task\ndecomposition and allocation process followed by an effective and efficient\nevaluation via a reward model. According to the evaluation results, the\nmeta-agent is also responsible for promptly making necessary adjustments to\nsub-tasks and scheduling. Besides, we integrate a feedback loop into AOP to\nfurther enhance the effectiveness and robustness of such a problem-solving\nprocess. Extensive experiments demonstrate the advancement of AOP in solving\nreal-world problems compared to both single-agent systems and existing planning\nstrategies for multi-agent systems. The source code is available at\nhttps://github.com/lalaliat/Agent-Oriented-Planning"
                },
                "authors": [
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Yuexiang Xie"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Fugee Tsung"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Yaliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yaliang Li"
                },
                "author": "Yaliang Li",
                "arxiv_comment": "Accepted by ICLR'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08311v1",
                "updated": "2025-03-11T11:21:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    21,
                    35,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:21:35Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    21,
                    35,
                    1,
                    70,
                    0
                ],
                "title": "Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM\n  Inference"
                },
                "summary": "Large language models have been widely adopted across different tasks, but\ntheir auto-regressive generation nature often leads to inefficient resource\nutilization during inference. While batching is commonly used to increase\nthroughput, performance gains plateau beyond a certain batch size, especially\nwith smaller models, a phenomenon that existing literature typically explains\nas a shift to the compute-bound regime. In this paper, through an in-depth\nGPU-level analysis, we reveal that large-batch inference remains memory-bound,\nwith most GPU compute capabilities underutilized due to DRAM bandwidth\nsaturation as the primary bottleneck. To address this, we propose a Batching\nConfiguration Advisor (BCA) that optimizes memory allocation, reducing GPU\nmemory requirements with minimal impact on throughput. The freed memory and\nunderutilized GPU compute capabilities can then be leveraged by concurrent\nworkloads. Specifically, we use model replication to improve serving throughput\nand GPU utilization. Our findings challenge conventional assumptions about LLM\ninference, offering new insights and practical strategies for improving\nresource utilization, particularly for smaller language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been widely adopted across different tasks, but\ntheir auto-regressive generation nature often leads to inefficient resource\nutilization during inference. While batching is commonly used to increase\nthroughput, performance gains plateau beyond a certain batch size, especially\nwith smaller models, a phenomenon that existing literature typically explains\nas a shift to the compute-bound regime. In this paper, through an in-depth\nGPU-level analysis, we reveal that large-batch inference remains memory-bound,\nwith most GPU compute capabilities underutilized due to DRAM bandwidth\nsaturation as the primary bottleneck. To address this, we propose a Batching\nConfiguration Advisor (BCA) that optimizes memory allocation, reducing GPU\nmemory requirements with minimal impact on throughput. The freed memory and\nunderutilized GPU compute capabilities can then be leveraged by concurrent\nworkloads. Specifically, we use model replication to improve serving throughput\nand GPU utilization. Our findings challenge conventional assumptions about LLM\ninference, offering new insights and practical strategies for improving\nresource utilization, particularly for smaller language models."
                },
                "authors": [
                    {
                        "name": "Pol G. Recasens"
                    },
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Eun Kyung Lee"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Pol G. Recasens, Ferran Agullo: equal contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08308v1",
                "updated": "2025-03-11T11:18:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    18,
                    53,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:18:53Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    18,
                    53,
                    1,
                    70,
                    0
                ],
                "title": "Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with\n  an Uncertainty-Aware Agentic Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with\n  an Uncertainty-Aware Agentic Framework"
                },
                "summary": "Multimodal large language models (MLLMs) show promise in tasks like visual\nquestion answering (VQA) but still face challenges in multimodal reasoning.\nRecent works adapt agentic frameworks or chain-of-thought (CoT) reasoning to\nimprove performance. However, CoT-based multimodal reasoning often demands\ncostly data annotation and fine-tuning, while agentic approaches relying on\nexternal tools risk introducing unreliable output from these tools. In this\npaper, we propose Seeing and Reasoning with Confidence (SRICE), a training-free\nmultimodal reasoning framework that integrates external vision models with\nuncertainty quantification (UQ) into an MLLM to address these challenges.\nSpecifically, SRICE guides the inference process by allowing MLLM to\nautonomously select regions of interest through multi-stage interactions with\nthe help of external tools. We propose to use a conformal prediction-based\napproach to calibrate the output of external tools and select the optimal tool\nby estimating the uncertainty of an MLLM's output. Our experiment shows that\nthe average improvement of SRICE over the base MLLM is 4.6% on five datasets\nand the performance on some datasets even outperforms fine-tuning-based\nmethods, revealing the significance of ensuring reliable tool use in an MLLM\nagent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) show promise in tasks like visual\nquestion answering (VQA) but still face challenges in multimodal reasoning.\nRecent works adapt agentic frameworks or chain-of-thought (CoT) reasoning to\nimprove performance. However, CoT-based multimodal reasoning often demands\ncostly data annotation and fine-tuning, while agentic approaches relying on\nexternal tools risk introducing unreliable output from these tools. In this\npaper, we propose Seeing and Reasoning with Confidence (SRICE), a training-free\nmultimodal reasoning framework that integrates external vision models with\nuncertainty quantification (UQ) into an MLLM to address these challenges.\nSpecifically, SRICE guides the inference process by allowing MLLM to\nautonomously select regions of interest through multi-stage interactions with\nthe help of external tools. We propose to use a conformal prediction-based\napproach to calibrate the output of external tools and select the optimal tool\nby estimating the uncertainty of an MLLM's output. Our experiment shows that\nthe average improvement of SRICE over the base MLLM is 4.6% on five datasets\nand the performance on some datasets even outperforms fine-tuning-based\nmethods, revealing the significance of ensuring reliable tool use in an MLLM\nagent."
                },
                "authors": [
                    {
                        "name": "Zhuo Zhi"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Adam Daneshmend"
                    },
                    {
                        "name": "Mine Orlu"
                    },
                    {
                        "name": "Andreas Demosthenous"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Da Li"
                    },
                    {
                        "name": "Ziquan Liu"
                    },
                    {
                        "name": "Miguel R. D. Rodrigues"
                    }
                ],
                "author_detail": {
                    "name": "Miguel R. D. Rodrigues"
                },
                "author": "Miguel R. D. Rodrigues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08302v1",
                "updated": "2025-03-11T11:13:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    13,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:13:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    13,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "General-Purpose Aerial Intelligent Agents Empowered by Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-Purpose Aerial Intelligent Agents Empowered by Large Language\n  Models"
                },
                "summary": "The emergence of large language models (LLMs) opens new frontiers for\nunmanned aerial vehicle (UAVs), yet existing systems remain confined to\npredefined tasks due to hardware-software co-design challenges. This paper\npresents the first aerial intelligent agent capable of open-world task\nexecution through tight integration of LLM-based reasoning and robotic\nautonomy. Our hardware-software co-designed system addresses two fundamental\nlimitations: (1) Onboard LLM operation via an edge-optimized computing\nplatform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W\npeak power; (2) A bidirectional cognitive architecture that synergizes slow\ndeliberative planning (LLM task planning) with fast reactive control (state\nestimation, mapping, obstacle avoidance, and motion planning). Validated\nthrough preliminary results using our prototype, the system demonstrates\nreliable task planning and scene understanding in communication-constrained\nenvironments, such as sugarcane monitoring, power grid inspection, mine tunnel\nexploration, and biological observation applications. This work establishes a\nnovel framework for embodied aerial artificial intelligence, bridging the gap\nbetween task planning and robotic autonomy in open environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) opens new frontiers for\nunmanned aerial vehicle (UAVs), yet existing systems remain confined to\npredefined tasks due to hardware-software co-design challenges. This paper\npresents the first aerial intelligent agent capable of open-world task\nexecution through tight integration of LLM-based reasoning and robotic\nautonomy. Our hardware-software co-designed system addresses two fundamental\nlimitations: (1) Onboard LLM operation via an edge-optimized computing\nplatform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W\npeak power; (2) A bidirectional cognitive architecture that synergizes slow\ndeliberative planning (LLM task planning) with fast reactive control (state\nestimation, mapping, obstacle avoidance, and motion planning). Validated\nthrough preliminary results using our prototype, the system demonstrates\nreliable task planning and scene understanding in communication-constrained\nenvironments, such as sugarcane monitoring, power grid inspection, mine tunnel\nexploration, and biological observation applications. This work establishes a\nnovel framework for embodied aerial artificial intelligence, bridging the gap\nbetween task planning and robotic autonomy in open environments."
                },
                "authors": [
                    {
                        "name": "Ji Zhao"
                    },
                    {
                        "name": "Xiao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Lin"
                },
                "author": "Xiao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08301v2",
                "updated": "2025-03-12T06:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    0,
                    27,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-11T11:13:11Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "title": "Large Language Model as Meta-Surrogate for Data-Driven Many-Task\n  Optimization: A Proof-of-Principle Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model as Meta-Surrogate for Data-Driven Many-Task\n  Optimization: A Proof-of-Principle Study"
                },
                "summary": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization."
                },
                "authors": [
                    {
                        "name": "Xian-Rong Zhang"
                    },
                    {
                        "name": "Yue-Jiao Gong"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08292v1",
                "updated": "2025-03-11T11:05:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    5,
                    42,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T11:05:42Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    5,
                    42,
                    1,
                    70,
                    0
                ],
                "title": "Large Language Models for Outpatient Referral: Problem Definition,\n  Benchmarking and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Outpatient Referral: Problem Definition,\n  Benchmarking and Challenges"
                },
                "summary": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Liu"
                    },
                    {
                        "name": "Qingying Xiao"
                    },
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Xiangyi Feng"
                    },
                    {
                        "name": "Xiangbo Wu"
                    },
                    {
                        "name": "Bairui Zhang"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Jian Chang"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08280v1",
                "updated": "2025-03-11T10:50:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    10,
                    50,
                    14,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T10:50:14Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    10,
                    50,
                    14,
                    1,
                    70,
                    0
                ],
                "title": "OminiControl2: Efficient Conditioning for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OminiControl2: Efficient Conditioning for Diffusion Transformers"
                },
                "summary": "Fine-grained control of text-to-image diffusion transformer models (DiT)\nremains a critical challenge for practical deployment. While recent advances\nsuch as OminiControl and others have enabled a controllable generation of\ndiverse control signals, these methods face significant computational\ninefficiency when handling long conditional inputs. We present OminiControl2,\nan efficient framework that achieves efficient image-conditional image\ngeneration. OminiControl2 introduces two key innovations: (1) a dynamic\ncompression strategy that streamlines conditional inputs by preserving only the\nmost semantically relevant tokens during generation, and (2) a conditional\nfeature reuse mechanism that computes condition token features only once and\nreuses them across denoising steps. These architectural improvements preserve\nthe original framework's parameter efficiency and multi-modal versatility while\ndramatically reducing computational costs. Our experiments demonstrate that\nOminiControl2 reduces conditional processing overhead by over 90% compared to\nits predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional\ngeneration scenarios. This efficiency enables the practical implementation of\ncomplex, multi-modal control for high-quality image synthesis with DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained control of text-to-image diffusion transformer models (DiT)\nremains a critical challenge for practical deployment. While recent advances\nsuch as OminiControl and others have enabled a controllable generation of\ndiverse control signals, these methods face significant computational\ninefficiency when handling long conditional inputs. We present OminiControl2,\nan efficient framework that achieves efficient image-conditional image\ngeneration. OminiControl2 introduces two key innovations: (1) a dynamic\ncompression strategy that streamlines conditional inputs by preserving only the\nmost semantically relevant tokens during generation, and (2) a conditional\nfeature reuse mechanism that computes condition token features only once and\nreuses them across denoising steps. These architectural improvements preserve\nthe original framework's parameter efficiency and multi-modal versatility while\ndramatically reducing computational costs. Our experiments demonstrate that\nOminiControl2 reduces conditional processing overhead by over 90% compared to\nits predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional\ngeneration scenarios. This efficiency enables the practical implementation of\ncomplex, multi-modal control for high-quality image synthesis with DiT models."
                },
                "authors": [
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Qiaochu Xue"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08271v1",
                "updated": "2025-03-11T10:40:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    10,
                    40,
                    39,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T10:40:39Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    10,
                    40,
                    39,
                    1,
                    70,
                    0
                ],
                "title": "LangTime: A Language-Guided Unified Model for Time Series Forecasting\n  with Proximal Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangTime: A Language-Guided Unified Model for Time Series Forecasting\n  with Proximal Policy Optimization"
                },
                "summary": "Recent research has shown an increasing interest in utilizing pre-trained\nlarge language models (LLMs) for a variety of time series applications.\nHowever, there are three main challenges when using LLMs as foundational models\nfor time series forecasting: (1) Cross-domain generalization. (2)\nCross-modality alignment. (3) Error accumulation in autoregressive frameworks.\nTo address these challenges, we proposed LangTime, a language-guided unified\nmodel for time series forecasting that incorporates cross-domain pre-training\nwith reinforcement learning-based fine-tuning. Specifically, LangTime\nconstructs Temporal Comprehension Prompts (TCPs), which include dataset-wise\nand channel-wise instructions, to facilitate domain adaptation and condense\ntime series into a single token, enabling LLMs to understand better and align\ntemporal data. To improve autoregressive forecasting, we introduce TimePPO, a\nreinforcement learning-based fine-tuning algorithm. TimePPO mitigates error\naccumulation by leveraging a multidimensional rewards function tailored for\ntime series and a repeat-based value estimation strategy. Extensive experiments\ndemonstrate that LangTime achieves state-of-the-art cross-domain forecasting\nperformance, while TimePPO fine-tuning effectively enhances the stability and\naccuracy of autoregressive forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown an increasing interest in utilizing pre-trained\nlarge language models (LLMs) for a variety of time series applications.\nHowever, there are three main challenges when using LLMs as foundational models\nfor time series forecasting: (1) Cross-domain generalization. (2)\nCross-modality alignment. (3) Error accumulation in autoregressive frameworks.\nTo address these challenges, we proposed LangTime, a language-guided unified\nmodel for time series forecasting that incorporates cross-domain pre-training\nwith reinforcement learning-based fine-tuning. Specifically, LangTime\nconstructs Temporal Comprehension Prompts (TCPs), which include dataset-wise\nand channel-wise instructions, to facilitate domain adaptation and condense\ntime series into a single token, enabling LLMs to understand better and align\ntemporal data. To improve autoregressive forecasting, we introduce TimePPO, a\nreinforcement learning-based fine-tuning algorithm. TimePPO mitigates error\naccumulation by leveraging a multidimensional rewards function tailored for\ntime series and a repeat-based value estimation strategy. Extensive experiments\ndemonstrate that LangTime achieves state-of-the-art cross-domain forecasting\nperformance, while TimePPO fine-tuning effectively enhances the stability and\naccuracy of autoregressive forecasting."
                },
                "authors": [
                    {
                        "name": "Wenzhe Niu"
                    },
                    {
                        "name": "Zongxia Xie"
                    },
                    {
                        "name": "Yanru Sun"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Man Xu"
                    },
                    {
                        "name": "Chao Hao"
                    }
                ],
                "author_detail": {
                    "name": "Chao Hao"
                },
                "author": "Chao Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01306v2",
                "updated": "2025-03-11T10:08:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    10,
                    8,
                    37,
                    1,
                    70,
                    0
                ],
                "published": "2024-10-02T08:01:05Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    8,
                    1,
                    5,
                    2,
                    276,
                    0
                ],
                "title": "Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1,\n  and ChatGPT 4) for Intelligent Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1,\n  and ChatGPT 4) for Intelligent Response Generation"
                },
                "summary": "Empathetic and coherent responses are critical in auto-mated\nchatbot-facilitated psychotherapy. This study addresses the challenge of\nenhancing the emotional and contextual understanding of large language models\n(LLMs) in psychiatric applications. We introduce Emotion-Aware Embedding\nFusion, a novel framework integrating hierarchical fusion and attention\nmechanisms to prioritize semantic and emotional features in therapy\ntranscripts. Our approach combines multiple emotion lexicons, including NRC\nEmotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs\nsuch as Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4. Therapy session\ntranscripts, comprising over 2,000 samples are segmented into hierarchical\nlevels (word, sentence, and session) using neural networks, while hierarchical\nfusion combines these features with pooling techniques to refine emotional\nrepresentations. Atten-tion mechanisms, including multi-head self-attention and\ncross-attention, further prioritize emotional and contextual features, enabling\ntemporal modeling of emotion-al shifts across sessions. The processed\nembeddings, computed using BERT, GPT-3, and RoBERTa are stored in the Facebook\nAI similarity search vector database, which enables efficient similarity search\nand clustering across dense vector spaces. Upon user queries, relevant segments\nare retrieved and provided as context to LLMs, enhancing their ability to\ngenerate empathetic and con-textually relevant responses. The proposed\nframework is evaluated across multiple practical use cases to demonstrate\nreal-world applicability, including AI-driven therapy chatbots. The system can\nbe integrated into existing mental health platforms to generate personalized\nresponses based on retrieved therapy session data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathetic and coherent responses are critical in auto-mated\nchatbot-facilitated psychotherapy. This study addresses the challenge of\nenhancing the emotional and contextual understanding of large language models\n(LLMs) in psychiatric applications. We introduce Emotion-Aware Embedding\nFusion, a novel framework integrating hierarchical fusion and attention\nmechanisms to prioritize semantic and emotional features in therapy\ntranscripts. Our approach combines multiple emotion lexicons, including NRC\nEmotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs\nsuch as Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4. Therapy session\ntranscripts, comprising over 2,000 samples are segmented into hierarchical\nlevels (word, sentence, and session) using neural networks, while hierarchical\nfusion combines these features with pooling techniques to refine emotional\nrepresentations. Atten-tion mechanisms, including multi-head self-attention and\ncross-attention, further prioritize emotional and contextual features, enabling\ntemporal modeling of emotion-al shifts across sessions. The processed\nembeddings, computed using BERT, GPT-3, and RoBERTa are stored in the Facebook\nAI similarity search vector database, which enables efficient similarity search\nand clustering across dense vector spaces. Upon user queries, relevant segments\nare retrieved and provided as context to LLMs, enhancing their ability to\ngenerate empathetic and con-textually relevant responses. The proposed\nframework is evaluated across multiple practical use cases to demonstrate\nreal-world applicability, including AI-driven therapy chatbots. The system can\nbe integrated into existing mental health platforms to generate personalized\nresponses based on retrieved therapy session data."
                },
                "authors": [
                    {
                        "name": "Abdur Rasool"
                    },
                    {
                        "name": "Muhammad Irfan Shahzad"
                    },
                    {
                        "name": "Hafsa Aslam"
                    },
                    {
                        "name": "Vincent Chan"
                    },
                    {
                        "name": "Muhammad Ali Arshad"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Arshad"
                },
                "author": "Muhammad Ali Arshad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12268v3",
                "updated": "2025-03-12T02:14:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    2,
                    14,
                    40,
                    2,
                    71,
                    0
                ],
                "published": "2024-06-18T04:52:20Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    4,
                    52,
                    20,
                    1,
                    170,
                    0
                ],
                "title": "Advancing Ubiquitous Wireless Connectivity through Channel Twinning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Ubiquitous Wireless Connectivity through Channel Twinning"
                },
                "summary": "As an emerging trend in channel acquisition (CA), the concept of channel\ntwinning (CT) has been proposed as a powerful enabler of ubiquitous\nconnectivity in next-generation (xG) wireless systems. By fusing multimodal\nsensor data, CT advocates a high-fidelity and low-overhead CA paradigm, which\nis promising to provide accurate channel prediction in cross-domain and\nhigh-mobility scenarios of ubiquitous xG networks. However, existing literature\nlacks a universal CT architecture to address the challenges of heterogeneous\nscenarios, data, and resources in xG networks, which hinders the widespread\ndeployment and applications of CT. This article discusses a new modularized CT\narchitecture to bridge scene recognition, cooperative sensing, and\ndecentralized training, comprising versatile model configuration, multimodal\ncooperative sensing, and lightweight twin modeling modules. Additionally, this\narticle presents a detailed concept, technical features, and case studies of\nCT, outlines mainstream trends of realization methods, followed by potential\napplications of CT-empowered ubiquitous connectivity, and issues requiring\nfuture investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an emerging trend in channel acquisition (CA), the concept of channel\ntwinning (CT) has been proposed as a powerful enabler of ubiquitous\nconnectivity in next-generation (xG) wireless systems. By fusing multimodal\nsensor data, CT advocates a high-fidelity and low-overhead CA paradigm, which\nis promising to provide accurate channel prediction in cross-domain and\nhigh-mobility scenarios of ubiquitous xG networks. However, existing literature\nlacks a universal CT architecture to address the challenges of heterogeneous\nscenarios, data, and resources in xG networks, which hinders the widespread\ndeployment and applications of CT. This article discusses a new modularized CT\narchitecture to bridge scene recognition, cooperative sensing, and\ndecentralized training, comprising versatile model configuration, multimodal\ncooperative sensing, and lightweight twin modeling modules. Additionally, this\narticle presents a detailed concept, technical features, and case studies of\nCT, outlines mainstream trends of realization methods, followed by potential\napplications of CT-empowered ubiquitous connectivity, and issues requiring\nfuture investigations."
                },
                "authors": [
                    {
                        "name": "Yashuai Cao"
                    },
                    {
                        "name": "Linglong Dai"
                    },
                    {
                        "name": "Jingbo Tan"
                    },
                    {
                        "name": "Jintao Wang"
                    },
                    {
                        "name": "Tianyue Zheng"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Ekram Hossain"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_doi": "10.1109/MCOM.001.2400476",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MCOM.001.2400476",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.12268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in IEEE Communications Magazine",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18113v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18113v5",
                "updated": "2025-03-11T10:03:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    10,
                    3,
                    46,
                    1,
                    70,
                    0
                ],
                "published": "2024-06-26T06:59:09Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    6,
                    59,
                    9,
                    2,
                    178,
                    0
                ],
                "title": "Chrono: A Simple Blueprint for Representing Time in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chrono: A Simple Blueprint for Representing Time in MLLMs"
                },
                "summary": "The recent success of Large Language Models (LLMs) has prompted the extension\nto the multimodal domain developing image-text Multimodal LLMs (MLLMs) and then\nvideo-text models. In this work, we investigate the challenge of contextual and\ntemporal comprehension in video-language models by exploring the task of\ntemporal localization in videos. To address this problem, prior works have\ndeveloped complex task-specific architectures, novel modules to embed time into\nMLLMs, or leveraged additional input signals such as video transcripts to best\nencode contextual and temporal information. Interestingly, we find that most of\nthese efforts are surpassed by a much simpler design. We introduce Chrono, a\nuniversal sequence blueprint that can be applied to an image-text pretrained\nMLLM. Through extensive ablations across different MLLM architectures,\nfinetuning and zero-shot settings, and different datasets, we achieve a new\nSOTA in moment retrieval on the most widely used benchmarks Charades-STA,\nQVHighlights, ActivityNet Captions, and grounded video question answering on\nNeXT-GQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent success of Large Language Models (LLMs) has prompted the extension\nto the multimodal domain developing image-text Multimodal LLMs (MLLMs) and then\nvideo-text models. In this work, we investigate the challenge of contextual and\ntemporal comprehension in video-language models by exploring the task of\ntemporal localization in videos. To address this problem, prior works have\ndeveloped complex task-specific architectures, novel modules to embed time into\nMLLMs, or leveraged additional input signals such as video transcripts to best\nencode contextual and temporal information. Interestingly, we find that most of\nthese efforts are surpassed by a much simpler design. We introduce Chrono, a\nuniversal sequence blueprint that can be applied to an image-text pretrained\nMLLM. Through extensive ablations across different MLLM architectures,\nfinetuning and zero-shot settings, and different datasets, we achieve a new\nSOTA in moment retrieval on the most widely used benchmarks Charades-STA,\nQVHighlights, ActivityNet Captions, and grounded video question answering on\nNeXT-GQA."
                },
                "authors": [
                    {
                        "name": "Boris Meinardus"
                    },
                    {
                        "name": "Hector Rodriguez"
                    },
                    {
                        "name": "Anil Batra"
                    },
                    {
                        "name": "Anna Rohrbach"
                    },
                    {
                        "name": "Marcus Rohrbach"
                    }
                ],
                "author_detail": {
                    "name": "Marcus Rohrbach"
                },
                "author": "Marcus Rohrbach",
                "arxiv_comment": "Code: https://github.com/sudo-Boris/mr-Blip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18113v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18113v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06749v2",
                "updated": "2025-03-11T09:47:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    47,
                    44,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-09T20:06:45Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    20,
                    6,
                    45,
                    6,
                    68,
                    0
                ],
                "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models"
                },
                "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of $\\sim$6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning\ncapabilities in LLMs purely through Reinforcement Learning (RL). Inspired by\nthis breakthrough, we explore how RL can be utilized to enhance the reasoning\ncapability of MLLMs. However, direct training with RL struggles to activate\ncomplex reasoning capabilities such as questioning and reflection in MLLMs, due\nto the absence of substantial high-quality multimodal reasoning data. To\naddress this issue, we propose the reasoning MLLM, Vision-R1, to improve\nmultimodal reasoning capability. Specifically, we first construct a\nhigh-quality multimodal CoT dataset without human annotations by leveraging an\nexisting MLLM and DeepSeek-R1 through modality bridging and data filtering to\nobtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as\ncold-start initialization data for Vision-R1. To mitigate the optimization\nchallenges caused by overthinking after cold start, we propose Progressive\nThinking Suppression Training (PTST) strategy and employ Group Relative Policy\nOptimization (GRPO) with the hard formatting result reward function to\ngradually refine the model's ability to learn correct and complex reasoning\nprocesses on a 10K multimodal math dataset. Comprehensive experiments show our\nmodel achieves an average improvement of $\\sim$6% across various multimodal\nmath reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely\nused MathVista benchmark, which is only 0.4% lower than the leading reasoning\nmodel, OpenAI O1. The datasets and code will be released in:\nhttps://github.com/Osilly/Vision-R1 ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Bohan Jia"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08223v1",
                "updated": "2025-03-11T09:41:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    41,
                    29,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T09:41:29Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    41,
                    29,
                    1,
                    70,
                    0
                ],
                "title": "Will LLMs Scaling Hit the Wall? Breaking Barriers via Distributed\n  Resources on Massive Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will LLMs Scaling Hit the Wall? Breaking Barriers via Distributed\n  Resources on Massive Edge Devices"
                },
                "summary": "The remarkable success of foundation models has been driven by scaling laws,\ndemonstrating that model performance improves predictably with increased\ntraining data and model size. However, this scaling trajectory faces two\ncritical challenges: the depletion of high-quality public data, and the\nprohibitive computational power required for larger models, which have been\nmonopolized by tech giants. These two bottlenecks pose significant obstacles to\nthe further development of AI. In this position paper, we argue that leveraging\nmassive distributed edge devices can break through these barriers. We reveal\nthe vast untapped potential of data and computational resources on massive edge\ndevices, and review recent technical advancements in distributed/federated\nlearning that make this new paradigm viable. Our analysis suggests that by\ncollaborating on edge devices, everyone can participate in training large\nlanguage models with small edge devices. This paradigm shift towards\ndistributed training on edge has the potential to democratize AI development\nand foster a more inclusive AI community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable success of foundation models has been driven by scaling laws,\ndemonstrating that model performance improves predictably with increased\ntraining data and model size. However, this scaling trajectory faces two\ncritical challenges: the depletion of high-quality public data, and the\nprohibitive computational power required for larger models, which have been\nmonopolized by tech giants. These two bottlenecks pose significant obstacles to\nthe further development of AI. In this position paper, we argue that leveraging\nmassive distributed edge devices can break through these barriers. We reveal\nthe vast untapped potential of data and computational resources on massive edge\ndevices, and review recent technical advancements in distributed/federated\nlearning that make this new paradigm viable. Our analysis suggests that by\ncollaborating on edge devices, everyone can participate in training large\nlanguage models with small edge devices. This paradigm shift towards\ndistributed training on edge has the potential to democratize AI development\nand foster a more inclusive AI community."
                },
                "authors": [
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Didi Zhu"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Chao Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08220v1",
                "updated": "2025-03-11T09:40:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    40,
                    28,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T09:40:28Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    40,
                    28,
                    1,
                    70,
                    0
                ],
                "title": "Bedrock Models in Communication and Sensing: Advancing Generalization,\n  Transferability, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bedrock Models in Communication and Sensing: Advancing Generalization,\n  Transferability, and Performance"
                },
                "summary": "Deep learning (DL) has emerged as a powerful tool for addressing the\nintricate challenges inherent in communication and sensing systems,\nsignificantly enhancing the intelligence of future sixth-generation (6G)\nnetworks. A substantial body of research has highlighted the promise of\nDL-based techniques in these domains. However, in addition to improving\naccuracy, new challenges must be addressed regarding the generalization and\ntransferability of DL-based systems. To tackle these issues, this paper\nintroduces a series of mathematically grounded and modularized models, referred\nto as bedrock models, specifically designed for integration into both\ncommunication and sensing systems. Due to their modular architecture, these\nmodels can be seamlessly incorporated into existing communication and sensing\nframeworks. For communication systems, the proposed models demonstrate\nsubstantial performance improvements while also exhibit strong transferability,\nenabling direct parameter sharing across different tasks, which greatly\nfacilitates practical deployment. In sensing applications, the integration of\nthe bedrock models into existing systems results in superior performance,\nreducing delay and Doppler estimation errors by an order of magnitude compared\nto traditional methods. Additionally, a pre-equalization strategy based on the\nbedrock models is proposed for the transmitter. By leveraging sensing\ninformation, the transmitted communication signal is dynamically adjusted\nwithout altering the communication model pre-trained in AWGN channels. This\nadaptation enables the system to effectively cope with doubly dispersive\nchannels, restoring the received signal to an AWGN-like condition and achieving\nnear-optimal performance. Simulation results substantiate the effectiveness and\ntransferability of the proposed bedrock models, underscoring their potential to\nadvance both communication and sensing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) has emerged as a powerful tool for addressing the\nintricate challenges inherent in communication and sensing systems,\nsignificantly enhancing the intelligence of future sixth-generation (6G)\nnetworks. A substantial body of research has highlighted the promise of\nDL-based techniques in these domains. However, in addition to improving\naccuracy, new challenges must be addressed regarding the generalization and\ntransferability of DL-based systems. To tackle these issues, this paper\nintroduces a series of mathematically grounded and modularized models, referred\nto as bedrock models, specifically designed for integration into both\ncommunication and sensing systems. Due to their modular architecture, these\nmodels can be seamlessly incorporated into existing communication and sensing\nframeworks. For communication systems, the proposed models demonstrate\nsubstantial performance improvements while also exhibit strong transferability,\nenabling direct parameter sharing across different tasks, which greatly\nfacilitates practical deployment. In sensing applications, the integration of\nthe bedrock models into existing systems results in superior performance,\nreducing delay and Doppler estimation errors by an order of magnitude compared\nto traditional methods. Additionally, a pre-equalization strategy based on the\nbedrock models is proposed for the transmitter. By leveraging sensing\ninformation, the transmitted communication signal is dynamically adjusted\nwithout altering the communication model pre-trained in AWGN channels. This\nadaptation enables the system to effectively cope with doubly dispersive\nchannels, restoring the received signal to an AWGN-like condition and achieving\nnear-optimal performance. Simulation results substantiate the effectiveness and\ntransferability of the proposed bedrock models, underscoring their potential to\nadvance both communication and sensing systems."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Luping Xiang"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08213v1",
                "updated": "2025-03-11T09:27:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    27,
                    56,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T09:27:56Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    27,
                    56,
                    1,
                    70,
                    0
                ],
                "title": "DeepRAG: Building a Custom Hindi Embedding Model for Retrieval Augmented\n  Generation from Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepRAG: Building a Custom Hindi Embedding Model for Retrieval Augmented\n  Generation from Scratch"
                },
                "summary": "In this paper, I present our work on DeepRAG, a specialized embedding model\nwe built specifically for Hindi language in RAG systems. While LLMs have gotten\nreally good at generating text, their performance in retrieval tasks still\ndepends heavily on having quality embeddings - something that's been lacking\nfor Hindi despite being one of the world's most spoken languages. We tackled\nthis by creating embeddings from the ground up rather than just fine-tuning\nexisting models. Our process involved collecting diverse Hindi texts (over 2.7M\nsamples), training a custom SentencePiece tokenizer that actually understands\nHindi morphology, designing transformer architecture with Hindi-specific\nattention mechanisms, and optimizing with contrastive learning. Results were\nhonestly better than I expected - we saw a 23% improvement in retrieval\nprecision compared to the multilingual models everyone's been using. The paper\ndetails our methodology, which I think could help others working with\nlow-resource languages where the one-size-fits-all multilingual models fall\nshort. We've also integrated our embeddings with LangChain to build complete\nHindi RAG systems, which might be useful for practitioners. While there's still\ntons more to explore, I believe this work addresses a critical gap for Hindi\nNLP and demonstrates why language-specific approaches matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, I present our work on DeepRAG, a specialized embedding model\nwe built specifically for Hindi language in RAG systems. While LLMs have gotten\nreally good at generating text, their performance in retrieval tasks still\ndepends heavily on having quality embeddings - something that's been lacking\nfor Hindi despite being one of the world's most spoken languages. We tackled\nthis by creating embeddings from the ground up rather than just fine-tuning\nexisting models. Our process involved collecting diverse Hindi texts (over 2.7M\nsamples), training a custom SentencePiece tokenizer that actually understands\nHindi morphology, designing transformer architecture with Hindi-specific\nattention mechanisms, and optimizing with contrastive learning. Results were\nhonestly better than I expected - we saw a 23% improvement in retrieval\nprecision compared to the multilingual models everyone's been using. The paper\ndetails our methodology, which I think could help others working with\nlow-resource languages where the one-size-fits-all multilingual models fall\nshort. We've also integrated our embeddings with LangChain to build complete\nHindi RAG systems, which might be useful for practitioners. While there's still\ntons more to explore, I believe this work addresses a critical gap for Hindi\nNLP and demonstrates why language-specific approaches matter."
                },
                "authors": [
                    {
                        "name": "Nandakishor M"
                    }
                ],
                "author_detail": {
                    "name": "Nandakishor M"
                },
                "author": "Nandakishor M",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16750v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16750v3",
                "updated": "2025-03-11T09:16:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    16,
                    6,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-23T23:35:15Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    23,
                    35,
                    15,
                    6,
                    54,
                    0
                ],
                "title": "Guardians of the Agentic System: Preventing Many Shots Jailbreak with\n  Agentic System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guardians of the Agentic System: Preventing Many Shots Jailbreak with\n  Agentic System"
                },
                "summary": "The autonomous AI agents using large language models can create undeniable\nvalues in all span of the society but they face security threats from\nadversaries that warrants immediate protective solutions because trust and\nsafety issues arise. Considering the many-shot jailbreaking and deceptive\nalignment as some of the main advanced attacks, that cannot be mitigated by the\nstatic guardrails used during the supervised training, points out a crucial\nresearch priority for real world robustness. The combination of static\nguardrails in dynamic multi-agent system fails to defend against those attacks.\nWe intend to enhance security for LLM-based agents through the development of\nnew evaluation frameworks which identify and counter threats for safe\noperational deployment. Our work uses three examination methods to detect rogue\nagents through a Reverse Turing Test and analyze deceptive alignment through\nmulti-agent simulations and develops an anti-jailbreaking system by testing it\nwith GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated\nadversarial scenarios. The detection capabilities are strong such as 94\\%\naccuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities\nwhen under long attacks as prompt length increases attack success rates (ASR)\nand diversity metrics become ineffective in prediction while revealing multiple\ncomplex system faults. The findings demonstrate the necessity of adopting\nflexible security systems based on active monitoring that can be performed by\nthe agents themselves together with adaptable interventions by system admin as\nthe current models can create vulnerabilities that can lead to the unreliable\nand vulnerable system. So, in our work, we try to address such situations and\npropose a comprehensive framework to counteract the security issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The autonomous AI agents using large language models can create undeniable\nvalues in all span of the society but they face security threats from\nadversaries that warrants immediate protective solutions because trust and\nsafety issues arise. Considering the many-shot jailbreaking and deceptive\nalignment as some of the main advanced attacks, that cannot be mitigated by the\nstatic guardrails used during the supervised training, points out a crucial\nresearch priority for real world robustness. The combination of static\nguardrails in dynamic multi-agent system fails to defend against those attacks.\nWe intend to enhance security for LLM-based agents through the development of\nnew evaluation frameworks which identify and counter threats for safe\noperational deployment. Our work uses three examination methods to detect rogue\nagents through a Reverse Turing Test and analyze deceptive alignment through\nmulti-agent simulations and develops an anti-jailbreaking system by testing it\nwith GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated\nadversarial scenarios. The detection capabilities are strong such as 94\\%\naccuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities\nwhen under long attacks as prompt length increases attack success rates (ASR)\nand diversity metrics become ineffective in prediction while revealing multiple\ncomplex system faults. The findings demonstrate the necessity of adopting\nflexible security systems based on active monitoring that can be performed by\nthe agents themselves together with adaptable interventions by system admin as\nthe current models can create vulnerabilities that can lead to the unreliable\nand vulnerable system. So, in our work, we try to address such situations and\npropose a comprehensive framework to counteract the security issues."
                },
                "authors": [
                    {
                        "name": "Saikat Barua"
                    },
                    {
                        "name": "Mostafizur Rahman"
                    },
                    {
                        "name": "Md Jafor Sadek"
                    },
                    {
                        "name": "Rafiul Islam"
                    },
                    {
                        "name": "Shehenaz Khaled"
                    },
                    {
                        "name": "Ahmedul Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Ahmedul Kabir"
                },
                "author": "Ahmedul Kabir",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16750v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16750v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08200v1",
                "updated": "2025-03-11T09:08:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    8,
                    7,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T09:08:07Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    8,
                    7,
                    1,
                    70,
                    0
                ],
                "title": "Route Sparse Autoencoder to Interpret Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Route Sparse Autoencoder to Interpret Large Language Models"
                },
                "summary": "Mechanistic interpretability of large language models (LLMs) aims to uncover\nthe internal processes of information propagation and reasoning. Sparse\nautoencoders (SAEs) have demonstrated promise in this domain by extracting\ninterpretable and monosemantic features. However, prior works primarily focus\non feature extraction from a single layer, failing to effectively capture\nactivations that span multiple layers. In this paper, we introduce Route Sparse\nAutoencoder (RouteSAE), a new framework that integrates a routing mechanism\nwith a shared SAE to efficiently extract features from multiple layers. It\ndynamically assigns weights to activations from different layers, incurring\nminimal parameter overhead while achieving high interpretability and\nflexibility for targeted feature manipulation. We evaluate RouteSAE through\nextensive experiments on Llama-3.2-1B-Instruct. Specifically, under the same\nsparsity constraint of 64, RouteSAE extracts 22.5% more features than baseline\nSAEs while achieving a 22.3% higher interpretability score. These results\nunderscore the potential of RouteSAE as a scalable and effective method for LLM\ninterpretability, with applications in feature discovery and model\nintervention. Our codes are available at https://github.com/swei2001/RouteSAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanistic interpretability of large language models (LLMs) aims to uncover\nthe internal processes of information propagation and reasoning. Sparse\nautoencoders (SAEs) have demonstrated promise in this domain by extracting\ninterpretable and monosemantic features. However, prior works primarily focus\non feature extraction from a single layer, failing to effectively capture\nactivations that span multiple layers. In this paper, we introduce Route Sparse\nAutoencoder (RouteSAE), a new framework that integrates a routing mechanism\nwith a shared SAE to efficiently extract features from multiple layers. It\ndynamically assigns weights to activations from different layers, incurring\nminimal parameter overhead while achieving high interpretability and\nflexibility for targeted feature manipulation. We evaluate RouteSAE through\nextensive experiments on Llama-3.2-1B-Instruct. Specifically, under the same\nsparsity constraint of 64, RouteSAE extracts 22.5% more features than baseline\nSAEs while achieving a 22.3% higher interpretability score. These results\nunderscore the potential of RouteSAE as a scalable and effective method for LLM\ninterpretability, with applications in feature discovery and model\nintervention. Our codes are available at https://github.com/swei2001/RouteSAEs."
                },
                "authors": [
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Gojun Ma"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08199v1",
                "updated": "2025-03-11T09:08:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    8,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T09:08:04Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    8,
                    4,
                    1,
                    70,
                    0
                ],
                "title": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging\n  Control Integrating Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cascading Cooperative Multi-agent Framework for On-ramp Merging\n  Control Integrating Large Language Models"
                },
                "summary": "Traditional Reinforcement Learning (RL) suffers from replicating human-like\nbehaviors, generalizing effectively in multi-agent scenarios, and overcoming\ninherent interpretability issues.These tasks are compounded when deep\nenvironment understanding, agent coordination and dynamic optimization are\nrequired. While Large Language Model (LLM) enhanced methods have shown promise\nin generalization and interoperability, they often neglect necessary\nmulti-agent coordination. Therefore, we introduce the Cascading Cooperative\nMulti-agent (CCMA) framework, integrating RL for individual interactions, a\nfine-tuned LLM for regional cooperation, a reward function for global\noptimization, and the Retrieval-augmented Generation mechanism to dynamically\noptimize decision-making across complex driving scenarios. Our experiments\ndemonstrate that the CCMA outperforms existing RL methods, demonstrating\nsignificant improvements in both micro and macro-level performance in complex\ndriving environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Reinforcement Learning (RL) suffers from replicating human-like\nbehaviors, generalizing effectively in multi-agent scenarios, and overcoming\ninherent interpretability issues.These tasks are compounded when deep\nenvironment understanding, agent coordination and dynamic optimization are\nrequired. While Large Language Model (LLM) enhanced methods have shown promise\nin generalization and interoperability, they often neglect necessary\nmulti-agent coordination. Therefore, we introduce the Cascading Cooperative\nMulti-agent (CCMA) framework, integrating RL for individual interactions, a\nfine-tuned LLM for regional cooperation, a reward function for global\noptimization, and the Retrieval-augmented Generation mechanism to dynamically\noptimize decision-making across complex driving scenarios. Our experiments\ndemonstrate that the CCMA outperforms existing RL methods, demonstrating\nsignificant improvements in both micro and macro-level performance in complex\ndriving environments."
                },
                "authors": [
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Zhenlong Fang"
                    },
                    {
                        "name": "Tianyi Wang"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08932v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08932v3",
                "updated": "2025-03-11T09:05:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    5,
                    50,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-13T03:16:18Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    16,
                    18,
                    2,
                    318,
                    0
                ],
                "title": "PyGen: A Collaborative Human-AI Approach to Python Package Creation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyGen: A Collaborative Human-AI Approach to Python Package Creation"
                },
                "summary": "The principles of automation and innovation serve as foundational elements\nfor advancement in contemporary science and technology. Here, we introduce\nPygen, an automation platform designed to empower researchers, technologists,\nand hobbyists to bring abstract ideas to life as core, usable software tools\nwritten in Python. Pygen leverages the immense power of autoregressive large\nlanguage models to augment human creativity during the ideation, iteration, and\ninnovation process. By combining state-of-the-art language models with\nopen-source code generation technologies, Pygen has significantly reduced the\nmanual overhead of tool development. From a user prompt, Pygen automatically\ngenerates Python packages for a complete workflow from concept to package\ngeneration and documentation. The findings of our work show that Pygen\nconsiderably enhances the researcher's productivity by enabling the creation of\nresilient, modular, and well-documented packages for various specialized\npurposes. We employ a prompt enhancement approach to distill the user's package\ndescription into increasingly specific and actionable. While being inherently\nan open-ended task, we have evaluated the generated packages and the\ndocumentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with\ndetailed results in the results section. Furthermore, we documented our\nresults, analyzed the limitations, and suggested strategies to alleviate them.\nPygen is our vision of ethical automation, a framework that promotes\ninclusivity, accessibility, and collaborative development. This project marks\nthe beginning of a large-scale effort towards creating tools where intelligent\nagents collaborate with humans to improve scientific and technological\ndevelopment substantially.\n  Our code and generated examples are open-sourced at\n[https://github.com/GitsSaikat/Pygen]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principles of automation and innovation serve as foundational elements\nfor advancement in contemporary science and technology. Here, we introduce\nPygen, an automation platform designed to empower researchers, technologists,\nand hobbyists to bring abstract ideas to life as core, usable software tools\nwritten in Python. Pygen leverages the immense power of autoregressive large\nlanguage models to augment human creativity during the ideation, iteration, and\ninnovation process. By combining state-of-the-art language models with\nopen-source code generation technologies, Pygen has significantly reduced the\nmanual overhead of tool development. From a user prompt, Pygen automatically\ngenerates Python packages for a complete workflow from concept to package\ngeneration and documentation. The findings of our work show that Pygen\nconsiderably enhances the researcher's productivity by enabling the creation of\nresilient, modular, and well-documented packages for various specialized\npurposes. We employ a prompt enhancement approach to distill the user's package\ndescription into increasingly specific and actionable. While being inherently\nan open-ended task, we have evaluated the generated packages and the\ndocumentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with\ndetailed results in the results section. Furthermore, we documented our\nresults, analyzed the limitations, and suggested strategies to alleviate them.\nPygen is our vision of ethical automation, a framework that promotes\ninclusivity, accessibility, and collaborative development. This project marks\nthe beginning of a large-scale effort towards creating tools where intelligent\nagents collaborate with humans to improve scientific and technological\ndevelopment substantially.\n  Our code and generated examples are open-sourced at\n[https://github.com/GitsSaikat/Pygen]"
                },
                "authors": [
                    {
                        "name": "Saikat Barua"
                    },
                    {
                        "name": "Mostafizur Rahman"
                    },
                    {
                        "name": "Md Jafor Sadek"
                    },
                    {
                        "name": "Rafiul Islam"
                    },
                    {
                        "name": "Shehenaz Khaled"
                    },
                    {
                        "name": "Md. Shohrab Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Md. Shohrab Hossain"
                },
                "author": "Md. Shohrab Hossain",
                "arxiv_comment": "33 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08932v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08932v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08195v1",
                "updated": "2025-03-11T09:00:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    0,
                    45,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T09:00:45Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    0,
                    45,
                    1,
                    70,
                    0
                ],
                "title": "Dialogue Injection Attack: Jailbreaking LLMs through Context\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Injection Attack: Jailbreaking LLMs through Context\n  Manipulation"
                },
                "summary": "Large language models (LLMs) have demonstrated significant utility in a wide\nrange of applications; however, their deployment is plagued by security\nvulnerabilities, notably jailbreak attacks. These attacks manipulate LLMs to\ngenerate harmful or unethical content by crafting adversarial prompts. While\nmuch of the current research on jailbreak attacks has focused on single-turn\ninteractions, it has largely overlooked the impact of historical dialogues on\nmodel behavior. In this paper, we introduce a novel jailbreak paradigm,\nDialogue Injection Attack (DIA), which leverages the dialogue history to\nenhance the success rates of such attacks. DIA operates in a black-box setting,\nrequiring only access to the chat API or knowledge of the LLM's chat template.\nWe propose two methods for constructing adversarial historical dialogues: one\nadapts gray-box prefilling attacks, and the other exploits deferred responses.\nOur experiments show that DIA achieves state-of-the-art attack success rates on\nrecent LLMs, including Llama-3.1 and GPT-4o. Additionally, we demonstrate that\nDIA can bypass 5 different defense mechanisms, highlighting its robustness and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant utility in a wide\nrange of applications; however, their deployment is plagued by security\nvulnerabilities, notably jailbreak attacks. These attacks manipulate LLMs to\ngenerate harmful or unethical content by crafting adversarial prompts. While\nmuch of the current research on jailbreak attacks has focused on single-turn\ninteractions, it has largely overlooked the impact of historical dialogues on\nmodel behavior. In this paper, we introduce a novel jailbreak paradigm,\nDialogue Injection Attack (DIA), which leverages the dialogue history to\nenhance the success rates of such attacks. DIA operates in a black-box setting,\nrequiring only access to the chat API or knowledge of the LLM's chat template.\nWe propose two methods for constructing adversarial historical dialogues: one\nadapts gray-box prefilling attacks, and the other exploits deferred responses.\nOur experiments show that DIA achieves state-of-the-art attack success rates on\nrecent LLMs, including Llama-3.1 and GPT-4o. Additionally, we demonstrate that\nDIA can bypass 5 different defense mechanisms, highlighting its robustness and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Wenlong Meng"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Wendao Yao"
                    },
                    {
                        "name": "Zhenyuan Guo"
                    },
                    {
                        "name": "Yuwei Li"
                    },
                    {
                        "name": "Chengkun Wei"
                    },
                    {
                        "name": "Wenzhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenzhi Chen"
                },
                "author": "Wenzhi Chen",
                "arxiv_comment": "17 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08193v1",
                "updated": "2025-03-11T08:57:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    57,
                    7,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T08:57:07Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    57,
                    7,
                    1,
                    70,
                    0
                ],
                "title": "Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of\n  Role-Playing Language Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guess What I am Thinking: A Benchmark for Inner Thought Reasoning of\n  Role-Playing Language Agents"
                },
                "summary": "Recent advances in LLM-based role-playing language agents (RPLAs) have\nattracted broad attention in various applications. While chain-of-thought\nreasoning has shown importance in many tasks for LLMs, the internal thinking\nprocesses of RPLAs remain unexplored. Understanding characters' inner thoughts\nis crucial for developing advanced RPLAs. In this paper, we introduce\nROLETHINK, a novel benchmark constructed from literature for evaluating\ncharacter thought generation. We propose the task of inner thought reasoning,\nwhich includes two sets: the gold set that compares generated thoughts with\noriginal character monologues, and the silver set that uses expert synthesized\ncharacter analyses as references. To address this challenge, we propose MIRROR,\na chain-of-thought approach that generates character thoughts by retrieving\nmemories, predicting character reactions, and synthesizing motivations. Through\nextensive experiments, we demonstrate the importance of inner thought reasoning\nfor RPLAs, and MIRROR consistently outperforms existing methods. Resources are\navailable at https://github.com/airaer1998/RPA_Thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLM-based role-playing language agents (RPLAs) have\nattracted broad attention in various applications. While chain-of-thought\nreasoning has shown importance in many tasks for LLMs, the internal thinking\nprocesses of RPLAs remain unexplored. Understanding characters' inner thoughts\nis crucial for developing advanced RPLAs. In this paper, we introduce\nROLETHINK, a novel benchmark constructed from literature for evaluating\ncharacter thought generation. We propose the task of inner thought reasoning,\nwhich includes two sets: the gold set that compares generated thoughts with\noriginal character monologues, and the silver set that uses expert synthesized\ncharacter analyses as references. To address this challenge, we propose MIRROR,\na chain-of-thought approach that generates character thoughts by retrieving\nmemories, predicting character reactions, and synthesizing motivations. Through\nextensive experiments, we demonstrate the importance of inner thought reasoning\nfor RPLAs, and MIRROR consistently outperforms existing methods. Resources are\navailable at https://github.com/airaer1998/RPA_Thought."
                },
                "authors": [
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "MingYu Wang"
                    },
                    {
                        "name": "XinTao Wang"
                    },
                    {
                        "name": "Dakuan Lu"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Wei Chu"
                    },
                    {
                        "name": "Yinghui Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yinghui Xu"
                },
                "author": "Yinghui Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08192v1",
                "updated": "2025-03-11T08:55:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    55,
                    52,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T08:55:52Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    55,
                    52,
                    1,
                    70,
                    0
                ],
                "title": "Automating Violence Detection and Categorization from Ancient Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Violence Detection and Categorization from Ancient Texts"
                },
                "summary": "Violence descriptions in literature offer valuable insights for a wide range\nof research in the humanities. For historians, depictions of violence are of\nspecial interest for analyzing the societal dynamics surrounding large wars and\nindividual conflicts of influential people. Harvesting data for violence\nresearch manually is laborious and time-consuming. This study is the first one\nto evaluate the effectiveness of large language models (LLMs) in identifying\nviolence in ancient texts and categorizing it across multiple dimensions. Our\nexperiments identify LLMs as a valuable tool to scale up the accurate analysis\nof historical texts and show the effect of fine-tuning and data augmentation,\nyielding an F1-score of up to 0.93 for violence detection and 0.86 for\nfine-grained violence categorization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Violence descriptions in literature offer valuable insights for a wide range\nof research in the humanities. For historians, depictions of violence are of\nspecial interest for analyzing the societal dynamics surrounding large wars and\nindividual conflicts of influential people. Harvesting data for violence\nresearch manually is laborious and time-consuming. This study is the first one\nto evaluate the effectiveness of large language models (LLMs) in identifying\nviolence in ancient texts and categorizing it across multiple dimensions. Our\nexperiments identify LLMs as a valuable tool to scale up the accurate analysis\nof historical texts and show the effect of fine-tuning and data augmentation,\nyielding an F1-score of up to 0.93 for violence detection and 0.86 for\nfine-grained violence categorization."
                },
                "authors": [
                    {
                        "name": "Alhassan Abdelhalim"
                    },
                    {
                        "name": "Michaela Regneri"
                    }
                ],
                "author_detail": {
                    "name": "Michaela Regneri"
                },
                "author": "Michaela Regneri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14856v2",
                "updated": "2025-03-11T08:54:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    54,
                    55,
                    1,
                    70,
                    0
                ],
                "published": "2025-02-20T18:58:10Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    58,
                    10,
                    3,
                    51,
                    0
                ],
                "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via\n  Frequency-Ranked Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FR-Spec: Accelerating Large-Vocabulary Language Models via\n  Frequency-Ranked Speculative Sampling"
                },
                "summary": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2. Code available at\nhttps://github.com/thunlp/FR-Spec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2. Code available at\nhttps://github.com/thunlp/FR-Spec."
                },
                "authors": [
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Tengyu Pan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Kaihuo Zhang"
                    },
                    {
                        "name": "Weilun Zhao"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08188v1",
                "updated": "2025-03-11T08:53:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    53,
                    53,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T08:53:53Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    53,
                    53,
                    1,
                    70,
                    0
                ],
                "title": "RigoChat 2: an adapted language model to Spanish using a bounded dataset\n  and reduced hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RigoChat 2: an adapted language model to Spanish using a bounded dataset\n  and reduced hardware"
                },
                "summary": "Large Language Models (LLMs) have become a key element of modern artificial\nintelligence, demonstrating the ability to address a wide range of language\nprocessing tasks at unprecedented levels of accuracy without the need of\ncollecting problem-specific data. However, these versatile models face a\nsignificant challenge: both their training and inference processes require\nsubstantial computational resources, time, and memory. Consequently, optimizing\nthis kind of models to minimize these requirements is crucial. In this article,\nwe demonstrate that, with minimal resources and in a remarkably short time, it\nis possible to enhance a state-of-the-art model, specifically for a given\nlanguage task, without compromising its overall capabilities using a relatively\nsmall pretrained LLM as a basis. Specifically, we present our use case,\nRigoChat 2, illustrating how LLMs can be adapted to achieve superior results in\nSpanish-language tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become a key element of modern artificial\nintelligence, demonstrating the ability to address a wide range of language\nprocessing tasks at unprecedented levels of accuracy without the need of\ncollecting problem-specific data. However, these versatile models face a\nsignificant challenge: both their training and inference processes require\nsubstantial computational resources, time, and memory. Consequently, optimizing\nthis kind of models to minimize these requirements is crucial. In this article,\nwe demonstrate that, with minimal resources and in a remarkably short time, it\nis possible to enhance a state-of-the-art model, specifically for a given\nlanguage task, without compromising its overall capabilities using a relatively\nsmall pretrained LLM as a basis. Specifically, we present our use case,\nRigoChat 2, illustrating how LLMs can be adapted to achieve superior results in\nSpanish-language tasks."
                },
                "authors": [
                    {
                        "name": "Gonzalo Santamaría Gómez"
                    },
                    {
                        "name": "Guillem García Subies"
                    },
                    {
                        "name": "Pablo Gutiérrez Ruiz"
                    },
                    {
                        "name": "Mario González Valero"
                    },
                    {
                        "name": "Natàlia Fuertes"
                    },
                    {
                        "name": "Helena Montoro Zamorano"
                    },
                    {
                        "name": "Carmen Muñoz Sanz"
                    },
                    {
                        "name": "Leire Rosado Plaza"
                    },
                    {
                        "name": "Nuria Aldama García"
                    },
                    {
                        "name": "David Betancur Sánchez"
                    },
                    {
                        "name": "Kateryna Sushkova"
                    },
                    {
                        "name": "Marta Guerrero Nieto"
                    },
                    {
                        "name": "Álvaro Barbero Jiménez"
                    }
                ],
                "author_detail": {
                    "name": "Álvaro Barbero Jiménez"
                },
                "author": "Álvaro Barbero Jiménez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08182v1",
                "updated": "2025-03-11T08:47:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    47,
                    13,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T08:47:13Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    47,
                    13,
                    1,
                    70,
                    0
                ],
                "title": "Mutation Testing via Iterative Large Language Model-Driven Scientific\n  Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation Testing via Iterative Large Language Model-Driven Scientific\n  Debugging"
                },
                "summary": "Large Language Models (LLMs) can generate plausible test code. Intuitively\nthey generate this by imitating tests seen in their training data, rather than\nreasoning about execution semantics. However, such reasoning is important when\napplying mutation testing, where individual tests need to demonstrate\ndifferences in program behavior between a program and specific artificial\ndefects (mutants). In this paper, we evaluate whether Scientific Debugging,\nwhich has been shown to help LLMs when debugging, can also help them to\ngenerate tests for mutants. In the resulting approach, LLMs form hypotheses\nabout how to kill specific mutants, and then iteratively generate and refine\ntests until they succeed, all with detailed explanations for each step. We\ncompare this method to three baselines: (1) directly asking the LLM to generate\ntests, (2) repeatedly querying the LLM when tests fail, and (3) search-based\ntest generation with Pynguin. Our experiments evaluate these methods based on\nseveral factors, including mutation score, code coverage, success rate, and the\nability to identify equivalent mutants. The results demonstrate that LLMs,\nalthough requiring higher computation cost, consistently outperform Pynguin in\ngenerating tests with better fault detection and coverage. Importantly, we\nobserve that the iterative refinement of test cases is important for achieving\nhigh-quality test suites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can generate plausible test code. Intuitively\nthey generate this by imitating tests seen in their training data, rather than\nreasoning about execution semantics. However, such reasoning is important when\napplying mutation testing, where individual tests need to demonstrate\ndifferences in program behavior between a program and specific artificial\ndefects (mutants). In this paper, we evaluate whether Scientific Debugging,\nwhich has been shown to help LLMs when debugging, can also help them to\ngenerate tests for mutants. In the resulting approach, LLMs form hypotheses\nabout how to kill specific mutants, and then iteratively generate and refine\ntests until they succeed, all with detailed explanations for each step. We\ncompare this method to three baselines: (1) directly asking the LLM to generate\ntests, (2) repeatedly querying the LLM when tests fail, and (3) search-based\ntest generation with Pynguin. Our experiments evaluate these methods based on\nseveral factors, including mutation score, code coverage, success rate, and the\nability to identify equivalent mutants. The results demonstrate that LLMs,\nalthough requiring higher computation cost, consistently outperform Pynguin in\ngenerating tests with better fault detection and coverage. Importantly, we\nobserve that the iterative refinement of test cases is important for achieving\nhigh-quality test suites."
                },
                "authors": [
                    {
                        "name": "Philipp Straubinger"
                    },
                    {
                        "name": "Marvin Kreis"
                    },
                    {
                        "name": "Stephan Lukasczyk"
                    },
                    {
                        "name": "Gordon Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Fraser"
                },
                "author": "Gordon Fraser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08179v2",
                "updated": "2025-03-12T08:46:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    46,
                    33,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-11T08:43:05Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    43,
                    5,
                    1,
                    70,
                    0
                ],
                "title": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models"
                },
                "summary": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks."
                },
                "authors": [
                    {
                        "name": "Zicheng Ma"
                    },
                    {
                        "name": "Chuanliu Fan"
                    },
                    {
                        "name": "Zhicong Wang"
                    },
                    {
                        "name": "Zhenyu Chen"
                    },
                    {
                        "name": "Xiaohan Lin"
                    },
                    {
                        "name": "Yanheng Li"
                    },
                    {
                        "name": "Shihao Feng"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Yi Qin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yi Qin Gao"
                },
                "author": "Yi Qin Gao",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08175v1",
                "updated": "2025-03-11T08:38:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    38,
                    45,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T08:38:45Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    38,
                    45,
                    1,
                    70,
                    0
                ],
                "title": "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems"
                },
                "summary": "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps://github.com/ZitongShi/EPEAgent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps://github.com/ZitongShi/EPEAgent"
                },
                "authors": [
                    {
                        "name": "Zitong Shi"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Wenke Huang"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Mang Ye"
                    },
                    {
                        "name": "Carl Yang"
                    }
                ],
                "author_detail": {
                    "name": "Carl Yang"
                },
                "author": "Carl Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08174v1",
                "updated": "2025-03-11T08:36:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    36,
                    37,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T08:36:37Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    36,
                    37,
                    1,
                    70,
                    0
                ],
                "title": "Investigating the Effectiveness of a Socratic Chain-of-Thoughts\n  Reasoning Method for Task Planning in Robotics, A Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Effectiveness of a Socratic Chain-of-Thoughts\n  Reasoning Method for Task Planning in Robotics, A Case Study"
                },
                "summary": "Large language models (LLMs) have demonstrated unprecedented capability in\nreasoning with natural language. Coupled with this development is the emergence\nof embodied AI in robotics. Despite showing promise for verbal and written\nreasoning tasks, it remains unknown whether LLMs are capable of navigating\ncomplex spatial tasks with physical actions in the real world. To this end, it\nis of interest to investigate applying LLMs to robotics in zero-shot learning\nscenarios, and in the absence of fine-tuning - a feat which could significantly\nimprove human-robot interaction, alleviate compute cost, and eliminate\nlow-level programming tasks associated with robot tasks.\n  To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot\nin Webots engine for an object search task. We evaluate the effectiveness of\nthree reasoning strategies based on Chain-of-Thought (CoT) sub-task list\ngeneration with the Socratic method (SocraCoT) (in order of increasing rigor):\n(1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was\nmeasured in terms of the proportion of tasks successfully completed and\nexecution time (N = 20). Our preliminary results show that when combined with\nchain-of-thought reasoning, the Socratic method can be used for code generation\nfor robotic tasks that require spatial awareness. In extension of this finding,\nwe propose EVINCE-LoC; a modified EVINCE method that could further enhance\nperformance in highly complex and or dynamic testing scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated unprecedented capability in\nreasoning with natural language. Coupled with this development is the emergence\nof embodied AI in robotics. Despite showing promise for verbal and written\nreasoning tasks, it remains unknown whether LLMs are capable of navigating\ncomplex spatial tasks with physical actions in the real world. To this end, it\nis of interest to investigate applying LLMs to robotics in zero-shot learning\nscenarios, and in the absence of fine-tuning - a feat which could significantly\nimprove human-robot interaction, alleviate compute cost, and eliminate\nlow-level programming tasks associated with robot tasks.\n  To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot\nin Webots engine for an object search task. We evaluate the effectiveness of\nthree reasoning strategies based on Chain-of-Thought (CoT) sub-task list\ngeneration with the Socratic method (SocraCoT) (in order of increasing rigor):\n(1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was\nmeasured in terms of the proportion of tasks successfully completed and\nexecution time (N = 20). Our preliminary results show that when combined with\nchain-of-thought reasoning, the Socratic method can be used for code generation\nfor robotic tasks that require spatial awareness. In extension of this finding,\nwe propose EVINCE-LoC; a modified EVINCE method that could further enhance\nperformance in highly complex and or dynamic testing scenarios."
                },
                "authors": [
                    {
                        "name": "Veronica Bot"
                    },
                    {
                        "name": "Zheyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zheyuan Xu"
                },
                "author": "Zheyuan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08168v1",
                "updated": "2025-03-11T08:30:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    30,
                    50,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T08:30:50Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    30,
                    50,
                    1,
                    70,
                    0
                ],
                "title": "TSCnet: A Text-driven Semantic-level Controllable Framework for\n  Customized Low-Light Image Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TSCnet: A Text-driven Semantic-level Controllable Framework for\n  Customized Low-Light Image Enhancement"
                },
                "summary": "Deep learning-based image enhancement methods show significant advantages in\nreducing noise and improving visibility in low-light conditions. These methods\nare typically based on one-to-one mapping, where the model learns a direct\ntransformation from low light to specific enhanced images. Therefore, these\nmethods are inflexible as they do not allow highly personalized mapping, even\nthough an individual's lighting preferences are inherently personalized. To\novercome these limitations, we propose a new light enhancement task and a new\nframework that provides customized lighting control through prompt-driven,\nsemantic-level, and quantitative brightness adjustments. The framework begins\nby leveraging a Large Language Model (LLM) to understand natural language\nprompts, enabling it to identify target objects for brightness adjustments. To\nlocalize these target objects, the Retinex-based Reasoning Segment (RRS) module\ngenerates precise target localization masks using reflection images.\nSubsequently, the Text-based Brightness Controllable (TBC) module adjusts\nbrightness levels based on the generated illumination map. Finally, an Adaptive\nContextual Compensation (ACC) module integrates multi-modal inputs and controls\na conditional diffusion model to adjust the lighting, ensuring seamless and\nprecise enhancements accurately. Experimental results on benchmark datasets\ndemonstrate our framework's superior performance at increasing visibility,\nmaintaining natural color balance, and amplifying fine details without creating\nartifacts. Furthermore, its robust generalization capabilities enable complex\nsemantic-level lighting adjustments in diverse open-world environments through\nnatural language interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based image enhancement methods show significant advantages in\nreducing noise and improving visibility in low-light conditions. These methods\nare typically based on one-to-one mapping, where the model learns a direct\ntransformation from low light to specific enhanced images. Therefore, these\nmethods are inflexible as they do not allow highly personalized mapping, even\nthough an individual's lighting preferences are inherently personalized. To\novercome these limitations, we propose a new light enhancement task and a new\nframework that provides customized lighting control through prompt-driven,\nsemantic-level, and quantitative brightness adjustments. The framework begins\nby leveraging a Large Language Model (LLM) to understand natural language\nprompts, enabling it to identify target objects for brightness adjustments. To\nlocalize these target objects, the Retinex-based Reasoning Segment (RRS) module\ngenerates precise target localization masks using reflection images.\nSubsequently, the Text-based Brightness Controllable (TBC) module adjusts\nbrightness levels based on the generated illumination map. Finally, an Adaptive\nContextual Compensation (ACC) module integrates multi-modal inputs and controls\na conditional diffusion model to adjust the lighting, ensuring seamless and\nprecise enhancements accurately. Experimental results on benchmark datasets\ndemonstrate our framework's superior performance at increasing visibility,\nmaintaining natural color balance, and amplifying fine details without creating\nartifacts. Furthermore, its robust generalization capabilities enable complex\nsemantic-level lighting adjustments in diverse open-world environments through\nnatural language interactions."
                },
                "authors": [
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Jun Yin"
                    },
                    {
                        "name": "Pengyu Zeng"
                    },
                    {
                        "name": "Yiqing Shen"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08165v1",
                "updated": "2025-03-11T08:29:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    29,
                    18,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T08:29:18Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    29,
                    18,
                    1,
                    70,
                    0
                ],
                "title": "Multimodal Generation of Animatable 3D Human Models with AvatarForge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Generation of Animatable 3D Human Models with AvatarForge"
                },
                "summary": "We introduce AvatarForge, a framework for generating animatable 3D human\navatars from text or image inputs using AI-driven procedural generation. While\ndiffusion-based methods have made strides in general 3D object generation, they\nstruggle with high-quality, customizable human avatars due to the complexity\nand diversity of human body shapes, poses, exacerbated by the scarcity of\nhigh-quality data. Additionally, animating these avatars remains a significant\nchallenge for existing methods. AvatarForge overcomes these limitations by\ncombining LLM-based commonsense reasoning with off-the-shelf 3D human\ngenerators, enabling fine-grained control over body and facial details. Unlike\ndiffusion models which often rely on pre-trained datasets lacking precise\ncontrol over individual human features, AvatarForge offers a more flexible\napproach, bringing humans into the iterative design and modeling loop, with its\nauto-verification system allowing for continuous refinement of the generated\navatars, and thus promoting high accuracy and customization. Our evaluations\nshow that AvatarForge outperforms state-of-the-art methods in both text- and\nimage-to-avatar generation, making it a versatile tool for artistic creation\nand animation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AvatarForge, a framework for generating animatable 3D human\navatars from text or image inputs using AI-driven procedural generation. While\ndiffusion-based methods have made strides in general 3D object generation, they\nstruggle with high-quality, customizable human avatars due to the complexity\nand diversity of human body shapes, poses, exacerbated by the scarcity of\nhigh-quality data. Additionally, animating these avatars remains a significant\nchallenge for existing methods. AvatarForge overcomes these limitations by\ncombining LLM-based commonsense reasoning with off-the-shelf 3D human\ngenerators, enabling fine-grained control over body and facial details. Unlike\ndiffusion models which often rely on pre-trained datasets lacking precise\ncontrol over individual human features, AvatarForge offers a more flexible\napproach, bringing humans into the iterative design and modeling loop, with its\nauto-verification system allowing for continuous refinement of the generated\navatars, and thus promoting high accuracy and customization. Our evaluations\nshow that AvatarForge outperforms state-of-the-art methods in both text- and\nimage-to-avatar generation, making it a versatile tool for artistic creation\nand animation."
                },
                "authors": [
                    {
                        "name": "Xinhang Liu"
                    },
                    {
                        "name": "Yu-Wing Tai"
                    },
                    {
                        "name": "Chi-Keung Tang"
                    }
                ],
                "author_detail": {
                    "name": "Chi-Keung Tang"
                },
                "author": "Chi-Keung Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08162v1",
                "updated": "2025-03-11T08:27:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    27,
                    1,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T08:27:01Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    27,
                    1,
                    1,
                    70,
                    0
                ],
                "title": "FASIONAD++ : Integrating High-Level Instruction and Information\n  Bottleneck in FAt-Slow fusION Systems for Enhanced Safety in Autonomous\n  Driving with Adaptive Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FASIONAD++ : Integrating High-Level Instruction and Information\n  Bottleneck in FAt-Slow fusION Systems for Enhanced Safety in Autonomous\n  Driving with Adaptive Feedback"
                },
                "summary": "Ensuring safe, comfortable, and efficient planning is crucial for autonomous\ndriving systems. While end-to-end models trained on large datasets perform well\nin standard driving scenarios, they struggle with complex low-frequency events.\nRecent Large Language Models (LLMs) and Vision Language Models (VLMs)\nadvancements offer enhanced reasoning but suffer from computational\ninefficiency. Inspired by the dual-process cognitive model \"Thinking, Fast and\nSlow\", we propose $\\textbf{FASIONAD}$ -- a novel dual-system framework that\nsynergizes a fast end-to-end planner with a VLM-based reasoning module. The\nfast system leverages end-to-end learning to achieve real-time trajectory\ngeneration in common scenarios, while the slow system activates through\nuncertainty estimation to perform contextual analysis and complex scenario\nresolution. Our architecture introduces three key innovations: (1) A dynamic\nswitching mechanism enabling slow system intervention based on real-time\nuncertainty assessment; (2) An information bottleneck with high-level plan\nfeedback that optimizes the slow system's guidance capability; (3) A\nbidirectional knowledge exchange where visual prompts enhance the slow system's\nreasoning while its feedback refines the fast planner's decision-making. To\nstrengthen VLM reasoning, we develop a question-answering mechanism coupled\nwith reward-instruct training strategy. In open-loop experiments, FASIONAD\nachieves a $6.7\\%$ reduction in average $L2$ trajectory error and $28.1\\%$\nlower collision rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring safe, comfortable, and efficient planning is crucial for autonomous\ndriving systems. While end-to-end models trained on large datasets perform well\nin standard driving scenarios, they struggle with complex low-frequency events.\nRecent Large Language Models (LLMs) and Vision Language Models (VLMs)\nadvancements offer enhanced reasoning but suffer from computational\ninefficiency. Inspired by the dual-process cognitive model \"Thinking, Fast and\nSlow\", we propose $\\textbf{FASIONAD}$ -- a novel dual-system framework that\nsynergizes a fast end-to-end planner with a VLM-based reasoning module. The\nfast system leverages end-to-end learning to achieve real-time trajectory\ngeneration in common scenarios, while the slow system activates through\nuncertainty estimation to perform contextual analysis and complex scenario\nresolution. Our architecture introduces three key innovations: (1) A dynamic\nswitching mechanism enabling slow system intervention based on real-time\nuncertainty assessment; (2) An information bottleneck with high-level plan\nfeedback that optimizes the slow system's guidance capability; (3) A\nbidirectional knowledge exchange where visual prompts enhance the slow system's\nreasoning while its feedback refines the fast planner's decision-making. To\nstrengthen VLM reasoning, we develop a question-answering mechanism coupled\nwith reward-instruct training strategy. In open-loop experiments, FASIONAD\nachieves a $6.7\\%$ reduction in average $L2$ trajectory error and $28.1\\%$\nlower collision rate."
                },
                "authors": [
                    {
                        "name": "Kangan Qian"
                    },
                    {
                        "name": "Ziang Luo"
                    },
                    {
                        "name": "Sicong Jiang"
                    },
                    {
                        "name": "Zilin Huang"
                    },
                    {
                        "name": "Jinyu Miao"
                    },
                    {
                        "name": "Zhikun Ma"
                    },
                    {
                        "name": "Tianze Zhu"
                    },
                    {
                        "name": "Jiayin Li"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Zheng Fu"
                    },
                    {
                        "name": "Yining Shi"
                    },
                    {
                        "name": "Boyue Wang"
                    },
                    {
                        "name": "Hezhe Lin"
                    },
                    {
                        "name": "Ziyu Chen"
                    },
                    {
                        "name": "Jiangbo Yu"
                    },
                    {
                        "name": "Xinyu Jiao"
                    },
                    {
                        "name": "Mengmeng Yang"
                    },
                    {
                        "name": "Kun Jiang"
                    },
                    {
                        "name": "Diange Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diange Yang"
                },
                "author": "Diange Yang",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]